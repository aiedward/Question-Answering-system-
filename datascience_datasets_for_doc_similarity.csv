"['machine-learning', 'classification', 'evaluation']",How to improve an existing (trained) classifier?,"<p>Adding more data does not always help. However, you can get an estimate if more data will help you by the following procedure: Make a plot. On the $x$-axis is the amount of training examples, starting at one example per class going to wherever you are currently. The $y$-axis shows the error. Now you should add two curves: Training and test error. For low $x$, the training error should be very low (almost 0) and the test error very high. With enough data, they should be ""about the same"". By plotting those curves you can make an educated guess how much more data will give you how much improvement.</p>

<blockquote>
  <p>When doing this should I add data to both the training set and the test set?</p>
</blockquote>

<p>Depends on what you want to achieve. If only getting a better classifier, then you can only add it to the training set. However, if you're doing this in a scientific setting this might be more difficult. (I assume that your test set is of reasonable size).</p>

<p>You might want to have a look at <a href=""https://en.wikipedia.org/wiki/Cross-validation_(statistics)"" rel=""nofollow"">cross-validation</a>.</p>
"
"['machine-learning', 'r', 'logistic-regression', 'random-forest']","Random Forest, Type - Regression, Calculation of Importance Example","<p>MSE is measure of error of the overall regression model, $\frac{1}{n}\sum\|y_i-\hat y_i\|^2$.</p>

<p>For an important variable, if it is replaced with random noise, you would imagine MSE with the faulty data to increase. <strong>IncMSE</strong> (Incremental MSE) for a particular variable is how much the MSE will increase if the variable is completely randomized.</p>

<p>This is usually computed on the out-of-bag data.</p>

<hr>

<p>Node purity is a measure of how homogeneous a node is. An example of node purity is information entropy, i.e. $-p_1\log p_1-p_0\log p_0$ if there are two classes.
For regression models, node <em>im</em>purity is usually taken as the variance in a node.</p>

<p>Everytime you split a node, you do it to make the new nodes homogeneous, hence the purity increases.</p>

<p><strong>IncPurity</strong> of a variable is weighted average of incremental purity because of each split by this variable was used to split, with the node population as the weight.</p>
"
['visualization'],How to analyze which site has most numbers,"<p>The most obvious way of visualizing this is to have the number of computers on the Y-axis and the size of the dots representing the percentages. The categories (or sites in your case) can be represented on the X-axis. The image below shows an example where the Y-axis represents a continuous value (can be mapped to number of computers in your case), the X-axis represents a discrete value (can be mapped to sites in your case), and the size of the dots represents another attribute (like percentage in your case). I have used the <code>R</code> package <code>ggplot2</code> for this. <img src=""http://i.stack.imgur.com/bXH9m.png"" alt=""enter image description here""></p>
"
['bigdata'],Privacy through fake data?,"<p>There's an inherent problem that anybody who chooses to use a system designed primarily for extreme privacy is interesting to the surveillance org.</p>

<p>So the surveillance org will likely learn about any such system or class of systems if it becomes significant, and develop means of identifying its traffic (and possibly filtering out the fake traffic), and then give special attention to the users sending that traffic.</p>

<p>You can also consider the opposite approach: instead of auto-generating traffic for existing users with false negatives, auto-generating new users with traffic full of false positives.  However it suffers from the same <a href=""https://en.wikipedia.org/wiki/Cat_and_mouse"" rel=""nofollow"">cat-and-mouse dynamic</a>.</p>

<p>The solution is for the tools used by most users (Android, iOS, telcos, Gmail, Facebook, Google Search, OEMs) to feature security capabilities client-side <em>by default</em> AND to <em>not</em> provide backdoors to surveillance orgs.  That would also make it more difficult to provide useful features like recommendations and well-targeted adverts.   For various reasons, in large part because most consumers do not demand it, they do not prioritise this, and that may not ever change.</p>

<p>Further reading: <a href=""https://blog.kaspersky.com/chrome_ext_encrypt_data_leaving_browser/5063/"" rel=""nofollow"">https://blog.kaspersky.com/chrome_ext_encrypt_data_leaving_browser/5063/</a></p>
"
"['r', 'data-wrangling']",When to choose character instead of factor in R?,"<p>Factors are stored as numbers and a table of levels. If you have categorical data, storing it as a factor may save lots of memory.</p>

<p>For example, if you have a vector of length 1,000 stored as character and the strings are all 100 characters long, it will take about 100,000 bytes. If you store it as a factor, it will take about 8,000 bytes plus the sum of the lengths of the different factors. </p>

<p>Comparisons with factors should be quicker too because equality is tested by comparing the numbers, not the character values.</p>

<p>The advantage of keeping it as character comes when you want to add new items, since you are now changing the levels. </p>

<p>Store them as whatever makes the most sense for what the data represent. If <code>name</code> is not categorical, and it sounds like it isn't, then use character.</p>
"
['r'],generate graph from .eps file (preferably using R),"<p>EPS is ""Encapsulated PostScript"". Its meant for embedding like an image in documents, or sending to printers. You can view it with a PostScript document viewer, and there are free PostScript document viewers for Linux, Windows, and Mac OSs. Ghostview, Evince, etc etc.</p>

<p>So although you can view the graphic once you've got a PostScript document viewer, you cannot load it into R as if you had just plotted it. </p>
"
"['machine-learning', 'svm']",Where exactly does $\geq 1$ come from in SVMs optimization problem constraint?,"<p><strong>First problem:</strong> Minimizing $\|w\|$ or $\|w\|^2$:</p>

<p>It is correct that one wants to maximize the margin. This is actually done by maximizing $\frac{2}{\|w\|}$. This would be the ""correct"" way of doing it, but it is rather inconvenient. Let's first drop the $2$, as it is just a constant. Now if $\frac{1}{\|w\|}$ is maximal, $\|w\|$ will have to be as small as possible. We can thus find the identical solution by <em>minimizing</em> $\|w\|$. </p>

<p>$\|w\|$ can be calculated by $\sqrt{w^T w}$. As the square root is a monotonic function, any point $x$ which maximizes $\sqrt{f(x)}$ will also maximize $f(x)$. To find this point $x$ we thus don't have to calculate the square root and can minimize $w^T w = \|w\|^2$.</p>

<p>Finally, as we often have to calculate derivatives, we multiply the whole expression by a factor $\frac{1}{2}$. This is done very often, because if we derive $\frac{d}{dx} x^2 = 2 x$ and thus $\frac{d}{dx} \frac{1}{2} x^2 = x$. 
This is how we end up with the problem: minimize $\frac{1}{2} \|w\|^2$.</p>

<p><em>tl;dr</em>: yes, minimizing $\|w\|$ instead of $\frac{1}{2} \|w\|^2$ would work.</p>

<p><strong>Second problem:</strong> $\geq 0$ or $\geq 1$:</p>

<p>As already stated in the question, $y_i \left( \langle w,x_i \rangle + b \right) \geq 0$ means that the point has to be on the correct side of the hyperplane. However this isn't enough: we want the point to be at least as far away as the margin (then the point is a support vector), or even further away.</p>

<p>Remember the definition of the hyperplane,</p>

<p>$\mathcal{H} = \{ x \mid \langle w,x \rangle + b = 0\}$.</p>

<p>This description however is not unique: if we scale $w$ and $b$ by a constant $c$, then we get an equivalent description of this hyperplane. To make sure our optimization algorithm doesn't just scale $w$ and $b$ by constant factors to get a higher margin, we define that the distance of a support vector from the hyperplane is always $1$, i.e. the margin is $\frac{1}{\|w\|}$. A support vector is thus characterized by $y_i \left( \langle w,x_i \rangle + b \right) = 1 $. </p>

<p>As already mentioned earlier, we want all points to be either a support vector, or even further away from the hyperplane. In training, we thus add the constraint $y_i \left( \langle w,x_i \rangle + b \right) \geq 1$, which ensures exactly that.</p>

<p><em>tl;dr</em>: Training points don't only need to be correct, they have to be on the margin or further away.</p>
"
"['machine-learning', 'regression', 'linear-regression']",How to select regression algorithm for noisy (scattered) data?,"<p>The model I would use is the one that minimizes the accumulated quadratic error. Both models you are using, linear and quadratic, looks good. You can compute which one has the lowest error.</p>

<p>If you want to use an advanced method you can use <a href=""https://en.m.wikipedia.org/wiki/RANSAC"" rel=""nofollow"">RANSAC</a>. It is an iterative method for regression that assumes that there are outliers and remove them from the optimization. So your model should be more accurate that just using the first approach I told you.</p>
"
"['scalability', 'graphs', 'neo4j']",Use Cases of Neo4J and Spark GraphX,"<p>Neo4j and Spark GraphX are meant for solving problem at different level and they are complimentary to each other.</p>

<p>They can be connected by Neo4j's Mazerunner extension:</p>

<blockquote>
  <p>Mazerunner is a Neo4j unmanaged extension and distributed graph
  processing platform that extends Neo4j to do big data graph processing
  jobs while persisting the results back to Neo4j.</p>
  
  <p>Mazerunner uses a message broker to distribute graph processing jobs
  to Apache Spark's GraphX module. When an agent job is dispatched, a
  subgraph is exported from Neo4j and written to Apache Hadoop HDFS.</p>
  
  <p>After Neo4j exports a subgraph to HDFS, a separate Mazerunner service
  for Spark is notified to begin processing that data. The Mazerunner
  service will then start a distributed graph processing algorithm using
  Scala and Spark's GraphX module. The GraphX algorithm is serialized
  and dispatched to Apache Spark for processing.</p>
  
  <p>Once the Apache Spark job completes, the results are written back to
  HDFS as a Key-Value list of property updates to be applied back to
  Neo4j.</p>
  
  <p>Neo4j is then notified that a property update list is available from
  Apache Spark on HDFS. Neo4j batch imports the results and applies the
  updates back to the original graph.</p>
</blockquote>

<p>Check out this tutorial to get an idea on how to combine the two:
<a href=""http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html"" rel=""nofollow"">http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html</a></p>
"
"['recommendation', 'information-retrieval']",Does click frequency account for relevance?,"<p><a href=""http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf"">Depends on the user's intent</a>, for starters. </p>

<p><a href=""http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm"">Users normally only view the first set of links</a>, which means that unless the link is viewable, it's not getting clicks; meaning you'd have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here's a <a href=""http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm"">click and attention distribution heat-map</a> for  Google search results:</p>

<p><img src=""http://i.stack.imgur.com/8kO5S.jpg"" alt=""Google SEPR Click and Attention distribution ‘heat-map’""></p>

<p>Further, using click frequency to account for relevance is not a direct measure of the resource's relevance. Also, using clicks is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter. </p>

<p>That said, if you're interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see ""<a href=""http://www.youtube.com/watch?v=BsCeNCVb-d8"">YouTube's head of engineering speaking about clicks vs engagement</a>"" for more information, though note that the <a href=""http://www.orbitmedia.com/blog/ideal-blog-post-length/"">size itself of the content is a factor</a> too.</p>

<p>Might be worth noting that historically Google was known for <a href=""http://en.wikipedia.org/wiki/PageRank"">PageRank algorithm</a> though it's possible your intent is only to review click-streams, so I won't delve <a href=""https://www.google.com/search?q=google%20ranking%20factors"">Google ranking factors</a>; if you are interested in the Google's approach, you might find a review of <a href=""http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf"">Google's Search Quality Rating Guidelines</a>.</p>
"
"['python', 'data', 'pandas']",Merging large CSV files in Pandas,"<p>No, there is not. You will have to use an alternative tool like <a href=""http://dask.pydata.org/en/latest/dataframe-groupby.html"" rel=""nofollow"">dask</a>, <a href=""https://drill.apache.org/docs/using-sql-functions-clauses-and-joins/"" rel=""nofollow"">drill</a>, <a href=""https://lab.getbase.com/pandarize-spark-dataframes/"" rel=""nofollow"">spark</a>, or a good old fashioned relational database.</p>

<p>Welcome to DataScience.SE!</p>
"
"['beginner', 'books']",From developper to data scientist,"<p>I suggest <a href=""https://github.com/open-source-society/data-science"" rel=""nofollow"">this guide</a> for a good introduction to the resources available to you to learn to do data science. You are starting at a good time, there are loads of excellent (and mostly free) resources to teach you this interesting field.</p>
"
['machine-learning'],Predict probability when model was trained in balanced dataset,"<p>Daniel,
What you did goes under the name of ""oversampling."" There is a sample of some ""real"" population, and you replace it with a sample from a ""manufactured"" population. The problem that makes sense in application is the estimation of</p>

<p>$$P_r(Y=1|X) = \text{probability of response=1 in the $\mathbf{real}$ population given the predictor $X$} $$</p>

<p>but by using an oversample you are estimating
$$P_m(Y=1|X) = \text{probability of response=1 in the $\mathbf{manufactured}$ population given the predictor $X$}$$</p>

<p>The two probabilities are related. I'll worked the details. I'll pretend the predicor $X$ is discrete. If $X$ takes numerical values one has to replace some probabilities by probability densities.
$$\dots\dots\dots$$</p>

<p>To simplify the notation, let $\pi_1 = P_r(Y=1)$ and $\mu_1 = P_m(Y=1)$ be the probabilities of response in the real and manufactured populations, let 
$$
L_r = 
\frac{P_r(X=x|Y=1)}{P_r(X=x|Y=0)} = \frac{\frac{P_r(Y=1|X=x)}{P_r(Y=0|X=x)}}{\frac{\pi_1}{1-\pi_1} }$$
be the odds ratio of $Y=1$, i.e.: the ratio of the odds among cases with $X=x$ and the odds in the general $\mathbf{real}$ population. Finally, let $L_m$ be the corresponding ratio in the $\mathbf{manufactured}$ population.</p>

<p>By Bayes' Theorem:
$$ P_r(Y=1|X=x) = \frac{P_r(Y=1,X=x)}{P_r(X=x)} =   \\
=\frac{P_r(X=x|Y=1)\space \pi_1}{P_r(X=x|Y=1)\space\pi_1 + P_r(X=x|Y=0)\space (1 - \pi_1)} = \\
=\frac{L_r\space \pi_1}{L_r\space\pi_1 + \space (1 - \pi_1)} \tag{1}
$$
In a similar way, we get an analogous result for the manufactured population:
$$
P_m(Y=1|X=x) = \frac{L_m\space \mu_1}{L_m\space\mu_1 + \space (1 - \mu_1)} \tag{2}
$$
Since the manufactured sample is a random sample, stratified by $Y$, the conditional distribution of X within responders is the same as in the real population. Same as for non responders, i.e.:<br>
$$
 P_r(X=x|Y=j) = P_r(X=x|Y=j) $$
for $j=0,1.$ If the sample stratified by values of Y was anything other than random sample these would not be true.
It follows that $\boxed{ L_r = L_m }$. Next we solve for $L_m$ in terms of $P_m(Y=1|X)$ from (2) and replace in (1). </p>

<p>$$\dots\dots\dots$$
$\mathbf{Digression}$:
Here is my easy way to carry the steps, without mess. Two non-zero vectors $\mathbf{v_1}$, $\mathbf{v_2}$ are parallel iff there is $\lambda \ne 0$ such that $\mathbf{v_1}=\lambda \mathbf{v_2}.$ Below I will use this idea, and I will not care about the exact value of $\lambda$, so I will be using ""$\lambda$"" as a short-hand for ""$\mathbf{\text{some non-zero mumber}}$.""
$\mathbf{\text{End Digression}}$:
$$\dots\dots\dots$$</p>

<p>The easy way to solve is to observed that for non-zero messy values of $\lambda$ (not the same in each occurrence!) one has:</p>

<p>$$
\begin{bmatrix}
 P_r(Y=1|X) \\
 1 \\
\end{bmatrix} = \lambda 
\begin{bmatrix}
       \pi_1 &amp;0\\
      \pi_1      &amp;1-\pi_1 
\end{bmatrix} 
\begin{bmatrix}
L_r\\
1 
\end{bmatrix} ,
$$
and
$$
\begin{bmatrix}
 P_m(Y=1|X) \\
 1 \\
\end{bmatrix} = \lambda 
\begin{bmatrix}
      \mu_1 &amp;0 \\
      \mu_1      &amp;1-\mu_1 
\end{bmatrix} 
\begin{bmatrix}
L_m\\
1 
\end{bmatrix}
. $$</p>

<p>Therefore,</p>

<p>$$
\begin{bmatrix}
L_m\\
1 
\end{bmatrix}
 = \lambda 
\begin{bmatrix}
      \mu_1  &amp;0 \\
      \mu_1      &amp;1-\mu_1 
\end{bmatrix}^{-1} 
\begin{bmatrix}
 P_m(Y=1|X) \\
 1 \\
\end{bmatrix} ,
$$</p>

<p>and so (remember that here $\lambda$ stands for ""some non-zero number"")</p>

<p>$ \space 
\begin{bmatrix}
  P_r \\
   1     
\end{bmatrix} = \lambda 
\begin{bmatrix}
       \pi_1 &amp;0 \\
      \pi_1      &amp;1-\pi_1 
\end{bmatrix} 
\begin{bmatrix}
L_r \\
1 
\end{bmatrix} = \\
\text{              }=\lambda 
\begin{bmatrix}
      \pi_1      &amp;0    \\
      \pi_1      &amp;1-\pi_1 
\end{bmatrix} 
\begin{bmatrix}
       \mu_1     &amp;0     \\
      \mu_1      &amp;1-\mu_1 
\end{bmatrix}^{-1} 
\begin{bmatrix}
 P_m \\
 1 \\
\end{bmatrix} = \\
\text{              }=\lambda 
\begin{bmatrix}
      \pi_1 (1- \mu_1)        &amp;0 \\
      \pi_1  - \mu_1     &amp; \mu_1 (1- \pi_1)
\end{bmatrix} 
\begin{bmatrix}
 P_m \\
 1 \\
\end{bmatrix} = 
\lambda 
\begin{bmatrix}
      \pi_1  (1-\mu_1)  P_m \\
      (\pi_1  - \mu_1) \; P_m   +  \mu_1 (1- \pi_1)
\end{bmatrix}. 
$<br>
Thus,
$$ P_r = \frac{\pi_1 (1- \mu_1) P_m}{(\pi_1 - \mu_1) \; P_m   +  \mu_1 (1- \pi_1) } $$</p>

<p>$$\dots\dots\dots$$ 
Example: Let's work the details of a Binomial model, 
$$P_m(Y=1|X) = \frac{e^{\beta_0 + \beta X}}{1+e^{\beta_0 + \beta X}} $$
or in the ""where $\lambda$ is some non-zero scalar"" notation (I would not had digressed before if I did not had ulterior motive.. :) ):</p>

<p>$$
 \begin{bmatrix}
 P_m \\
 1 \\
\end{bmatrix} = 
\lambda 
\begin{bmatrix}
e^{\beta_0 + \beta X} \\
1 + e^{\beta_0 + \beta X}
\end{bmatrix}
$$</p>

<p>What is the implied model in the real population?  </p>

<p>$ \space 
\begin{bmatrix}
     P_r \\
      1     
\end{bmatrix} = 
\lambda 
\begin{bmatrix}
      \pi_1 (1- \mu_1)   &amp;0 \\
      \pi_1  - \mu_1     &amp;\mu_1 (1- \pi_1)
\end{bmatrix} 
\begin{bmatrix} 
  P_m \\
   1 
\end{bmatrix} =
$  </p>

<p>$\;$
$
= \lambda 
\begin{bmatrix}
      \pi_1 (1- \mu_1)   &amp;0 \\
      \pi_1  - \mu_1     &amp;\mu_1 (1- \pi_1)
\end{bmatrix} 
\begin{bmatrix}
e^{\beta_0 + \beta X} \\
1 + e^{\beta_0 + \beta X}
\end{bmatrix}=
$  </p>

<p>$\;$
$
= \lambda 
\begin{bmatrix}
      \pi_1 (1- \mu_1)   e^{\beta_0 + \beta X} \\
      \pi_1 (1- \mu_1)   e^{\beta_0 + \beta X} + \mu_1 (1- \pi_1)
\end{bmatrix} = 
\lambda
\begin{bmatrix}
\frac{\pi_1 (1- \mu_1)}{\mu_1 (1- \pi_1)} e^{\beta_0 + \beta X} \\
1 + \frac{\pi_1 (1- \mu_1)}{\mu_1 (1- \pi_1)} e^{\beta_0 + \beta X}
\end{bmatrix}
.$  </p>

<p>If we let $\tau = \ln(\frac{\pi_1 (1- \mu_1)}{\mu_1 (1- \pi_1)})$, we can absorve this constant in the exponents to get:
$$ 
\begin{bmatrix}
     P_r \\
      1     
\end{bmatrix} = 
\lambda 
\begin{bmatrix}
e^{\tau + \beta_0 + \beta X} \\
1 + e^{\tau + \beta_0 + \beta X}
\end{bmatrix}
.$$<br>
Taking the ratio and simplifying the non-zero constant in numerator and denominator we get that fitting a logistic model to the manufactured population results in an implied logistic model for the real population, $\mathbf{\text{with the same coefficients for X}}$ and with a difference in the constant (in the logistic model) given by:
$$ \beta_{real} = \tau + \beta_0 $$</p>

<p>$$\dots$$
Note that, according to your reference, the ratio of $\gamma_1 = Pr(Z=1|Y=1)$ and $\gamma_0 = Pr(Z=1|Y=0)$ should come up. Indeed:<br>
$$ \gamma_1 = Pr(Z=1|Y=1) = \frac{P(Z=1,Y=1)}{P(Y=1)} = \frac{P_r(Y=1|Z=1)P_r(Z=1)}{P_r(Y=1)} = \frac{P_m(Y=1)}{P_r(Y=1)} P_r(Z=1)= 
\frac{\mu_1}{\pi_1}P_r(Z=1) $$
likewise (i.e. change Y to 1-Y),
$$ \gamma_0 = \frac{1-\mu_1}{1-\pi_1}P_r(Z=1) $$ 
so 
$$ \ln(\frac{\gamma_1}{\gamma_0}) = - \ln(\frac{\pi_1 (1-\mu_1)}{\mu_1 (1-\pi_1}) = \tau $$</p>

<p>$$\dots\dots\dots$$
Notes for full disclosure: I worked with the probability model. When one works with finite samples the example above suggests two ways of estimating the coefficients:
* estimate coefficients using the sample from the real population
* estimate coefficients using the manufactored populations</p>

<p>It terns out that this two estimators are not the same (it is obvious if one consideres one estimator is based on more cases than the other). Both estimators are asymtopically consistent, but it can be shown the one based on the manufactored population is more biased (forgot the reference :( ).</p>

<p>In the data science space we are more concern with the quality of the predictions than the parameters of the parameters used to make those predictions, so as long as you check results properly (e.g.: using a testing set to build models and another to validate them), the bias in the parameters should not deter us from using oversampling. </p>

<p>$$\dots\dots\dots$$</p>
"
"['dataset', 'statistics', 'algorithms']",Versatile data structure for combined statistics,"<p>It sounds like you would like the <a href=""http://www.boost.org/doc/libs/1_56_0/doc/html/accumulators.html"" rel=""nofollow"">Boost Accumulators</a> library:</p>

<blockquote>
  <p>Boost.Accumulators is both a library for incremental statistical
  computation as well as an extensible framework for incremental
  calculation in general. The library deals primarily with the concept
  of an accumulator, which is a primitive computational entity that
  accepts data one sample at a time and maintains some internal state.
  These accumulators may offload some of their computations on other
  accumulators, on which they depend. Accumulators are grouped within an
  accumulator set. Boost.Accumulators resolves the inter-dependencies
  between accumulators in a set and ensures that accumulators are
  processed in the proper order.</p>
</blockquote>
"
"['python', 'correlation']","data processing, correlation calculation","<p>Pandas is the best thing since sliced bread (for data science, at least). </p>

<p>an example:</p>

<pre><code>import pd
In [22]: df = pd.read_csv('yourexample.csv')

In [23]: df
Out[23]:
   user   item1   item2
0     a        2      4
1     b        1      3
2     c        5      6

In [24]: df.columns
Out[24]: Index([u'user ', u'item1 ', u'item2'], dtype='object')

In [25]: df.corr()
Out[25]:
          item1      item2
item1   1.000000  0.995871
item2   0.995871  1.000000

In [26]: df.cov()
Out[26]:
          item1      item2
item1   4.333333  3.166667
item2   3.166667  2.333333
</code></pre>

<p>Bingo!</p>
"
"['machine-learning', 'programming']",From where should I start Machine Learning?,"<p>I am a book person so I would recommend one of the following books:</p>

<ol>
<li>Elements of Statistical Learning (Hastie and Tibshirani). </li>
<li>Pattern Recognition and Machine Learning (Bishop). </li>
</ol>

<p>The first book is available as a free download from the authors' website. You may download and start reading it. You will get an idea about your deficiencies. If it's too difficult, then you need to improve your statistics and linear algebra skills. For Linear Algebra I recommend:</p>

<ul>
<li>Linear Algebra and its Applications (David Lay). </li>
</ul>

<p>For statistics I like:</p>

<ul>
<li>Discovering Statistics (Andy Fields).</li>
</ul>

<p>Stay away from the recipe books if your aims are long-term. </p>
"
"['python', 'scikit', 'nltk', 'gensim']",Document Categorization Problem,"<p>Except for the OCR part, the right bundle would be <code>pandas</code> and <code>sklearn</code>.</p>

<p>You can check this <a href=""https://gist.github.com/herokyar/5ffb653df69139a3e2b2"" rel=""nofollow"">ipython notebook</a> which uses TfidfVectorizer and SVC Classifier.</p>

<p>This classifier can make <a href=""http://scikit-learn.org/stable/modules/multiclass.html"" rel=""nofollow"">one-vs-one or one-vs-the-rest</a> multiclass predictions, and if you use the <code>predict_proba</code> method instead of <code>predict</code>, you would have the confidence level of each category.</p>

<p>If you're looking for performances and you don't need prediction confidence levels, you should use <code>LinearSVC</code> which is way faster.</p>

<p>Sklearn is very well documented and you will find everything you need for text classification.</p>
"
"['r', 'time-series']",forecast monthly shipment (time series) for 300 products individually in R,"<p>It is difficult to answer this question with the limited information provided here. Nonetheless, I have following recommendations.</p>

<p>Do you have a (data backed) reason to believe that you do not need 300 different models? For example, are the shipments of different products correlated? Alternatively, can you instead model the problem at higher levels like product categories? If either case, you can take a look at hierarchical time series modeling. <a href=""http://robjhyndman.com/papers/hierarchical/"" rel=""nofollow"">This journal article by Rob Hyndman, author of forecast package in R</a> can get you started on hierarchical time series modeling.</p>
"
"['dataset', 'feature-selection', 'correlation']",Use of Correlation score,"<p>Correlation should be as less as possible between different features, because correlated features mean that those features are giving out same kind of information/trend for the predictor to learn. Thus only one of them is actually useful for prediction. </p>

<p>Keeping more number of uninformative features (correlated features) would result in degraded accuracy if your sample size is similar to you feature set size. Feature selection using Recursive Feature elimination or PCA etc. can help you reduce your feature set to optimal size.</p>

<p>We calculate correlation score in predictive analysis between features and Target variable. When using linear regression to model a data set, we first see if the plot between different features and target variable values follow an upward (+ve correlation) or downward trend (-ve correlation) and not scattered randomly. If such a relationship exists then regression modelling on the data would work well.</p>
"
"['python', 'visualization']",What to consider before learning a new language for data analysis,"<p>Personally going to make a strong argument in favor of Python here. There are a large number of reasons for this, but I'm going to build on some of the points that other people have mentioned here:</p>

<ol>
<li><strong>Picking a single language:</strong> It's definitely possible to mix and match languages, picking <code>d3</code> for your visualization needs, <code>FORTRAN</code> for your fast matrix multiplies, and <code>python</code> for all of your networking and scripting. You can do this down the line, but keeping your stack as simple as possible is a good move, especially early on.</li>
<li><strong>Picking something bigger than you:</strong> You never want to be pushing up against the barriers of the language you want to use. This is a huge issue when it comes to languages like <code>Julia</code> and <code>FORTRAN</code>, which simply don't offer the full functionality of languages like <code>python</code> or <code>R</code>.</li>
<li><strong>Pick Community</strong>: The one most difficult thing to find in any language is community. <code>Python</code> is the clear winner here. If you get stuck, you ask something on SO, and someone will answer in a matter of minutes, which is simply not the case for most other languages. If you're learning something in a vacuum you will simply learn much slower.</li>
</ol>

<p>In terms of the minus points, I might actually push back on them.</p>

<p>Deepening your knowledge of one language is a decent idea, but knowing <em>only</em> one language, without having practice generalizing that knowledge to other languages is a good way to shoot yourself in the foot. I have changed my entire favored development stack three time over as many years, moving from <code>MATLAB</code> to <code>Java</code> to <code>haskell</code> to <code>python</code>. Learning to transfer your knowledge to another language is far more valuable than just knowing one.</p>

<p>As far as feasibility, this is something you're going to see again and again in any programming career. Turing completeness means you could technically do everything with <code>HTML4</code> and <code>CSS3</code>, but you want to pick the right tool for the job. If you see the ideal tool and decide to leave it by the roadside you're going to find yourself slowed down wishing you had some of the tools you left behind.</p>

<p>A great example of that last point is trying to deploy <code>R</code> code. 'R''s networking capabilities are hugely lacking compared to <code>python</code>, and if you want to deploy a service, or use slightly off-the-beaten path packages, the fact that <code>pip</code> has an order of magnitude more packages than <code>CRAN</code> is a huge help.</p>
"
"['bigdata', 'hadoop', 'research']",Masters thesis topics in big data,"<p>Since it's a master's thesis, how about writing something regarding decision trees, and their ""upgrades"": boosting and Random Forests? And then integrate that with Map/Reduce, together with showing how to scale a Random Forest on Hadoop using M/R?</p>
"
"['data-mining', 'bigdata', 'dataset', 'apache-spark', 'social-network-analysis']",Need to prepare the data to Link Analysis project?,"<p>One option is that you could make a bipartite graph from your TLOG and then implement some link analysis. Depending on the requirements that you need (<em>volume of the data</em>) there are different frameworks that you can use. One which is quite popular for not so large data is <code>networkx</code> where (<a href=""https://networkx.github.io/documentation/networkx-1.9.1/reference/algorithms.html"" rel=""nofollow"">manual</a>) you can find already implemented algorithms for link analysis and link prediction.</p>

<p>Maybe, the community would be able to help you more if you try to be more specific about what kind of link analysis you want and what kind of problem you try to solve (<em>does it has to do probably with Supervised or Unsupervised learning</em>).</p>
"
"['machine-learning', 'classification', 'r', 'clustering']",R Script to generate random dataset in 2d space,"<p>None of the algorithms you mention are good with data that has uniform distribution.</p>

<pre><code>size &lt;- 20             #length of random number vectors
set.seed(1) 
x &lt;- runif(size)          # generate samples from uniform distribution (0.0, 1.0)
y &lt;-runif(size) 
df &lt;-data.frame(x,y)

# other distributions: rpois, rmvnorm, rnbinom, rbinom, rbeta, rchisq, rexp, rgamma, rlogis, rstab, rt, rgeom, rhyper, rwilcox, rweibull.
</code></pre>

<p>See <a href=""http://statistics.ats.ucla.edu/stat/r/modules/prob_dist.htm"" rel=""nofollow"">this page</a> for tutorial on generating random samples from distributions.</p>

<hr>

<p>For specific set of randomized data sets that are 'hard' for these methods (e.r. linearly inseparable <em>n</em>-classes XOR patterns), see this blog post (incl. R code): <a href=""http://tjo-en.hatenablog.com/entry/2014/01/06/234155"" rel=""nofollow"">http://tjo-en.hatenablog.com/entry/2014/01/06/234155</a>.</p>
"
"['python', 'apache-spark', 'pyspark']",key parameter in max function in Pyspark,"<p>You pass a function to the key parameter that it will virtually map your rows on to check for the maximum value. In this case you pass the str function which converts your floats to strings. Since '5.0' > '14.0' due to the nature of string comparisons, this is returned. What is usually a more likely use is using the key parameter as follows:</p>

<pre><code>test = sc.parallelize([(1, 2), (4,3), (2,4)])
test.max(key = lambda x: -x[1])
(1,2)
</code></pre>

<p>Because of the - we sort descending and the x[1] means we use the second entry in our tuples as the key.</p>
"
"['data-mining', 'r', 'dataset', 'beginner']",Data transposition code in R,"<p>You can use the <code>reshape2</code> package for this task.</p>

<p>First, transform the data to the long format with <code>melt</code>:</p>

<pre><code>library(reshape2)
dat_m &lt;- melt(dat, measure.vars = c(""fruit_amt"", ""veg_amt""))
</code></pre>

<p>where <code>dat</code> is the name of your data frame.</p>

<p>Second, cast to the wide format:</p>

<pre><code>dcast(dat_m, ID ~ timeframe + variable + date)
</code></pre>

<p>The result:</p>

<pre><code>    ID after_fruit_amt_04/16/2014 after_fruit_amt_05/23/2013 after_fruit_amt_05/24/2014 after_veg_amt_04/16/2014
1 4352                          0                       0.06                         NA                        0
2 5002                         NA                         NA                       0.06                       NA
  after_veg_amt_05/23/2013 after_veg_amt_05/24/2014 before_fruit_amt_05/23/2013 before_fruit_amt_05/24/2014
1                     0.25                       NA                        0.25                          NA
2                       NA                     0.25                          NA                        0.75
  before_veg_amt_05/23/2013 before_veg_amt_05/24/2014
1                      0.75                        NA
2                        NA                      0.25
&gt; 
</code></pre>
"
['machine-learning'],What does 'contextual' mean in 'contextual bandits'?,"<p>A contextual bandit algorithm not only adapts to the user-click feedback as the algorithm progresses, it also utilizes pre-existing information about the user's (and similar users) browsing patterns to select which content to display.</p>

<p>So, rather than starting with no prediction (cold start) with what the user will click (traditional bandit and also traditional A/B testing), it takes other data into account (warm start) to help predict which content to display during the bandit test.</p>

<p>See: <a href=""http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf"" rel=""nofollow"">http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf</a></p>
"
"['apache-spark', 'apache-mahout', 'recommender-system']",Unknown program 'spark-itemsimilarity' chosen,"<p>Spark support for Mahout came from <a href=""http://mahout.apache.org/release-notes/Apache-Mahout-0.10.0-Release-Notes.pdf"" rel=""nofollow"">Mahout 0.10 release</a> while you are using 0.9 release. So this should explain why you get the <code>unknown program</code> error. I would suggest using a higher version of Mahout.</p>
"
['decision-trees'],Decision Stumps with same value leaf nodes,"<p>What is the overall response rate? If it's low (even 15-20%) it may be difficult to find decision stumps that contain one leaf with > 50% response! </p>

<p>You could consider oversampling or changing cutoff probability, but I think if your using only 2 leaf trees, your model is bound to struggle.</p>
"
"['social-network-analysis', 'time-series', 'javascript', 'visualization']",How to animate growth of a social network?,"<p>It turned out that this task was quite easy to accomplish using <a href=""http://visjs.org/"" rel=""nofollow"">vis.js</a>. <a href=""http://visjs.org/examples/graph/20_navigation.html"" rel=""nofollow"">This</a> was the best example code which I have found.</p>

<p>The example of what I have built upon this is <a href=""http://laboratoriumdanych.pl/jak-powstaje-siec/"" rel=""nofollow"">here</a> (scroll to the bottom of this post). This graph represents the growth of a subnetwork of Facebook friends. Green dots are females, blue ones are males. The darker the colour, the older the user. By clicking ""Dodaj węzły"" you can add more nodes and edges to the graph.</p>

<p>Anyway, I am still interested in other ways to accomplish this task, so I won't accept any answer as for now.</p>

<p>Thanks for your contributions!</p>
"
"['neuralnetwork', 'recommendation', 'image-classification']",How to adapt the softmax layer for multiple labels?,"<p>I have read related tutorials over the past several days.</p>

<p>There are two possible solutions</p>

<ol>
<li>transfer the problem into several multi-classication problem.</li>
<li>use algorithms that can handle multi-label problem.</li>
</ol>

<p>On the software side:</p>

<ol>
<li>extract network parameters except the last layer.</li>
<li>use a SVM(or other methods) to handle the last layer.</li>
</ol>

<p>Reference:</p>

<ol>
<li>stanford computer vision online course</li>
<li>caffe tutorial</li>
</ol>
"
['statistics'],How can I show the relations between travel destinations?,"<p>You can try Graph databases (Neo4j/Orient DB etc). Store location and connection between the location as nodes and edges. Then do analysis over graph data. Based on your need, you can use additional attributes (like count) and assign weights for edges etc. Neo4j supports collaborative filtering also.</p>
"
['classification'],Clasification - ROC Curve with very high number of false negatives,"<p>Notice how the <code>precision</code> is very high and all of the other metrics are very low.  Now look at the class balance of your problem:</p>

<p>$$TP+FN=Actual Positive=31,245$$
$$TN+FP=Actual Negative=508$$</p>

<p>So your data is heavily skewed toward positives.  To have gotten a model that is producing this poorly, I think you may have provided the model with the <code>precision</code> as the cross validation metric.  The <code>precision</code> is a very bad cross validation metric in this case since it will result in poor <code>accuracy</code> and poor <code>recall</code>.  <code>accuracy</code> is also not a good metric as your model could classify everything as positive and get an accuracy of:</p>

<p>$$AC=\frac{31,245}{31,763}=.984$$</p>

<p>For cases like this where the classes are grossly weighted toward one value, I suggest using the <code>F1-score</code> as your cross validation metric.  The <code>F1-score</code> is the <code>harmonic mean</code> of <code>precision</code> and <code>recall</code> and hence balances these two factors nicely.  Wikipedia actually has a <a href=""https://en.wikipedia.org/wiki/Precision_and_recall#Definition_.28classification_context.29"" rel=""nofollow"">very nice explanation of classification metrics here</a> and <a href=""http://rali.iro.umontreal.ca/rali/sites/default/files/publis/SokolovaLapalme-JIPM09.pdf"" rel=""nofollow"">this paper is top notch</a> if you even need to understand multi-class metrics and confusion matrices.</p>

<p>Hope this helps!</p>
"
"['machine-learning', 'cross-validation']",Overfitting/Underfitting with Data set size,"<p>So, the underfitting means that you still have capacity for improving your learning while overfitting means that you have used a capacity more than needed for learning.</p>

<p>Green area is where testing error is rising i.e. you should continue providing capacity (either data points or model complexity) to gain better results. More green line goes, more flat it becomes i.e. you are reaching the point where the provided capacity (which is data) is enough and better to try providing the other type of capacity which is model complexity. </p>

<p>If it does not improve your test score or even reduce it that means that the combination of Data-Complexity was somehow optimal and you can stop training.</p>
"
"['python', 'feature-extraction', 'image-recognition']",Feature extraction of images in Python,"<p>In images, some frequently used techniques for feature extraction are <strong>binarizing</strong> and <strong>blurring</strong></p>

<p><strong>Binarizing:</strong> converts the image array into 1s and 0s. This is done while converting the image to a 2D image. Even gray-scaling can also be used. It gives you a numerical matrix of the image. Grayscale takes much lesser space when stored on Disc.</p>

<p>This is how you do it in Python:</p>

<pre><code>from PIL import Image

%matplotlib inline  

#Import an image
image = Image.open(""xyz.jpg"")

image
</code></pre>

<p>Example Image: </p>

<p><a href=""http://i.stack.imgur.com/mkf97.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mkf97.jpg"" alt=""enter image description here""></a></p>

<p>Now, convert into gray-scale:</p>

<pre><code>im = image.convert('L')

im
</code></pre>

<p>will return you this image:</p>

<p><a href=""http://i.stack.imgur.com/AGxy6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AGxy6.png"" alt=""enter image description here""></a></p>

<p>And the matrix can be seen by running this:</p>

<pre><code>array(im)
</code></pre>

<p>The array would look something like this:</p>

<pre><code>array([[213, 213, 213, ..., 176, 176, 176],
       [213, 213, 213, ..., 176, 176, 176],
       [213, 213, 213, ..., 175, 175, 175],
       ..., 
       [173, 173, 173, ..., 204, 204, 204],
       [173, 173, 173, ..., 205, 205, 204],
       [173, 173, 173, ..., 205, 205, 205]], dtype=uint8)
</code></pre>

<p>Now, use a histogram plot and/or a contour plot to have a look at the image features:</p>

<pre><code>from pylab import *

# create a new figure
figure()
gray()
# show contours with origin upper left corner
contour(im, origin='image')
axis('equal')
axis('off')


figure()


hist(im_array.flatten(), 128)

show()
</code></pre>

<p>This would return you a plot, which looks something like this:</p>

<p><a href=""http://i.stack.imgur.com/56A0K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/56A0K.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/tWpNy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tWpNy.png"" alt=""enter image description here""></a></p>

<p><strong>Blurring:</strong> Blurring algorithm takes weighted average of neighbouring pixels to incorporate surroundings color into every pixel. It enhances the contours better and helps in understanding the features and their importance better.</p>

<p>And this is how you do it in Python:</p>

<pre><code>from PIL import *


figure()
p = image.convert(""L"").filter(ImageFilter.GaussianBlur(radius = 2))
p.show()
</code></pre>

<p>And the blurred image is:</p>

<p><a href=""http://i.stack.imgur.com/0Dx8q.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0Dx8q.jpg"" alt=""enter image description here""></a></p>

<p>So, these are some ways in which you can do feature engineering. And for advanced methods, you have to understand the basics of Computer Vision and neural networks, and also the different types of filters and their significance and the math behind them.</p>
"
"['bigdata', 'r', 'optimization', 'processing']",Running huge datasets with R,"<p>Although your question is not very specific so I'll try to give you some generic solutions. There are couple of things you can do here:</p>

<ul>
<li>Check sparseMatrix from Matrix package as mentioned by @Sidhha</li>
<li>Try running your model in parallel using packages like snowfall, <a href=""https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf"">Parallel</a>. Check this <a href=""http://cran.r-project.org/web/views/HighPerformanceComputing.html"">list of packages on Cran</a> which can help you runnning your model in multicore parallel mode.</li>
<li>You can also try <a href=""http://datatable.r-forge.r-project.org/"">data.table</a> package. It is quite phenomenal in speed.</li>
</ul>

<p>Good reads:</p>

<ol>
<li><a href=""http://theodi.org/blog/fig-data-11-tips-how-handle-big-data-r-and-1-bad-pun"">11 Tips on How to Handle Big Data in R (and 1 Bad Pun)</a></li>
<li><a href=""http://adv-r.had.co.nz/Performance.html"">Why R is slow &amp; how to improve its Performance?</a></li>
</ol>
"
"['machine-learning', 'python', 'neuralnetwork']",Best python library for neural networks,"<p>UPDATE: the landscape has changed quite a bit since I answered this question in July '14, and some new players have entered the space.  In particular I would recommend checking out:</p>

<ul>
<li>TensorFlow: <a href=""https://github.com/tensorflow/tensorflow"">https://github.com/tensorflow/tensorflow</a></li>
<li>Blocks: <a href=""https://github.com/mila-udem/blocks"">https://github.com/mila-udem/blocks</a></li>
<li>Lasagne: <a href=""https://github.com/Lasagne/Lasagne"">https://github.com/Lasagne/Lasagne</a></li>
<li>Keras: <a href=""https://github.com/fchollet/keras"">https://github.com/fchollet/keras</a></li>
<li>Deepy: <a href=""https://github.com/uaca/deepy"">https://github.com/uaca/deepy</a></li>
<li>Nolearn: <a href=""https://github.com/dnouri/nolearn"">https://github.com/dnouri/nolearn</a></li>
</ul>

<p>They each have their strengths and weaknesses, so give them all a go and see which best suits your use case.  Although I would have recommended using pylearn2 a year ago, the community is no longer active so I would recommend looking elsewhere.  My original response to the answer is included below, but is largely irrelevant at this point.</p>

<hr>

<p><a href=""http://deeplearning.net/software/pylearn2/"">Pylearn2</a> is generally considered the library of choice for neural networks and deep learning in python.  It's designed for easy scientific experimentation rather than ease of use, so the learning curve is rather steep, but if you take your time and follow the tutorials I think you'll be happy with the functionality it provides.  Everything from standard Multilayer Perceptrons to Restricted Boltzmann Machines to Convolutional Nets to Autoencoders is provided.  There's great GPU support and everything is built on top of Theano, so performance is typically quite good.  The source for Pylearn2 is available <a href=""https://github.com/lisa-lab/pylearn2"">on github</a>.</p>

<p>Be aware that Pylearn2 has the opposite problem of pybrain at the moment -- rather than being abandoned, Pylearn2 is under active development and is subject to frequent changes.</p>
"
"['neuralnetwork', 'cross-validation', 'training']",Cross validation when training neural network?,"<p>I couldn't say what the authors refer to by</p>

<blockquote>
  <p>best by cross-validation</p>
</blockquote>

<p>but I'll mention a simple and general procedure that's out there:</p>

<p>You are correct that analyzing <strong>one</strong> estimate of the generalization performance using <strong>one</strong> training and <strong>one</strong> test set is quite simplistic. Cross-validation can help us understand how this performance varies across datasets, instead of wonder whether we got lucky/unlucky with our choice of train/test datasets. </p>

<p>Split the whole dataset into <em>k</em> folds (or partitions), and train/test the model <em>k</em> times using different folds. When you're done, you can compute the mean performance and a variance that will be of utmost importance assessing confidence in the generalization performance estimate.</p>
"
"['python', 'visualization', 'javascript', 'weka']",How to create visualisation on medical data?,"<p>If you have good working knowledge in Python than you are good for it. <a href=""https://d3js.org/"" rel=""nofollow"">D3.js</a> actually has a Python comparable called <a href=""http://bokeh.pydata.org/en/latest/"" rel=""nofollow"">Bokeh</a>. You question implies a lot of ramifications and does explain fully what you are trying to achieve so let's go by parts.</p>

<p><strong>Some 2D Scientific Plotting Python Libraries:</strong></p>

<ul>
<li><a href=""http://matplotlib.org/"" rel=""nofollow"">Matplotlib</a></li>
<li><a href=""http://ggplot.yhathq.com/"" rel=""nofollow"">ggplot</a></li>
<li><a href=""http://bokeh.pydata.org/en/latest/"" rel=""nofollow"">Bokeh</a></li>
<li><a href=""http://code.enthought.com/projects/chaco/"" rel=""nofollow"">Chaco</a></li>
<li><a href=""http://www.pyqtgraph.org/"" rel=""nofollow"">pyQtGraph</a> (This one has some interesting features in volume slicing if it suits you)</li>
</ul>

<p><strong>Some UI development Python libraries:</strong></p>

<ul>
<li><a href=""https://www.riverbankcomputing.com/software/pyqt/download5"" rel=""nofollow"">PyQt5</a></li>
<li><a href=""https://www.riverbankcomputing.com/software/pyqt/download"" rel=""nofollow"">PyQt4</a> (there are significant changes to PyQt5, thus the suggestion)</li>
<li><a href=""http://wiki.qt.io/PySide"" rel=""nofollow"">PySide</a> (also a Qt port)</li>
<li><a href=""http://www.wxpython.org/"" rel=""nofollow"">wxPython</a></li>
</ul>

<p><strong>Some Python numerical libraries:</strong></p>

<ul>
<li><a href=""http://pandas.pydata.org/"" rel=""nofollow"">Pandas</a></li>
<li><a href=""http://www.numpy.org/"" rel=""nofollow"">Numpy</a></li>
<li><a href=""https://www.scipy.org/"" rel=""nofollow"">Scipy</a></li>
<li><a href=""http://scikit-learn.org/stable/"" rel=""nofollow"">Scikit-learn</a> (You can do some serious data analysis with this)</li>
</ul>

<p>Using combinations of the above you can build some very powerful software solutions. Any of the UI libraries suggested has spreadsheet widgets. There are libraries to read <a href=""https://docs.python.org/2/library/csv.html"" rel=""nofollow"">csv</a> and <a href=""http://www.python-excel.org/"" rel=""nofollow"">excel</a>, among others (<a href=""https://docs.python.org/2/library/json.html"" rel=""nofollow"">JSON</a>, <a href=""http://docs.h5py.org/en/latest/"" rel=""nofollow"">HDF5</a>, etc.).</p>

<p>The data problem can be resolved by either building your own randomized data or by using resources on the net like <a href=""http://www.healthdata.gov/dataset"" rel=""nofollow"">this</a> or <a href=""https://www.nlm.nih.gov/hsrinfo/datasites.html"" rel=""nofollow"">this</a>.</p>
"
"['classification', 'apache-spark', 'multilabel-classification']",SPARK 1.5.1: Convert multi-labeled data into binary vector,"<p>Spark only recently implemented <a href=""https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/feature/CountVectorizer.html"" rel=""nofollow"">CountVectorizer</a>, which will take the labels (as strings) and encode them as your 100-dimensional vector (assuming all 100 labels show up somewhere in your dataset).  Once you have those vectors, it should be a simple step to threshold them to make them 0/1 instead of a frequency.</p>
"
"['machine-learning', 'classification']",What are Hybrid Classifiers used in Sentiment Analysis?,"<p>In sentiment analysis you may want to combine a number of classifiers. Let's say: a separate classifier for emoticons, another one for emotionally loaded terms, another one for some special linguistic patterns and - let's say - yet another one to detect and filter out spam messages. It's all up to you.</p>

<p>You can use either SVM, Naive Bayes, or anything else that best suits your problem. You may use majority voting, weights (for example based on cross validation results), or any other more advanced technique to decide which class is the most appropriate one.</p>

<p>Also, googling for <em>hybrid sentiment</em> returns tons of papers containing answers to the questions that you have stated. Please, don't ask us to rewrite this papers here.</p>
"
"['gradient-descent', 'word-embeddings']",Gradient Descent Step for word2vec negative sampling,"<p>Looks good to me. This derivative is also presented in the paper (equations 56-58).</p>

<p>The paper you're linking to is the most advanced attempt - at least to best of my knowledge - to explain how word2vec works, but there is also a lot of other resources on the topic (just search for word2vec on <a href=""http://arxiv.org/find/all/1/all:+word2vec/0/1/0/all/0/1"" rel=""nofollow"">arxiv.org</a>). If you're interested in word2vec, you may find GloVe interesting too (see: <a href=""http://arxiv.org/pdf/1411.5595v2.pdf"" rel=""nofollow"">Linking GloVe with word2vec</a>).</p>
"
"['python', 'classification', 'random-forest', 'pandas', 'unbalanced-classes']",How does class_weights work in RandomForestClassifier,"<p>Maybe try to encode your target values as binary. Then, this <code>class_weight={0:1,1:2}</code> should do the job. Now, class 0 has weight 1 and class 1 has weight 2.</p>
"
"['bigdata', 'hadoop']",Do I need to learn Hadoop to be a Data Scientist?,"<p>Different people use different tools for different things.  Terms like Data Science are generic for a reason.  A data scientist could spend an entire career without having to learn a particular tool like hadoop.  Hadoop is widely used, but it is not the only platform that is capable of managing and manipulating data, even large scale data.</p>

<p>I would say that a data scientist should be familiar with concepts like MapReduce, distributed systems, distributed file systems, and the like, but I wouldn't judge someone for not knowing about such things.  </p>

<p>It's a big field.  There is a sea of knowledge and most people are capable of learning and being an expert in a single drop.  The key to being a scientist is having the desire to learn and the motivation to know that which you don't already know.</p>

<p>As an example:  I could hand the right person a hundred structured CSV files containing information about classroom performance in one particular class over a decade.  A data scientist would be able to spend a year gleaning insights from the data without ever needing to spread computation across multiple machines.  You could apply machine learning algorithms, analyze it using visualizations, combine it with external data about the region, ethnic makeup, changes to environment over time, political information, weather patterns, etc.  All of that would be ""data science"" in my opinion.  It might take something like hadoop to test and apply anything you learned to data comprising an entire country of students rather than just a classroom, but that final step doesn't necessarily make someone a data scientist.  And not taking that final step doesn't necessarily disqualify someone from being a data scientist.</p>
"
['regression'],Regression model for a count proces,"<p>I have to quote Tukey, perhaps the grandfather of data science:</p>

<blockquote>
  <p>The combination of some data and an aching desire for an answer does
  not ensure that a reasonable answer can be extracted from a given body
  of data.</p>
</blockquote>

<p>I see nothing wrong with your Poisson model. In fact its a pretty good fit to the data. The data is noisy. There is nothing you can do about it. Perhaps the noise if due to whatever else is on TV at the time, or the weather, or the phase of the moon. Whatever it is, its not in your data.</p>

<p>If you reasonably think the weather might be affecting your data, get the weather data and add it. If it decreases the log-likelihood enough for each degree of freedom then it's doing a good job and you leave it in. This is regression modelling 101. </p>

<p>Of course there's a zillion other things you can do. Scale the data by any old transformation you want. Fit a quadratic. A quartic. A quintic. A spline.  You could include the date and possible temporal correlation effects. But always bear in mind what Tukey was saying - if your data is noisy, you won't get anything much out of it. So it goes.</p>
"
"['neuralnetwork', 'keras']",Properties for building a Multilayer Perceptron Neural Network using Keras?,"<p>1) Activation is an architecture choice, which boils down to a hyperparameter choice. You can make a theoretical argument for using any function, but the best way to determine this is to try several and evaluate on a validation set. It's also important to remember you can mix and match activations of various layers.</p>

<p>2) In theory yes, many random initializations would be the same if your data was extremely well behaved and your network ideal. But in practice initializations seek to ensure the gradient starts off reasonable and the signal can be backpropagated correctly. Likely in this case any of those initializations would perform similarly, but the best approach is to try them out, switching if you get undesirable results.</p>
"
['machine-learning'],Machine Learning: Single input to variable number of outputs,"<p>I would try to set a <a href=""http://scikit-learn.org/stable/modules/multiclass.html"" rel=""nofollow"">multilabel classification algorithm</a> and make the output standard by adding zeros. So if your data is like this: &lt;1, <a href=""http://scikit-learn.org/stable/modules/multiclass.html"" rel=""nofollow"">1</a>>, &lt;2, [1, 1]>, &lt;3, [2, 1]>, &lt;4, [1, 2, 1, 1]>, &lt;5, [1, 1, 1, 2, 2, 1]>. The maximum number of output is 6. So you could transform your data into something like: 
&lt;1, [1,0,0,0,0,0]>, &lt;2, [1, 1,0,0,0,0]>, &lt;3, [2, 1,0,0,0,0]>, &lt;4, [1, 2, 1, 1,0,0]>, &lt;5, [1, 1, 1, 2, 2, 1]></p>

<p>Another option that occurs to me is to add the limit dynamically. Let say you have your training and test set. You can search for the biggest length and create an algorithm that adds the zeros to both datasets. Then let's say a new data you want to predict has a bigger length, then you'll need to recompute all training and test with for this new prediction. You can even check how extending the limit affects your model.</p>
"
"['predictive-modeling', 'regression', 'feature-selection', 'feature-engineering', 'azure-ml']",Improve a regression model and feature selection,"<p>I agree with @Hoap. Your features might be low for the amount of training observations you have. Instead of excluding columns, see if you're missing more features. Feature Engineering, rather than Feature Selection. <br>
However, if you are looking for Feature Selection, then Azure ML has a Feature Selection Module with the option to specify how many features you'd like to keep.<br></p>

<p>Some simple verifications to do before you jump into modeling: <br></p>

<ul>
<li>Visualize your dataset for any non-linear relationships. <br></li>
<li>You could also perform a simple correlation analysis to check for multi-collinearity. <br></li>
<li>I also think that normalizing all of your data between 0 to 1 for consistent comparable values between features would be helpful.</li>
</ul>

<p>Hopefully one of these will show some unexpected pattern in your data. I apologize if you've already performed these checks. Just wanted to put them out there.</p>

<p>Looks like you pretty much used every regression model in the Azure ML library. </p>
"
['svm'],Please enlighten me with Platt's SMO algorithm (for SVM),"<p>Your understanding is correct. The point is that equation (8)
$$y_i(&lt;\textbf{w}, \phi_i&gt; + b) - 1 = 0$$
is not exactly an equation, but a system of equations, one for each $i$ index of the support vectors (those for each $0&lt;\alpha_i&lt;C$. </p>

<p>The point is that you cannot compute $b$ during the optimization of the dual problem since it does not matter for optimization, you have to go back and compute $b$ from all the other equations you have (one possible way is (8)). </p>

<p>Vapnick suggestion is to not use only one of those equations, but two of them, specifically one support vector for a negative observation and one for a positive observation. In other words two support vectors which have opposite signs for $y_i$. </p>

<p>Let's name $A$ the index of one support vector and $B$ the index of a suport vector of opposite side, baiscally you select from the system of equations at (8) only two of them. Evaluate both of them and take the mean.</p>

<p>From:
$$y_A(&lt;\textbf{w},\phi_A&gt;+b)=1$$
$$y_B(&lt;\textbf{w}, \phi_B&gt;+b)=1$$
We get:
$$b_A=\frac{1}{y_A}-&lt;\textbf{w},\phi_A&gt;$$
$$b_B=\frac{1}{y_B}-&lt;\textbf{w},\phi_B&gt;$$
Where $b_A$ and $b_B$ are two estimations, then the mean is
$$b = (b_A+b_B)/2 = -\frac{1}{2}(&lt;\textbf{w},\phi_A&gt;+&lt;\textbf{w},\phi_B&gt;)=-\frac{1}{2}\sum_{i=1}^{n}y_i\alpha_i(&lt;\phi(x_i),\phi(x_A)&gt;+&lt;\phi(x_i),\phi(x_B)&gt;)$$</p>
"
"['machine-learning', 'nlp', 'text-mining', 'algorithms']",How to determine if character sequence is English word or noise,"<p>During NLP and text analytics, several varieties of features can be extracted from a document of words to use for predictive modeling. These include the following.</p>

<p><strong>ngrams</strong></p>

<p>Take a random sample of words from <em>words.txt</em>. For each word in sample, extract every possible <a href=""https://en.wikipedia.org/wiki/Bigram"" rel=""nofollow"">bi-gram</a> of letters. For example, the word <strong>strength</strong> consists of these bi-grams: {<strong>st</strong>, <strong>tr</strong>, <strong>re</strong>, <strong>en</strong>, <strong>ng</strong>, <strong>gt</strong>, <strong>th</strong>}. Group by bi-gram and compute the frequency of each bi-gram in your corpus. Now do the same thing for tri-grams, ... all the way up to n-grams. At this point you have a rough idea of the frequency distribution of how Roman letters combine to create English words. </p>

<p><strong>ngram + word boundaries</strong></p>

<p>To do a proper analysis you should probably create tags to indicate n-grams at the start and end of a word, (<strong>dog</strong> -> {<strong>^d</strong>, <strong>do</strong>, <strong>og</strong>, <strong>g^</strong>}) - this would allow you to capture phonological/orthographic constraints that might otherwise be missed (e.g., the sequence <strong>ng</strong> can never occur at the beginning of a native English word, thus the sequence <strong>^ng</strong> is not permissible - one of the reasons why Vietnamese names like <strong>Nguyễn</strong> are hard to pronounce for English speakers). </p>

<p>Call this collection of grams the <em>word_set</em>. If you reverse sort by frequency, your most frequent grams will be at the top of the list -- these will reflect the most common sequences across English words. Below I show some (ugly) code using package <a href=""https://cran.r-project.org/web/packages/ngram/ngram.pdf"" rel=""nofollow"">{ngram}</a> to extract the letter ngrams from words then compute the gram frequencies:</p>

<pre><code>#' Return orthographic n-grams for word
#' @param w character vector of length 1
#' @param n integer type of n-gram
#' @return character vector
#' 
getGrams &lt;- function(w, n = 2) {
  require(ngram)
  (w &lt;- gsub(""(^[A-Za-z])"", ""^\\1"", w))
  (w &lt;- gsub(""([A-Za-z]$)"", ""\\1^"", w))


  # for ngram processing must add spaces between letters
  (ww &lt;- gsub(""([A-Za-z^'])"", ""\\1 \\2"", w))
  w &lt;- gsub(""[ ]$"", """", ww)

  ng &lt;- ngram(w, n = n)
  grams &lt;- get.ngrams(ng)
  out_grams &lt;- sapply(grams, function(gram){return(gsub("" "", """", gram))}) #remove spaces
  return(out_grams)
}

words &lt;- list(""dog"", ""log"", ""bog"", ""frog"")
res &lt;- sapply(words, FUN = getGrams)
grams &lt;- unlist(as.vector(res))
table(grams)

## ^b ^d ^f ^l bo do fr g^ lo og ro 
##  1  1  1  1  1  1  1  4  1  4  1 
</code></pre>

<p>Your program will just take an incoming sequence of characters as input, break it into grams as previously discussed and compare to list of top grams. Obviously you will have to <strong>reduce your top <em>n picks</em> to fit the program size requirement</strong>.</p>

<p><strong>consonants &amp; vowels</strong> </p>

<p>Another possible feature or approach would be to look at consonant vowel sequences. Basically convert all words in consonant vowel strings (e.g., <strong>pancake</strong> -> <strong>CVCCVCV</strong>) and follow the same strategy previously discussed. This program could probably be much smaller but it would suffer from accuracy because it abstracts phones into high-order units.</p>

<p><strong>nchar</strong></p>

<p>Another useful feature will be string length, as the possibility for legitimate English words decreases as the number of characters increases.</p>

<pre><code>library(dplyr)
library(ggplot2)

file_name &lt;- ""words.txt""
df &lt;- read.csv(file_name, header = FALSE, stringsAsFactors = FALSE)
names(df) &lt;- c(""word"")

df$nchar &lt;- sapply(df$word, nchar)
grouped &lt;- dplyr::group_by(df, nchar)
res &lt;- dplyr::summarize(grouped, count = n())
qplot(res$nchar, res$count, geom=""path"", 
      xlab = ""Number of characters"", 
      ylab = ""Frequency"", 
      main = ""Distribution of English word lengths in words.txt"",
      col=I(""red""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/QpJDv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QpJDv.png"" alt=""distribution of words lengths""></a></p>

<p><strong>Error Analysis</strong></p>

<p>The type of errors produced by this type of machine should be <a href=""https://en.wikipedia.org/wiki/Nonsense_word"" rel=""nofollow"">nonsense</a> words - words that look like they should be English words but which aren't (e.g., <strong>ghjrtg</strong> would be correctly rejected (true negative) but <strong>barkle</strong> would incorrectly classified as an English word (false positive)).</p>

<p>Interestingly, <strong>zyzzyvas</strong> would be incorrectly rejected (false negative), because <strong>zyzzyvas</strong> is a real English word (at least according to <a href=""https://raw.githubusercontent.com/hola/challenge_word_classifier/master/words.txt"" rel=""nofollow"">words.txt</a>), but its gram sequences are extremely rare and thus not likely to contribute much discriminatory power.</p>
"
"['clustering', 'algorithms', 'visualization', 'pca', 'distance']",an algorithm for clustering visually separable clusters,"<p>With appropriate parameters, DBSCAN and single linkage hierarchical agglomerative clustering should work very well. Epsilon=0.2 or so.</p>

<p>But why? You know the data, just <strong>use a threshold</strong>.</p>

<p>If you just want an algorithm to ""confirm"" your desired outcome then you are using it wrong. Be honest: if you want your result to be ""if $F-factor-1 > 1.5 then cluster1 else cluster2"", then just say so, instead of attempting to find a clustering algorithm to fit to your desired solution!</p>
"
"['clustering', 'k-means', 'apache-spark', 'distributed', 'scala']",Distributed k-means in Spark,"<p>In that link you posted, you can look at the python full solution <a href=""http://www.cs.berkeley.edu/~rxin/ampcamp-ecnu/machine-learning-with-spark.html#solution_11"" rel=""nofollow"">here</a> at the end and go through it to see what all is distributed. 
In short, some parts are distributed, like reading data from the file, but the very important parts like the distance computation are not.</p>

<p>Running down, we see:</p>

<blockquote>
  <p><code>sc = SparkContext(""local[6]"", ""PythonKMeans"")</code></p>
</blockquote>

<p>This instantiates the context and creates a local cluster which the jobs will be submitted to</p>

<blockquote>
  <p><code>lines = sc.textFile(..)</code></p>
</blockquote>

<p>This is still setting up. No operations have taken place yet. You can verify this by putting timing statements in the code</p>

<blockquote>
  <p><code>data = lines.map(lambda x: (x.split(""#"")[0], parseVector(x.split(""#"")[1])))</code></p>
</blockquote>

<p>The lambda here will be applied to lines, so this operation will split the file in parallel. Note that the actual line also has a <code>cache()</code> at the end (see <a href=""http://spark.apache.org/docs/latest/quick-start.html#caching"" rel=""nofollow"">cache</a>]). <code>data</code> is just a reference to the spark object in memory. (I may be wrong here, but I think the operation still doesn't happen yet)</p>

<blockquote>
  <p>count = data.count()</p>
</blockquote>

<p>This forces the parallel computation to start, and the count to be stored. At the end, the reference data is still valid, and we'll use it for further computations. I'll stop with detailed explanations here, but wherever <code>data</code> is being used is a possible parallel computation. The python code itself is single threaded, and interfaces with the Spark cluster.</p>

<p>An interesting line is:</p>

<blockquote>
  <p>tempDist = sum(np.sum((centroids[x] - y) ** 2) for (x, y) in newCentroids.iteritems())</p>
</blockquote>

<p><code>centroids</code> is an object in python memory, as is <code>newCentroids</code>. So, at this point, all computations are being done in memory (and <strong>on the client</strong>, typically clients are slim, i.e. have limited capabilities, or the client is an SSH shell, so the computers resources are shared. You should ideally <em>never</em> do any computation here), so no parallelization is being used. You could optimize this method further by doing this computation in parallel. Ideally you want the python program to never directly handle individual points' $x$ and $y$ values.</p>
"
"['machine-learning', 'linear-regression']",Why do cost functions use the square error?,"<p>Your loss function here would not work because it incentivizes putting both $\theta_0$ and $\theta_1$ to $-\infty$.</p>

<p>Lets call $r(x,y)=h_\theta(x) -y$ the residual (as is often done). You're asking to make it as small as possible, i.e. $h_\theta$ as negative as possible since $y$, the ground truth, is just a constant. The squared error sidesteps this issue because it forces $h(x)$ and $y$ to match, i.e. $(u-v)^2$ is minimized when $u=v$, if possible, and is always $&gt;0$ otherwise, because it's a square.</p>

<p>One justification for using the squared error is that it relates to Gaussian Noise. Suppose the residual, which measures the error is the sum of many small independent noise terms. By the <a href=""https://en.wikipedia.org/wiki/Central_limit_theorem"" rel=""nofollow"">Central Limit Theorem</a>, it should be distributed Normally. So, we want to pick $\theta$ where this noise distribution - the things your model cannot explain - has the smallest variance. This corresponds to the least squares loss.</p>

<p>Let $R$ be a random variable that follows a Gaussian distribution $\mathcal{N}(\mu,\sigma)$. Then, the variance of $R$ is $E[R^2] = \sigma^2$. See <a href=""http://math.stackexchange.com/questions/518281/how-to-derive-the-mean-and-variance-of-a-gaussian-random-variable"">here</a></p>

<p>Regarding your second question, the 1/2 does not matter and actually, the $m$ doesn't matter either :) . The optimal value of $\theta$ would remain the same in both cases, it is put in so that when you take the derivative, the expression is prettier, because the 2 cancels out the 2 from the square term. The $m$ is useful if you solve this problem with gradient descent. Then your gradient is the sum of $m$ terms <em>divided</em> by $m$, so it is like an average over your points. This lets you handle all sizes of datasets, so your gradients won't overflow the integers if you scale up the dataset. </p>

<p>It's also used to maintain consistency with future equations where you'll add <a href=""http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf"" rel=""nofollow"">regularization</a> terms. The slides I linked here don't do it, but you can see that if you do, the regularization parameter $\lambda$ will not depend on the dataset size $m$ so it'll be more interpretable.</p>
"
"['bigdata', 'data-mining', 'dimensionality-reduction']",How to do SVD and PCA with big data?,"<p>First of all, <strong>dimensionality reduction</strong> is used when you have <strong>many covariated dimensions</strong> and want to reduce problem size by rotating data points into new orthogonal basis and taking only axes with largest variance. With 8 variables (columns) your space is already low-dimensional, reducing number of variables further is unlikely to solve technical issues with memory size, but may affect dataset quality a lot. In you concrete case it's more promising to take a look at <a href=""http://en.wikipedia.org/wiki/Online_machine_learning""><strong>online learning</strong></a> methods. Roughly speaking, instead of working with the whole dataset, these methods take a little part of them (often referred to as ""mini-batches"") at a time and build a model incrementally. (I personally like to interpret word ""online"" as a reference to some infinitely long source of data from Internet like Twitter feed, where you just can't load the whole dataset at once). </p>

<p>But what if you really wanted to apply dimensionality reduction technique like PCA to a dataset that doesn't fit into a memory? Normally dataset is represented as a data matrix <em>X</em> of size <em>n</em> x <em>m</em>, where <em>n</em> is number of observations (rows) and <em>m</em> is a number of variables (columns). Typically problem with memory comes from only one of these two numbers. </p>

<h2>Too many observations (n >> m)</h2>

<p>When you have <strong>too many observations</strong>, but number of variables is from small to moderate, you can <strong>build covariance matrix incrementally</strong>. Indeed, typical PCA consists of constructing covariance matrix of size <em>m</em> x <em>m</em> and applying singular value decomposition to it. With <em>m</em>=1000 variables of type float64 covariance matrix has size 1000*1000*8 ~ 8Mb, which easily fits into memory and may be used with SVD. So you need only to build covariance matrix without loading entire dataset into memory - <a href=""http://rebcabin.github.io/blog/2013/01/22/covariance-matrices/"">pretty tractable task</a>. </p>

<p>Alternatively, you can select small representative sample from your dataset and <strong>approximate covariance matrix</strong>. This matrix will have all the same properties as normal, just a little bit less accurate. </p>

<h2>Too many variables (n &lt;&lt; m)</h2>

<p>On another hand, sometimes, when you have <strong>too many variables</strong>, covariance matrix itself will not fit into memory. E.g. if you work with 640x480 images, every observation has 640*480=307200 variables, which results in 703Gb covariance matrix! That's definitely not what you would like to keep in memory of your computer. Or even in memory of your cluster. So we need to reduce dimensions without building covariance matrix at all. </p>

<p>My favourite method for doing it is <a href=""http://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf""><strong>Random Projection</strong></a>. In short, if you have dataset <em>X</em> of size <em>n</em> x <em>m</em>, you can multiply it by some sparse random matrix <em>R</em> of size <em>m</em> x <em>k</em> (with <em>k</em> &lt;&lt; <em>m</em>) and obtain new matrix <em>X'</em> of a much smaller size <em>n</em> x <em>k</em> with <em>approximately same properties</em> as original one. Why it works? Well, you should know that PCA aims to find set of orthogonal axes (principal components) and project your data onto first <em>k</em> of them. It turns out, that sparse random vectors are <em>nearly orthogonal</em> and thus may also be used as a new basis. </p>

<p>And, of course, you don't have to multiply the whole dataset <em>X</em> by <em>R</em> - you can translate every observation <em>x</em> into new basis separately or in mini-batches.</p>

<p>There's also somewhat similar algorithm called <strong>Random SVD</strong>. I don't have any real experience with it, but you can find example code with explanations <a href=""http://stats.stackexchange.com/a/11934/3305"">here</a>.</p>

<hr>

<p>As a bottom line, here's short check list for dimensionality reduction of big datasets: </p>

<ol>
<li>If you have not that many dimensions (variables), simply use online learning algorithms. </li>
<li>If there are many observations, but moderate number of variables (covariance matrix fits into memory), construct matrix incrementally and use normal SVD. </li>
<li>If number of variables is too high, use incremental algorithms. </li>
</ol>
"
"['dataset', 'recommendation', 'graphs']",Trouble representing a problem,"<p>I would definitely use a graph (Though, this clearly depends on the final application, maybe you could add more information) </p>

<p>For the nodes, you should consider as nodes not only tanks but also points were pipelines change name or bifurcate. For instance, following your example:</p>

<pre><code>    e1     e2        e3          e4        e5          e7           
    +---+----------.---------+----------.----------+-----------      
    |          |         |                     |          |      
[R tank 1]  [S tank 1] [S tank 2]              |e6     [S tank 3]
                                               |                 
                                           [S tank 4]            
</code></pre>

<p>Now adding the nodes:  </p>

<pre><code>    e1 n2  e2     n4   e3   n6   e4    n8  e5     n9   e7           
n1 +---+----------.---------+----------.----------+----------+      
   |          |         |                     |              |      
  n3         n5        n7                     |e6           n11     
                                              |                 
                                             n10                
</code></pre>

<p>Lastly, you need some kind of mapping. 
Some of the nodes will map to tanks:</p>

<pre><code>[R tank 1]     n3                                                   
[S tank 1]     n5                                                   
[S tank 2]     n7                                                   
[S tank 4]     n10                                                  
[S tank 3]     n11                                                  
</code></pre>

<p>And the pipelines will be represented by paths in the graph</p>

<pre><code>Pipeline 1     e1 e2                                                
Pipeline 2     e3 e4                                                
Pipeline 3     e5 e7                                                
Pipeline 4     e6                                                   
</code></pre>
"
"['machine-learning', 'classification', 'python', 'sklearn']",Passing TFIDF Feature Vector to a SGDClassifier from sklearn,"<p>It's useful to do this with a <code>Pipeline</code>:</p>

<pre><code>import numpy as np
from sklearn import linear_model, pipeline, feature_extraction

X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
Y = np.array(['C++', 'C#', 'java','python'])
clf = pipeline.make_pipeline(
        feature_extraction.text.TfidfTransformer(use_idf=True),
        linear_model.SGDClassifier())
clf.fit(X, Y)
print(clf.predict([[1.7, 0.7]]))
</code></pre>
"
"['python', 'svm', 'scikit']","Reduce dimension, then apply SVM","<p>I'd recommend spending more time thinking about feature selection and representation for your SVM than worrying about the number of dimensions in your model. Generally speaking, SVM tends to be very robust to uninformative features (e.g., see <a href=""https://eldorado.tu-dortmund.de/bitstream/2003/2595/1/report23_ps.pdf"" rel=""nofollow"">Joachims, 1997</a>, or <a href=""https://eldorado.tu-dortmund.de/bitstream/2003/2596/1/report24.pdf"" rel=""nofollow"">Joachims, 1999</a> for a nice overview). In my experience, SVM doesn't often benefit as much from spending time on feature selection as do other algorithms, such as Naïve Bayes. The best gains I've seen with SVM tend to come from trying to encode your own expert knowledge about the classification domain in a way that is computationally accessible. Say for example that you're classifying publications on whether they contain information on protein-protein interactions. Something that is lost in the bag of words and tfidf vectorization approaches is the concept of proximity—two protein-related words occurring close to each other in a document are more likely to be found in documents dealing with protein-protein interaction. This can sometimes be achieved using $n$-gram modeling, but there are better alternatives that you'll only be able to use if you think about the characteristics of the types of documents you're trying to identify. </p>

<p>If you still want to try doing feature selection, <a href=""http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html"" rel=""nofollow"">I'd recommend $\chi^{2}$</a> (chi-squared) feature selection. To do this, you rank your features with respect to the objective</p>

<p>\begin{equation}
\chi^{2}(\textbf{D},t,c) = \sum_{e_{t}\in{0,1}}\sum_{e_{c}\in{0,1}}\frac{(N_{e_{t}e_{c}}-E_{e_{t}e_{c}})^{2}}{E_{e_{t}}e_{c}},
\end{equation}
where $N$ is the observed frequency of a term in $\textbf{D}$, $E$ is its expected frequency, and $t$ and $c$ denote term and class, respectively. You can <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html"" rel=""nofollow"">easily compute this in sklearn</a>, unless you want the educational experience of coding it yourself $\ddot\smile$</p>
"
"['machine-learning', 'python', 'feature-engineering', 'feature-scaling', 'normalization']",Ways to deal with longitude/latitude feature,"<p>Lat long coordinates have a problem that they are 2 features that represent a three dimensional space. This means that the long coordinate goes all around, which means the two most extreme values are actually very close together. I've dealt with this problem a few times and what I do in this case is map them to x, y and z coordinates. This means close points in these 3 dimensions are also close in reality. Depending on the use case you can disregard the changes in height and map them to a perfect sphere. These features can then be standardized properly.</p>
"
"['text-mining', 'preprocessing']",Problem in constructing co-occurence matrix,"<p>Whether to window only over sentences or over the whole corpus depends on the context of your problem and on the structure of your corpus. In this corpus the order of your sentences does not matter, if they were switched around it would still be the same corpus. This means that you should stick to only the lines and not see it as one line. If your corpus would be a book for example, spilling over sentences makes a lot more sense. If you are unsure, try both, it is just a hyperparameter of your model.</p>
"
"['machine-learning', 'python', 'svm', 'supervised-learning', 'discriminant-analysis']",How are Hyperplane Heatmaps created and how should they be interpreted?,"<p>I think I can answer that, since I implement such a thing in my own library, even if I really don't know how it's implemented in other libraries. Although I am confident that if there are other ways, they don't differ too much. </p>

<p>It took my a few weeks to understand how such a graph can be drawn. </p>

<p>Let's start with a general function $f:\mathbb{R} \times \mathbb{R} \to \mathbb{R}$. What you want is to draw points with a colour which signifies the value of the function. One way would be to simplify the whole problem and draw one point for each pixel. This would work, but will draw only shaded surfaces and it's impossible to draw lines with various formats (dotted lines with some colours and line widths. </p>

<p>The real solution which I found makes two simplifications. The first one would the that instead of colouring with a gradient you can colour bands. Suppose that your function $f$ takes values in $[-1, 1]$. You can split your co-domain into many subintervals like: $[-1, -0.9]$, $[-0.9, -0.8]$ and so on. Now what you have to do would be to paint a polygon filled with appropriate color, for each interval. So your original problem is simplified to draw multiple instances of a simpler problem. Note that when your intervals are small enough it will look like a gradient and even a trained eye would not notice.</p>

<p>The second simplification would be to split the space which needs to be drawn into a grid of small rectangles. So instead of drawing a polygon on the whole surface, you need to fill each small rectangle with appropriate much simpler polygon. </p>

<p>If it's not obvious, the problem is much much simplified. Take for example a one of the rectangle. This rectangle has four corners and you can take an additional point in the center of that rectangle (you might need that point in some certain situations). </p>

<p>The question is how to fill with appropriate colour the proper region? 
You need to evaluate function in all four corners and in the center. </p>

<p>There are some specific cases:</p>

<ul>
<li>function evaluated in all corners are smaller than the beginning of the interval => you need to do nothing </li>
<li>functions evaluated in all corners are greater than the end of the interval => you need to do nothing</li>
<li>functions evaluated in all corners are within interval => fill the whole rectangle with an appropriate color</li>
</ul>

<p>You can stop here if you want, but your figures would looks non-smooth. You can go further:</p>

<ul>
<li>left-up, left-down, right-down in interval, right-up bigger => there are two points, one on up side and one on right side which contains the function evaluated at max value from interval => those two points together with top-right corner forms a triangle which can be filled</li>
<li>many other cases which requires only common judgement to decide which polygons to be formed and should be filled.</li>
</ul>

<p>Using this algorithm you can fill polygons or draw lines. In the specific case of SVM you need to know that the line which corresponds with $f(x,y)=0$ is the line which classifies points into positive and negative samples. Also, lines which evaluates the function at $-1$ or $1$ corresponds with the maximal margins of SVM.</p>

<p>After some time I found that this kind of approach is named iso lines, or iso curves. Perhaps are more similar algorithms like that.</p>

<p>My implementation is named mesh contour (I did not found a proper name at that time in the beginning) and you can find source <a href=""https://github.com/padreati/rapaio/blob/master/src/main/java/rapaio/graphics/plot/plotcomp/MeshContour.java"">here</a>.</p>

<p>Some examples:</p>

<p><a href=""http://i.stack.imgur.com/oE7rK.png""><img src=""http://i.stack.imgur.com/oE7rK.png"" alt=""Mesh curves for grained colour step""></a></p>

<p><a href=""http://i.stack.imgur.com/WQsvq.png""><img src=""http://i.stack.imgur.com/WQsvq.png"" alt=""Mesh curves for fine colour steps""></a></p>

<p><a href=""http://i.stack.imgur.com/lhlhf.png""><img src=""http://i.stack.imgur.com/lhlhf.png"" alt=""SVM on synthetic data""></a></p>
"
"['machine-learning', 'weighted-data']",Machine learning technique to calculate weighted average weights?,"<p>Yes it is definitely possible to calculate optimised weightings provided you have some training examples where you know the document fields, the query, and either the outcome (relevant/not-relevant) or the desired score. </p>

<p>I think your training feature set should be the query score in range [0.0,1.0] for each field of each example. The training label should be either relevance 0 or 1 for each example, or the relevance score that the example has.</p>

<p><strong>If you have a target score for each example</strong></p>

<p>You want to determine the weights $W_i$ to use for each field $i$. Your calculated relevance score would be $\hat{y} = \sum_{i=1}^{N_{fields}} W_i * X_i$ where the caret signifies this is the estimate from your function and $N_{fields}$ is the number of field. Note I am ignoring your original idea of dividing by the sum of all $W_i$, because it makes things more complex. You can either add that term or force the sum to be equal to 1.0 if you wish (I am not going to show you how though, as this answer would get too long, and it probably won't help you much)</p>

<p>With a target score and training data, the simplest approach is to find the weights which cause the lowest error when used with the training data. This is a very common goal in supervised learning. You need a <a href=""https://en.wikipedia.org/wiki/Loss_function"" rel=""nofollow"">loss function</a>. Having a target scalar value means you can use difference from target and a very common loss function for this kind of regression problem is the mean squared error:</p>

<p>$$E = \frac{1}{N_{examples}} \sum_{j=1}^{N_{examples}} (\hat{y}_j - y_j)^2$$</p>

<p>Where $\hat{y}_j$ is your calculated relevance score for example $j$ and $y_j$ is your training label for the same example.</p>

<p>There are a few different ways to solve for lowest $E$ with this loss function, and it is one of the simplest to solve. If you express your weights as a vector $W$ length $N_{fields}$ your example features as a matrix $X$ size $N_{examples} \times N_{fields}$ and the labels as a vector $Y$ length $N_{examples}$ then you can get an exact solution to minimise loss using the <a href=""https://en.wikipedia.org/wiki/Least_squares#Linear_least_squares"" rel=""nofollow"">linear least squares equation</a></p>

<p>$$W = (X^TX)^{-1}X^TY$$</p>

<p>There are other approaches that work too - gradient descent or other function optimisers. You can look these up and see which you would prefer to use for your problem. Most programming languages will have a library with this already implemented.</p>

<p>Note that you will likely get scores greater than 1.0 or less than 0.0 from some document/query pairs.</p>

<p>You will have to use a adjust the technique if you want to divide by total of all weights or want sum of all weights equal to 1 in your scoring system.</p>

<p><strong>If you have a relevance 0 or 1 for each example</strong></p>

<p>You have a classification problem, relevant or not are your two classes. This can still be made to work, but you will want to change how you calculate your weighted score and use <a href=""https://en.wikipedia.org/wiki/Logistic_regression"" rel=""nofollow"">logistic regression</a>. </p>

<p>Your weighted score under logistic regression would be:</p>

<p>$$\hat{y} = \frac{1}{1 + e^{-(b + \sum_{i=1}^{N_{fields}} W_i * X_i)}}$$ </p>

<p>Where $b$ is a <em>bias</em> term. This looks complicated, but really it is just the same as before but mapped by a sigmoid function to better represent class probabilities - the result is always between 0 and 1.</p>

<p>You can look up solvers for logistic regression, and most stats or ML libraries will have functions ready to use.</p>

<p><strong>Caveats</strong></p>

<p>You have made a starting assumption that a simple combined relevance score will lead to a useful end result for your users performing search. This has led to simple linear models looking like a good solution. However, this may not be the case in practice, and you may need to re-visit that assumption. </p>
"
"['machine-learning', 'r', 'logistic-regression']",Logistic regression on biased data,"<p><strong>Background</strong></p>

<p>I'll start with some background to help you research the solution yourself and then will add some specifics.  What you refer to as ""biased data"" is more commonly known as <code>unbalanced classes</code> in the data science world.  Also ""customer turnover"" is often referred to as <code>churn</code>.</p>

<p><strong>Metrics</strong></p>

<p>As hoards of <a href=""https://class.coursera.org/ml-003/lecture"" rel=""nofollow"">Ng'ian devotees</a> will undoubtably point out you need to start by designing a set of metrics that work better with <code>unbalanced classes</code> than <code>accuracy</code>. Accuracy does a poor job in testing the quality of predictions for unbalanced classes e.g. a cancer test for a cancer that occurs in 0.05 % of the population is 99.95% accurate if it always predicts ""no cancer"". I suggest using the <a href=""http://datascience.stackexchange.com/questions/6817/clasification-roc-curve-with-very-high-number-of-false-negatives/6822#6822""><code>F1-score</code> as the key metric in cross-validating your model</a>.  The <code>F1-score</code> is the harmonic mean of <code>precision</code> and <code>recall</code> and tends to work both for balanced and unbalanced classes.  There are other rations of harmonic mean, that could work in special cases, so <a href=""https://en.wikipedia.org/wiki/F1_score"" rel=""nofollow"">be aware of these</a>.</p>

<p><a href=""http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf"" rel=""nofollow"">There are other metrics you should learn about also</a>. ROC-AUC is likely at the top of the list for other metrics you should understand and know about.</p>

<p><strong>Model Selection and Cross-Validation</strong></p>

<p>Beginning a classification task with <code>Logistic Regression</code> is a fantastic strategy. I make a point to always use a linear regression for regression tasks and a logistic regression for classification tasks. The linear model provides significant insight into the <code>feature importance</code> and helps frame the problem.</p>

<p>But following this initial survey you should move on to other, more sophisticated models. Many will give you a litany of things to try. You should perhaps focus on one or two and develop the model while paying very careful attention to <code>bias</code> and <code>variance</code> as you <code>cross-validate</code> and test your model. A <a href=""http://scikit-learn.org/stable/modules/learning_curve.html"" rel=""nofollow"">full <code>bias-variance decomposition</code></a> may be unnecessary, once you develop better intuition, but is a great place for newbs to start.</p>

<p>I suggest starting with an <code>SVM</code> and also eventually trying a <code>random forest</code> or <code>naive Bayes</code> model as this will traverse several regimes of model types (analogy, decision trees, bagging, Bayesian).</p>

<p><strong>Finally... Unbalanced Classes</strong></p>

<p>There are two typical methods for dealing with unbalanced classes. These include <a href=""http://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation"" rel=""nofollow"">oversampling the minority class</a>, and fixing the model by <a href=""http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html"" rel=""nofollow"">altering the hyperplane (SVM)</a> or <a href=""http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf"" rel=""nofollow"">changing priors (Bayes)</a>.</p>

<p>There are <a href=""http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"" rel=""nofollow"">lots</a> of <a href=""http://stats.stackexchange.com/questions/131255/class-imbalance-in-supervised-machine-learning"">summaries</a> of this <a href=""http://pages.stern.nyu.edu/~fprovost/Papers/skew.PDF"" rel=""nofollow"">problem and solution</a> if you search for ""<code>unbalanced classes</code>"". <strong>But</strong>, it can still be a tricky problem despite all the literature.  Good luck...</p>

<p>Hope this helps!</p>
"
"['r', 'clustering']",Feeding R agnes object into cutree,"<p>I found an answer in a related post <a href=""http://stackoverflow.com/questions/9457473/strange-error-of-hierarchical-clustering-in-r"">here</a>. Another user was getting a similar error about ordering <code>height</code>, but theirs came with the suggestion to apply <code>as.hclust()</code> first. I converted the <code>agnes</code> object to an <code>hclust</code> and passed that directly into <code>cutree</code>. That seemed to solve the problem. </p>
"
['r'],Create multiple matrices from 2 bigger ones in R,"<p>This will extract the rows from matrix <strong>A</strong> and <strong>B</strong>. </p>

<pre><code>matrix(c(A[x,],rep(B[x,],times=3)),nrow=2,byrow=T)
</code></pre>

<p>If you want to get them into a list (recommend)</p>

<pre><code>single&lt;-lapply(c(1:1000), function(x) matrix(c(A[x,],rep(B[x,],times=3)),nrow=2,byrow=T))
</code></pre>

<p>To put them all in diagonal matrix with <em>adiag</em> from <strong>magic</strong> package</p>

<pre><code>d&lt;-adiag(single[[1]])

for(i in 2:1000){
  d&lt;-adiag(d,single[[i]])
}
</code></pre>

<p>I could make it work without the loop (anyone any suggestions?)</p>
"
"['logistic-regression', 'regularization']",Good explanation for why regularisation works,"<p>The ""same angle hyperplane"" does not have the same cost. It is the same decision boundary as you describe it, but perpendicular distances to it are larger wrt the norm of the weights. In effect with higher weights in the same ratio (i.e. without any regularisation effect), the classifier will be more confident in all of its decisions. That means the classifier will be more sensitive to getting as many observations as possible in the training set on the ""right"" side of the boundary. In turn this makes it sensitive to noise in the observations.</p>

<p>Your estimated probability for being in the positive class is:</p>

<p>$$p(y=1|X) = \frac{1}{1+e^{-W^TX}}$$</p>

<p>This includes $w_0$ and fixed value 1 for $x_0$. If you take the midpoint, decision line where $W^TX$ is zero (and the output is at the threshold value 0.5), that defines your decision hyperplane in $X$ space.</p>

<p>When $W$ has the same factors but a larger norm with $w_0$ compensating to make the same hyperplane, then $X$ values that were on the decision hyperplane still give threshold value of 0.5. However, $X$ values away from the hyperplane will deviate more strongly. If instead of 0 you had $W^TX=1.0$ and doubled the weights keeping the same hyperplane, you would get $W^TX=2.0$ for that example. Which changes your confidence from 0.73 to 0.88.</p>

<p>The usual cost function without regularisation for logistic regression with example vectors $X_j$ and targets $y_j$ is:</p>

<p>$$J = - \sum_{\forall j} y_jlog(\frac{1}{1+e^{-W^TX_j}}) + (1 -y_j)(1  - log(\frac{1}{1+e^{-W^TX_j}}))$$</p>

<p>The cost is more sensitive to distances from the hyperplane for larger weight values. Looking your example for the imaginary item (with 0.73 or 0.88 confidence), when the categorisation is correct (i.e. y=1) the score would improve by 0.19 for that example if the weights doubled. When the categorisation is wrong (y=0) then the score would worsen by 0.81. In other words for <em>higher</em> weights, with same weight ratio, the same miscategorisations are punished more than correct categorisations are rewarded.</p>

<p>When training, weights will converge to specific balanced weight vector for the minimum cost, <strong>not</strong> to a specific ratio that forms a ""best decision hyperplane"". That's because the hyperplane does not correspond to a single value of the cost function. </p>

<p>You can demonstrate this effect. Train a logistic regression classifier - without any regularisation to show it has nothing to do with that. Take the weight vector and multiply by some factor e.g. 0.5.Then re-train starting with those weights. You will end up with the same weights as before. The cost function minimum clearly defines <em>specific</em> weight values, not a ratio.</p>

<p>When you add regularisation, that changes the cost and how the weights will converge. Higher regularisation in effect makes the classifier prefer a boundary with lower confidence in all its predictions, it penalises ""near misses"" less badly because the weights are forced down where possible. When viewed as a hyperplane, the boundary will likely be different.</p>
"
"['beginner', 'tools', 'career', 'reference-request']",Why do internet companies prefer Java/Python for data scientist job?,"<p>So you can integrate with the rest of the code base. It seems your company uses a mix of Java and python. What are you going to do if a little corner of the site needs machine learning; pass the data around with a database, or a cache, drop to R, and so on? Why not just do it all in the same language? It's faster, cleaner, and easier to maintain.</p>

<p>Know any online companies that run solely on R? Neither do I...</p>

<p>All that said Java is the last language I'd do data science in.</p>
"
"['machine-learning', 'neuralnetwork']",Which type of machine learning to use,"<p>With using R, You could look at trees / randomforests. Since you have correlated variables, you could look into Projection pursuit classification trees (R package pptree). And there soon will be a ppforest package. But this is still under  development.  You could also combine randomforest with the package forestFloor to see the curvature of the randomforest and work from there.</p>
"
"['open-source', 'dataset']",Publicly Available Datasets,"<p>There is, in fact, a very reasonable list of publicly-available datasets, supported by different enterprises/sources. Here are some of them:</p>

<ul>
<li><a href=""http://aws.amazon.com/publicdatasets/"">Public Datasets on Amazon WebServices</a>;</li>
<li><a href=""http://fimi.ua.ac.be/data/"">Frequent Itemset Mining Implementation Repository</a>;</li>
<li><a href=""http://archive.ics.uci.edu/ml/datasets.html"">UCI Machine Learning Repository</a>;</li>
<li><a href=""http://www.kdnuggets.com/datasets/index.html"">KDnuggets</a> -- big list of lots of public repositories.</li>
</ul>

<p>Now, two considerations on your question. First one, regarding policies of database sharing. From personal experience, there are some databases that can't be made publicly available, either for involving privacy restraints (as for some social network informations), or for concerning government information (like health system databases).</p>

<p>Another point concerns the usage/application of the dataset. Although some bases can be reprocessed to suit the needs of the application, it would be great to have some <em>nice organization</em> of the datasets by purpose. The <em>taxonomy</em> should involve social graph analysis, itemset mining, classification, and lots of other reasearch areas there may be.</p>
"
"['bigdata', 'machine-learning', 'databases', 'statistics', 'education']",Is Data Science just a trend or is a long term concept?,"<p>The one thing that you can say for sure is: Nobody can say this for sure. And it might indeed be opinion-based to some extent. The introduction of terms like ""Big Data"" that some people consider as ""hypes"" or ""buzzwords"" don't make it easier to flesh out an appropriate answer here. But I'll try. </p>

<p>In general, interdisciplinary fields often seem to have the problem of not being taken serious by either of the fields they are spanning. However, the more research is invested into a particular field, the greater is the urge to split this field into several sub-topics. And these sub-topics sonner of later have to be re-combined in new ways, in order to prevent an overspecialization, and to increase and broaden the applicability of techniques that are developed by the (over?)specialized experts in the different fields. </p>

<p>And I consider ""Data Science"" as such an approach to combine the expertise and findings from different fields. You described it as</p>

<blockquote>
  <p>...a mix of computer science and statistics techniques</p>
</blockquote>

<p>And indeed, several questions here aim at the differentiation between data science and statistics. But a pure statistician will most likely not be able to set up a Hadoop cluster and show the results of his analysis in an interactive HTML5 dashboard. And someone who can implement a nice HTML5 dashboard might not be so familiar with the mathematical background of a Chi-Squared-Test. </p>

<p>It is reasonable to assume that giving students enough knowledge to <em>apply</em> the most important techniques from the different fields that are covered by data science will lead to new applications of these techniques, and be beneficial - also for the ""purists"" in these fields. The combination of these techniques is not straightforward in many cases, and can justify an own branch of research. </p>

<p>You also asked whether in 10 years, data science will be considered as ""just a topic inside computer science"". Again: Nobody can say for sure. But I wonder at which point people stopped asking the question whether ""Computer Science"" will one day only be considered only as a mix of (or a subject of) Electrical Engineering and Mathematics...</p>
"
['machine-learning'],Is supervised machine learning by definition predictive?,"<p>A description as any statistic drawn from your sample data, say the sample mean, quantiles, etc.. Inference is a conclusion drawn from your sample data about the population, e.g., rejecting or accepting some hypothesis or stating that a model is suitable or not for describing your data. Prediction is simply a guess about future observations, which hopefully uses your data and some function/model of the data in a way to formulate that guess.</p>

<p>Both unsupervised and supervised learning methods aim to learn a function of the data that predicts another variable (typically called y) so both are drawing an inference (i.e., a model is well suited to describe your data, see the first sentence <a href=""https://en.wikipedia.org/wiki/Supervised_learning"" rel=""nofollow"">here</a>). However, these two methods differ in what data is available. In supervised learning you are able to use an observed sample of y for training your model and in unsupervised learning, y is unobserved.</p>

<p>Hope that helps!</p>
"
"['machine-learning', 'online-learning']","Is there a difference between on-line learning, incremental learning and sequential learning?","<p>So, the following answer is just based on different opinions of collegues and professors from the field. I want to try to summarize it briefly:</p>

<p>Sequential and online learning is mostly associated with Bayesian updating. The sequential learning is used widely for an order in time of the data, meaning that $x_1$ is coming always first, then $x_2$, then $x_3$ and so on. The Dataset each has a certain order in that sense. 
In contrast to that incremental may be a whole block of data at time x and another block of data at time y. While the block internally may be randomly ordered. </p>

<p>Concerning online learning, the people mostly referred to a data stream, hence a online learning is always incremental learning but incremental learning does not have to be online. </p>

<p>These are quite fuzzy definitions, and in my opinion there is not clear definition though. I still hope that helps.</p>
"
"['text-mining', 'definitions', 'terminology']","What is an alternative name for ""Unstructured Data""?","<p>It is a bad idea to counterpose ""unstructure data"" to, say, tabular data (as in ""non-tabular data""), as you will have to elliminate other alternatives as well (e.g., ""non-tabular and non-graph and ... data""). ""Plain text"" (-- my choice) or ""raw text"" or ""raw data"" sound fine.</p>
"
"['machine-learning', 'python']",Bigdata cluster compatible distributed predictive model,"<p>Spark does have a pretty good Python API, check out this <a href=""https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python"" rel=""nofollow"">tutorial</a>.</p>

<p>For traditional Hadoop stack, take a look at <a href=""https://github.com/Yelp/mrjob"" rel=""nofollow"">mrjob</a>, it lets you write MapReduce jobs in Python and run them on several platforms.</p>
"
"['machine-learning', 'data-mining', 'python']",What is the best way to propose an item from a set based on previous choices?,"<p>There are many great ways to handle this problem. It is a recommendation problem, not a classification problem, as pointed out by others. There are many ways to do recommendation with a data set like this. I'll point out a few methods and you can choose one or try them all.</p>

<p>The first method is called user-based collaborative filtering. The basic idea is to give users recommendations based on the tastes of like-minded users. So, you'd be trying to recommend music based on the listening history of users who have listened to the same songs. Such data can be modeled as a graph or sparse matrix. Then, you choose the exact algorithm depending on how you want to model your data. </p>

<p>The second method is called item-based collaborative filtering. Rather than associating users together, this strategy looks at the set of items a user has 'rated' (the songs a user has listened to) and calculates how similar they are to a specific target item (song), or even to all the songs in your data set. It grabs the set of most-similar items and uses various methods to predict how much a user will like the song. </p>

<p>In this case, you only have binary data (user listened to it or they did not). These calculations tend to work best with actual rating scores (like a 5 star system) because this gives more detailed variation amongst items in the data set. </p>

<p>The third option is to model your data in a graph database like Neo4J and write graph traversal queries in order to find similar items. If you like graph theory, this can be a lot of fun. The sky is the limit in regards to what kinds of traversals will return good results. To get started, think of the users and songs as nodes in the graph, and 'listened' as the edge. $user-&gt;listened-&gt;$song  </p>

<p>Because of ratings and item-based filtering, and because there are probably many songs in your data set, and each user only listens to a very small portion of them, I'd first try a user-based collaborative filtering method which uses sparse matrix operations to calculate recommendations. If your data set is large, these computations scale horizontally so you can leverage parallel processing if you run into performance issues.</p>

<p>You can find more detail about collaborative filtering in this paper: <a href=""http://files.grouplens.org/papers/www10_sarwar.pdf"" rel=""nofollow"">http://files.grouplens.org/papers/www10_sarwar.pdf</a></p>
"
"['r', 'predictive-modeling']",Predicting future value with regression Model,"<p>You should distinct between a <em>time series</em> prediction, where from a known history of some attribute the future is predicted and <em>model prediction</em> where based on the predictor variables the target variable is calculated.</p>

<p>In your case you could combine both approaches, i.e. use time series prediction on the customer balances and apply the regression model to calculate the profit on the result.</p>
"
"['machine-learning', 'data-mining', 'dataset', 'databases']",Data repositories like UCI,"<p>There're lots and tons of data sets for biological data.</p>

<ul>
<li><a href=""http://www.ncbi.nlm.nih.gov/genbank/"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/genbank/</a></li>
<li><a href=""http://www.1000genomes.org/"" rel=""nofollow"">http://www.1000genomes.org/</a></li>
<li><a href=""http://www.ddbj.nig.ac.jp/"" rel=""nofollow"">http://www.ddbj.nig.ac.jp/</a></li>
<li><a href=""https://en.wikipedia.org/wiki/List_of_biological_databases"" rel=""nofollow"">https://en.wikipedia.org/wiki/List_of_biological_databases</a></li>
</ul>
"
"['data-mining', 'competitions', 'overfitting']",numer.ai: how does their leaderboard system work?,"<p>I found the answer in the <a href=""https://blog.numer.ai/2016/06/16/New-Rules-and-Round-6-Winners"" rel=""nofollow"">comments section</a> of their blog:</p>

<blockquote>
  <p>Earnings listed on the public-score leaderboard are potential winnings. Actual winnings are determined by the private-score leaderboard. At the time of withdrawal and when new datasets are released, your actual winnings will be revealed to you.</p>
</blockquote>
"
"['machine-learning', 'r', 'similarity', 'correlation']",How to find similarity between different factors in a dataset,"<p>One way is to normalize your quantitative  values (play, eat, drink, sleep rates) so they all have the same range (say, 0 -> 1), then assign each game to its own ""dimension"", that takes value 0 or 1. Turn each row into a vector and normalize the length to 1. Now, you can compare the inner product of any two people's normalized vectors as a measure of similarity. Something like this is used in text mining quite often</p>

<hr>

<p>R Code for Similarity Matrix</p>

<p><em>Assumes you've saved your dataframe to the variable ""D""</em></p>

<pre><code>#Get normalization factors for quantitative measures
maxvect&lt;-apply(D[,1:4],MARGIN=2,FUN=max)
minvect&lt;-apply(D[,1:4],MARGIN=2,FUN=min)
rangevect&lt;-maxvect-minvect
#Normalize quantative factors
D_matrix &lt;- as.matrix(D[,1:4])
NormDMatrix&lt;-matrix(nrow=10,ncol=4)
colnames(NormDMatrix)&lt;-colnames(D_matrix)
for (i in 1:4) NormDMatrix[,i]&lt;-(D_matrix[,i]-minvect[i]*rep(1,10))/rangevect[i]
gamenames&lt;-unique(D[,""game""])
#Create dimension matrix for games
Ngames&lt;-length(gamenames)
GameMatrix&lt;-matrix(nrow=10,ncol=Ngames)
for (i in 1:Ngames) GameMatrix[,i]&lt;-as.numeric(D[,""game""]==gamenames[i])
colnames(GameMatrix)&lt;-gamenames
#combine game matrix with normalized quantative matrix
People&lt;-D[,""name""]
RowVectors&lt;-cbind(GameMatrix,NormDMatrix)
#normalize each row vector to length of 1 and then store as a data frame with person names
NormRowVectors&lt;-t(apply(RowVectors,MARGIN=1,FUN=function(x) x/sqrt(sum(x*x))))
dfNorm&lt;-data.frame(People,NormRowVectors)

#create person vectors via addition of appropriate row vectors
PersonMatrix&lt;-array(dim=c(length(unique(People)),ncol(RowVectors)))
rownames(PersonMatrix)&lt;-unique(People)
for (p in unique(People)){
  print(p)
  MatchIndex&lt;-(dfNorm[,1]==p)*seq(1,nrow(NormRowVectors))
  MatchIndex&lt;-MatchIndex[MatchIndex&gt;0]
  nclm&lt;-length(MatchIndex)
  SubMatrix&lt;-matrix(NormRowVectors[MatchIndex,],nrow=length(MatchIndex),ncol=dim(NormRowVectors)[2])
  CSUMS&lt;-colSums(SubMatrix)
  NormSum&lt;-sqrt(sum(CSUMS*CSUMS))
  PersonMatrix[p,]&lt;-CSUMS/NormSum
}
colnames(PersonMatrix)&lt;-colnames(NormRowVectors)
#Calculate matrix of dot products
Similarity&lt;-(PersonMatrix)%*%t(PersonMatrix)
</code></pre>
"
"['machine-learning', 'neuralnetwork', 'deep-learning']",Bagging vs Dropout in Deep Neural Networks,"<p>Bagging and dropout do not achieve quite the same thing, though both are types of model averaging.</p>

<p><a href=""https://en.wikipedia.org/wiki/Bootstrap_aggregating"">Bagging</a> is an operation across your entire dataset which trains models on a subset of the training data.  Thus some training examples are not shown to a given model.</p>

<p><a href=""https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"">Dropout</a>, by contrast, is applied to features within each training example.  It is true that the result is functionally equivalent to training exponentially many networks (with shared weights!) and then equally weighting their outputs.  But dropout works on the feature space, causing certain features to be unavailable to the network, not full examples.  Because each neuron cannot completely rely on one input, representations in these networks tend to be more distributed and the network is less likely to overfit.</p>
"
"['machine-learning', 'dataset', 'unbalanced-classes']",Should I go for a 'balanced' dataset or a 'representative' dataset?,"<p>I would say the answer depends on your use case. Based on my experience:</p>

<ul>
<li>If you're trying to build a representative model -- one that describes the data rather than necessarily predicts -- then I would suggest using a representative sample of your data.</li>
<li>If you want to build a predictive model, particularly one that performs well by measure of AUC or rank-order and plan to use a basic ML framework (i.e. Decision Tree, SVM, Naive Bayes, etc), then I would suggest you feed the framework a balanced dataset. Much of the literature on class imbalance finds that random undersampling (down sampling the majority class to the size of the minority class) can drive performance gains.</li>
<li>If you're building a predictive model, but are using a more advanced framework (i.e. something that determines sampling parameters via wrapper or a modification of a bagging framework that samples to class equivalence), then I would suggest again feeding the representative sample and letting the algorithm take care of balancing the data for training.</li>
</ul>
"
"['visualization', 'graphs']",Visualizing a graph with a million vertices,"<p>I also suggest <code>Gephi</code> software (<a href=""https://gephi.github.io"">https://gephi.github.io</a>), which seems to be quite powerful. Some additional information on using <code>Gephi</code> with <strong>large networks</strong> can be found <a href=""https://forum.gephi.org/viewtopic.php?t=1554"">here</a> and, more generally, <a href=""https://forum.gephi.org/viewforum.php?f=25"">here</a>. <code>Cytoscape</code> (<a href=""http://www.cytoscape.org"">http://www.cytoscape.org</a>) is an alternative to <code>Gephi</code>, being an another popular platform for complex network analysis and visualization.</p>

<p>If you'd like to work with networks <strong>programmatically</strong> (including visualization) in R, Python or C/C++, you can check <code>igraph</code> collection of libraries. Speaking of R, you may find interesting the following blog posts: on <strong>using R with Cytoscape</strong> (<a href=""http://www.vesnam.com/Rblog/viznets1"">http://www.vesnam.com/Rblog/viznets1</a>) and on <strong>using R with Gephi</strong> (<a href=""http://www.vesnam.com/Rblog/viznets2"">http://www.vesnam.com/Rblog/viznets2</a>).</p>

<p>For <strong>extensive lists</strong> of <em>network analysis and visualization software</em>, including some comparison and reviews, you might want to check the following pages: 1) <a href=""http://wiki.cytoscape.org/Network_analysis_links"">http://wiki.cytoscape.org/Network_analysis_links</a>; 2) <a href=""http://www.kdnuggets.com/software/social-network-analysis.html"">http://www.kdnuggets.com/software/social-network-analysis.html</a>; 3) <a href=""http://www.activatenetworks.net/social-network-analysis-sna-software-review"">http://www.activatenetworks.net/social-network-analysis-sna-software-review</a>.</p>
"
"['nosql', 'databases']",is this a good case for NOSQL?,"<p>Three tables: <strong>animal</strong>, <strong>observation</strong>, and <strong>sibling</strong>. The observation has an <strong>animal_id</strong> column which links to the animal table, and the sibling table has <strong>animal_1_id</strong> and <strong>animal_2_id</strong> columns that indicates two animals are siblings for each row.</p>

<p>Even with 5000 animals and 100000 observations I don't think query time will be a problem for something like PostgreSQL for most reasonable queries (obviously you can construct unreasonable queries but you can do that in any system). </p>

<p>So I don't see how this is ""relatively painful"". Relative to what? The only complexity is the sibling table. In NOSQL you might store the full list of siblings in the record for each animal, but then when you add a sibling relationship you have to add it to both sibling's animal records. With the relational table approach I've outlined, it only exists once, but at the expense of having to test against both columns to find an animal's siblings. </p>

<p>I'd use PostgreSQL, and that gives you the option of using PostGIS if you have location data - this is a geospatial extension to PostgreSQL that lets you do spatial queries (point in polygon, points near a point etc) which might be something for you. </p>

<p>I really don't think the properties of NOSQL databases are a problem here for you - you aren't changing your schema every ten minutes, you probably <strong>do</strong> care that your database is ACID-compliant, and you don't need something web-scale.</p>

<p><a href=""http://www.mongodb-is-web-scale.com/"">http://www.mongodb-is-web-scale.com/</a> [warning: strong language]</p>
"
"['machine-learning', 'reinforcement-learning']",Does reinforcement learning always work on grid world?,"<p>The short answer is no! Reinforcement Learning is not limited to discrete spaces. But most of the introductory literature does deal with discrete spaces.</p>

<p>As you might know by now that there are three important components in any Reinforcement Learning problem: Rewards, States and Actions. The first is a scalar quantity and theoretically the latter two can either be discrete or continuous. The convergence proofs and analyses of the various algorithms are easier to understand for the discrete case and also the corresponding algorithms are easier to code. That is one of the reasons, most introductory material focuses on them.</p>

<p>Having said that, it should be interesting to note that the early research on Reinforcement Learning actually focussed on continuous state representations. It was only in the the 90s since the literature started representing all the standard algorithms for discrete spaces as we had a lot of proofs for them.</p>

<p>Finally, if you noticed carefully, I said continuous states only. Mapping continuous states and continuous actions is hard. Nevertheless, we do have some solutions for now. But it is an active area of Research in RL. </p>

<p>This <a href=""http://webdocs.cs.ualberta.ca/~sutton/papers/SSR-98.pdf"" rel=""nofollow"">paper by Sutton</a> from '98 should be a good start for your exploration!</p>
"
"['machine-learning', 'data-mining', 'classification', 'bigdata', 'statistics']",How to detect overfitting of a stock screener,"<p>Learning curves or bias-variance decomposition are the gold standard for detecting high variance, aka: overfitting.  Separate your data (in your case the ""back data"") into 60% training data and 40% testing data.  Fit the model on the training data as you usually would and see how well it is working with the test data.  </p>

<p>Finally, when you think you have the model that you want, split each of the training and test sets into 10-100 subsets and retrain and test with incrementally larger sets.  Apply your favorite performance metric and plot the results of performance vs. the number of cases used for testing and training.  </p>

<p>The curves will never come together if the model is overfit (high variance).  The curves will come together but the performance will be lower than desired if the model is underfit (high bias) and the lines will come together at an acceptable performance for a well performing model that is not overfit.</p>

<p>Here is an example of overfitting and underfitting with root mean square error as the performance metric:
<img src=""http://i.stack.imgur.com/m3a90.png"" alt=""Bias-Variance decomposition via learning curves"">  </p>

<p><a href=""https://followthedata.wordpress.com/2012/06/02/practical-advice-for-machine-learning-bias-variance/"">Here is a pretty good link</a> on the process and <a href=""http://www.astroml.org/sklearn_tutorial/practical.html"">here is another one</a>.  Hope this helps!</p>
"
"['machine-learning', 'algorithms']",Logbook: Machine Learning approaches,"<blockquote>
  <p>How are you solving this? How are you keeping track of the work done?
  What's your logbook tool?</p>
</blockquote>

<p>This might not be the best approach. But, this is how my team does it. We believe that for pulling off an end-to-end data science experiment, proper conscience is very important. So, we use <a href=""https://slack.com/"">Slack</a> for the same for our discussions and the meetings.</p>

<p>In addition to them, we have Rmd (R markdown) files for documenting the planning and the analysis parts.</p>
"
"['clustering', 'distance']",What methods are exist for distance calculation in clustering? when we should use each of them?,"<p>Well, there is a book called</p>

<blockquote>
  <p>Deza, Michel Marie, and Elena Deza.<br /> <strong>Encyclopedia of distances</strong>.<br /> Springer Berlin Heidelberg, 2009. ISBN 978-3-642-00233-5</p>
</blockquote>

<p>I guess that book answers your question better than I can...</p>

<p>Choose the distance function most appropriate for your data.</p>

<p>For example, on latitude and longitude, use a distance like Haversine. If you have enough CPU, you can use better approximations such as Vincenty's.</p>

<p>On histograms, use a distribution-baes distance. Earth-movers (EMD), divergences, histogram intersection, quadratic form distances, etc.</p>

<p>On binary data, for example Jaccard, Dice, or Hamming make a lot of sense.</p>

<p>On non-binary sparse data, such as text, various variants of tf-idf weights and cosine are popular.</p>

<p>Probably the best tool to experiment with different distance functions and clustering is ELKI. It has <a href=""http://elki.dbs.ifi.lmu.de/wiki/DistanceFunctions"">many many distances</a>, and many clustering algorithms that can be used with all of these distances (e.g. OPTICS). For example Canberra distance worked very well for me. That is probably what I would choose as ""default"".</p>
"
['information-retrieval'],Typing error handling n-gram character index and vector space model,"<p>Trigram models can be more powerful for document retrieval than unigram models, but if you want to handle spelling errors, they will not be of much help.  You need some form of <a href=""https://en.wikipedia.org/wiki/Approximate_string_matching"" rel=""nofollow"">fuzzy matching</a> for that.</p>

<p>For example the string, ""I like dosg too"" would fool a unigram model because ""dosg"" is likely ""dogs"" misspelled, and it will encode it as <code>""dosg"" : 1</code>.  But you have the same problem in a trigram model.  It will encode <code>""I like dosg"" : 1</code>, <code>""like dosg too"" : 1</code>.  Which is not really better, as it will still not match any trigrams with the word ""dogs"" in it.</p>
"
"['machine-learning', 'algorithms']",Algorithm for gesture classification in a wearable,"<p>It might be really useful to provide a bit more details regarding the actual data, produced by your sensor. Can you provide a simple example for any simple gesture? </p>

<p>Anyway, let's try to address the problem in the following way: </p>

<ul>
<li>sensor can be represented as a function <code>f(t)</code></li>
<li>range of the function will be <code>{1,2,3,..100}</code> </li>
</ul>

<p>So we can draw something like the following (can we?): </p>

<p><a href=""http://i.stack.imgur.com/okVvh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/okVvh.png"" alt=""enter image description here""></a></p>

<p>You can extract various high-level features out of that data, but I would suggest to go with inflection points as the most straightforward option:</p>

<p><a href=""http://i.stack.imgur.com/qBHIA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qBHIA.png"" alt=""enter image description here""></a></p>

<p>Those points, basically, define a pattern. I'll use a figure from the <a href=""http://stackoverflow.com/a/19071980/1435741"">different answer</a> to illustrate the idea. For example, the naive pattern below is used to identify a circle. </p>

<p><a href=""http://i.stack.imgur.com/qzN7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qzN7M.png"" alt=""enter image description here""></a> </p>

<p>In your case situation is a bit simpler, because sensor output is the only data you can work with. Naturally, you can combine multiple sensors in order to define a bit more complex interaction scenarios:</p>

<p><a href=""http://i.stack.imgur.com/MKJn1.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MKJn1.png"" alt=""enter image description here""></a></p>

<p>Also you can take into account <code>f'(t)</code> in order to distinguish between slow and fast movements, etc.  </p>

<p><strong>Update</strong></p>

<p>Thanks for the data. It does look like these three gestures looks pretty much the same from the data perspective (see visualization below). </p>

<p><a href=""http://i.stack.imgur.com/uhdcR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uhdcR.png"" alt=""enter image description here""></a></p>

<p>It really looks like several features are required to identify the gesture.</p>
"
['statistics'],How should I create a single score with two values as input?,"<p>If you're looking for something where A and B are equally represented, consider trying something like Z score normalization (or <a href=""http://en.wikipedia.org/wiki/Standard_score"" rel=""nofollow"">standard score</a>):</p>

<pre><code>c = (a-u_a)/sigma_a + (b-u_b)/sigma_b
</code></pre>

<p>That score equally represents the two, but would be on a smaller scale.  It really shouldn't matter since the numbers are arbitrary, however, if you need to scale it up, you could do something like:</p>

<pre><code>c2 = (sigma_a+sigma_b)*(c) + u_a + u_b
</code></pre>
"
"['topic-model', 'lda']",replicability / reproducibility in topic modeling (LDA),"<p>LDA is Bayesian model. This means the desired result is a posterior probability distribution over the random vectors of interest (probability of topics etc. having seen some data). </p>

<p>Inference for many Bayesian models is done by Markov Chain Monte Carlo. Indeed the <a href=""https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"" rel=""nofollow"">wiki</a> on LDA suggests that Gibbs sampling is a popular inference technique for LDA. MCMC draws random samples to provide an approximation of the posterior distribution.</p>

<p>Variational inference methods should typically be deterministic, but I'm not too familiar with the <a href=""https://www.cl.cam.ac.uk/teaching/1011/L101/ml4lp-lect8.pdf"" rel=""nofollow"">VB inference</a> for this particular model.</p>

<p>Also one can typically replicate runs of random algorithms by setting of random number generation seeds (if your purpose is scientific).</p>

<p>In either case if the results from a Bayesian model show huge variability in the the parameters of interest, it may be telling you that the model is not a good fit for the dataset, or the dataset is not big enough for the model you are fitting.</p>

<p>EDIT: I don't know which inference method (Gibbs, VB etc.) the backend of your software is using, so it's not possible to determine what type (if any) of randomisation is going on. </p>

<p>For scientific purposes, you'll probably want to read up some more on Bayesian inference. Standard software (e.g. the LDA in scikits.learn) will give you a summary of the outputs of the inference (e.g. most coders just want the best assignment of docs to topics). There's more information hanging around behind the scenes which you might be able to get access to, and could be useful.</p>

<p>E.g. (roughly) for scientific applications of Gibbs sampling methods we'd typically run multiple chains, drop the first N samples generated by each, and check if the resulting samples look like they came from the same distribution. If you're concerned about dependence on seeds etc. and your backend is the Gibbs sampler you will want to check out MCMC convergence diagnostics for this model.</p>
"
"['neuralnetwork', 'matlab']",Is it possible to connect three neural networks in Matlab?,"<p>If you want to combine the results from three different Neural Networks to ""boost"" the performance :) , you might want to look at the different Ensemble Learning Methods as I mentioned earlier. </p>

<p>Which method you should use, depends on how you share or divide the training data between the three NNs. For example if the NNs are trained on same data but have different parameters, you can look at simple voting ( if you are doing a classification task) or averaging ( if you are using them for regression).</p>

<p>The more advanced methods like AdaBoost divide the training data between the classifiers. You can read about it in <a href=""http://www.iro.umontreal.ca/~lisa/pointeurs/ada-nc.pdf"" rel=""nofollow"">Boosting Neural Networks</a></p>
"
"['data', 'education']",Domain-specific data science programs,"<p>Unsurprisingly most programs in data science focus mostly on business or marketing applications. I too would like to see more programs focused on Biotechnology, Gaming or Energy for example but I think that in the future those kind of programs will become more and more available.
However there are a few I know of that offer a somewhat different approach.</p>

<p>Regarding health informatics there are many offers where you can find anything between Public health to Computational Biology and Quantitative Genetics. Take a look at the full list.</p>

<p><a href=""http://www.mastersindatascience.org/specialties/health-informatics/"" rel=""nofollow"">http://www.mastersindatascience.org/specialties/health-informatics/</a></p>

<p>On that list there are a few certification programs but there also are full fledged programs that lead to a M.Sc.</p>

<p>If you are more interested in Marketing Analytics you can find the same list but for marketing related programs. </p>

<p><a href=""http://www.mastersindatascience.org/specialties/marketing-analytics/"" rel=""nofollow"">http://www.mastersindatascience.org/specialties/marketing-analytics/</a></p>

<p>other options like Information Systems are also present, just browse that site and you might find something that interests you.</p>
"
"['machine-learning', 'optimization']",Machine learning for state-based transforms?,"<p>If i get it correctly:</p>

<ul>
<li><p>You have an input polygon</p></li>
<li><p>As a first step you want to ""match"" that against a list of previously seen templates. If this is successful, you pick it's corresponding output and move on.</p></li>
<li><p>If not, you wish to find some optimal transformation, in order for it to satisfy some constraints that you have (your ""objective function""). Then add the original+transformed shape to the templates list and move on.</p></li>
</ul>

<p>Is this correct? I'll risk an answer anyways:</p>

<p>For the first part, I believe that there is a <a href=""https://www.google.com.tr/search?rls=en&amp;q=shape%20matching%20algorithm&amp;ie=UTF-8&amp;oe=UTF-8"" rel=""nofollow"">slew of literature out there</a>. It's not my expertise, but first thing that comes to mind is measuring the distance in feature space between your shape and each template, and picking the closest one, it the distance is below a threshold that you set. ""Feature"" here would be either some low-level polygon property, e.g. x and y coordinates of vertices, or an abstraction, e.g. perimeter, area, no. vertices, mean side length/side length variance, etc.</p>

<p>For the second part, it really depends on the nature of your constraints/objective functions. Are they convex? Uni- or multi-modal? Single or multi-objective? Do you want to incorporate some domain knowledge (i.e. knowledge about what ""good"" transformations would be?)? One can really not tell without further details. Evolutionary algorithms are quite versatile but expensive methods (although some argue on that). If you can spare the possibly large amount of function evaluations, you could try EAs as a first step, and then refine your approach.</p>

<p>Finally, while not exactly related to what you describe in your process, I believe you may benefit by taking a look into auto-associative networks (and models in general); these are models that are able to perform constraint-satisfaction on their input, effectively enforcing learned relationships on input values. I could see this being used in your case by inputing a shape, and having a transformed shape as an output, which would be ""legal"", i.e. satisfying the constraints learned by the auto associative model. Thus, you would eliminate the need for a template matching + optimization altogether.</p>
"
"['python', 'apache-spark', 'pyspark', 'jupyter']",Use spark_csv inside Jupyter and using Python,"<p>Assuming the rest of your configuration is correct all you have to do is to make <code>spark-csv</code> jar available to your program. There are a few ways you can achieve this:</p>

<ul>
<li><p>manually download required jars including  <a href=""http://central.maven.org/maven2/com/databricks/spark-csv_2.10/1.3.0/spark-csv_2.10-1.3.0.jar"" rel=""nofollow""><code>spark-csv</code></a> and csv parser (for example <a href=""http://mvnrepository.com/artifact/org.apache.commons/commons-csv/1.2"" rel=""nofollow""><code>org.apache.commons.commons-csv</code></a>) and put them somewhere on the <code>CLASSPATH</code>.</p></li>
<li><p>using <code>--packages</code> option (use Scala version which has been used to build Spark. Pre-built versions use 2.10):</p>

<ul>
<li><p>using <code>PYSPARK_SUBMIT_ARGS</code> environmental variable:</p>

<pre><code>export PACKAGES=""com.databricks:spark-csv_2.11:1.3.0""
export PYSPARK_SUBMIT_ARGS=""--packages ${PACKAGES} pyspark-shell""
</code></pre></li>
<li><p>adding Gradle string to <code>spark.jars.packages</code> in <code>conf/spark-defaults.conf</code>:</p>

<pre><code>spark.jars.packages    com.databricks:spark-csv_2.11:1.3.0
</code></pre></li>
</ul></li>
</ul>
"
"['machine-learning', 'r', 'data-mining', 'bigdata', 'predictive-modeling']",What model should I use to find a common pattern for a specific user group based on the other dimensions?,"<p>The most common approach is to create business rules handmade, based on the univariate and multivariate analysis of the variable.</p>

<p>Basically, do some frequency count, see if you could isolate some subset of your data just looking at one or two variables.</p>

<p>Then when you have your labels, create a linear or so model with this new variable as output. For exemple, a <a href=""https://en.wikipedia.org/wiki/Linear_discriminant_analysis"" rel=""nofollow"">linear discriminant analysis</a>. The analysis will supply you with new insights on your group.</p>

<p>If you want to rely on an algorithm, two solutions:</p>

<p>As you don't seems to have a lot of variables, a non-supervised segmentation could do the job. For exemple, a k-Nearest Neighbor or a decision tree are basic and good aproachs.</p>

<p>With a few more variables, I like is to do a <a href=""https://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow"">principal component analysis</a> then a non-supervised classification to define your group on the result of the PCA.
Note that a PCA + handmade rules based on the analysis of your PCA result may be enough.</p>

<p>Each time, in the end, a discriminant analysis and a profile of your groups to asses the quality of your results.</p>
"
"['python', 'visualization']",Are there any python based data visualization toolkit?,"<p>There is a Tablaeu API and you can use Python to use it, but maybe not in the sense that you think. There is a Data Extract API that you could use to import your data into Python and do your visualizations there, so I do not know if this is going to answer your question entirely.</p>

<p>As in the first comment you can use Matplotlib from <a href=""http://www.matplotlib.org"">Matplotlib website</a>, or you could install Canopy from Enthought which has it available, there is also Pandas, which you could also use for data analysis and some visualizations. There is also a package called <code>ggplot</code> which is used in <code>R</code> alot, but is also made for Python, which you can find here <a href=""https://pypi.python.org/pypi/ggplot"">ggplot for python</a>.</p>

<p>The Tableau data extract API and some information about it can be found <a href=""http://www.tableausoftware.com/new-features/data-engine-api-0"">at this link</a>. There are a few web sources that I found concerning it using duckduckgo <a href=""https://duckduckgo.com/?q=tableau%20PYTHON%20API&amp;kp=1&amp;kd=-1"">at this link</a>.
Here are some samples:</p>

<p><a href=""https://www.interworks.com/blogs/bbickell/2012/12/06/introducing-python-tableau-data-extract-api-csv-extract-example"">Link 1</a></p>

<p><a href=""http://ryrobes.com/python/building-tableau-data-extract-files-with-python-in-tableau-8-sample-usage/"">Link 2</a></p>

<p><a href=""http://nbviewer.ipython.org/github/Btibert3/tableau-r/blob/master/Python-R-Tableau-Predictive-Modeling.ipynb"">Link 3</a></p>

<p>As far as an API like matplotlib, I cannot say for certain that one exists. Hopefully this gives some sort of reference to help answer your question.</p>

<p>Also to help avoid closure flags and downvotes you should try and show some of what you have tried to do or find, this makes for a better question and helps to illicit responses.</p>
"
"['classification', 'confusion-matrix', 'accuracy']",How to get an aggregate confusion matrix from n different classifications,"<p>I do not know a standard answer to this, but I thought about it some times ago and I have some ideas to share. </p>

<p>When you have one confusion matrix, you have more or less a picture of how you classification model confuse (mis-classify) classes. When you repeat classification tests you will end up having multiple confusion matrices. The question is how to get a meaningful aggregate confusion matrix. The answer depends on what is the meaning of meaningful (pun intended). I think there is not a single version of meaningful.</p>

<p>One way is to follow the rough idea of multiple testing. In general, you test something multiple times in order to get more accurate results. As a general principle one can reason that averaging on the results of the multiple tests reduces the variance of the estimates, so as a consequence, it increases the precision of the estimates. You can proceed in this way, of course, by summing position by position and then dividing by the number of tests. You can go further and instead of estimating only a value for each cell of the confusion matrix, you can also compute some confidence intervals, t-values and so on. This is OK from my point of view. But it tell only one side of the story.</p>

<p>The other side of the story which might be investigated is how stable are the results for the same instances. To exemplify that I will take an extreme example. Suppose you have a classification model for 3 classes. Suppose that these classes are in the same proportion. If your model is able to predict one class perfectly and the other 2 classes with random like performance, you will end up having 0.33 + 0.166 + 0.166 = 0.66 misclassification ratio. This might seem good, but even if you take a look on a single confusion matrix you will not know that your performance on the last 2 classes varies wildly. Multiple tests can help. But averaging the confusion matrices would reveal this? My belief is not. The averaging will give the same result more or less, and doing multiple tests will only decrease the variance of the estimation. However it says nothing about the wild instability of prediction.</p>

<p>So another way to do compose the confusion matrices would better involve a prediction density for each instance. One can build this density by counting for each instance, the number of times it was predicted a given class. After normalization, you will have for each instance a prediction density rather a single prediction label. You can see that a single prediction label is similar with a degenerated density where you have probability of 1 for the predicted class and 0 for the other classes for each separate instance. Now having this densities one can build a confusion matrix by adding the probabilities from each instance and predicted class to the corresponding cell of the aggregated confusion matrix. </p>

<p>One can argue that this would give similar results like the previous method. However I think that this might be the case sometimes, often when the model has low variance, the second method is less affected by how the samples from the tests are drawn, and thus more stable and closer to the reality. </p>

<p>Also the second method might be altered in order to obtain a third method, where one can assign as prediction the label with highest density from the prediction of a given instance. </p>

<p>I do not implemented those things but I plan to study further because I believe might worth spending some time. </p>
"
"['random-forest', 'accuracy', 'gbm', 'ensemble-modeling']",Why isn't dimension sampling used with gradient boosting machines (GBM)?,"<p>sklearn's <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"" rel=""nofollow"">GradientBoostingClassifier</a> / <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"" rel=""nofollow"">GradientBoostingRegressor</a> have a <em>max_features</em> parameter and <a href=""https://github.com/dmlc/xgboost/blob/master/doc/parameter.md"" rel=""nofollow"">XGBoost</a> has <em>colsample_bylevel</em> and <em>colsample_bytree</em> parameters that control how many features are sampled for each tree / split.</p>
"
['dataset'],Confused about description of YearPrediction Dataset,"<p>You are right, the covariance matrix should have n^2 elements. However, since cov_{i,j} = cov_{j,i}, there is no need to have a repeated feature cov_{j,i} if cov_{i,j} is already accounted for. Hence there will be only n*(n+1)/2 = 12*13/2 = 78 unique covariances and thus only 78 unique covariance based features (n of those will be variances). </p>
"
"['random-forest', 'supervised-learning', 'unsupervised-learning']",Research in random forest algorithms able to switch data sets,"<p><strong>Semi-Supervised Learning</strong></p>

<p>The combination of unsupervised learning and supervised learning is referred to as <strong><code>semi-supervised learning</code></strong>, which is the concept that I believe you are searching for.</p>

<p><a href=""http://www.slideshare.net/dav009/label-propagation-semisupervised-learning-with-applications-to-nlp"" rel=""nofollow""><code>Label propagation</code></a> is often cited when outlining the heuristics of <code>semi-supervised learning</code>.  The essence is to employ clustering, but to use a tiny set of known cases in order to derive (or propogate) the labels of the clusters.  Hence one is able to use a small set of labeled cases to classify a much larger set of unsupervised data.</p>

<p><strong>Here are some references:</strong></p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Semi-supervised_learning"" rel=""nofollow"">Wikipedia has an entry on the <code>semi-supervised learning</code></a>.</li>
<li><a href=""http://scikit-learn.org/stable/modules/label_propagation.html"" rel=""nofollow"">The scikit learn User Guide is often a useful starting point and has a label propogation routine</a>.</li>
<li>There are, in fact, papers treating <a href=""http://www.icg.tugraz.at/publications/pdf/srf.pdf"" rel=""nofollow""><code>semi-supervised</code> <code>random forest</code> models</a>. </li>
<li><a href=""http://research.microsoft.com/pubs/155552/decisionForests_MSR_TR_2011_114.pdf"" rel=""nofollow"">Another one here</a></li>
</ul>

<p>Hope this helps!</p>
"
"['feature-selection', 'sklearn', 'scikit']",Does scikit-learn have forward selection/stepwise regression algorithm?,"<p>No, sklearn doesn't seem to have a forward selection algorithm. However, it does provide recursive feature elimination, which is a greedy feature elimination algorithm similar to sequential backward selection. See the documentation here:</p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"">http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html</a></p>
"
['recommendation'],"In a SVD with user/video bias, why is the UV contribute so small?","<p>It means the average predictions of the user across all items, and the average of each item across all users accurately predicts the ratings -- you have an easy data set.</p>
"
"['classification', 'ensemble-modeling']",how to choose classifer,"<p>It's usually not that clear cut; there's typically not one universally best approach.</p>

<p>Having said that, there are some prototyped ensemble approaches that are supposed to always be better than their underlying component algorithms, notably Erin LeDell's binary ensemble classifier for H2O. However, even in those cases you still need to optimize the first stage algorithms for the ensemble to be universally better.</p>

<p>Thus if you're willing to spend a lot of extra time, let's say 2 weeks for an ensemble instead of the 1 week it might take you for your single-stage algorithm, then it's possible (especially for binary classification) to find an ensemble that will definitely be better than your single-stage classifier.</p>

<p>However this is rarely the case and the way you framed the question implies there's a choice between </p>

<ol>
<li><p>building 1 really good single-stage model, selected from many candidate models (and by the way be sure to avoid overfitting while making those selections)  and</p></li>
<li><p>throwing an ensemble at the problem without completing #1 above for each component of the ensemble (or completing #1 but not also optimizing the 2nd stage of the ensemble)</p></li>
</ol>

<p>If that's the decision then -- while there's no 1 universally right answer -- I'd say that in the <strong>vast</strong> majority of the cases it's better to stick with #1.</p>
"
"['bigdata', 'nosql', 'mongodb']",Uses of NoSQL database in data science,"<p>To be perfectly honest, most NoSQL databases are not very well suited to applications in big data. For the vast majority of all big data applications, the performance of <a href=""https://en.wikipedia.org/wiki/MongoDB"" rel=""nofollow"">MongoDB</a> compared to a relational database like <a href=""http://en.wikipedia.org/wiki/MySQL"" rel=""nofollow"">MySQL</a> is <a href=""http://www.moredevs.com/mysql-vs-mongodb-performance-benchmark/"" rel=""nofollow"">significantly</a> is poor enough to warrant staying away from something like MongoDB entirely.</p>

<p>With that said, there are a couple of really useful properties of NoSQL databases that certainly work in your favor when you're working with large data sets, though the chance of those benefits outweighing the generally poor performance of NoSQL compared to <a href=""http://en.wikipedia.org/wiki/SQL"" rel=""nofollow"">SQL</a> for read-intensive operations (most similar to typical big data use cases) is low.</p>

<ul>
<li><strong>No Schema</strong> - If you're working with a lot of unstructured data, it might be hard to actually decide on and rigidly apply a schema. NoSQL databases in general are very supporting of this, and will allow you to insert schema-less documents on the fly, which is certainly not something an SQL database will support.</li>
<li><strong><a href=""http://en.wikipedia.org/wiki/JSON"" rel=""nofollow"">JSON</a></strong> - If you happen to be working with JSON-style documents instead of with <a href=""http://en.wikipedia.org/wiki/Comma-separated_values"" rel=""nofollow"">CSV</a> files, then you'll see a lot of advantage in using something like MongoDB for a database-layer. Generally the workflow savings don't outweigh the increased query-times though.</li>
<li><strong>Ease of Use</strong> - I'm not saying that SQL databases are always hard to use, or that <a href=""http://en.wikipedia.org/wiki/Apache_Cassandra"" rel=""nofollow"">Cassandra</a> is the easiest thing in the world to set up, but in general NoSQL databases are easier to set up and use than SQL databases. MongoDB is a particularly strong example of this, known for being one of the easiest database layers to use (outside of <a href=""http://en.wikipedia.org/wiki/SQLite"" rel=""nofollow"">SQLite</a>). SQL also deals with a lot of normalization and there's a large legacy of SQL best practices that just generally bogs down the development process.</li>
</ul>

<p>Personally I might suggest you also check out <a href=""http://en.wikipedia.org/wiki/Graph_database"" rel=""nofollow"">graph databases</a> such as <a href=""http://en.wikipedia.org/wiki/Neo4j"" rel=""nofollow"">Neo4j</a> that show really good performance for certain types of queries if you're looking into picking out a backend for your data science applications.</p>
"
['data-cleaning'],How should I analyze this data from reddit (sample text included),"<p>Data Science is not an algorithm to run on your data. It is the process that helps you answer a specific question. The key to be a data scientist is to ask the right questions. So, first, since you want to be familiar with machine learning, examine your data and try to understand what they can answer for you.</p>

<p>Examples:</p>

<ol>
<li>Can I cluster the reddit questions in categories?</li>
<li>Can you predict the score of a question based on the number of comments?</li>
<li>Can you predict the sub-reddit that a user will ask a question?</li>
</ol>
"
"['machine-learning', 'svm', 'visualization', 'performance', 'supervised-learning']",Geometric Interpretation of Whether SVMs are performing well or not,"<p>A most interesting paper! Following <em>Opper and Haussler</em><sup>$\dagger$</sup>, the authors define a <em>version space</em>; the set of unit vectors which separate the training samples:</p>

<p>$\mathcal V \equiv \{ \mathtt w | y_i f(\mathtt x_i) &gt;0 , i= 1, \ldots, n, \| \mathtt w \|_2 = 1 \}$</p>

<p>Remember that we are dealing with a classification problem where $y_i \in \{-1, +1\}$, so we want our classifier $f(x_i)$ to have the same sign as $y_i$. It might help to recall that $\mathtt w = \sum_i \alpha_i \phi(\mathtt x_i)$ and $f(\mathtt x) = \left&lt; \mathtt w, \phi(\mathtt x) \right&gt; = \sum_i \alpha_i k(\mathtt x_i, \mathtt x)$</p>

<p>Normally $y_i \left( \left&lt; \mathtt w , \phi(\mathtt x_i) \right&gt; + b \right) \ge 1$. What they've done is to set $b=0$ and eliminate the margin (RHS). The length constraint is to ensure uniqueness.</p>

<blockquote>
  <p>The version space is illustrated as a region on the sphere as shown in
  Figs. 5 and 6. If the version space is shaped as in Fig. 5, the SVM
  solution is near to the optimal point. However, if it has an elongated
  shape as in Fig. 6, the SVM solution is far from the optimal one.</p>
</blockquote>

<p>The reason there is an inscribed sphere is that:</p>

<blockquote>
  <p>the SVM solution coincides with the Tchebycheff-center of the version
  space, which is the center of the largest sphere contained
  in $\mathcal V$. However, the theoretical optimal point in version
  space yielding a Bayes-optimal decision boundary is the
  Bayes point, which is known to be closely approximated by
  the center of mass.</p>
</blockquote>

<p>In other words, they are saying that the SVM classifier is not always Bayes optimal. Please see the references for proof. The hypersphere comes about when you consider the margin $\gamma \equiv \min y_i f(\mathtt x_i)$. The support vectors are then the boundaries of the version space which are tangent to the hypersphere.</p>

<p>A cited paper* follows this idea to develop an algorithm to actually estimate the center of mass ""by averaging over the trajectory of a billiard ball bouncing in version space."" If you're interested in diagnosing SVM failures, perhaps it would help to read that paper too, since it claims to offer a better algorithm instead.</p>

<hr>

<p>$\dagger$ M. Opper and D. Haussler, “Generalization performance of Bayes optimal
classification algorithm for learning a perceptron,” <em>Phys. Rev. Lett.</em>,
vol. 66, p. 2677, 1991.</p>

<p>* T. Graepel, R. Herbrich, and C. Campbell, “Bayes point machines: Estimating
the bayes point in kernel space,” in Proc. IJCAI Workshop Support Vector Machines, 1999, pp. 23–27</p>
"
"['predictive-modeling', 'feature-selection']",Use forecast weather data or actual weather data for prediction?,"<p>Using historical weather data implicitely means that you trust meteorologists and weather forecasters to improve their model over time and you let them the full responsability of it; however once your own model deployed a bias in the forecast may create a bias in your model response. </p>

<p>Using weather forecasts instead should give better results because your model will directly capture potential bias in the forecasts; however if the weather forecasters update their forecasting model, and if you miss this update, your model response may suffer from it. </p>

<p>I wouldn't use both historical weather data and weather forecasts in the same model; I would consider to build two models, one with historical data and one with weather forecasts, then go for historical data if the improvement of using weather forecasts is not significant. </p>
"
"['machine-learning', 'data-mining', 'nlp', 'text-mining', 'similarity']",What are some standard ways of computing the distance between documents?,"<p>There's a number of different ways of going about this depending on exactly how much semantic information you want to retain and how easy your documents are to tokenize (html documents would probably be pretty difficult to tokenize, but you could conceivably do something with tags and context.)</p>

<p>Some of them have been mentioned by ffriend, and the paragraph vectors by user1133029 is a really solid one, but I just figured I would go into some more depth about plusses and minuses of different approaches.</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Cosine_similarity"">Cosine Distance</a> - Tried a true, cosine distance is probably the most common distance metric used generically across multiple domains. With that said, there's very little information in cosine distance that can actually be mapped back to anything semantic, which seems to be non-ideal for this situation.</li>
<li><a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"">Levenshtein Distance</a> - Also known as <code>edit distance</code>, this is usually just used on the individual token level (words, bigrams, etc...). In general I wouldn't recommend this metric as it not only discards any semantic information, but also tends to treat very different word alterations very similarly, but it is an extremely common metric for this kind of thing</li>
<li><a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"">LSA</a> - Is a part of a large arsenal of techniques when it comes to evaluating document similarity called <code>topic modeling</code>. LSA has gone out of fashion pretty recently, and in my experience, it's not quite the strongest topic modeling approach, but it is relatively straightforward to implement and has a few open source implementations</li>
<li><a href=""http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"">LDA</a> - Is also a technique used for <code>topic modeling</code>, but it's different from <code>LSA</code> in that it actually learns internal representations that tend to be more smooth and intuitive. In general, the results you get from <code>LDA</code> are better for modeling document similarity than <code>LSA</code>, but not quite as good for learning how to discriminate strongly between topics.</li>
<li><a href=""http://en.wikipedia.org/wiki/Pachinko_allocation"">Pachinko Allocation</a> - Is a really neat extension on top of LDA. In general, this is just a significantly improved version of <code>LDA</code>, with the only downside being that it takes a bit longer to train and open-source implementations are a little harder to come by</li>
<li><a href=""https://code.google.com/p/word2vec/"">word2vec</a> - Google has been working on a series of techniques for intelligently reducing words and documents to more reasonable vectors than the sparse vectors yielded by techniques such as <code>Count Vectorizers</code> and <code>TF-IDF</code>. Word2vec is great because it has a number of open source implementations. Once you have the vector, any other similarity metric (like cosine distance) can be used on top of it with significantly more efficacy.</li>
<li><a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"">doc2vec</a> - Also known as <code>paragraph vectors</code>, this is the latest and greatest in a series of papers by Google, looking into dense vector representations of documents. The <code>gensim</code> library in python has an implementation of <code>word2vec</code> that is straightforward enough that it can pretty reasonably be leveraged to build <code>doc2vec</code>, but make sure to keep the license in mind if you want to go down this route</li>
</ul>

<p>Hope that helps, let me know if you've got any questions.</p>
"
['r'],How to subset rows from a data frame with comparison operators in R,"<p>You want</p>

<pre><code>data_frame[data_frame$X1 &gt; 30, c(""X1"",""X2"",""X4"")]
</code></pre>

<p>that will just print it, you probably want to update <code>data_frame</code> or store it in something else:</p>

<pre><code>data_frame = data_frame[data_frame$X1 &gt; 30, c(""X1"",""X2"",""X4"")]
</code></pre>

<p>also you probably want to try asking this on StackOverflow, or reading a bit more basic R documentation because it should be well covered. Its a bit simple to be ""data science"".</p>
"
"['experiments', 'marketing']",Attributing causality to single quasi-independent variable,"<p>I would suggest you to consider either direct <em>dimensionality reduction</em> approach. Check <a href=""http://datascience.stackexchange.com/a/4946/2452"">my relevant answer</a> on this site. Another valid option is to use <em>latent variable modeling</em>, for example, <em>structural equation modeling</em>. You can start with relevant articles on Wikipedia (<a href=""http://en.wikipedia.org/wiki/Latent_variable_model"" rel=""nofollow"">this</a> and <a href=""http://en.wikipedia.org/wiki/Structural_equation_modeling"" rel=""nofollow"">this</a>, correspondingly) and then, as needed, read more specialized or more practical articles, papers and books.</p>
"
"['machine-learning', 'data-mining', 'graphs']",Learning time of arrival (ETA) from historical location data of vehicle,"<p>Based on what I figured out from your problem:</p>

<h2>1</h2>

<ul>
<li>You can easily convert your data to a graph using <a href=""https://networkx.github.io/"" rel=""nofollow"">Networkx</a>, <a href=""http://igraph.org/"" rel=""nofollow"">igraph</a> or any other tool/library/software. Then what you need is a <a href=""http://en.wikipedia.org/wiki/Shortest_path_problem"" rel=""nofollow"">Shortest Path Algorithm</a> (<a href=""http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"" rel=""nofollow"">Dijkstra</a> is widely used and implemented in all graph/network analysis softwares). Once you created the graph you can simply calculate the average estimated time.</li>
<li>For turning the problem into a <em>Learning Problem</em>, you can use historical time estimations for different paths and assign a weight to an edge proportional to the property of that edge (e.g. traffic jam probability, time conditions) and try to predict the ETA for a new query. </li>
</ul>

<h2>2</h2>

<ul>
<li>You can also turn it into a <em>Network Science Problem</em> and use Graph Theoretc approaches to approach the question. You can start with statistical analysis of nodes and edges attributes e.g. passing time distribution, shortest path length distribution, probabilistic modeling of traffic jam and so on to see if some meaningful insight leads you the next step. </li>
<li>Another idea is to use graph clustering algorithms to extract most connected parts of the town and go through the analysis of them i.e. calculate the ETA for different parts instead of whole the data and assign the estimated time to the members of corresponding cluster and reduce the computational complexty if your algorithm.</li>
</ul>

<h2>3</h2>

<ul>
<li>The last but not least is having a look at <a href=""https://www.arangodb.com/"" rel=""nofollow"">ArangoDB</a>. It's a new database model which is based on graphs and you can run queries on millions of edges in an amazing speed! all what you need is a bit of javascript knowledge and even if you don't have it you can use AQL language designed for arangoDB. The interesting point is that it uses JSON files as the standard data format so you are already half way through ;)</li>
</ul>

<p>Hope i could help :)
Good Luck!</p>
"
"['python', 'nlp', 'nltk']",Complex Chunking with NLTK,"<p>your grammar is correct!</p>

<pre><code>grammar = """"""MEDIA: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}
           RELATION: {&lt;V.*&gt;}
                     {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}
           ENTITY: {&lt;NN.*&gt;}""""""
</code></pre>

<p>by specifying </p>

<pre><code>RELATION: {&lt;V.*&gt;}
          {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}
</code></pre>

<p>you are indicating that there are two ways to generate the <code>RELATION</code> chunk i.e. <code>{&lt;V.*&gt;}</code> or <code>{&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}</code></p>

<p>so </p>

<pre><code>grammar = """"""MEDIA: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}
               RELATION: {&lt;V.*&gt;}
                         {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}
               ENTITY: {&lt;NN.*&gt;}""""""
    chunkParser = nltk.RegexpParser(grammar)
    tagged = nltk.pos_tag(nltk.word_tokenize(""adventure movies between 2000 and 2015 featuring performances by daniel craig""))

    tree = chunkParser.parse(tagged)

    for subtree in tree.subtrees():
        if subtree.label() == ""RELATION"": 
            print(""RELATION: ""+str(subtree.leaves()))
</code></pre>

<p>gives</p>

<pre><code>RELATION: [('featuring', 'VBG')]
</code></pre>
"
['recommendation'],Recommendation model that can recommend already bought item,"<p>Usually recommendation algorithms provides the confidence that a user will like an item. Items that the user already bought should get high confidence - the user bought them since he liked them.</p>

<p>In most cases filtering out item the user already bough is done in the application level, not in the algorithm level. </p>

<p>So , you can use regular recommendation algorithms in order to know if the user will like the item.</p>

<p>Please note that you might face a different problem - will the user buy the item for the second/third time. 
In order to cope with this problem, using some domain knowledge might be beneficial. If the products are usually bough many times (e.g., milk), just use the recommendation algorithm. If all the products have the same tendency to be bought few times, build a model for that (e.g., probability for buying another time given already buying x times) and combine the models.
If your products are very different from this aspect you might need to get into a lower level. Different domains require different solution but you might split the product by the buying behavior and train few recommendation systems, add the number of buying as a feature, etc. </p>
"
"['machine-learning', 'feature-selection', 'feature-construction']",Is there any difference between feature extraction and feature learning?,"<p>Yes I think so. Just by looking at <a href=""https://en.wikipedia.org/wiki/Feature_learning"" rel=""nofollow"">Feature Learning</a> and <a href=""https://en.wikipedia.org/wiki/Feature_extraction"" rel=""nofollow"">Feature extraction</a> you can see it's a different problem.</p>

<p><strong>Feature extraction</strong> is just transforming your raw data into a sequence of feature vectors (e.g. a dataframe) that you can work on. </p>

<p>In <strong>feature learning</strong>, you don't know what feature you can extract from your data. In fact, you will probably apply machine learning techniques just to  discover what are good features to extract from your dataset. Then you can extract them them apply machine learning to the extracted features. Deep learning techniques are one example of this.</p>

<p>In the <strong>word2vec</strong> toolkit, for instance, you extract vectors from documents which can't be easily interpreted by a human, you can't look at it and tell what <em>features</em> have been extracted at all. It's just a mass of vectors which, for some reason, give good empirical results.</p>
"
"['statistics', 'ab-test']",A/B testing randomization step,"<p>As far as I understand you simply don't have to worry about that. </p>

<p>So, you have a sample split randomly in control and treatment. You measure something before treatment, and after, for the same individuals. Because you measure on the same individuals than you have paired measures: delta = before - after. You are interested to measure if the mean of delta for control sample is significantly different than the mean of delta for control sample. This is done with paired sample test known also as dependent test. </p>

<p>If you assume a normal distribution you can use <a href=""https://en.wikipedia.org/wiki/Student&#39;s_t-test#Dependent_t-test_for_paired_samples"" rel=""nofollow"">paired t test</a>. If you can't reasonably assume the normal distribution than you can use <a href=""https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test"" rel=""nofollow"">Wilcoxon signed rank test</a>. </p>

<p>The idea of pairings in test is to eliminate the effect of confounders. What you computed can be the effect of a confounder, but if the sample was split randomly in control and treatment you should not worry about that, unless you have strong reasons to doubt the randomization procedure.  In the later case you perhaps should include confounders in the equation and take the route of random effects modeling.</p>
"
"['machine-learning', 'classification', 'algorithms', 'svm', 'supervised-learning']",How do linear learning systems classify datapoints that fall on the hyperplane,"<p>Linear, binary classifiers can choose either class (but consistently) when the datapoint which is to classify is on the hyperplane. It just depends on how you programmed it.</p>

<p>Also, it doesn't really matter. This is very unlikely to happen. In fact, if we had arbitrary precision computing and normal distributed features, there would be a probability of 0 (exactly, not rounded) that this would happen. We have IEEE 754 floats, so the probability is not 0, but still so small that there are much more important factors to worry about.</p>
"
['machine-learning'],What are the current killing machine learning methods?,"<p>The question is very general. However, there are some studies being conducted to test which algorithms perform relatively well in a broad range of problems (I'll add link to papers later), concerning regression and classification.</p>

<p>Lately Random Decision Forests, Support Vector Machines and certain variations of Neural Networks are being said to achieve the best results for very broad variety of problems.</p>

<p>This does not mean that these are ""the best algorithms"" for any problem, that does not exist, and actually is not very realistic to pursue. Also it must be observed that both RDF and SVM are rather-easy methods to initially grasp and obtain good results, so they are becoming really popular. NN have been used intensively since couple of decades (after they revived), so they appear often in implementations.</p>

<p>If you are interested in learning further you should look for an specific area and deal with a problem that can be solved nicely by machine learning to understand the main idea (and why is impossible to find  <strong>the method</strong>).</p>

<p>You will find common the task to try to predict the expected behavior of something given some known or observable characteristics (to learn the function that models the problem given input data), the issues related to dealing with data in high-dimensional spaces, the need for good quality data, the notable improvements that can give data pre-processing, and many others.</p>
"
"['clustering', 'k-means', 'distance']",Can k-means clustering get shells as clusters?,"<p>You cannot just use arbitrary distance functions with k-means.</p>

<p>because the algorithm is <strong>not based on metric properties but on variance</strong>.</p>

<p><a href=""http://stats.stackexchange.com/q/81481/7828"">http://stats.stackexchange.com/q/81481/7828</a></p>

<p>Fact is that k-means minimizes the <strong>sum of squares</strong>. This does not even give you the ""smallest distances"" but only the smallest <em>squared</em> distances. This is <em>not</em> the same (see: difference between median and mean) - if you want to minimize Euclidean distances, use k-medians or if you want other distances PAM (k-medoids).</p>

<p>You can generalize k-means to use a few more distances known as ""Bergman divergences"" and you can do some variant of the kernel trick. But that is not very powerful, because you don't have labels for optimizing the kernel parameters! Still, that could be what this exercise question is up to... If your ""shells"" are indeed centered at 0, then you can transform your data (read: kernel trick done wrong) to angle+distance from origin, and k-means may be able to cluster the projected data (dependent on the not well defined scaling of the axes).
Or the textbook did <em>not</em> realize that a kernel k-means has been proposed long ago. Then the argument is probably this: <strong>the mean of each shell is 0, and thus the shells cannot be distinguished</strong>. This clearly holds for <em>unmodified</em> k-means.</p>
"
"['classification', 'dataset', 'unbalanced-classes']",Ratio of positive to negative sample in data set for best classification,"<p>I guess you are not limited to these 100 samples. Generate more, and let each 5th be negative. Then reduce number of positives by random removing 4/5 of them.</p>

<p>And check this out <a href=""http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets"">Training imbalanced data set</a></p>

<p>This is small quantity, you'd better have 50:50 negative vs positive.</p>
"
"['neuralnetwork', 'optimization', 'backpropagation']","Backpropagation: In second-order methods, would ReLU derivative be 0? and what its effect on training?","<p>Yes the ReLU second order derivative is 0. Technically, neither $\frac{dy}{dx}$ nor $\frac{d^2y}{dx^2}$ are defined at $x=0$, but we ignore that - in practice an exact $x=0$ is rare and not especially meaningful, so this is not a problem. Newton's method does not work on the ReLU transfer function because it has no stationary points. It also doesn't work meaningfully on most other common transfer functions though - they cannot be minimised or maximised for finite inputs.</p>

<p>When you combine multiple ReLU functions with layers of matrix multiplications in a structure such as a neural network, and wish to minimise an objective function, the picture is more complicated. This combination does have stationary points. Even a single ReLU neuron and a mean square error objective will have different enough behaviour such that the second-order derivative of the single weight will vary and is not guaranteed to be 0.</p>

<p>Nonlinearities when multiple layers combine is what creates a more interesting optimisation surface. This also means that it is harder to calculate useful second-order partial derivatives (or <a href=""https://en.wikipedia.org/wiki/Hessian_matrix"" rel=""nofollow"">Hessian matrix</a>), it is not just a matter of taking second order derivatives of the transfer functions. </p>

<p>The fact that $\frac{d^2y}{dx^2} = 0$ for the transfer function will make some terms zero in the matrix (for the second order effect from same neuron activation), but the majority of terms in the Hessian are of the form $\frac{\partial^2E}{\partial x_i\partial x_j}$ where E is the objective and $x_i$, $x_j$ are <em>different</em> parameters of the neural network. A fully-realised Hessian matrix will have $N^2$ terms where $N$ is number of parameters - with large neural networks having upwards of 1 million parameters, even with a simple calculation process and many terms being 0 (e.g. w.r.t. 2 weights in same layer) this may not be feasible to compute.</p>

<p>There are techniques to estimate effects of second-order derivatives used in some neural network optimisers. RMSProp can be viewed as roughly estimating second-order effects, for example. The ""Hessian-free"" optimisers more explicitly calculate the impact of this matrix.</p>
"
"['machine-learning', 'data-mining', 'nlp', 'data-cleaning']",Stackoverflow API Structure data storage,"<p>What do you want to define as your document? You could define a document as a single question, each question-answer pair, or a question with all its answers. How you define a document depends on why you want to cluster documents and how you plan to use this information. From reading your question, my guess is that you want to define a question with all its answer as a document. </p>

<p>Next, it sounds like you want to apply bag of words techniques to extract features (e.g. tf-idf). If so, you would first want to concatenate the question title, question body, and each answer body into one string object. Then I would suggest using a Python package like scikit learn or nltk to do the other preprocessing steps. For example in scikit learn you can then apply <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow"">TfidfVectorizer</a> to convert a string to a vector. </p>

<p>This is unrelated to your question, but it may be easier to use the <a href=""https://data.stackexchange.com/"" rel=""nofollow"">data explorer</a> instead of the StackOverflow API. The data explorer allows you to download questions and answers in a large batch and does not have a quota. Unless you need to collect data in real time, the data explorer may be more efficient. It is updated every week and you can use SQL queries to output data tables as csv. </p>
"
"['machine-learning', 'classification']",Predict set elements based on other elements,"<p>Maybe you could use some algorithm that solves ""Market Basket Analysis"". The problem is explained here: </p>

<blockquote>
  <p>Market Basket Analysis is a modelling technique based upon the theory
  that if you buy a certain group of items, you are more (or less)
  likely to buy another group of items. For example, if you are in an
  English pub and you buy a pint of beer and don't buy a bar meal, you
  are more likely to buy crisps (US. chips) at the same time than
  somebody who didn't buy beer.</p>
</blockquote>

<p><a href=""http://albionresearch.com/data_mining/market_basket.php"" rel=""nofollow"">http://albionresearch.com/data_mining/market_basket.php</a></p>

<p>One example of such an algorithm is: <a href=""https://en.wikipedia.org/wiki/Association_rule_learning"" rel=""nofollow"">https://en.wikipedia.org/wiki/Association_rule_learning</a></p>
"
"['machine-learning', 'statistics']",root mean square error - significance of square root,"<p>It depends on what you are using the RMSE for. If you are merely trying to compare two models/estimators, then there is no significance to the square root. However, if you are trying to plot the error in terms of the same units as you made the measurements/estimates, then you need to take the square root to transform the squared units to the original units (much like variance vs standard deviation)</p>
"
"['nlp', 'data-cleaning', 'nltk', 'google-prediction-api']",Create training data,"<p>Assuming you are doing supervized learning to train a model that when deployed will take text as input and output a label (e.g., topic) or class probability, then what you probably want to do is balanced, <a href=""https://en.wikipedia.org/wiki/Stratified_sampling"" rel=""nofollow"">stratified sampling</a>. Assuming sufficient labelled data, ensure that your final training set has a balanced number of text examples for each class/label. Depending on your situation, you may need to over/under sample or somehow deal with the problem of highly imbalanced classes (see <a href=""http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"" rel=""nofollow"">8 tactics to combat imbalanced classes</a>).</p>

<p>The simplest NLP approach to use a <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow"">bag of words</a> technique, simply indicating the presence/absence of a word in the sentence. Thus each sentence becomes represented as vector of length n, where n = the number of unique words in your data set. </p>

<pre><code>data_set &lt;- c(""the big dog"", ""the small cat"", ""the big and fat cow"")
words &lt;- strsplit(data_set, split = "" "") #tokenize sentences
words
##[[1]]
##[1] ""the"" ""big"" ""dog""
##
##[[2]]
##[1] ""the""   ""small"" ""cat""  
##
##[[3]]
##[1] ""the"" ""big"" ""and"" ""fat"" ""cow""


vec &lt;- unique(unlist(words)) #vector representation of sentences
##[1] ""the""   ""big""   ""dog""   ""small"" ""cat""   ""and""  
##[7] ""fat""   ""cow"" 

m &lt;- matrix(nrow = length(data_set), ncol = length(vec))

for (i in 1:length(words)) { #iterate the index of tokenized sentences
  vec_rep &lt;- vec %in% words[[i]] #create binary word-feature vector
  m[i,] &lt;- vec_rep #update matrix
}

df &lt;- data.frame(m, row.names = NULL)
names(df) &lt;- vec
df
##   the   big   dog small   cat   and   fat   cow
##1 TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE
##2 TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE
##3 TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE
</code></pre>

<p>Sometimes you can increase performance by adding <a href=""https://en.wikipedia.org/wiki/Bigram"" rel=""nofollow"">bi-gram</a> and tri-gram features. </p>

<pre><code>##given: ""the big dog""
##unigrams &lt;- {the, big, dog}
##bigrams &lt;- {the big, big dog}
##trigrams &lt;- {the big dog}
</code></pre>

<p>Sometimes weighting words by their frequency improves performance or computing the <a href=""http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/"" rel=""nofollow"">tf-idf</a>. </p>

<p>Another way to increase performance, in my experience, has been custom language feature engineering, especially if the data is from social media sources replete with spelling errors, acronyms, slang, and other word variants. Standard NLP approaches will typically remove <a href=""http://datascience.stackexchange.com/questions/5893/how-to-create-a-good-list-of-stopwords?rq=1"">stop words</a> (e.g., the, a/an, this/that, etc.) from vector representations (because closed class, high frequency words often don't help discriminate among class/label boundaries). Because vector representations are typically highly dimensional (approximately num of unique words in the corpus/data set), dimensionality reductions techniques can increase performance. For example, one can compute chi-sq, info gain, etc. on a word feature's distribution across classes -- only keep those features/words above some threshold or below some pre-established p value). </p>
"
"['r', 'statistics', 'correlation']",r - How to determine the correlation between unordered categorical variables and individuals?,"<p>If you have data sets $X_1,\cdots,X_n$ and $Y_1,\cdots,Y_n$, then you can compute their correlation with the following formula:</p>

<p>$$Cor(X,Y) = \frac{\sum (X_i-\bar{X})(Y_i-\bar{Y})}{\sqrt{\sum (X_i-\bar{X})^2\sum(Y_i-\bar{Y})^2}}$$</p>

<p>(where $\bar{X}$ denotes the average value of the $X_i$'s).  This is accomplished in $R$ with the following command:</p>

<pre><code>cor(x,y)
</code></pre>

<p>That being said, it is unclear what two data sets you are trying to find the correlation for.  Finding the correlation between a type (A,B,C) and a condition (copper, gold, etc.) would not make any sense.  You could, however, find the correlation between two different types (A and B, for example), or between conditions (copper and gold).</p>

<p><strong>Edit</strong>: I think you might want to do a test for independence between categorical variables...if this is the case then <a href=""http://www.r-tutor.com/elementary-statistics/goodness-fit/chi-squared-test-independence"" rel=""nofollow"">this</a> is what you are looking for.</p>
"
"['machine-learning', 'neuralnetwork', 'online-learning']",is neural networks an online algorithm by nature?,"<p>You can train after each example, or after each epoch. This is the difference between stochastic gradient descent, and batch gradient descent. See pg 84 of Sebastian Raschka's Python Machine Learning book for more.</p>
"
"['machine-learning', 'regression', 'feature-selection', 'feature-construction', 'missing-data']",What to do when testing data has less features than training data?,"<p>Use the extra features for unsupervised learning. You might enjoy Vladimir Vapnik's take on this in the context of SVMs, which he calls privileged learning: <a href=""http://research.microsoft.com/apps/video/default.aspx?id=259655&amp;l=i"" rel=""nofollow"">Learning with Intelligent Teacher: Similarity Control and Knowledge Transfer</a></p>
"
"['machine-learning', 'r', 'predictive-modeling']",Combining data sets without using row.name,"<p>You need neither to use the row names or to create an additonal ID column. Here is an approach based on the indices of the training set.</p>

<p>An example data set:</p>

<pre><code>set.seed(1)
dat &lt;- data.frame(Y = rnorm(10),
                  X1 = rnorm(10),
                  X2 = rnorm(10),
                  Z1 = rnorm(10),
                  Z2 = rnorm(10))
</code></pre>

<p>Now, your steps:</p>

<ol>
<li><p>Create an analysis data set without the <code>Z</code> variables</p>

<pre><code>dat2 &lt;- dat[grep(""Z"", names(dat), invert = TRUE)]
dat2
#             Y          X1          X2
# 1  -0.6264538  1.51178117  0.91897737
# 2   0.1836433  0.38984324  0.78213630
# 3  -0.8356286 -0.62124058  0.07456498
# 4   1.5952808 -2.21469989 -1.98935170
# 5   0.3295078  1.12493092  0.61982575
# 6  -0.8204684 -0.04493361 -0.05612874
# 7   0.4874291 -0.01619026 -0.15579551
# 8   0.7383247  0.94383621 -1.47075238
# 9   0.5757814  0.82122120 -0.47815006
# 10 -0.3053884  0.59390132  0.41794156
</code></pre></li>
<li><p>Break this data set into random training and test sets</p>

<pre><code>train_idx &lt;- sample(nrow(dat2), 0.8 * nrow(dat2))
train_idx
# [1]  7  4  3 10  9  2  1  5

train &lt;- dat2[train_idx, ]
train
#             Y          X1          X2
# 7   0.4874291 -0.01619026 -0.15579551
# 4   1.5952808 -2.21469989 -1.98935170
# 3  -0.8356286 -0.62124058  0.07456498
# 10 -0.3053884  0.59390132  0.41794156
# 9   0.5757814  0.82122120 -0.47815006
# 2   0.1836433  0.38984324  0.78213630
# 1  -0.6264538  1.51178117  0.91897737
# 5   0.3295078  1.12493092  0.61982575

test_idx &lt;- setdiff(seq(nrow(dat2)), train_idx)
test_idx
# [1] 6 8

test &lt;- dat2[test_idx, ]
test
#            Y          X1          X2
# 6 -0.8204684 -0.04493361 -0.05612874
# 8  0.7383247  0.94383621 -1.47075238
</code></pre></li>
<li><p>Find my best model</p>

<p>...</p></li>
<li><p>Predict on both the training and test sets using this model</p>

<p>...</p></li>
<li><p>Recombine the training and test sets by rows</p>

<pre><code>idx &lt;- order(c(train_idx, test_idx))
dat3 &lt;- rbind(train, test)[idx, ]
identical(dat3, dat2)
# [1] TRUE
</code></pre></li>
<li><p>Recombine these data with the <code>Z</code> variables, by column</p>

<pre><code>dat4 &lt;- cbind(dat3, dat[grep(""Z"", names(dat))])
identical(dat, dat4)
# [1] TRUE
</code></pre></li>
</ol>

<p>In summary, we can use the indices of the training and test data to combine the data in the rows in the original order.</p>
"
"['r', 'predictive-modeling', 'scoring']",R in production,"<p>Speed of code execution is rarely an issue.  The important speed in business is almost always the speed of designing, deploying, and maintaining the application.  An experienced programmer can optimize where necessary to get code execution fast enough.  In these cases, R can make a lot of sense in production. </p>

<p>In cases where speed of execution IS an issue, you are already going to find an optimized C++ or some such real-time decision engine.  So your choices are integrate an R process, or add the bits you need to the engine.  The latter is probably the only option, not because of the speed of R, but because you don't have the time to incorporate any external process.  If the company has nothing to start with, I can't imagine everyone saying ""let's build our time critical real-time engine in R because of the great statistical libraries"".  </p>

<p>I'll give a few examples from my corporate experiences, where I use R in production:</p>

<ul>
<li>Delivering Shiny applications dealing with data that is not/ not yet institutionalized.  I will generally load already-processed data frames and use Shiny to display different graphs and charts.  Computation is minimal.   </li>
<li>Decision making analysis that requires heavy use of advanced libraries (mcclust, machine learning) but done on a daily or longer time-scale.  In this case there is no reason to use any other language.  I've already done the prototyping in R, so my fastest and best option is to keep things there.</li>
</ul>

<p>I did not use R for production when integrating with a real-time C++ decision engine.  Issues:</p>

<ul>
<li>An additional layer of complication to spawn R processes and integrate the results</li>
<li>A suitable machine-learning library (Waffles) was available in C++</li>
</ul>

<p>The caveat in the latter case: I still use R to generate the training files.</p>
"
"['statistics', 'predictive-modeling']",Interested in Mathematical Statistics... where to start from?,"<p>When looking for texts to learn advanced topics, I start with a web search for relevant grad courses and textbooks, or background tech/math books like those from Dover.</p>

<p>To wit, Theoretical Statistics by Keener looks relevant:
<a href=""http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-93838-7"" rel=""nofollow"">http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-93838-7</a></p>

<p>And this:
""Looking for a good Mathematical Statistics self-study book (I'm a physics student and my class &amp; current book are useless to me)""
<a href=""http://www.reddit.com/r/statistics/comments/1n6o19/looking_for_a_good_mathematical_statistics/"" rel=""nofollow"">http://www.reddit.com/r/statistics/comments/1n6o19/looking_for_a_good_mathematical_statistics/</a></p>
"
"['machine-learning', 'neuralnetwork', 'nlp', 'rnn']",What is the difference between word-based and char-based text generation RNNs?,"<p>Here is what I learnt recently.</p>

<p>Obviously, when talking about text generation RNNs we are talking about RNN language models. When asking about <strong>word/char-based</strong> text generation RNNs, we are asking about <strong>word/char-based</strong> RNN language models (LM).</p>

<p><strong>Word-based</strong> LMs display <strong>higher accuracy</strong> and <strong>lower computational cost</strong> than <strong>char-based</strong> LMs. </p>

<blockquote>
  <p><a href=""http://arxiv.org/abs/1511.06303"" rel=""nofollow"">This drop of performance is <strong>unlikely</strong> due to the difficulty for character level model to capture longer short term memory, since also the Longer Short Term Memory (LSTM) recurrent networks work better with word-based input.</a></p>
</blockquote>

<p>This is because char-based RNN LMs require much bigger hidden layer to successfully model long-term dependencies which means higher computational costs.</p>

<p>Therefore, we can say that </p>

<blockquote>
  <p><a href=""http://arxiv.org/abs/1511.06303"" rel=""nofollow"">one of the <strong>fundamental differences</strong> between the <strong>word level</strong> and <strong>character level</strong> models is in <strong>the number of parameters</strong> the RNN has to access during the training and test. The smaller is the input and output layer of RNN, the larger needs to be the fully connected hidden layer, which makes the training of the model expensive.</a></p>
</blockquote>

<p>However, <strong>char-based</strong> RNN LMs better model languages with a rich morphology such as Finish, Turkish, Russian etc. Using <strong>word-based</strong> RNN LMs to model such languages is difficult if possible at all and is not advised.</p>

<p>The above analysis makes sense especially when you look at the output text, generated by <strong>char-based</strong> RNNs:</p>

<blockquote>
  <p><a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow""><em>The surprised in investors weren’t going to raise money. I’m not the company with the time there are all interesting quickly, don’t have to get off the same programmers.</em></a></p>
</blockquote>

<p>While simple <a href=""http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139"" rel=""nofollow"">char-based Maximum Likelihood LM</a> with a 13-character window delivers this:</p>

<blockquote>
  <p><em>And when she made many solid bricks. He stacked them in piles and stomped her
  feet. The doctor diagnosed him with a bat. The girl and her boyfriend asked
  her out.</em></p>
</blockquote>

<p>Of course I cherry-picked the example (actually most ML LM examples looked better than any RNN generated text I've read so far) and this tiny ML LM was trained on a simpler corpus but you get the idea: straightforward <em>conditional probability</em> generates better texts than far more complex <strong>char-based</strong> RNN.</p>

<p><strong>Char-based</strong> RNN LMs can mimic grammatically correct sequences for a wide range of languages, require bigger hidden layer and computationally more expensive while <strong>word-based</strong> RNN LMs train faster and generate more coherent texts and yet even these generated texts are far from making actual sense.</p>
"
"['machine-learning', 'r', 'clustering', 'decision-trees']",How to retrieve the clustering results of rpart,"<p>I found an answer to my question using the package partykit, I post it to help others.</p>

<pre><code>tree_party &lt;- as.party(tree) #coercing the rpart.object into partykit

tree_fit &lt;- fitted(tree_party) #a two-column data.frame with the fitted node numbers and the observed responses on the training data.

tree_pred &lt;- predict(tree_party, newdata = data, type = ""node"")
</code></pre>
"
"['data-mining', 'clustering', 'k-means']",K-means: What are some good ways to choose an efficient set of initial centroids?,"<p>An approach that yields more consistent results is <a href=""http://en.wikipedia.org/wiki/K-means%2B%2B"">K-means++</a>.  This approach acknowledges that there is probably a better choice of initial centroid locations than simple random assignment. Specifically, K-means tends to perform better when centroids are seeded in such a way that doesn't clump them together in space. </p>

<p>In short, the method is as follows:</p>

<ol>
<li>Choose one of your data points at random as an initial centroid.</li>
<li>Calculate $D(x)$, the distance between your initial centroid and all other data points, $x$.</li>
<li>Choose your next centroid from the remaining datapoints with probability proportional to $D(x)^2$</li>
<li>Repeat until all centroids have been assigned.</li>
</ol>

<p>Note: $D(x)$ should be updated as more centroids are added.  It should be set to be the distance between a data point and the nearest centroid.</p>

<p>You may also be interested to read <a href=""http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf"">this paper</a> that proposes the method and describes its overall expected performance.</p>
"
"['machine-learning', 'classification', 'bigdata', 'dataset']",What types of features are used in a large-scale click-through rate prediction problem?,"<p>That really is a nice question, although once you're Facebook or Google etc., you have the opposite problem: how to reduce the number of features from many billions, to let's say, a billion or so.</p>

<p>There really are billions of features out there.</p>

<p>Imagine, that in your feature vector you have billions of possible phrases that the user could type in into search engine. Or, that you have billions of web sites a user could visit. Or millions of locations from which a user could log in to the system. Or billions of mail accounts a user could send mails to or receive mails from.</p>

<p>Or, to swich a bit to social networking site-like problem. Imagine that in your feature vector you have billions of users which a particular user could either know or be in some degree of separation from. You can add billions of links that user could post in his SNS feed, or millions of pages a user could 'like' (or do whatever the SNS allows him to do).</p>

<p>Similar problems may be found in many domains from voice and image recognition, to various branches of biology, chemistry etc. I like your question, because it's a good starting point to dive into the problems of dealing with the abundance of features. Good luck in exploring this area!</p>

<p><strong>UPDATE due to your comment:</strong></p>

<p>Using features other than binary is just one step further in imagining things. You could somehow cluster the searches, and count frequencies of searches for a particular cluster.</p>

<p>In a SNS setting you could build a vector of relations between users defined as degree of separation instead of a mere binary feature of being or not being friends.</p>

<p>Imagine logs that global corporations are holding on millions of their users. There's a whole lot of stuff that can be measured in a more detailed way than binary.</p>

<p>Things become even more complicated once we're considering an online setting. In such a case you do not have time for complicated computations and you're often left with binary features since they are cheaper.</p>

<p>And no, I am not saying, that the problem becomes tractable once it's reduced to a magical number of billion features. I am only saying that a billion of features is something you may end up after a lot of effort in reducing the number of dimensions.</p>
"
"['r', 'gradient-descent']",Gradient descent parameter estimation Package for R,"<p>Ok, after a lot of looking I found the ""optim"" routine which is in ""stats"", one of the packages that is always loaded. It has quite a few methods including conjugate gradients and BGGS and a few others and worked well on the first few examples I tried. It doesn't seem to get a lot of attention strangely. I guess optimization people tend to use Matlab.</p>

<p>I knew there had to be something.</p>
"
"['python', 'neuralnetwork', 'keras']",MLP on Iris Data not working but it does fine on MNIST - Keras,"<p>By default <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html"" rel=""nofollow"">sklearn.preprocessing.normalize</a> normalizes samples, not features. Replace sklearn.preprocessing.normalize with <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html"" rel=""nofollow"">sklearn.preprocessing.scale</a>. This will center and scale (to unit variance) every feature.</p>

<p>Also give it more than 10 epochs. Here are learning curves (log loss) for 5000 epochs:</p>

<p><a href=""http://i.stack.imgur.com/9ME3N.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9ME3N.png"" alt=""Learning Curves""></a></p>

<p>This should end up with an accuracy of about 96%.</p>
"
"['machine-learning', 'scalability', 'sklearn', 'apache-spark']",Scalable open source machine learning library written in python,"<p>Is there a specific reason beside the fact that you would like to contribute? I am asking because there is always <a href=""https://github.com/apache/spark/tree/master/python/pyspark/mllib"" rel=""nofollow"">pyspark</a> that you can use, the Spark python API that exposes the Spark programming model to Python.</p>

<p>For deep learning specifically, there are a lot of frameworks built on top of <a href=""https://github.com/Theano/Theano"" rel=""nofollow"">Theano</a> -which is a python library for mathematical expressions involving multi-dimensional arrays-, like Lasagne, so they are able to use GPU for intense training. Getting an EC2 instance with GPU on AWS is always an option.</p>
"
"['machine-learning', 'classification']",Adjusting Probabilities When Using Alternative Cutoffs For Classification,"<p>First of all, you cannot always consider what a machine learning algorithm outputs as a ""probability"". Logistic regression outputs a sigmoid activation on a <code>(0, 1)</code> scale, but that doesn't magically make it so! </p>

<p>We simply often scale things to a <code>(0, 1)</code> scale in ML as a measure of <strong>confidence</strong>. </p>

<p>Also in your example, if the events are mutually exclusive (like classification), just think of them as ""event 1"" and ""NOT event 1"". Something like <code>p(e1) + p(~e1) = 1</code>. </p>

<p>So when your book tells you to lower the threshold, it is simply saying that you require a smaller level of <strong>confidence</strong> to choose e1 over e2. This doesn't mean you are choosing one with smaller likelihood, you are simply making a conscious choice to adjust your <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow"">precision-recall curve</a>. </p>

<p>There are other ways to combat class imbalance, but changing the threshold to be more sensitive to any indication of <strong>confidence</strong> of one class over another is certainly a way to do that. </p>
"
"['neuralnetwork', 'matlab']","Does the network learn based on previous training or does it restart? Matlab, neuralnetworks","<p>It trains again based on what it learned the first time you did <code>OP = train(OP,inputsVals,targetVals)</code>. More generally, <code>train</code> uses your network's weights, i.e. it does not initialize the weights. The weight initialization happens in <code>feedforwardnet</code>.</p>

<p>Example:</p>

<pre><code>% To generate reproducible results 
% http://stackoverflow.com/a/7797635/395857
rng(1234,'twister')

% Prepare input and target vectors
[x,t] = simplefit_dataset;

% Create ANN
net = feedforwardnet(10);

% Loop to see where train() initializes the weights
for i = 1:10

    % Learn
    net.trainParam.epochs = 1;
    net = train(net,x,t);

    % Score
    y = net(x);
    perf = perform(net,y,t)
end
</code></pre>

<p>yields</p>

<pre><code>perf =

    0.4825


perf =

    0.0093


perf =

    0.0034


perf =

    0.0034


perf =

    0.0034


perf =

    0.0034


perf =

    0.0034


perf =

    0.0034


perf =

    0.0034


perf =

    0.0028
</code></pre>
"
"['dataset', 'data-cleaning']",How do I factor in features which are IDs?,"<p>These IDs should not be represented as numerical values to your model. If you would, your model thinks 2 and 3 are closer together than 2 and 2000, while it's just an ID, the number is just a name.</p>

<p>Some models can deal with them but then they need to be factors or categories (like decision trees). However most models cannot deal with categories at all, there are however numerous ways to solve this problem. The most used one is probably one-hot encoding, which means for every category in your feature you add a column, and you put a 1 if it's that category and a 0 otherwise. Example:</p>

<pre><code>ID | target
1  | 0
1  | 1
2  | 3
3  | 2
</code></pre>

<p>To:</p>

<pre><code>ID_1 | ID_2 | ID_3 | target
1    | 0    | 0    | 0
1    | 0    | 0    | 1
0    | 1    | 0    | 3
0    | 0    | 1    | 2
</code></pre>

<p>This work very well if you have few categories, however in the case of thousands of IDs this will increase your dimensionality too much. What you can do is collect statistics about the target and other features per group and join these onto your set and then remove your categories. This is what is usually done with a high number of categories. You have to be careful not to leak any information about your target into your features though (problem called label leaking).</p>
"
"['bigdata', 'machine-learning', 'databases', 'clustering', 'data-mining']",Human activity recognition using smartphone data set problem,"<p>The data set definitions are on the page here:</p>

<p><a href=""http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones#"">Attribute Information at the bottom</a></p>

<p>or you can see inside the ZIP folder the file named activity_labels, that has your column headings inside of it, make sure you read the README carefully, it has some good info in it. You can easily bring in a <code>.csv</code> file in R using the <code>read.csv</code> command.</p>

<p>For example if you name you file <code>samsungdata</code> you can open R and run this command:</p>

<pre><code>data &lt;- read.csv(""directory/where/file/is/located/samsungdata.csv"", header = TRUE)
</code></pre>

<p>Or if you are already inside of the working directory in R you can just run the following</p>

<pre><code>data &lt;- read.csv(""samsungdata.csv"", header = TRUE)
</code></pre>

<p>Where the name <code>data</code> can be changed to whatever you want to call your data set.</p>
"
"['statistics', 'predictive-modeling', 'regression']",How to fit an odd relationship with a function?,"<p>The definition you gave <em>is</em> the definition of the function. This is called the <a href=""http://en.wikipedia.org/wiki/Heaviside_step_function"" rel=""nofollow"">Heaviside Step Function</a>. There is not a simple <em>analytic</em> way to express it (like as a ratio, product, or composition  of trigonometric functions, exponentials, or polynomials). Note that it is neither continuous nor differentiable at x = 0. </p>

<p>There are a couple of cool ways to represent it. The coolest and most intuitive way is as an integral of a <a href=""http://en.wikipedia.org/wiki/Dirac_delta_function"" rel=""nofollow"">Dirac Delta Function</a>:</p>

<p>$$ 
H(x) = \int_{-\infty}^x { \delta(s)} \, \mathrm{d}s 
$$</p>

<p>Note, though, that a Dirac Delta Function is itself not an ""official"" function, since it is not well-defined at x = 0. Check out <a href=""http://en.wikipedia.org/wiki/Distribution_(mathematics)"" rel=""nofollow"">Distribution Theory</a> for some cool info on weird ""functions"" like this.</p>

<p>Now, I think you may be trying to approximate this function, because you asked how to ""fit"" it. Taken straight from Wikipedia:</p>

<pre><code>For a smooth approximation to the step function, one can use the logistic   function
</code></pre>

<p>$$
H(x) \approx \frac{1}{2} + \frac{1}{2}\tanh(kx) = \frac{1}{1+\mathrm{e}^{-2kx}},
$$</p>

<pre><code>where a larger k corresponds to a sharper transition at x = 0.
</code></pre>
"
"['r', 'visualization']",visualize a horizontal box plot in R,"<p>Let's start by creating some fake dataset. </p>

<pre><code>software = sample(c(""Windows"",""Linux"",""Mac""), n=100, replace=T) 
salary = runif(n=100,min=1,max=100) 
test = data.frame(software, salary)
</code></pre>

<p>This should create a dataframe <code>test</code> that will look like somewhat like: </p>

<pre><code>    software    salary
1    Windows 96.697217
2      Linux 29.770905
3    Windows 94.249612
4        Mac 71.188701
5      Linux 94.028326
6      Linux  7.482632
7        Mac 98.841689
8        Mac 81.152623
9    Windows 54.073761
10   Windows  1.707829
</code></pre>

<p><strong>EDIT based on comment</strong> Note that if the data does not already exist in the above format, it can be changed to this format. Let's take a data frame provided in the original question and lets assume the dataframe is called <code>raw_test</code>.</p>

<pre><code>    windows sql excel salary
1     yes  no   yes    100
2      no  yes  yes    200
3     yes  no   yes    300
4     yes  no    no    400
5      no  no   yes    500
</code></pre>

<p>Now, using the <code>melt</code> function/ method from the <code>reshape</code> package in <code>R</code>, first create the dataframe <code>test</code> (that will be used for final plotting) as follows: </p>

<pre><code># use melt to convert from wide to long format 
test = melt(raw_test,id.vars=c(""salary""))
# subset to only select where value is ""yes""
test = subset(test, value == 'yes')
# replace column name from ""variable"" to ""software"" 
names(test)[2] = ""software""   
</code></pre>

<p>Now, you will get a datframe <code>test</code> that looks like: </p>

<pre><code>  salary software value
1     100  windows   yes
3     300  windows   yes
4     400  windows   yes
7     200      sql   yes
11    100    excel   yes
12    200    excel   yes
13    300    excel   yes
15    500    excel   yes
</code></pre>

<p>Having created the dataset. We will now generate the plot. </p>

<p>First, create the bar plot on the left based on the counts of software that represents usage rate. </p>

<pre><code>p1 &lt;- ggplot(test, aes(factor(software))) + geom_bar() + coord_flip()
</code></pre>

<p>Next, create the boxplot on the right. </p>

<pre><code>p2 &lt;- ggplot(test, aes(factor(software), salary)) + geom_boxplot() + coord_flip()
</code></pre>

<p>Finally, place both these plots next to each other. </p>

<pre><code>require('gridExtra')
grid.arrange(p1,p2,nrow=1)
</code></pre>

<p>This should create a plot like: </p>

<p><img src=""http://i.stack.imgur.com/AbyXC.png"" alt=""On the left pane shows the counts of how different software is being used represented through bar plots and on the right pane shows the distribution of salaries grouped by software used represented through box plots.""></p>
"
"['neuralnetwork', 'convnet']",How are 1x1 convolutions the same as a fully connected layer?,"<h1>Your Example</h1>

<p>In your example we have 3 input and 2 output units. To apply convolutions, think of those units having shape: <code>[1,1,3]</code> and <code>[1,1,2]</code>, respectively. In CNN terms, we have <code>3</code> input and <code>2</code> output feature maps, each having spatial dimensions <code>1 x 1</code>. </p>

<p>Applying an <code>n x n</code> convolution to a layer with <code>k</code> feature maps, requires you to have a kernel of shape <code>[n,n,k]</code>. Hence the kernel of you <code>1x1</code> convolutions have shape <code>[1, 1, 3]</code>. You need <code>2</code> of those kernels (or filters) to produce the <code>2</code> output feature maps. Please Note: $1 \times 1$ convolutions really are  $1 \times 1 \times \text{number of channels of the input}$ convolutions. The last one is only rarely mentioned.</p>

<p>Indeed if you choose as kernels and bias:<br>
$$
\begin{align}
w_1 &amp;= 
\begin{pmatrix}
 0 &amp; 1 &amp; 1\\
\end{pmatrix} \in \mathbb{R}^{3}\\
w_2 &amp;= 
\begin{pmatrix}
2 &amp; 3 &amp; 5\\
\end{pmatrix} \in \mathbb{R}^{3}\\
b &amp;= \begin{pmatrix}8\\ 13\end{pmatrix} \in \mathbb{R}^2
\end{align}
$$</p>

<p>The conv-layer will then compute $f(x) = ReLU\left(\begin{pmatrix}w_1 \cdot x\\ w_2 \cdot x\end{pmatrix} + \begin{pmatrix}b_1\\ b_2\end{pmatrix}\right)$ with $x \in \mathbb{R}^3$. </p>

<h1>Transformation in real Code</h1>

<p>For a real-life example, also have a look at my <a href=""https://github.com/MarvinTeichmann/tensorflow-fcn/blob/d04bc268ac6e84f03afc4332d7f54ecff22d1732/fcn32_vgg.py"" rel=""nofollow"">vgg-fcn</a> implementation. The Code provided in this file takes the VGG weights, but transforms every fully-connected layer into a convolutional layers. The resulting network yields the same output as <code>vgg</code> when applied to input image of shape <code>[244,244,3]</code>. (When applying both networks without padding).  </p>

<p>The transformed convolutional layers are introduced in the function <code>_fc_layer</code> (line 145). They have kernel size <code>7x7</code> for FC6 (which is maximal, as <code>pool5</code> of VGG outputs a feature map of shape <code>[7,7, 512]</code>. Layer <code>FC7</code> and <code>FC8</code> are implemented as <code>1x1</code> convolution.</p>

<h1>""Full Connection Table""</h1>

<p>I am not 100% sure, but he might refer to a filter/kernel which has the same dimension as the input feature map. In both cases (Code and your Example) the spatial dimensions are maximal in the sense, that the spatial dimension of the filter is the same as the spatial dimension as the input.</p>
"
"['machine-learning', 'predictive-modeling', 'feature-extraction']",Smoothing Proportions :: Massive User Database,"<p>A simple way would be to consider Laplace Smoothing (<a href=""http://en.wikipedia.org/wiki/Additive_smoothing"" rel=""nofollow"">http://en.wikipedia.org/wiki/Additive_smoothing</a> ) or something like it. </p>

<p>Basically, instead of calculating your response rate as (Clicks)/(Impressions) you calculate (Clicks + X)/(Impressions + Y), with X and Y chosen, for example, so that X/Y is the global average of clicks/impressions. </p>

<p>When Clicks and Impressions are both high, this smoothed response rate is basically equal to the true response rate (signal dominates the prior). When Clicks and Impressions are both low, the this smoothed response rate will be close to the global average response rate - a good guess when you have little data and don't want to put much weight on it!</p>

<p>The absolute scale of X and Y will determine how many data points you consider ""enough data"". It's been argued that the right thing to do is set X to 1, and Y appropriately given that. </p>
"
"['data-mining', 'predictive-modeling', 'visualization', 'scikit', 'decision-trees']",Decision Tree generating leaves for only one case,"<p>You are predicting binary class: 0 - sale failed, 1 - sale succeed. In the decision tree, the information <code>value</code> is sorted. So for example in the first node, you have 35011 predictions of 0 and 1785 predictions of class 1.</p>

<p>But then, in your code you have this: <code>class_names=[""Won"",""Lost""])</code>. So you are telling your decision tree that name of class 0 is ""Won"" and name of class 1 is ""Lost"". I assume it should be inverted.</p>

<p>So in fact, the model always predict 0 - sale failed. Which seems reasonable as you said that out of 38000 samples, only 1700 are class 1.</p>

<p>Further, your dataset is highly unbalanced. So when cross validation gives accuracy of 95%, it says nothing, because only if your model gives all prediction 0, you will have accuracy of...95%.
For those cases, use models which can put weights to classes (SVM, Logistic regression or Random Forest classifier in scikit have this possibility). Think about undersample your dataset. Or add more of class 1 - dig in your database or do it artificially (SMOTE),</p>

<p>Plot confusion matrix and give CV different score measure - precision or recall, depends on your situation.</p>

<p>Last thing, don't expect that your decision tree with depth of 3 will perform well. It is nice to show to business, but it gives poor prediction.</p>
"
"['machine-learning', 'dataset', 'recommendation']",Machine learning for Point Clouds Lidar data,"<p>First, CNNs are great for image recognition, where you usually take sub sampled windows of about 80 by 80 pixels, 27,000 input neurons is too large and it will take you forever to train a CNN on that.</p>

<p>Furthermore, why did you choose CNN? Why don't you try some more down to earth algorithms fisrst? Like SVMs, or Logistic regressions.</p>

<p>4500 Data points and 27000 features seems unrealistic to me, and very prone to over fitting.</p>

<p>Check this first.</p>

<p><a href=""http://scikit-learn.org/stable/tutorial/machine_learning_map/"" rel=""nofollow"">http://scikit-learn.org/stable/tutorial/machine_learning_map/</a></p>
"
['machine-learning'],How to compute F1 score?,"<p>First you need to learn about Logistic Regression, it is an algorithm that will assign weights to different features given some training data. Read the wiki intro, is quite helpful, basically the Betas there are the same as the Ws in the paper. </p>

<p>The formula you have is correct, and those value do seem off. It also depends on the number of significant figures you have, perhaps they are making their calculations with more than the ones they are reporting.</p>

<p>But honestly, you can't understand much of the paper unless you understand LR</p>
"
"['machine-learning', 'prediction']",Prediction with non-scalar output (label),"<p>Neural Networks can have a vector or matrix as output layer, image segmentation is a well researched topic and deep learning (as most things concerning images) are the state-of-the-art. You will need (a lot of) training examples where the pattern is found, and where. This could be a bounding box, or per pixel if it is part of the pattern or not (this will generate a matrix equally sized to your input). To see if the pattern is found you could construct a second network that is just a binary classifier, or you could try to see if your pixel-based network will output almost only zeros in case of no pattern. In this case you will need negative examples as well.</p>
"
"['classification', 'cross-validation']",Which cross-validation type best suits to binary classification problem,"<p>You will have best results if you care to build the folds so that each variable (and most importantly the target variable) is approximately identically distributed in each fold. This is called, when applied to the target variable, stratified k-fold. One approach is to cluster the inputs and make sure each fold contains the same number of instances from each cluster proportional to their size.</p>
"
['ensemble-modeling'],Stacked features not helping,"<p>As far as I understood stacking does not add features to the original data set. The point is to train several models on the training data and use their predictions on training data as input features to another model. </p>

<p>First such kind of construction used logistic regression as a final ensemble and and class probabilities from each base learner as input features. Now, what I have described is a technical layout, the intuition behind is the following: considering that there are no models which are good over all joint probability space of features, one can combine their results in order to get the best from each one. In other words we can state the we explore the richness of models (seen as function spaces) to get a combined thing. This strategy does not work always but often it works. </p>

<p>I think you do something wrong. I think is better to use original features only for base learners. Be careful to use scores or probabilities if possible, instead of final classifications from base learners, it gives more space for improvements. Often is better to stack learners from different families, not the same model with different parameters (better to use a gradient boost and random forest than two gradient boosts). All of those advices are not rules which cannot be broken, and even if you take them all there is no guarantee that there will be improvements.  </p>
"
"['machine-learning', 'predictive-modeling', 'apache-spark']",How to start prediction from dataset?,"<p>The problem you are facing is a <a href=""https://en.wikipedia.org/wiki/Time_series"" rel=""nofollow"">time series</a> problem.
Your events are categorial which is a specific case (so most common techniques like <a href=""https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average"" rel=""nofollow"">arima</a> and <a href=""https://en.wikipedia.org/wiki/Fourier_transform"" rel=""nofollow"">Fourier transform</a> are irrelevant).</p>

<p>Before getting into the analysis, try to find out whether the events among nodes are independent. If they are independent, you can break them into sequences per node and analyze them. If they are not (e.g., ""Main power supply has a fault alarm"" on node x indicates the same event on node y) you should the combined sequence. Sometime even when the the sequence are dependent you can gain from using the per node sequence as extra data.</p>

<p>You data set is quite large, which means that computation will take time. You data is probably noisy, so you will probably have some mistakes. Therefore, I recommend advancing in small steps from simple models to more complex ones.</p>

<p>Start with descriptive statistics, just to explore the data. How many events do you have? How common are they? What is the probability of the events that you try to predict? Can you remove some of the events as meaningless (e.g., by using domain knowledge)?</p>

<p>In case you have domain knowledge that indicates that recent events are the important ones, I would have try predicting based on the n last events. Start with 1 and grow slowly since the number of combinations will grow very fast and the number of samples you will have for each combination will become small and might introduce errors.</p>

<p>Incase that the important event are not recent, try to condition on these events in the past.</p>

<p>In most cases such simple model will help you get a bit above the baseline but not too much. Then you will need more complex models. I recommend using <a href=""https://en.wikipedia.org/wiki/Association_rule_learning"" rel=""nofollow"">association rules</a> that fit your case and have plenty of implementations.</p>

<p>You can further advance more but try these technique first.</p>

<p>The techniques mentioned before will give you a model the will predict the probability that a node will be down, answering your question (ii). Running it on the sequence of the nodes will enable you to predict the number of nodes that will fail answering question (i).</p>
"
"['neuralnetwork', 'terminology']",Looking for a rough explanation of additive hidden nodes and radial basis functions,"<p>Albeit not wrong, Huang seems to be the only person in the world to use the term ""additive hidden nodes"". By this, he means that the neuron computes the sum of weighted inputs. In other words, the kind of neural network you're already used to.</p>

<p>An RBF neuron, on the other hand, computes a distance (usually the Euclidean distance) from input to some center (which can be thought as the weights if you see them as a vector) and applies a <code>exp(-dist²)</code> function in order to obtain a Gaussian activation. Thus, RBF neurons have maximum activation when the center/weights are equal to the inputs.
<a href=""http://i.stack.imgur.com/9u5fQ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9u5fQ.jpg"" alt=""enter image description here""></a></p>
"
"['apache-spark', 'nltk', 'pyspark']",Unable to load NLTK in spark using PySpark,"<p>It looks like you installed it only on the driver/gateway and not on the nodes/workers itself. The test you ran in the shell is running it locally, once you map a function via your SparkContext it gets distributed to the workers which don't have NLTK installed.</p>
"
"['classification', 'time-series']",One-class classifier for time series data classification,"<p>Apart from the approach @Rolf Schorpion mentioned, there are others.
For example, you could use a deep neural network, specifically, an auto-encoder (see <a href=""https://github.com/h2oai/h2o-training-book/blob/master/hands-on_training/anomaly_detection.md"" rel=""nofollow"">here</a> for a tutorial).  </p>

<p>But there's an important catch to all purely ""data-driven"" approaches: if the figure of 30 time series you mention in the comments is a typical order of magnitude for your training set, the results will be more or less arbitrary. If you don't define ""accordance"" in any way, this is a classification problem with only positive training data which consists of 30 data points with more than 100 features each. </p>

<p>Unless your data is very, very special (e.g., all time series are identical), there is just too much freedom in the solution of this problem. Different algorithms (or different parameter sets for the same algorithm) will use this freedom in very different ways. So you will probably see very different solutions when you experiment with different methods. </p>

<p>So if you don't want to do more or less arbitrary experiments and choose from the solutions afterwards, you have to use a method in which you somehow define the meaning of ""in accordance"" in advance. You may try and use traditional time series techniques to find a common model that fits each of the time series well. You could then postulate that ""accordance"" is a good fit to this model. </p>

<p>Or you might just do some exploratory analysis of your data to decide upon a simple rule which could be used to detect accordance. There's lots of possibilities, and without further details on the application it's difficult to decide which one is appropriate. Delegating the decision of what's ""in accordance"" to some algorithm doesn't seem to be a good choice in this case, though.</p>
"
"['machine-learning', 'data-mining', 'predictive-modeling', 'recommender-system']",recommendation system for eCommerce healthcare portal suggestion,"<p>Your recommendation system will be designed to tell the customer what product they should choose, however, this doesn't account for what products the customer likes.  A ML method could take all the input parameters from the recommendation system and provide a recommended product based on what similar users liked.</p>

<p>There's no specific ML technique for considering HIPPA constraints.  This would come into play more in the pre-processing stage.  For example, you might not be able to acquire specific birth dates and addresses, but maybe you can get an age range and a zip code (first 3 digits only).</p>

<p>Logistic regression is frequently used in healthcare for binary classification.</p>

<p>There are lots of tutorials online for building ML models.  Kaggle has a good tutorial for Python and R using titanic survival data.</p>
"
"['machine-learning', 'bigdata']",Handling a regularly increasing feature set,"<p>In an ideal world, you retain all of your historical data, and do indeed run a new model with the new feature extracted retroactively from historical data. I'd argue that the computing resource spent on this is quite useful actually. Is it really a problem?</p>

<p>Yes, it's a widely accepted technique to build an ensemble of classifiers and combine their results. You can build a new model in parallel just on new features and average in its prediction. This should add value, but, you will never capture interaction between the new and old features this way, since they will never appear together in a classifier. </p>
"
"['tools', 'experiments']",Book keeping of experiment runs and results,"<p>you might want to look at <a href=""http://deeplearning.net/software/jobman/intro.html"" rel=""nofollow"">http://deeplearning.net/software/jobman/intro.html</a></p>

<p>it was designed for deep learning (I guess), but it is application agnostic. It is effectively an API version of SeanEasters approach</p>
"
"['classification', 'data-cleaning', 'apache-spark', 'scala', 'text']",How to down-weight non-words in text classification?,"<p>Usually, those non-sensical words are not problematic because they appear in one or two documents, and people usually just filter words with such low frequencies.</p>

<p>Are you not able to do this? Or do your non-sensical words appear that much?</p>

<p>Anyhow, your suggestion is what people use:</p>

<blockquote>
  <p>outright ignore tokens that aren't discernable by a dictionary. Is there?</p>
</blockquote>

<p>Exactly. There is no magic way to know if a word is English or not. What word processors do is to use a dictionary, as you yourself suggested.</p>

<p>In python, before stemming, you could filter based on <a href=""https://pypi.python.org/pypi/pyenchant/"" rel=""nofollow"">pyenchant</a>.</p>

<pre><code>import enchant
d = enchant.Dict(""en_US"")
d.check('hello')  # True
d.check('mmmmmmmm')  # False
</code></pre>

<p>I bet this would be good enough.</p>

<p>But, if there are a lot of false negatives, you could ask the dictionary for the most similar words and then apply a word distance measure. Here I will use <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow"">Levenshtein distance </a> as <a href=""https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python"" rel=""nofollow"">implemented in python</a>:</p>

<pre><code>&gt;&gt;&gt; r = d.suggest('helo')
&gt;&gt;&gt; r
['hole', 'help', 'helot', 'hello', 'halo', 'hero', 'hell', 'held', 'helm', 'he lo', 'he-lo', 'heel', 'loathe', 'Helios', 'helicon']
&gt;&gt;&gt; min([LD('helo', w) for w in r])
1
</code></pre>

<p>The distance will be higher for the non-sensical words you suggested:</p>

<pre><code>&gt;&gt;&gt; d.suggest('mmmmmmmm')
['mammogram', ""Mammon's""]
&gt;&gt;&gt; min([LD('mmmmmmmm', w) for w in d.suggest('mmmmmmmm')])
5
&gt;&gt;&gt; min([LD('aaannnammmmrrr', w) for w in d.suggest('aaannnammmmrrr')])
12
&gt;&gt;&gt; min([LD('hdhhdhhhhhjjj', w) for w in d.suggest('hdhhdhhhhhjjj')])
10
</code></pre>

<p>To recap:</p>

<ol>
<li>filter words that appear in less than 3 documents</li>
<li>if that's not enough or it is not a possibility for your use-case, use a dictionary</li>
</ol>

<p>I think this last suggestion of mine of using Levenshtein distance to the closest English words is overkill.</p>

<p>If your documents involve a lot of urban words or hip terms, you could also filter based on the number of Google results, instead of using a common dictionary. Your word <a href=""https://www.google.pt/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8&amp;client=ubuntu#q=hdhhdhhhhhjjj"" rel=""nofollow"">hdhhdhhhhhjjj</a>, for instance, gives me only two Google results, one of which is this stackexchange. Data science is about being creative. :)</p>

<p>There probably are heuristics or statistics based on the number of consonants in a word, and letter combinations, which you could use to probabilistic shrink your documents' dictionary, but I wouldn't go there. It will be too much work and very brittle.</p>
"
"['clustering', 'k-means']",Binning long-tailed / pareto data before clustering,"<p>There are several approaches. You can start from the second one.</p>

<p><strong> Equal-width (distance) partitioning:</strong></p>

<ul>
<li><p>It divides the range into N intervals of equal size: uniform grid</p></li>
<li><p>if A and B are the lowest and highest values of the attribute, the width of intervals will be: <code>W = (B-A)/N</code>.</p></li>
<li><p>The most straightforward
- Outliers may dominate presentation
- Skewed data is not handled well.</p></li>
</ul>

<p><strong> Equal-depth (frequency) partitioning:</strong></p>

<ul>
<li>It divides the range into 
N intervals, each containing approximately 
same number of samples</li>
<li>Good data scaling</li>
<li>Managing categorical attributes can be tricky.</li>
</ul>

<p><strong>Other Methods</strong></p>

<ul>
<li><code>Rank</code>: The rank of a number is its size relative to other values of a numerical variable. First, we sort the list of values, then we assign the position of a value as its rank. Same values receive the same rank but the presence of duplicate values affects the ranks of subsequent values (e.g., 1,2,3,3,5). Rank is a solid binning method with one major drawback, values can have different ranks in different lists.</li>
<li><code>Quantiles (median, quartiles, percentiles, ...)</code>: Quantiles are also very useful binning methods but like Rank, one value can have different quantile if the list of values changes.</li>
<li><code>Math functions</code>: For example, logarithmic binning is an effective method for the numerical variables with highly skewed distribution (e.g., income).</li>
</ul>

<p><strong>Entropy-based Binning</strong></p>

<p><a href=""http://www.saedsayad.com/supervised_binning.htm"" rel=""nofollow"">Entropy based method</a> uses a split approach. The entropy (or the information content) is calculated based on the class label. Intuitively, it finds the best split so that the bins are as pure as possible that is the majority of the values in a bin correspond to have the same class label. Formally, it is characterized by finding the split with the maximal information gain. </p>
"
"['bigdata', 'clustering', 'text-mining']",General approahces for grouping a continuous variable based on text data?,"<p>Since it is general methodological question, let's assume we have only one text-based variable - total number of words in a sentence. First of all, it's worth to <strong>visualize</strong> your data. I will pretend I have following data: </p>

<p><img src=""http://i.stack.imgur.com/KciVJ.png"" alt=""number of words vs. age""></p>

<p>Here we see slight dependency between age and number of words in responses. We may assume that young people (approx. between 12 and 25) tend to use 1-4 words, while people of age 25-35 try to give longer answers. But how do we split these points? I would do it something like this: </p>

<p><img src=""http://i.stack.imgur.com/uect9.png"" alt=""enter image description here""></p>

<p>In 2D plot it looks pretty straightforward, and this is how it works most of the time in practise. However, you asked for splitting data by a single variable - age. That is, something like this: </p>

<p><img src=""http://i.stack.imgur.com/G10GI.png"" alt=""enter image description here""></p>

<p>Is it a good split? I don't know. In fact, it depends on your actual needs and interpretation of the ""cut points"". That's why I asked about <em>concrete</em> task. Anyway, this interpretation is up to you. </p>

<p>In practise, you will have much more text-based variables. E.g. you can use every word as a feature (don't forget to <a href=""http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"" rel=""nofollow"">stem or lemmatize</a> it first) with values from zero to a number of occurrences in the response. Visualizing high-dimensional data is not an easy task, so you need a way to discover groups of data without plotting them. <a href=""http://en.wikipedia.org/wiki/Cluster_analysis"" rel=""nofollow""><strong>Clustering</strong></a> is a general approach for this. Though clustering algorithms may work with data of arbitrary dimensionality, we still have only 2D to plot it, so let's come back to our example. </p>

<p>With algorithm like <a href=""http://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow""><strong>k-means</strong></a> you can obtain 2 groups like this: </p>

<p><img src=""http://i.stack.imgur.com/Tp4aK.png"" alt=""enter image description here""></p>

<p>Two dots - red and blue - show cluster centres, calculated by k-means. You can use coordinates of these points to split your data by any subset of axes, even if you have 10k dimensions. But again, the most important question here is: <em>what linguistic features will provide reasonable grouping of ages</em>. </p>
"
"['python', 'algorithms']",Newton-Raphson or EM Algorithm in Python,"<p>scikit learn has the EM algorithm <a href=""http://scikit-learn.org/stable/modules/mixture.html"" rel=""nofollow"">here</a>.</p>

<p>Source code is available.</p>

<p>And if you are an R fan the <code>mclust</code> package is available <a href=""http://www.stat.washington.edu/mclust/"" rel=""nofollow"">here</a>.</p>
"
"['neuralnetwork', 'deep-learning', 'convnet']",Convolutional neural network for sparse one-hot representation,"<p>I would not apply convolutional neural networks to your problem (at least from what I can gather from the description).</p>

<p>Convolutional nets' strengths and weaknesses are related to a core assumption in the model class: Translating patterns of features in a regular way either has a minor impact on the outcome, or has a specific useful meaning. So a pattern <code>1 0 1</code> seen in features 9,10,11 is similar in some way to the same pattern seen in features 15,16,17. Having this assumption built in to the model allows you to train a network with far fewer free parameters when dealing with e.g. image data, where this is a key property of data captured by scanners and cameras. </p>

<p>With one-hot encoding of features, you assign a feature vector index from a value or category essentially at random (via some hashing function). There is no meaning to translations between indices of the feature vectors. The patterns <code>0 0 1 0 1 0 0</code>  and <code>0 0 0 1 0 1 0</code> can represent entirely different things, and any associations between them are purely by chance. You can treat a sparse one-hot encoding as an image if you wish, but there is no good reason to do so, and models that assume translations can be made whilst preserving meaning will not do well.</p>

<p>For such a small sparse feature vector, assuming you want to try a neural network model, use a simple fully-connected network.</p>
"
"['topic-model', 'lda', 'parameter']",What does the alpha and beta hyperparameters contribute to in Latent Dirichlet allocation?,"<p>The Dirichlet distribution is a multivariate distribution. We can denote the parameters of the Dirichlet as a vector of size K of the form ~ 1/B(a) * Product(x_i ^ (a_i-1)), where a is the vector of size K of the parameters, and sum of x_i = 1.</p>

<p>Now the LDA uses some constructs like:
- a document can have multiple topics (because of this multiplicity, we need the Dirichlet distribution); and there is a Dirichlet distribution which models this relation
- words can also belong to multiple topics, when you consider them outside of a document; so here we need another Dirichlet to model this</p>

<p>The previous two are distributions which you do not really see from data, this is why is called latent, or hidden.</p>

<p>Now, in Bayesian inference you use the Bayes rule to infer the posterior probability. For simplicity, let's say you have data <em>x</em> and you have a model for this data governed by some parameters theta. In order to infer values for this parameters, in full Bayesian inference you will infer the posterior probability of these parameters using Bayes' rule with <em>p(theta|x) = p(x|theta)p(theta|alpha)/p(x|alpha)</em>. In plain words is <em>posterior probability = likelihood x prior probability / marginal likelihood</em>. Note that here comes an <em>alpha</em>. This is your initial belief about this distribution, and is the parameter of the prior distribution. Usually this is chosen in such a way that will have a conjugate prior (so the distribution of the posterior is the same with the distribution of the prior) and often to encode some knowledge if you have one or to have maximum entropy if you know nothing.</p>

<p>The parameters of the prior are called <em>hyperparameters</em>. So, in LDA, both topic distributions, over documents and over words have also correspondent priors, which are denoted usually with alpha and beta, and because are the parameters of the prior distributions are called hyperparameters. </p>

<p>Now about choosing priors. If you plot some Dirichlet distributions you will note that if the individual parameters <em>alpha_k</em> have the same value, the pdf is symmetric in the simplex defined by the <em>x</em> values, which is the minimum or maximum for pdf is at the center. </p>

<p>If all the alpha_k have values lower than unit the maximum is found at corners</p>

<p><img src=""http://i.stack.imgur.com/5khZE.png"" width=""200"" height=""200""></p>

<p>or can if all values alpha_k are the same and greater than 1 the maximum will be found in center like </p>

<p><img src=""http://research.microsoft.com/en-us/um/people/cmbishop/prml/prmlfigs-png/Figure2.5c.png"" width=""200"" height=""200""></p>

<p>It is easy to see that if values for alpha_k are not equal the symmetry is broken and the maximum will be found near bigger values. </p>

<p>Additional, please note that values for priors parameters produce smooth pdfs of the distribution as the values of the parameters are near 1. So if you have great confidence that something is clearly distributed in a way you know, with a high degree of confidence, than values far from 1 in absolute value are to be used, if you do not have such kind of knowledge than values near 1 would be encode this lack of knowledge. It is easy to see why 1 plays such a role in Dirichlet distribution from the formula of the distribution itself.</p>

<p>Another way to understand this is to see that prior encode prior-knowledge. In the same time you might think that prior encode some prior seen data. This data was not saw by the algorithm itself, it was saw by you, you learned something, and you can model prior according to what you know (learned). So in the prior parameters (hyperparameters) you encode also how big this data set you apriori saw, because the sum of alpha_k can be that also as the size of this more or less imaginary data set. So the bigger the prior data set, the bigger is the confidence, the bigger the values of alpha_k you can choose, the sharper the surface near maximum value, which means also less doubts. </p>

<p>Hope it helped.</p>

<p>PS: It's a hell to write something without LaTeX notation. I hope moderators/administrators will do something.</p>
"
"['machine-learning', 'classification']",Algorithm for generating classification rules,"<p>C45 made by Quinlan is able to produce rule for prediction. Check this <a href=""http://en.wikipedia.org/wiki/C4.5_algorithm"">Wikipedia</a> page. I know that in <a href=""http://www.cs.waikato.ac.nz/~ml/weka/"">Weka</a> its name is J48. I have no idea which are implementations in R or Python. Anyway, from this kind of decision tree  you should be able to infer rules for prediction.</p>

<p><em>Later edit</em></p>

<p>Also you might be interested in algorithms for directly inferring rules for classification. RIPPER is one, which again in Weka it received a different name JRip. See the original paper for RIPPER: <a href=""http://www.google.ro/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCYQFjAA&amp;url=http://www.cs.utsa.edu/~bylander/cs6243/cohen95ripper.pdf&amp;ei=-XJ-U-7pGoqtyAOej4Ag&amp;usg=AFQjCNFqLnuJWi3gGXVCrugmv3NTRhHHLA&amp;bvm=bv.67229260,d.bGQ&amp;cad=rja"">Fast Effective Rule Induction, W.W. Cohen 1995</a> </p>
"
['neuralnetwork'],Multi layer back propagation Neural network for classification,"<p>Suppose that you need to classify something in K classes, where K > 2. In this case the most often setup I use is one hot encoding. You will have K output columns, and in the training set you will set all values to 0, except the one which has the category index, which could have value 1. Thus, for each training data set instance you will have all outputs with values 0 or 1, all outputs sum to 1 for each instance.</p>

<p>This looks like a probability, which reminds me of a technique used often to connect some outputs which are modeled as probability. This is called softmax function, more details <a href=""http://en.wikipedia.org/wiki/Softmax_activation_function"" rel=""nofollow"">on Wikipedia</a>. This will allow you to put some constraints on the output values (it is basically a logistic function generalization) so that the output values will be modeled as probabilities. </p>

<p>Finally, with or without softmax you can use the output as a discriminant function to select the proper category.</p>

<p>Another final thought would be to avoid to encode you variables in a connected way. For example you can have the binary representation of the category index. This would induce to the learner an artificial connection between some outputs which are arbitrary. The one hot encoding has the advantage that is neutral to how labels are indexed.</p>
"
"['python', 'statistics', 'visualization', 'pandas']",Calculation and Visualization of Correlation Matrix with Pandas,"<p><strong>I suggest some sort of play on the following:</strong></p>

<p>Using the UCI Abalone data for this example...</p>

<pre><code>import matplotlib
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# Read file into a Pandas dataframe
from pandas import DataFrame, read_csv
f = 'https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'
df = read_csv(f)
df=df[0:10]
df
</code></pre>

<p><a href=""http://i.stack.imgur.com/efITk.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/efITk.png"" alt=""enter image description here""></a></p>

<p><strong>Correlation matrix plotting function:</strong> </p>

<p># Correlation matric plotting function </p>

<pre><code>def correlation_matrix(df):
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm as cm

    fig = plt.figure()
    ax1 = fig.add_subplot(111)
    cmap = cm.get_cmap('jet', 30)
    cax = ax1.imshow(df.corr(), interpolation=""nearest"", cmap=cmap)
    ax1.grid(True)
    plt.title('Abalone Feature Correlation')
    labels=['Sex','Length','Diam','Height','Whole','Shucked','Viscera','Shell','Rings',]
    ax1.set_xticklabels(labels,fontsize=6)
    ax1.set_yticklabels(labels,fontsize=6)
    # Add colorbar, make sure to specify tick locations to match desired ticklabels
    cbar = fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])
    plt.show()

correlation_matrix(df)
</code></pre>

<p><a href=""http://i.stack.imgur.com/YZsF8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YZsF8.png"" alt=""enter image description here""></a></p>

<p>Hope this helps!</p>
"
"['machine-learning', 'feature-selection', 'scikit', 'sklearn']",Sklearn feature selection stopping criterion (SelectFromModel),"<p>Some regression/classification models can also calculate feature importances - for example <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""nofollow"">RandomForestClassifier</a> models have a property <strong>feature_importances_</strong> and <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""nofollow"">LogisticRegression</a> models have a <strong>coef_</strong> property. There are many more models that can provide feature importances - but all of them either have a coef_ or feature_importances_ property.</p>

<p>What SelectFromModel does is to check if the object you pass as ""estimator"" has one of these two properties and to return features where this property is higher than the specified threshold (or the mean/median).</p>

<p>For example if you pass a RandomForestClassifier to SelectFromModel, it will return all features where the random forest's feature_importances_ property is higher than the specified threshold. The same happens if you pass a LogisticRegression model, except that it'll compare the coef_ property with the threshold instead.</p>

<p>Selecting the best value for the threshold can be done using a grid- or randomized search.</p>
"
"['predictive-modeling', 'scikit', 'categorical-data', 'multiclass-classification', 'missing-data']",Scikit Learn Missing Data - Categorical values,"<p>We would need more information on the prediction problem and the features to be able to give something more precise.</p>

<p>Anyhow, I am surprised no answer so far included all possible options since they aren't that many:</p>

<ol>
<li><strong>get rid</strong> of incomplete observations or features --- obviously, only viable if there are few incomplete cases since you lose too much information otherwise</li>
<li><strong>replace</strong> NAs with some value like -1 --- this depends on the classifier you use; if your classifier supports categorical variables, you can create a new category for those NAs for example. In some continuous variables, sometimes there are some values that make sense (for instance, in text mining classification, if you have a <code>title-length</code> feature but you have no title, it might make sense to replace with <code>title-length=0</code>)</li>
<li><strong>fill up</strong> the missing data</li>
</ol>

<p>This last point encompasses too many things:</p>

<ol start=""4"">
<li>replace NAs with the median (this is the usual lazy approach; sklearn <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html"" rel=""nofollow"">has a class</a> for this)</li>
<li>if time series, replace with an average of the previous and following values -- in pandas, this can be done using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html"" rel=""nofollow"">DataFrame.resample()</a>.</li>
<li>use the $k$ closest neighbors. build a KNN model using the other variables and then do the average of those neighbors (if you use euclidean distance, you probably should normalize first). I never seen this done, but you probably could try predicting the missing NAs using another model as well.</li>
</ol>

<p>But all this depends very much on what you are doing. For instance, if you have performed clustering analysis and you know your data is made up of clusters, you could use the median within each cluster.</p>

<p>Possibly other solutions could include things like multimodal or multiview models. These are recent techniques that can cope with missing modalities, and you can see a feature, or subset of features, as a modality. For instance, you could build a different classifier for various subsets of your features (using the complete cases in each of those subsets) and then build another classifier on top of that to merge those probabilities. I would only try these techniques if <em>most</em> of your data is missing. There are more advanced deep learning versions of this using autoencoders.</p>
"
['r'],ML packages in R: caret v e1071,"<p>I think that <code>caret</code> and <code>e1071</code> serve difference purposes. First lets discuss <code>caret</code>, its closest competitor is the <code>mlr</code> package. Both are meta packages that allow to optimize models across parameters. Take for example a problem where you are not sure whether you would want to use Lasso or Ridge to create a model. As explained <a href=""https://rstudio-pubs-static.s3.amazonaws.com/22067_48fad02fb1a944e9a8fb1d56c55119ef.html#ridge-regression-and-lasso"">here</a> <code>caret</code> allows to choose the optimal lambda based on different type of cross validations. </p>

<p>The <code>e1071</code> package instead is a bag of functions developed by the TU Wien. It is probably one of the most popular packages for support vector machines which are used by <code>caret</code>. But its goal is very different than <code>caret</code>, it implements learning algorithms and other functions whereas <code>caret</code> seeks to find the best parameter in learning models.</p>
"
"['xgboost', 'weighted-data']",xgboost: give more importance to recent samples,"<p>You could try building multiple xgboost models, with some of them being limited to more recent data, then weighting those results together.  Another idea would be to make a customized evaluation metric that penalizes recent points more heavily which would give them more importance.</p>
"
"['python', 'linear-regression', 'xgboost']",XGBoost Linear Regression output incorrect,"<p>It seems that XGBoost uses <strong>regression trees</strong> as base learners by default. XGBoost (or Gradient boosting in general) work by combining multiple of these base learners. Regression trees can not extrapolate the patterns in the training data, so any input above 3 or below 1 will not be predicted correctly in your case. Your model is trained to predict outputs for inputs in the interval <code>[1,3]</code>, an input higher than 3 will be given the same output as 3, and an input less than 1 will be given the same output as 1.</p>

<p>Additionally, regression trees do not really see your data as a <em>straight line</em> as they are non-parametric models, which means they can theoretically fit any shape that is more complicated than a straight line. Roughly, a regression tree works by assigning your new input data to some of the training data points it have seen during training, and produce the output based on that. </p>

<p>This is in contrast to parametric regressors (like <strong>linear regression</strong>) which actually look for the best parameters of a hyperplane (straight line in your case) to fit your data. Linear regression <strong>does</strong> see your data as a straight line with a slope and and intercept.</p>

<p>You can change the base learner of your XGB model to a GLM (generalised linear model) by adding <code>""booster"":""gblinear""</code> to your model <code>params</code> :</p>

<pre><code>import pandas as pd
import xgboost as xgb

df = pd.DataFrame({'x':[1,2,3], 'y':[10,20,30]})
X_train = df.drop('y',axis=1)
Y_train = df['y']
T_train_xgb = xgb.DMatrix(X_train, Y_train)

params = {""objective"": ""reg:linear"", ""booster"":""gblinear""}
gbm = xgb.train(dtrain=T_train_xgb,params=params)
Y_pred = gbm.predict(xgb.DMatrix(pd.DataFrame({'x':[4,5]})))
print Y_pred
</code></pre>

<p>In general, to debug why your XGBoost model is behaving in a particular way, see the model parameters :</p>

<pre><code>gbm.get_dump()
</code></pre>

<p>If your base learner is linear model, the get_dump output is :</p>

<pre><code>['bias:\n4.49469\nweight:\n7.85942\n']
</code></pre>

<p>In your code above, since you tree base learners, the output will be :</p>

<pre><code>['0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=2.85\n\t\t4:leaf=5.85\n\t2:leaf=8.85\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=1.995\n\t\t4:leaf=4.095\n\t2:leaf=6.195\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=1.3965\n\t\t4:leaf=2.8665\n\t2:leaf=4.3365\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.97755\n\t\t4:leaf=2.00655\n\t2:leaf=3.03555\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.684285\n\t\t4:leaf=1.40458\n\t2:leaf=2.12489\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.478999\n\t\t4:leaf=0.983209\n\t2:leaf=1.48742\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.3353\n\t\t4:leaf=0.688247\n\t2:leaf=1.04119\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.23471\n\t\t4:leaf=0.481773\n\t2:leaf=0.728836\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.164297\n\t\t4:leaf=0.337241\n\t2:leaf=0.510185\n',
 '0:[x&lt;2] yes=1,no=2,missing=1\n\t1:leaf=0.115008\n\t2:[x&lt;3] yes=3,no=4,missing=3\n\t\t3:leaf=0.236069\n\t\t4:leaf=0.357129\n']
</code></pre>

<p>Tip : I actually prefer to use xgb.XGBRegressor or xgb.XGBClassifier classes, since they follow the <a href=""http://scikit-learn.org/"">sci-kit learn</a> API. And because sci-kit learn has so many machine learning algorithm implementations, using XGB as an additional library does not disturb my workflow only when I use the sci-kit interface of XGB.</p>
"
"['neuralnetwork', 'backpropagation']",Torch and conceptual question about neural nets backpropagation,"<p>I have been using torch for a few months now but I will give it a go (apologies if incorrect).</p>

<p>Yes a weight $w$ is updated as follows;</p>

<p>$$ w_{new} = w_{old} - \gamma \partial E/ \partial w_{old}  $$</p>

<p>where $ \gamma $ is your learning rate and $E$ is the error calculated using something like <code>criterion:forward(output,target)</code>. The criterion could be, for example, <code>nn.MSECriterion()</code>.</p>

<p>To calculate $\partial E/\partial W$  you need $\partial E/\partial y$ <code>gradOutput = criterion:backward(output,target)</code>(gradient respect to output) as well as the input <code>input</code> to the net i.e. your $X$ (e.g. image data) to generate the recursive set of equations which multiply with <code>gradOutput</code>.</p>

<p><code>model:backward(input, gradOutput)</code> therefore serves to update the weights so that they are ready for the next <code>model:forward(input)</code> as it generates a big derivative tensor $dE/dW_{old}$. </p>

<p>This is then combined with a optimiser such as <code>optim.sgd</code> using <code>optimMethod</code> and the old weights $W_{old}$ to generate the new weights in the first equation. Of course you can just update the weights without an optimiser with <code>model:updateParameters(learningRate)</code> but you miss useful stuff like momentum, weight decay etc.</p>

<p>Got a bit side tracked there but hope this helps.</p>
"
"['scikit', 'decision-trees', 'ensemble-modeling']","what is the difference between ""fully developed decision trees"" and ""shallow decision trees""?","<p>[<strong>Later edit - Rephrase everything</strong>]</p>

<h3>Types of trees</h3>

<p>A <em>shallow tree</em> is a small tree (most of the cases it has a small depth). A <em>full grown tree</em> is a big tree (most of the cases it has a large depth). </p>

<p>Suppose you have a training set of data which looks like a non-linear structure. </p>

<p><a href=""http://i.stack.imgur.com/7Y2k1.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7Y2k1.png"" alt=""Synthetic sample with 2 regression trees. Red line is for tree with depth 3, blue line is for tree with depth 10""></a>  </p>

<h3>Bias variance decomposition as a way to see the learning error</h3>

<p>Considering bias variance decomposition we know that the learning error has 3 components:</p>

<p>$$Err = \text{Bias}^2 + \text{Var} + \epsilon$$</p>

<p><strong>Bias</strong> is the error produced by the fit model when it is not capable to represent the true function; it is in general associated with <em>underfitting</em>. </p>

<p><strong>Var</strong> is the error produced by the fit model due to sampled data, it describes how unstable a model is if the training data changes; it is in general associated with <em>overfitting</em>.</p>

<p><strong>$\epsilon$</strong> is the irreducible error which envelops the true function; this can't be learned</p>

<p><a href=""https://onlinecourses.science.psu.edu/stat857/sites/onlinecourses.science.psu.edu.stat857/files/lesson04/model_complexity.png"" rel=""nofollow""><img src=""https://onlinecourses.science.psu.edu/stat857/sites/onlinecourses.science.psu.edu.stat857/files/lesson04/model_complexity.png"" alt=""Model compexity""></a></p>

<p>Considering our shallow tree we can say that the model has low variance since changing the sample does not change too much the model. It needs too many changed data points to be considered unstable. At the same time we can say that has a high bias, since it really can't represent the sine function which is the true model. We can say also that it has a low complexity. It can be described by 3 constants and 3 regions. </p>

<p>Consequently, the full grown tree has low bias. It is very complex since it can be described only using many regions and many constants on those regions. This is why it has low bias. The complexity of the model impacts also variance which is high. In some regions at least, a single point sampled differently can change the shape of the fitted tree. </p>

<p>As a general rule of thumb when a model has low bias it has also high variance and when it has low variance it has high bias. This is not true always, but it happens very often. And intuitively is a correct idea. The reason is that when you are getting close to the points in the sample you learn the patterns but also learn the errors from sample, when you are far away from sample you are instead very stable since you do not incorporate errors from sample. </p>

<h3>How can we build ensembles using those kind of trees?</h3>

<p><strong>Bagging</strong></p>

<p>The statistical model behind bagging is based on bootstrapping, which is a statistical procedure to evaluate the error of a statistic. Assuming you have a sample and you want to evaluate the error of a statistical estimation, bootstrapping procedure allows you to approximate the distribution of the estimator. But trees are only a simple function of sample ""split the space into regions and predict with the average, a statistic"". Thus if one builds multiple trees from bootstrap samples and averages the trees can be considered i.i.d. and the same principle works to reduce variance. Because of that bagging allows one to reduce the variance without affecting too much the bias. Why it needs full depth trees? Well it is a simple reason, perhaps not so obvious at first sight. It need classifiers with high variance in order to reduce it. This procedure does not affect the bias. If the underlying models have low variance high bias, the bagging will reduce slightly the already small variance and produce nothing for bias. </p>

<p><strong>Bootstrapping</strong></p>

<p>How bootstrapping works? Many compare bootstrapping with model averaging, but the comparison is flawed. The idea of bootstrapping is that the ensemble is an iterative procedure. It is true that the final rule for classification looks like a weighted average of some weak models, but the point is that those models were built iteratively. Has nothing to do with bagging and how it works. Any $k$ tree is build using information learned from all previous $k-1$ trees. So we have initially a weak classifier which we fit to data, where all the points have the same importance. This importance can be changed by weights like in adaboost or by residuals like in gradient boosting, it really does not matter. The next weak classifier will not treat in the same way all the points, but those previously classified correctly has smaller importance than those classifier incorrectly. The consequence is that the model enriches it's complexity, it's ability to reproduce more complex surfaces. This is translated in the fact that it reduces the bias, since it can go closer to data. A similar intuition is behind: if the classifier has already a low bias, what will happen when I boost it? Probably a much unbearable overfit, that's all.</p>

<h3>Which one is better?</h3>

<p>There is no clear winner. It depends too much on data set and on other parameters. For example bagging can't hurt. It is possible to be useless ut usually does not hurt performance. Boosting can lead to overfit. That is because you can go eventually too close to data. </p>

<p>The is a lot of literature which says that when the irreducible error is high, the bagging is much better and boosting does not progress too much. </p>

<h3>Can we decrease both variance and bias?</h3>

<p>Pure bootstrap and bagging approaches serves a single purpose. Either reduce variance either reduce bias. However modern implementations changed various things in how those approaches works. Sampling can be used in boosting and it seems to work towards reducing also the variance. There are bagging procedures which takes some ideas prom boosting, for example iterative bagging (or adaptive bagging) published by Breiman. So, the answer is yes, is possible.</p>

<h3>Can we use other learners other than trees?</h3>

<p>Of course. Often you will see boosting to use this approach. I read some papers on bagging svm or another learners also. I tried myself to bag some svms but without much success. The trees are the preferred way, however, for a very simple reason, they are simple to build, simple to adapt and simple to control their effect. My personal opinion is that it is not everything said regarding ensembles of trees.</p>

<p>PS: a last note on the number of weak classifiers: this depends entirely on the complexity of the data set mixed with the complexity of the learner. There is no recipe. Often 20 of them are enough to get most of the information and additional ones are only for tiny tuning. </p>
"
"['data-mining', 'graphs', 'terminology']",What is a Recurrent Heavy Subgraph?,"<p>The term may best be expressed as a <strong>Recurrent</strong>, <strong>Heavy</strong> <strong>Subgraph</strong>. That is, a subgraph which is both <strong>Recurrent</strong> <em>and</em> <strong>Heavy</strong>.</p>

<p><strong>Heaviness</strong> of a subgraph refers to heavily connected vertices- that is, nodes which are connected many times (""many"" being relative to the network in question). </p>

<p><strong>Recurrent</strong> refers to the propensity of a subgraph to occur more than once. </p>

<p>Thus, a <strong>Recurrent Heavy Subgraph</strong>  is a densely connected set of vertices which occurs several times in the overall network.</p>

<p>These subgraphs are often used to determine properties of a network. </p>

<p>For example: In a network of emails interactions within a company organized into 4-person teams with one member acting as the lead, each team's email activity (if they email between themselves sufficiently to be considered ""heavy"") could be described as a <strong>Heavy</strong> <strong>Subgraph</strong>. The fact that these subgraphs occur many times in the network make them <strong>Recurrent Heavy Subgraphs</strong>. If one was searching for structure in the network, noticing that these recurrent, heavy subgraphs exist would go a long way toward determining the organization of the network as a whole.</p>
"
"['machine-learning', 'svm', 'supervised-learning', 'unsupervised-learning']",What kinds of learning problems are suitable for Support Vector Machines?,"<p>SVM can be used for classification (distinguishing between several groups or classes) and regression (obtaining a mathematical model to predict something). They can be applied to both linear and non linear problems. </p>

<p>Until 2006 they were the best general purpose algorithm for machine learning.  I was trying to find a paper that compared many implementations of the most known algorithms: svm, neural nets, trees, etc. I couldn't find it sorry (you will have to believe me, bad thing). In the paper the algorithm that got the best performance was svm, with the library libsvm.</p>

<p>In 2006 Hinton came up with deep learning and neural nets. He improved the current state of the art by at least 30%, which is a huge advancement. However deep learning only get good performance for huge training sets. If you have a small training set I would suggest to use svm.</p>

<p>Furthermore you can find here a useful infographic about <a href=""http://scikit-learn.org/stable/tutorial/machine_learning_map/"" rel=""nofollow"">when to use different machine learning algorithms</a> by scikit-learn. However, to the best of my knowledge there is no agreement among the scientific community about if a problem has X,Y and Z features then it's better to use svm. I would suggest to try different methods. Also, please don't forget that svm or neural nets is just a method to compute a model. It is very important as well the features you use.</p>
"
['tensorflow'],Question about train example code for TensorFlow,"<p>If I understood you correctly, you are asking about this line of code:</p>

<pre><code>train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
</code></pre>

<p>Here you only specify which part of batch is used for features and which for your predicted class.</p>
"
"['nlp', 'text-mining', 'feature-extraction']",Unsupervised Feature Learning for NER,"<p>Yes, it is entirely possible to combine unsupervised learning with the CRF model.  In particular, I would recommend that you explore the possibility of using <a href=""https://code.google.com/p/word2vec/"">word2vec</a> features as inputs to your CRF.</p>

<p>Word2vec trains a  to distinguish between words that are appropriate for a given context and words that are randomly selected.  Select weights of the model can then be interpreted as a dense vector representation of a given word.  </p>

<p>These dense vectors have the appealing property that words that are semantically or syntactically similar have similar vector representations.  Basic vector arithmetic even reveals some interesting learned relationships between words.<br>
For example, vector(""Paris"") - vector(""France"") + vector(""Italy"") yields a vector that is quite similar to vector(""Rome"").  </p>

<p>At a high level, you can think of word2vec representations as being similar to LDA or LSA representations, in the sense that you can convert a sparse input vector into a dense output vector that contains word similarity information. </p>

<p>For that matter, LDA and LSA are also valid options for unsupervised feature learning -- both attempt to represent words as combinations of ""topics"" and output dense word representations.  </p>

<p>For English text Google distributes word2vec models pretrained on a huge 100 billion word Google News dataset, but for other languages you'll have to train your own model.</p>
"
"['python', 'nlp', 'pandas', 'topic-model', 'lda']",How to get columns from unsorted rows in Pandas? (MALLET),"<p>So one way to do it is to use python slice operators to grab every other value in the line and zip them (along with the filename) into 3-tuples, e.g.:</p>

<pre><code>data = []
malletOutput = open('doc-topics','r').readlines()

for line in malletOutput:
    line = line.split('\t')[1:-1] # slicing out useless leading index and trailing \n
    _id = line[0]

    tIndicies = map(int,line[1::2])
    tVals = map(float, line[2::2])
    topics = sorted(zip(tIndicies, tVals))
    topics = [t + tuple([_id]) for t in topics] # add _id
    for t in topics:
        data.append(t)
</code></pre>

<p>Now you have a list of 3-tuples which you can add to a dataframe. A simple pivot from there results in the desired shape:</p>

<pre><code>df = pd.DataFrame(data)
df = df.pivot(index=2,columns=0,values=1)
</code></pre>
"
"['deep-learning', 'convnet', 'backpropagation']",Trying to figure out how to set weights for convolutional networks,"<p>You are right there are just 10 params in your example.</p>

<p>For determining gradients, you just add up all the deltas from backpropagation in each location - i.e. you run backpropagation 30x30 = 900 times, for each position the 3x3 kernel is used, for every example in your batch (or just for one example if you are running most simple onine stochastic gradient descent), and for each position you add those delta values into a suitably-sized buffer (10 values for weight deltas, or 9 values for previous layer activation deltas). You will end up with one set of summed deltas matching your single 3x3 filter (plus a delta bias term). You then apply the summed version to update the weights of your single filter + bias.</p>

<p>Note this is a general rule you can apply whenever multiple gradient sources from backpropagation can be applied to any parameter - they just add. This occurs in RNNs too, or in any structure where you can set an objective function for non-output neurons.</p>
"
['predictive-modeling'],problem of choosing right statistical method for scheduler prediction,"<p>Do you know if the scheduler has a memory?</p>

<p>Let us assume for a moment that the scheduler has no memory.  This is a straightforward classification (supervised learning) problem: the inputs are X, the outputs are the schedules (N->M maps).  Actually, if <em>every</em> N gets scheduled and the only question is which M it gets, the outputs are lists which channel (or none) is scheduled to each block, and there is only a certain possible number of those, so you can model them as discrete outputs (classes) with their own probabilities.  Use whatever you like (AdaBoost, Naive Bayes, RBF SVM, Random Forest...) as a classifier.  I think you will quickly learn about the general behavior of the scheduler.</p>

<p>If the scheduler has a memory, then things get complicated.  I think you might approach that as a hidden Markov model: but the number of individual states may be quite large, and so it may be essentially impossible to build a complete map of transition probabilities.</p>
"
"['neuralnetwork', 'image-classification', 'preprocessing', 'convnet']",How to prepare images for neural network?,"<p>The idea with Neural Networks is that they need little pre-processing since the heavy lifting is done by the algorithm which is the one in charge of learning the features.</p>

<p>The winners of the Data Science Bowl 2015 have a great write-up regarding their approach, so most of this answer's content was taken from:
<a href=""https://benanne.github.io/2015/03/17/plankton.html"">Classifying plankton with deep neural networks</a>. I suggest you read it, specially the part about <strong>Pre-processing and data augmentation</strong>.</p>

<p><strong>- Resize Images</strong></p>

<p>As for different sizes, resolutions or distances you can do the following. You can simply rescale the largest side of each image to a fixed length.</p>

<p>Another option is to use openCV or scipy.
and this will resize the image to have 100 cols (width) and 50 rows (height):</p>

<pre><code>resized_image = cv2.resize(image, (100, 50)) 
</code></pre>

<p>Yet another option is to use scipy module, by using:</p>

<pre><code>small = scipy.misc.imresize(image, 0.5)
</code></pre>

<p><strong>- Data Augmentation</strong></p>

<p>Data Augmentation always improves performance though the amount depends on the dataset. If you want to augmented the data to artificially increase the size of the dataset you can do the following if the case applies (it wouldn't apply if for example were images of houses or people where if you rotate them 180degrees they would lose all information but not if you flip them like a mirror does):</p>

<ul>
<li>rotation: random with angle between 0° and 360° (uniform)</li>
<li>translation: random with shift between -10 and 10 pixels (uniform)</li>
<li>rescaling: random with scale factor between 1/1.6 and 1.6
(log-uniform)</li>
<li>flipping: yes or no (bernoulli)</li>
<li>shearing: random with angle between -20° and 20° (uniform)</li>
<li>stretching: random with stretch factor between 1/1.3 and 1.3
(log-uniform)</li>
</ul>

<p>You can see the results on the Data Science bowl images.</p>

<p><strong>Pre-processed images</strong></p>

<p><a href=""http://i.stack.imgur.com/0S0Y0.png""><img src=""http://i.stack.imgur.com/0S0Y0.png"" alt=""Pre-processed images""></a></p>

<p><strong>augmented versions of the same images</strong></p>

<p><a href=""http://i.stack.imgur.com/KJXZK.png""><img src=""http://i.stack.imgur.com/KJXZK.png"" alt=""enter image description here""></a></p>

<p><strong>-Other techniques</strong></p>

<p>These will deal with other image properties like lighting and are already related to the main algorithm more like a simple pre-processing step. Check the full list on: <a href=""http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/"">UFLDL Tutorial</a></p>
"
"['python', 'scikit', 'pandas', 'unbalanced-classes', 'anomaly-detection']",Outlier detection for Imbalanced classes,"<p>you need to distinguish between these cases:</p>

<ol>
<li>Data Imbalance</li>
<li>Data Imbalance + Very few number of samples (minority class)</li>
<li>Severe Data Imbalance + Very few number of samples (minority class)</li>
</ol>

<p>20:60   vs.  10:20    vs. 100:1000  vs. 10:100</p>

<p>and these cases:</p>

<ol>
<li><p>similarities between different classes.</p></li>
<li><p>wide variations within the same class.</p></li>
</ol>

<p>You need to understand to which of these cases your problem belong.</p>

<p>if you have very severe data imbalance + very few number of samples + wide variation within the majority class and similarities between different classes. regular oversampling or down sampling techniques will not help you as well as most of the synthetic oversampling techniques designed specifically to deal with the data imbalance but the assumption is to have enough number of samples.</p>

<p>Try to focus more on ensemble techniques that designed mainly to deal with data imbalance. 
SMOTE-Boost
RUSBoost
SMOTEBagging
IIIVote
EasyEnsemble</p>
"
"['machine-learning', 'data-mining', 'statistics']",What statistical model should I use to analyze the likelihood that a single event influenced longitudinal data,"<p>For the record, I think this is the type of question that's perfect for the data science Stack Exchange. I hope we get a bunch of real world examples of data problems and several perspectives on how best to solve them.</p>

<p>I would encourage you <em>not</em> to use p-values as they can be pretty misleading (<a href=""http://andrewgelman.com/2013/03/12/misunderstanding-the-p-value/"">1</a>, <a href=""http://occamstypewriter.org/boboh/2008/08/19/why_p_values_are_evil/"">2</a>). My approach hinges on you being able to summarize traffic on a given page before and after some intervention. What you care about is the difference in the <em>rate</em> before and after the intervention. That is, how does the number of hits per day change? Below, I explain a first stab approach with some simulated example data. I will then explain one potential pitfall (and what I would do about it).</p>

<p>First, let's think about one page before and after an intervention. Pretend the intervention increases hits per day by roughly 15%:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def simulate_data(true_diff=0):
    #First choose a number of days between [1, 1000] before the intervention
    num_before = np.random.randint(1, 1001)

    #Next choose a number of days between [1, 1000] after the intervention
    num_after = np.random.randint(1, 1001)

    #Next choose a rate for before the intervention. How many views per day on average?
    rate_before = np.random.randint(50, 151)

    #The intervention causes a `true_diff` increase on average (but is also random)
    rate_after = np.random.normal(1 + true_diff, .1) * rate_before

    #Simulate viewers per day:
    vpd_before = np.random.poisson(rate_before, size=num_before)
    vpd_after = np.random.poisson(rate_after, size=num_after)

    return vpd_before, vpd_after

vpd_before, vpd_after = simulate_data(.15)

plt.hist(vpd_before, histtype=""step"", bins=20, normed=True, lw=2)
plt.hist(vpd_after, histtype=""step"", bins=20, normed=True, lw=2)
plt.legend((""before"", ""after""))
plt.title(""Views per day before and after intervention"")
plt.xlabel(""Views per day"")
plt.ylabel(""Frequency"")
plt.show()
</code></pre>

<p><img src=""http://i.stack.imgur.com/FJJqD.png"" alt=""Distribution of hits per day before and after the intervention""></p>

<p>We can clearly see that the intervention increased the number of hits per day, on average. But in order to quantify the difference in rates, we should use one company's intervention for multiple pages. Since the underlying rate will be different for each page, we should compute the percent change in rate (again, the rate here is hits per day).</p>

<p>Now, let's pretend we have data for <code>n = 100</code> pages, each of which received an intervention from the same company. To get the percent difference we take (mean(hits per day before) - mean(hits per day after)) / mean(hits per day before):</p>

<pre><code>n = 100

pct_diff = np.zeros(n)

for i in xrange(n):
    vpd_before, vpd_after = simulate_data(.15)
    # % difference. Note: this is the thing we want to infer
    pct_diff[i] = (vpd_after.mean() - vpd_before.mean()) / vpd_before.mean()

plt.hist(pct_diff)
plt.title(""Distribution of percent change"")
plt.xlabel(""Percent change"")
plt.ylabel(""Frequency"")
plt.show()
</code></pre>

<p><img src=""http://i.stack.imgur.com/CAitf.png"" alt=""Distribution of percent change""></p>

<p>Now we have the distribution of our parameter of interest! We can query this result in different ways. For example, we might want to know the mode, or (approximation of) the most likely value for this percent change:</p>

<pre><code>def mode_continuous(x, num_bins=None):
    if num_bins is None:
        counts, bins = np.histogram(x)
    else:
        counts, bins = np.histogram(x, bins=num_bins)

    ndx = np.argmax(counts)
    return bins[ndx:(ndx+1)].mean()

mode_continuous(pct_diff, 20)
</code></pre>

<p>When I ran this I got 0.126, which is not bad, considering our true percent change is 15. We can also see the number of positive changes, which approximates the probability that a given company's intervention improves hits per day:</p>

<pre><code>(pct_diff &gt; 0).mean()
</code></pre>

<p>Here, my result is 0.93, so we could say there's a pretty good chance that this company is effective.</p>

<p>Finally, a potential pitfall: Each page probably has some underlying trend that you should probably account for. That is, even without the intervention, hits per day may increase. To account for this, I would estimate a simple linear regression where the outcome variable is hits per day and the independent variable is day (start at day=0 and simply increment for all the days in your sample). Then subtract the estimate, y_hat, from each number of hits per day to de-trend your data. Then you can do the above procedure and be confident that a positive percent difference is not due to the underlying trend. Of course, the trend may not be linear, so use discretion! Good luck!</p>
"
"['machine-learning', 'python', 'nlp', 'neuralnetwork', 'recommendation']",Word2Vec for Named Entity Recognition,"<p>Instead of ""recursive neural nets with back propagation"" you might consider the approach used by Frantzi, et. al. at National Centre for Text Mining (NaCTeM) at University of Manchester for <em>Termine</em> (see: <a href=""http://www.nactem.ac.uk/index.php"" rel=""nofollow"">http://www.nactem.ac.uk/index.php</a> and <a href=""http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf"" rel=""nofollow"">http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf</a>) Instead of deep neural nets, they ""combine linguistic and statistical information"".</p>
"
"['machine-learning', 'data-mining', 'classification', 'predictive-modeling', 'svm']",Behaviour of Learning Algorithms on Random Data,"<p>They will of course still learn some best decision boundary. We know it will be meaningless, but there will still be better and best coefficients for the algorithm to learn when fitting to this particular instance of data from this random process. It may produce better than 50% accuracy on the data set, but of course this is purely due to overfitting whatever the data happens to be. It will not predict future outcomes with more than 50% accuracy.</p>
"
"['neuralnetwork', 'svm', 'supervised-learning', 'training', 'overfitting']",Does the phenomenon of over-fitting of data varies with training algorithms?,"<p>A model is over-fitting if it makes good predictions on a test set but bad predictions on new data. This is generally a good indication that the used model is too complex. </p>

<p>The complexity of the model is often quantified as the number of free parameters. These are the parameters that need to be set in order to fit the data. More parameters allow more flexibility in what can be expressed but also increase the change of over-fitting. </p>

<p>The type of model also restricts what functions can be learned. For example, linear models can only learn linear functions (really).</p>

<p>The simple answer to your question is yes. For example, a SVM with little parameters might fit data well while a ANN with many parameters might over-fit on the same data. </p>

<p>A fair comparison however compares models with the same number of parameters. But the answer remains yes. One model might be better suited than another to fit the intrinsic structure of the data. For example, on the same data, a decision tree with 100 nodes might over-fit, a linear model might under-fit and a ANN might work perfectly. It all depends on the underlying structure that you want to model. </p>
"
"['neuralnetwork', 'supervised-learning', 'performance', 'matlab', 'accuracy']",Accuracy value constant even after different runs,"<p>I see that you have set a Random Number Generator(rng) seed in the following line in your code: <code>rng(1)</code>.</p>

<p>So, this splits the data in the same way no matter whichever run. So, that is the reason why you are getting the same error values.</p>

<p>Try removing the line. Then, the data shall be slit randomly (as there is no seed now). You shall get different error then, depending on how the split is done (which is, randomly).</p>

<p><strong><a href=""http://stats.stackexchange.com/questions/222279/should-random-forests-based-on-same-data-but-different-random-seeds-be-compared#comment420513_222279"">A model which generalizes well should be robust to the choice of seed</a></strong></p>
"
"['machine-learning', 'data-mining', 'statistics', 'algorithms', 'recommendation']",Mahout Similarity algorithm comparison,"<p>For those not familiar, item-item recommenders calculate similarities between items, as opposed to user-user (or user-based) recommenders, which calculate similarities between users. Although some algorithms can be used for both, this question is in regard to item-item algorithms (thanks for being specific in your question).</p>

<p>Accuracy or effectiveness of recommenders is evaluated based on comparing recommendations to a previously collected data set (training set). For example, I have shopping cart data from the last six months; I'll use the first 5 months as training data, then run my various algorithms, and compare the quality against what really happened during the 6th month.</p>

<p>The reason Mahout ships with so many algorithms is because different algorithms are more or less effective in each data set you may work with. So, ideally, you do some testing as I described with many algorithms and compare the accuracy, then choose the winner.</p>

<p>Interestingly, you can also take other factors into account, such as the need to minimize the data set (for performance reasons), and run your tests only with a certain portion of the training data available. In such a case, one algorithm may work better with the smaller data set, but another may work with the complete set. Then, you get to weigh performance VS accuracy VS challenge of implementation (such as deploying on a Hadoop cluster).</p>

<p>Therefore, different algorithms are suited for different project. However, there are some general rules:</p>

<ol>
<li>All algorithms always do better with unreduced data sets (more data is better).</li>
<li>More complex algorithms aren't necessarily better.</li>
</ol>

<p>I suggest starting with a simple algorithm and ensuring you have high quality data. If you have additional time, you can implement more complex algorithms and create a comparison which is unique to your data set. </p>

<p>Most of my info comes from <a href=""http://ai.arizona.edu/intranet/papers/comparative.ieeeis.pdf"" rel=""nofollow"">This study</a>. You'll find lots of detail about implementation there.</p>
"
"['machine-learning', 'data-mining', 'clustering', 'algorithms', 'data-cleaning']",Prepping Data For Usage Clustering,"<p>I believe your problem boils down to clustering time-series of <em>different</em> lengths. According to your question, you want the longer time-series of a power user to be considered similar to time-series of similar pattern but much shorter.<br>
Therefore you should look into clustering techniques and distance metrics which allow for these properties. I don't know your language of choice but here are some of the many packages in <code>R</code> that you might find interesting :<br>
- <a href=""https://en.wikipedia.org/wiki/Fr%C3%A9chet_distance"" rel=""nofollow"">Fréchet distance</a> - one of the packages offering this is <a href=""https://mran.revolutionanalytics.com/web/packages/kmlShape/kmlShape.pdf"" rel=""nofollow"">kmlShape</a><br>
- <a href=""https://en.wikipedia.org/wiki/Dynamic_time_warping"" rel=""nofollow"">Dynamic Time Warping</a> included in base <code>R</code><br>
- <a href=""http://pubman.mpdl.mpg.de/pubman/item/escidoc:2222505/component/escidoc:2222571/id10000557.pdf"" rel=""nofollow"">Permutation Distribution Clustering</a> - package <a href=""http://pubman.mpdl.mpg.de/pubman/item/escidoc:2222505/component/escidoc:2222571/id10000557.pdf"" rel=""nofollow"">pdc</a><br>
This would also solve your data formatting problem as to setting values to <code>-1</code> or <code>NULL</code> would not be needed anymore. hth. </p>
"
"['classification', 'python']","minimization with a negative cost function: works in MATLAB, not in Python","<p>The problem is with division on python 2.x.  In python 2.x, division involving two integers produces an integer result.  So <code>1/2==0</code>.  Python 3.x does not have this problem, <code>1/2==.5</code>.</p>

<p>There are two ways to avoid this.  First, you can always convert one value you a float.  So <code>1./2==0.5</code> and <code>1/2.==0.5</code>.  However, you have to remember to do this everywhere, and if you forget it can lead to hard-to-find errors.</p>

<p>The more reliable method is to always put this at the top of your code: <code>from __future__ import division</code>.  This will switch python 2.x to the python 3 behavior, so <code>1/2==.5</code>.  In python 3.x it does nothing, so it also makes your code python 3.x compatible in this regard.</p>
"
['machine-learning'],What are the 'hottest' future areas of Machine Learning and Data Science?,"<p>There is a brilliant answer by Yann LeCunn recently on a Quora session. 
<a href=""https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning"" rel=""nofollow"">What are some recent and potentially upcoming breakthroughs in deep learning?</a> As he quotes, adversarial training proposed by Ian Goodfellow is one of the hottest area. Apart from that, I think Memory Networks, RNNs are widely used right now and can solve various problems up ahead.</p>

<p>Coming to what companies might do, I see a lot of text analytic companies. Considering the amount of consumer data we have now, there is a great need to be able to understand and analyze it. Computer Vision using ML or Deep learning is fast catching up. Using cameras for face detection, object tracking, fraud detection, tracking number plates is a huge field for companies to handle. </p>

<p>Also, I think <a href=""https://www.quora.com/What%E2%80%99s-your-advice-for-undergraduate-student-who-aspires-to-be-a-research-scientist-in-deep-learning-or-related-field-one-day"" rel=""nofollow"">this</a> might help you to prepare. To catch up with the overlooked developments, refer <a href=""https://www.quora.com/What-are-some-exciting-but-overlooked-developments-in-ML-research"" rel=""nofollow"">this</a> which might give you an insight about what can develop in the future.</p>
"
"['random-forest', 'decision-trees']",Minimum number of trees for Random Forest classifier,"<p>This is not necessarily an answer to your question. Just general thoughts about cross-validating the number of decision trees within a random forest.</p>

<p>I see a lot of people in kaggle and stackexchange cross-validating the number of trees in a random forest. I have also asked a couple of colleagues and they tell me it is important to cross-validate them to avoid overfitting.</p>

<p>This never made sense to me. Since each decision tree is trained independently, adding more decision trees should just make your ensemble more and more robust.</p>

<p>(This is different from gradient boosting trees, which are a particular case of ada boosting, and therefore there is potential for overfitting since each decision tree is trained to weight residuals more heavily.)</p>

<p>I did a simple experiment:</p>

<pre><code>from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier
from sklearn.grid_search import GridSearchCV
import numpy as np
import matplotlib.pyplot as plt
plt.ioff()

df = load_digits()
X = df['data']
y = df['target']

cv = GridSearchCV(
    RandomForestClassifier(max_depth=4),
    {'n_estimators': np.linspace(10, 1000, 20, dtype=int)},
    'accuracy',
    n_jobs=-1,
    refit=False,
    cv=50,
    verbose=1)
cv.fit(X, y)
scores = np.asarray([s[1] for s in cv.grid_scores_])
trees = np.asarray([s[0]['n_estimators'] for s in cv.grid_scores_])
o = np.argsort(trees)
scores = scores[o]
trees = trees[o]
plt.clf()
plt.plot(trees, scores)
plt.xlabel('n_estimators')
plt.ylabel('accuracy')
plt.savefig('trees.png')
plt.show()
</code></pre>

<p><a href=""http://i.stack.imgur.com/IluRe.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IluRe.png"" alt=""performance""></a></p>

<p>I am not saying your are committing this fallacy about thinking more trees can cause overfitting. You clearly are not since you have asked for a lower bound. This is just something that has been bugging me for awhile, and I think it is important to keep in mind.</p>

<p>(Addendum: <a href=""http://statweb.stanford.edu/~tibs/ElemStatLearn/"" rel=""nofollow"">Elements of Statistical Learning</a> discusses this at page 596, and is in agreement on this with me. «It is certainly true that increasing B [B=number of trees] does not cause the random forest sequence to overfit». The author does make the observation that «this limit can overfit the data». In other words, since <strong>other</strong> hyperparameters may lead to overfitting, creating a robust model does not rescue you from overfit. You have to pay attention when cross-validating your other hyperparameters.)</p>

<p>To answer your question, adding decision trees will always be beneficial to your ensemble. It will always make it more and more robust. But, of course, it is dubious whether the marginal 0.00000001 reduction in variance is worth the computational time.</p>

<p>Your question therefore, as I understand, is whether you can somehow calculate or estimate the amount of decision trees to reduce the error variance to below a certain threshold.</p>

<p>I very much doubt it. We do not have clear answers for many broad questions in data mining, much less specific questions like that. As Leo Breiman (the author of random forests) wrote, <a href=""http://www.math.snu.ac.kr/~hichoi/machinelearning/(Breiman)%20Statistical%20Modeling--The%20Two%20Cultures.pdf"" rel=""nofollow"">there are two cultures in statistical modeling</a>, and random forests is the type of model that he says  has few assumptions, but is also very data-specific. That is why, he says, we cannot resort to hypothesis testing, we have to go with brute-force cross-validation.</p>
"
"['text-mining', 'clustering', 'sklearn', 'k-means']",Algorithms for text clustering,"<p>Check the <strong>Stanford NLP Group</strong>'s open source software (<a href=""http://www-nlp.stanford.edu/software"">http://www-nlp.stanford.edu/software</a>), in particular, <strong>Stanford Classifier</strong> (<a href=""http://www-nlp.stanford.edu/software/classifier.shtml"">http://www-nlp.stanford.edu/software/classifier.shtml</a>). The software is written in <code>Java</code>, which will likely delight you, but also has bindings for some other languages. Note, the <em>licensing</em> - if you plan to use their code in commercial products, you have to acquire commercial license.</p>

<p>Another interesting set of open source libraries, IMHO suitable for this task and much more, is <strong>parallel framework for machine learning GraphLab</strong> (<a href=""http://select.cs.cmu.edu/code/graphlab"">http://select.cs.cmu.edu/code/graphlab</a>), which includes <strong>clustering library</strong>, implementing various clustering algorithms (<a href=""http://select.cs.cmu.edu/code/graphlab/clustering.html"">http://select.cs.cmu.edu/code/graphlab/clustering.html</a>). It is especially suitable for <strong>very large volume of data</strong> (like you have), as it implements <code>MapReduce</code> model and, thus, supports <em>multicore</em> and <em>multiprocessor</em> <strong>parallel processing</strong>.</p>

<p>You most likely are aware of the following, but I will mention it just in case. <strong>Natural Language Toolkit (NLTK)</strong> for <code>Python</code> (<a href=""http://www.nltk.org"">http://www.nltk.org</a>) contains modules for clustering/classifying/categorizing text. Check the relevant chapter in the <code>NLTK Book</code>: <a href=""http://www.nltk.org/book/ch06.html"">http://www.nltk.org/book/ch06.html</a>.</p>

<p><strong>UPDATE:</strong></p>

<p>Speaking of <strong>algorithms</strong>, it seems that you've tried most of the ones from <code>scikit-learn</code>, such as illustrated in this topic extraction example: <a href=""http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html"">http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html</a>. However, you may find useful other libraries, which implement a wide variety of <em>clustering algorithms</em>, including <em>Non-Negative Matrix Factorization (NMF)</em>. One of such libraries is <strong>Python Matrix Factorization (PyMF)</strong> with home page at <a href=""https://code.google.com/p/pymf"">https://code.google.com/p/pymf</a> and source code at <a href=""https://github.com/nils-werner/pymf"">https://github.com/nils-werner/pymf</a>. Another, even more interesting, library, also Python-based, is <strong>NIMFA</strong>, which implements various <em>NMF algorithms</em>: <a href=""http://nimfa.biolab.si"">http://nimfa.biolab.si</a>. Here's a research paper, describing <code>NIMFA</code>: <a href=""http://jmlr.org/papers/volume13/zitnik12a/zitnik12a.pdf"">http://jmlr.org/papers/volume13/zitnik12a/zitnik12a.pdf</a>. Here's an example from its documentation, which presents the solution for very similar text processing problem of <em>topic clustering</em>: <a href=""http://nimfa.biolab.si/nimfa.examples.documents.html"">http://nimfa.biolab.si/nimfa.examples.documents.html</a>.</p>
"
"['neuralnetwork', 'svm', 'sklearn']",how to make sklearn pipeline using custom model?,"<p>Implementing a custom transformer is simple. You have to implement the fit and transform methods like below. Since your ANN is already trained (right?) the fit method has to do nothing, just return self. And the transform method has to pass the incoming data to the ANN and return its output.</p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin

class MyANNTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, ann):
        self.ann = ann

    def fit(self, X, y):
        return self

    def transform(self, X)
        return self.ann.predict(X)
</code></pre>

<p>Now you can include that in pipelines:</p>

<pre><code>from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC

pipe = make_pipeline(MyANNTransformer(ann),
                     SVC())

pipe.fit(Xtrain, ytrain)
pipe.predict(Xtest)
</code></pre>
"
"['machine-learning', 'logistic-regression']",Linear regression with non-symmetric cost function?,"<p>If I understand you correctly, you want to err on the side of overestimating. If so, you need an appropriate, asymmetric cost function. One simple candidate is to tweak the squared loss:</p>

<p>$\mathcal L: (x,\alpha) \to x^2 \left( \mathrm{sgn} x + \alpha \right)^2$</p>

<p>where $-1 &lt; \alpha &lt; 1$ is a parameter you can use to trade off the penalty of underestimation against overestimation. Positive values of $\alpha$ penalize overestimation, so you will want to set $\alpha$ negative. In python this looks like <code>def loss(x, a): return x**2 * (numpy.sign(x) + a)**2</code></p>

<p><a href=""http://i.stack.imgur.com/zCSNb.png""><img src=""http://i.stack.imgur.com/zCSNb.png"" alt=""Loss functions for two values of a""></a></p>

<p>Next let's generate some data:</p>

<pre><code>import numpy
x = numpy.arange(-10, 10, 0.1)
y = -0.1*x**2 + x + numpy.sin(x) + 0.1*numpy.random.randn(len(x))
</code></pre>

<p><a href=""http://i.stack.imgur.com/XNhIG.png""><img src=""http://i.stack.imgur.com/XNhIG.png"" alt=""Arbitrary function""></a></p>

<p>Finally, we will do our regression in <code>tensorflow</code>, a machine learning library from Google that supports automated differentiation (making gradient-based optimization of such problems simpler). I will use <a href=""https://github.com/nlintz/TensorFlow-Tutorials/blob/master/1_linear_regression.py"">this example</a> as a starting point.</p>

<pre><code>import tensorflow as tf

X = tf.placeholder(""float"") # create symbolic variables
Y = tf.placeholder(""float"") 

w = tf.Variable(0.0, name=""coeff"")
b = tf.Variable(0.0, name=""offset"")
y_model = tf.mul(X, w) + b

cost = tf.pow(y_model-Y, 2) # use sqr error for cost function
def acost(a): return tf.pow(y_model-Y, 2) * tf.pow(tf.sign(y_model-Y) + a, 2)

train_op = tf.train.AdamOptimizer().minimize(cost)
train_op2 = tf.train.AdamOptimizer().minimize(acost(-0.5))

sess = tf.Session()
init = tf.initialize_all_variables()
sess.run(init)

for i in range(100):
    for (xi, yi) in zip(x, y): 
#         sess.run(train_op, feed_dict={X: xi, Y: yi})
        sess.run(train_op2, feed_dict={X: xi, Y: yi})

print(sess.run(w), sess.run(b))
</code></pre>

<p><code>cost</code> is the regular squared error, while <code>acost</code> is the aforementioned asymmetric loss function. </p>

<p>If you use <code>cost</code> you get</p>

<blockquote>
  <p>1.00764 -3.32445</p>
</blockquote>

<p><a href=""http://i.stack.imgur.com/5pW89.png""><img src=""http://i.stack.imgur.com/5pW89.png"" alt=""cost""></a></p>

<p>If you use <code>acost</code> you get</p>

<blockquote>
  <p>1.02604 -1.07742</p>
</blockquote>

<p><a href=""http://i.stack.imgur.com/s6IDA.png""><img src=""http://i.stack.imgur.com/s6IDA.png"" alt=""acost""></a></p>

<p><code>acost</code> clearly tries not to underestimate. I did not check for convergence, but you get the idea.</p>
"
['finance'],Resources for data science applications in finance/banking,"<p>Take a look at <a href=""http://www.oreilly.com/data/free/"" rel=""nofollow"">O'Reilly free Ebooks</a> There are a couple of resources for banking/finance/fintech.</p>

<p>There are some sites from which you can work on for learning more about predictive modelling in this sector:</p>

<p>1.) <a href=""https://inclass.kaggle.com/c/name-that-loan-open/data"" rel=""nofollow"">https://inclass.kaggle.com/c/name-that-loan-open/data</a> - Kaggle dataset   for prediction of interest rates</p>

<p>2.) <a href=""https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/"" rel=""nofollow"">https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/</a> - Dataset on Analytics Vidhya for loan prediction</p>

<p>3.) There is also a twitter handle with the name @StatCan_eng posting datasets related to Canada specifically.  </p>
"
"['r', 'data-cleaning']",removing strings after a certain character in a given text,"<p>For instance:</p>

<pre><code> rs&lt;-c(""copyright @ The Society of mo"",""I want you to meet me @ the coffeshop"")
 s&lt;-gsub(""@.*"","""",rs)
 s
 [1] ""copyright ""             ""I want you to meet me ""
</code></pre>

<p>Or, if you want to keep the @ character:</p>

<pre><code> s&lt;-gsub(""(@).*"",""\\1"",rs)
 s
 [1] ""copyright @""             ""I want you to meet me @""
</code></pre>

<p>EDIT: If what you want is to remove everything from the last @ on you just have to follow this previous example with the appropriate regex. Example:</p>

<pre><code>rs&lt;-c(""copyright @ The Society of mo located @ my house"",""I want you to meet me @ the coffeshop"")
s&lt;-gsub(""(.*)@.*"",""\\1"",rs)
s
[1] ""copyright @ The Society of mo located "" ""I want you to meet me ""
</code></pre>

<p>Given the matching we are looking for, both sub and gsub will give you the same answer.</p>
"
"['machine-learning', 'r', 'classification', 'decision-trees']",Aggregating Decision Trees,"<p>Have a look on HMM (Hidden Markov Model) also. A concrete example Of HMM is available in <a href=""https://en.wikipedia.org/wiki/Hidden_Markov_model"" rel=""nofollow"">wikipedia</a>. Decision tree is better
in generalising and applying learned data in another context and a Markov-model is better to recall the exact
learned machine state.</p>
"
['r'],Package that is similar to R's caret?,"<p><strong>MLR</strong> is similar to <strong>caret</strong>, mlr offers a high-level interface to various statistical and machine learning packages.</p>

<p>According to the package description:</p>

<blockquote>
  <p>Interface to a large number of classification and regression
  techniques, including machine-readable parameter descriptions. There
  is also an experimental extension for survival analysis, clustering
  and general, example-specific cost-sensitive learning. Generic
  resampling, including cross-validation, bootstrapping and subsampling.
  Hyperparameter tuning with modern optimization techniques, for single-
  and multi-objective problems. Filter and wrapper methods for feature
  selection. Extension of basic learners with additional operations
  common in machine learning, also allowing for easy nested resampling.
  Most operations can be parallelized.</p>
</blockquote>

<p>Please check the <a href=""https://cran.r-project.org/web/packages/mlr/index.html"">reference</a></p>
"
['data'],First steps when analyzing a company's data,"<p>The thing you need to understand as completely as possible is how they expect a data analysis to enable them to achieve their objective. They are a business, so their overall objective is likely related to maximising profit. However, there will be a more immediate objective underneath that heading. To maximise profit you can either reduce costs or increase sales. In turn, to increase sales you can increase the number of customers or increase the amount of sales to each customer etc.</p>

<p>The question then turns on how you can use data science to perform one those objectives.</p>

<p>For example, questions that can almost be answered with data science could be 'how do I better identify potential customers?' or 'how do increase existing custmers' spend?' These are still very high level questions, but they are the sort of questions that you need to have in mind as you start to do your descriptive stats etc.</p>

<p>Bear in mind that this is an iterative process and it is completely normal to start off in a fuzzy sort of area. At this stage it is almost the case that having a question in mind is a McGuffin - it will kick things off, but it may not be the question you end up answering.</p>

<p>The CRISP-DM process is a process that has been built for data mining that discusses how to iteratively use results from analyses and models to increase your understanding of the customer's situation, and hence drive the development of a better business objective for use in a data science project.</p>
"
"['machine-learning', 'predictive-modeling', 'statistics', 'algorithms', 'regression']",How to decide power of independent variables in case of non-linear polynomial regression?,"<p>If all you care about is the quality of predictions (as opposed to explanatory power), skip linear models altogether and use gradient boosted trees instead. Gradient boosting can generally learn polynomial splines with ease, and you don't have to manually make a bunch of polynomial predictors yourself. </p>

<p>By the way, gradient boosting is implemented in Python's scikit-learn library, R's caret library, and Java/Scala's Weka library. </p>
"
"['machine-learning', 'r', 'hadoop', 'random-forest', 'predictive-modeling']",Differences in scoring from PMML model on different platforms,"<p>The difference was, it appears, due to the different implementation of Random Forests in <code>R</code> and <code>Cascading Pattern</code> (as well as <code>openscoring</code> which I tried later) with respect to ties in the tree voting - i.e. when an even number of trees are built (say, 500) and exactly half classify an application as <code>Good</code>, and the other half as <code>Bad</code>, the handling of those situations differs. Solved it by growing and odd (501) number of trees.</p>
"
"['bigdata', 'data-mining', 'algorithms']",Scalable Outlier/Anomaly Detection,"<p>I would take a look at <a href=""https://github.com/tdunning/t-digest"">t-digest algorithm</a>. It's <a href=""https://issues.apache.org/jira/browse/MAHOUT-1361"">been merged into mahout</a> and also a part of some other libraries (github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/stream/quantile/TDigest.java) for big data streaming. You can get more about this algorithm particularly and big data anomaly detection in general in next resources:</p>

<ol>
<li>Practical machine learning anomaly detection book.(info.mapr.com/rs/mapr/images/Practical_Machine_Learning_Anomaly_Detection.pdf)</li>
<li>Webinar: Anomaly Detection When You Don't Know What You Need to Find (youtube.com/watch?v=i-mSV63Q9rA#t=757)</li>
<li>Anomaly Detection in Elasticsearch (info.prelert.com/anomaly-detection-in-elasticsearch)</li>
<li>Beating Billion Dollar Fraud Using Anomaly Detection: A Signal Processing Approach using Argyle Data on the Hortonworks Data Platform with Accumulo (oreilly.com/pub/e/3211)</li>
</ol>
"
"['machine-learning', 'classification', 'python', 'clustering']",Clustering large number of strings based on tags,"<p>Using the Levenshtein distance does not make a lot of sense in this context, as it is made for comparing distances between words.</p>

<p>A commonly used representation for texts is the bag-of-words representation, where a text is converted to a vector where every element in the vector represents the count of the corresponding word. In your case you could represent a text as a bag-of-tags.
The vector representation makes calculating distances a lot easier. However, I believe this is not necessary as you can classify the bags of words with Naive Bayes.</p>

<p>Once you have tried bag-of-words you can try more complicated representations like LDA, word2vec, and the like.</p>
"
['naive-bayes-classifier'],Handling underflow in a Gaussian Naive Bayes classifier,"<p>The standard answer is to work in log space, and manipulate the log of probabilities instead of probabilities, for exactly this reason. This classifier involves products of probabilities which just become sums of log probabilities.</p>

<p>You allude to that already, but the problem you suggest isn't a problem. Internally you don't calculate a probability and then take the log again. It stays in log space. So for very small P, log P is a large negative number while P itself may underflow to 0.0. But you may never need to evaluate P internally.</p>
"
"['machine-learning', 'sklearn']",Can I fine tune the xgboost model instead of re-training it?,"<p>I see that in the current version of python wrapper of xgboost you can specify file name or existing xgboost model (class Booster) in train function.</p>
"
"['beginner', 'career']",Data scientist light?,"<p><strong>Business intelligence</strong> is perfect for you; you already have the business background. If you want to become a bona fide data scientist brush up on your computer science, linear algebra, and statistics. I consider these the bare essentials.</p>

<p>I don't know about Scandinavia, but in the U.S., data science covers a broad spectrum of tasks ranging from full-time software development to full-time data analysis, often with domain expertise required in various niches, such as experimental design. You have to decide where your strengths and interests lie to pick a position on this spectrum, and prepare accordingly. Useful activities include participating in Kaggle competitions, and contributing to open source data science libraries.</p>
"
"['statistics', 'feature-scaling']",PCA and maintaining relationship with target variable,"<p>The short answer is that the y_original and x_reduced are still connected to each other, so it is safe to train your data using y_original and x_reduced.  While x_reduced is on a different scale, as you mentioned via eigenvectors, it still is representative of the data that was attached to that observation, just in a different format.  You lose a lot of interpretability as far as what the actual numbers mean which is why it may seem confusing, but it's just a transformed representation of the x_original that (hopefully) contains enough of the x_original variability to make it useful.</p>
"
"['text-mining', 'data-cleaning', 'data-wrangling']",Detecting boilerplate in text samples,"<p>This might get you started. Phrase length is determined by the range() function. Basically this tokenizes and creates n-grams. Then it counts each token. Tokens with a high mean over all documents (occurs often across documents) is printed out in the last line.</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import nltk

text = """"""DESCRIPTION PROVIDED BY AUTHOR: The goal of my a...
Author provided: The goal of my b...
The goal of my c... END OF TRANSCRIPT
The goal of my d... END SPONSORED BY COMPANY XYZ
The goal of my e... SPONSORED: COMPANY XYZ All rights reserved date: 10/21
""""""

def todocuments(lines):
    for line in lines:
        words = line.lower().split(' ')
        doc = """"
        for n in range(3, 6):
            ts = nltk.ngrams(words, n)
            for t in ts: doc = doc + "" "" + str.join('_', t) 
        yield doc

cv = CountVectorizer(min_df=.5)

fit = cv.fit_transform(todocuments(text.splitlines()))
vocab_idx = {b: a for a, b in cv.vocabulary_.items()}

means = fit.mean(axis=0)
arr = np.squeeze(np.asarray(means))
[vocab_idx[idx] for idx in np.where(arr &gt; .95)[0]]
# ['goal_of_my', 'the_goal_of', 'the_goal_of_my']
</code></pre>
"
"['python', 'neuralnetwork', 'predictive-modeling']",Denormalise data in Neural Networks,"<p>Normalise using your training data statistics. Save the values used (e.g. mean and sd per feature), treating them as part of your model. Once you have used these values to transform input, they become fixed translate/scale factors in the model. </p>

<p>Use the same values to normalise test data or new inputs as required. You do not need to calculate new normalisation constants for new data. In fact doing so will most likely reduce the effectiveness of your model.</p>

<p>The same principle applies to interpreting output values if you need to scale those into range that your model produces. Scale according to your training data.</p>
"
['data-mining'],What is the use of user data collection besides serving ads?,"<p>A couple of days ago developers from one product company asked me how they can understand why new users were leaving their website. My first question to them was what these users' profiles looked like and how they were different from those who stayed. </p>

<p>Advertising is only top of an iceberg. User profiles (either filled by users themselves or computed from users' behaviour) hold information about: </p>

<ul>
<li><strong>user categories</strong>, i.e. what kind of people tend to use your website/product</li>
<li><strong>paying client portraits</strong>, i.e. who is more likely to use your paid services</li>
<li><strong>UX component performance</strong>, e.g. how long it takes people to find the button they need</li>
<li><strong>action performance comparison</strong>, e.g. what was more efficient - lower price for a weekend or propose gifts with each buy, etc. </li>
</ul>

<p>So it's more about improving product and making better user experience rather than selling this data to advertisers. </p>
"
"['r', 'text-mining']",R error using package tm (text-mining),"<p>You have to tell Corpus what kind of source you are using.  Try:</p>

<pre><code>Corpus(VectorSource(d1$Yes))
</code></pre>
"
['r'],Hardware requirements for Linux server to run R & RStudio,"<p>There is no ideal configuration, for <code>R</code> or in general - product selection is always a difficult task and many factors are at play. I think that the solution is rather simple - get the best computer that your budget allows.</p>

<p>Having said that, since you want to focus on <code>R</code> development and one of <code>R</code>'s pressing issues is its <a href=""http://stackoverflow.com/q/5171593/2872891"">critical dependence</a> on the amount of available <em>physical memory</em> (RAM), I would suggest favoring more RAM to other parameters. The second most important parameter, in my opinion, would be <em>number of cores</em> (or <em>processors</em> - see details below), due to your potential <em>multiprocessing</em> focus. Finally, the two next most important criteria I'd pay attention to would be <em>compatibility</em> with Linux and system/manufacturer's <em>quality</em>.</p>

<p>As far as the <em>storage</em> goes, I suggest considering <em>solid state drives (SSD)</em>, if you'd rather prefer to have a bit more more speed than more space (however, if your work will involve intensive disk operations, you might want to investigate the issue of SSD <em>reliability</em> or consult with people, knowledgeable in this matter). However, I think that for R-focused work, disk operations are much less critical than memory ones, as I've mentioned above.</p>

<p>When choosing a specific <em>Linux distribution</em>, I suggest using a well-supported one, such as Debian or, even better, Ubuntu (if you care more about support, choose their LTS version). I'd rather not buy parts and assemble custom box, but some people would definitely prefer that route - for that you really need to know hardware well, but potential compatibility could still be an issue. The next paragraph provides some examples for both <em>commercial-off-the-shelf (COTS)</em> and <em>custom</em> solutions.</p>

<p>Should you be interested in the <em>custom system</em> route, <a href=""http://www.tomshardware.com/forum/295552-28-multiprocessor-multicore-system-rendering-workstation"" rel=""nofollow"">this discussion</a> might be worth reading, as it contains some interesting pricing numbers (just to get an idea of potential savings) and also sheds some light on <em>multiprocessor vs. multi-core</em> alternatives (obviously, the <em>context</em> is different, but nevertheless could be useful). As I said, I would go the COTS route, mainly due to reliability and compatibility issues. In terms of single-processor multi-core systems, your budget is more than enough. However, when we go to multiprocessor workstations (I'm not even talking about servers), even two-processor configurations can go over your budget easily. Some, not far away, such as <a href=""http://www8.hp.com/us/en/campaigns/workstations/z820.html"" rel=""nofollow"">HP Z820 Workstation</a>. It starts from 2439 USD, but in minimal configuration. When you upgrade it to match your desired specs (if it's even possible), I'm sure that we'll be talking about 5K USD price range (extrapolating from the series' higher-level models). What I like about HP Z820, though, is the fact that this system is <a href=""http://www.ubuntu.com/certification/hardware/201011-6754"" rel=""nofollow"">Ubuntu certified</a>. Considering system compatibility and assuming your desire to run Ubuntu, the best way to approach your problem is to go through <a href=""http://www.ubuntu.com/certification"" rel=""nofollow"">Ubuntu-certified hardware lists</a> and shortlist systems that you like. Just for the sake of completeness, take a look at this <a href=""http://www.mediaworkstations.net/i-x2.html"" rel=""nofollow"">interesting multiprocessor system</a>, which in compatible configuration might cost less than from HP or other major vendors. However, it's multimedia-oriented as well as it's reliability and compatibility are unknown, not to mention that it's way over your specified budget.</p>

<p>In terms of <code>R</code> and <code>R</code>-focused software, I highly recommend you to use <em>RStudio Server</em> instead of <em>RStudio</em>, as that will provide you with an opportunity to be able to work from any Internet-enabled location (provided you computer will be running, obviously). Another advice that I have is to keep an eye on <em>alternative <code>R</code> distributions</em>. I'm not talking about commercial expensive ones, but about emerging open source projects, such as <code>pqR</code>: <a href=""http://www.pqr-project.org"" rel=""nofollow"">http://www.pqr-project.org</a>. Will update as needed. I hope this is helpful.</p>
"
"['nlp', 'performance']",Accuracy of Stanford NER,"<p><a href=""http://en.wikipedia.org/wiki/Named-entity_recognition#Formal_evaluation"" rel=""nofollow"">http://en.wikipedia.org/wiki/Named-entity_recognition#Formal_evaluation</a> :</p>

<blockquote>
  <p>To evaluate the quality of a NER system's output, several measures
  have been defined. While accuracy on the token level is one
  possibility, it suffers from two problems: the vast majority of tokens
  in real-world text are not part of entity names as usually defined, so
  the baseline accuracy (always predict ""not an entity"") is
  extravagantly high, typically >90%; and mispredicting the full span of
  an entity name is not properly penalized (finding only a person's
  first name when their last name follows is scored as ½ accuracy).</p>
  
  <p>In academic conferences such as CoNLL, a variant of the F1 score has
  been defined as follows:</p>
  
  <ul>
  <li>Precision is the number of predicted entity name spans that line up exactly with spans in the gold standard evaluation data. I.e. when
  [Person Hans] [Person Blick] is predicted but [Person Hans Blick] was
  required, precision for the predicted name is zero. Precision is then
  averaged over all predicted entity names.</li>
  <li>Recall is similarly the number of names in the gold standard that appear at exactly the same location in the predictions.</li>
  <li>F1 score is the harmonic mean of these two.</li>
  </ul>
  
  <p>It follows from the above definition that any prediction that misses a
  single token, includes a spurious token, or has the wrong class,
  ""scores no points"", i.e. does not contribute to either precision or
  recall.</p>
</blockquote>
"
"['machine-learning', 'classification']",Splitting binary classification into smaller susbsets,"<p>If you try to get the best accuracy, etc... for a given question you should always learn on a training set that is labeled exactly according to your questions. You shouldn't expect to get better results if you are using more granular class labels. The classifier then would then try to pick up the differences in the classes and try to separate them apart. Since in practice your variables in the training set will not perfectly explain the more granular classification question you shouldn't expect to get a better answer for your less granular classification problem.</p>

<p>If you are not happy with the accuracy of your model you try the following instead:</p>

<ol>
<li>review the explanatory variables. Think about what might influence the classification problem. Maybe there us a clever way to construct new variables (from your existing ones) that helps. It's nowpossible to give a general advise on that since you have to consider the properties of your classifier</li>
<li>if your class distribution is very skewed you might consider over/undersampling </li>
<li>you might run more different classifiers and then classify based on the majority vote. Note that you will most likely sacrifice explainability of your model.</li>
</ol>

<p>Also you seem to have some missunderstanding, when you write 'you would assign it to human if it doesn't fall into any of the granular classes'. Note that you always try to pick class labels covering the whole universe (all possible classes). This can be always defined as the complement of the other classes. Also you will have to have instances for each class in your training set.</p>
"
"['neuralnetwork', 'decision-trees', 'normalization']",How to normalize data for Neural Network and Decision Forest,"<p>I disagree with the other comments.</p>

<p>First of all, I see no need to normalize data for <strong>decision trees</strong>. Decision trees work by calculating a score (usually entropy) for each different division of the data $(X\leq x_i,X&gt;x_i)$. Applying a transformation to the data that does not change the order of the data makes no difference.</p>

<p><strong>Random forests</strong> are just a bunch of decision trees, so it doesn't change this rationale.</p>

<p><strong>Neural networks</strong> are a different story. First of all, in terms of prediction, it makes no difference. The neural network can easily counter your normalization since it just scales the weights and changes the bias. The big problem is in the training.</p>

<p>If you use an algorithm like <a href=""https://en.wikipedia.org/wiki/Rprop"" rel=""nofollow"">resilient backpropagation</a> to estimate the weights of the neural network, then it makes <em>no</em> difference. The reason is because it uses the sign of the gradient, not its magnitude, when changing the weights in the direction of whatever minimizes your error. This is the default algorithm for the <code>neuralnet</code> package in R, by the way.</p>

<p>When does it make a difference? When you are using traditional backpropagation with sigmoid activation functions, it can <strong>saturate</strong> the sigmoid derivative.</p>

<p>Consider the sigmoid function (green) and its derivative (blue):</p>

<p><a href=""http://i.stack.imgur.com/OuyeO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OuyeO.png"" alt=""sigmoid""></a></p>

<p>What happens if you do not normalize your data is that your data is multiplied by the random weights and you get things like $s'(9999)=0$. The derivative of the sigmoid is (approximately) zero and the training process does not move along. The neural network that you end up with is just a neural network with random weights (there is no training).</p>

<p>Does this help us to know what the best normalization function is? But of course! First of all, it is crucial to use a normalization that centers your data because most implementation initialize bias at zero. I would normalize between -0.5 and 0.5, $\frac{X-\min{X}}{\max{X}-\min{X}}-0.5$. But <a href=""https://en.wikipedia.org/wiki/Standard_score"" rel=""nofollow"">standard score</a> is also good.</p>

<p>The actual normalization is not very crucial because it only influences the initial iterations of the optimization process. As long as it is centered and most of your data is below 1, then it might mean you have to use slightly less or more iterations to get the same result. But the result will be the same, as long as you avoid the saturation problem I mentioned.</p>

<p>There is something not here discussed which is <strong>regularization</strong>. If you use regularization in your objective function, the way you normalize your data <strong>will</strong> affect the resulting model. I'm assuming your are already familiar with this. If you know that one variable is more prone to cause overfitting, your normalization of the data should take this into account. This is of course completely independent of neural networks being used.</p>
"
"['clustering', 'feature-selection']",Does it make sense to apply clustering on aggregation of data?,"<p>Since some features are missing for specific sources, the missing values are not missing-at-random but are systematically missing. In this situation, I'd advise against doing clustering on the combined data set with all available features. If missing values were occurring at random, you could have used some missing value imputation method before performing cluster analysis. However, since the values are systematically missing, imputation would be difficult to tackle. (You could try to predict those missing values, but I am afraid that will add a lot of unnecessary noise in the data.)</p>

<p>I'd recommend choosing from one of these two options:</p>

<ol>
<li>Perform clustering on the combined data set, <em>but</em> use only those features that are non-missing across all sources. </li>
<li>Perform three different cluster analysis, one for each source. This way, you can ensure that you are using as many features (information) as possible. The determination of ""abnormal"" behavior can then be determined within each source. This can be an added benefit since it would allow you to be more specific about why a use might be abnormal, as you have more features that can be used to explain this. The results can also be then summarized across all sources to create one consolidated report.</li>
</ol>
"
"['machine-learning', 'r', 'random-forest']",Creating obligatory combinations of variables for drawing by random forest,"<p>You may want to consider gradient boosted trees rather than random forests. They're also an ensemble tree-based method, but since this method doesn't sample dimensions, it won't run in to the problem of not having a useful predictor available to split on at any particular time. </p>

<p>Different implementations of GBDT have different ways of handling missing values, which will make a big difference in your case; I believe R does ternary splits which is likely to work fine. </p>
"
['machine-learning'],Machine learning libraries for Ruby,"<p>I'll go ahead and post an answer for now; if someone has something better I'll accept theirs. </p>

<p>At this point the most powerful option appears to be accessing WEKA using jRuby. We spent yesterday scouring the 'net, and this combination was even used by a <a href=""http://www.confreaks.com/videos/867-railsconf2012-practical-machine-learning-and-rails"">talk at RailsConf 2012</a>, so I would guess if there were a comparable pure ruby package, they would have used it.</p>

<p>Note that if you know exactly what you need, there are plenty of individual libraries that either <a href=""https://github.com/febeling/rb-libsvm"">wrap standalone packages like libsvm</a> or <a href=""https://github.com/alexandru/stuff-classifier"">re-implement some individual algorithms like Naive Bayes in pure Ruby</a> and will spare you from using jRuby.</p>

<p>But for a general-purpose library, WEKA and jRuby seem to be the best bet at this time.</p>
"
"['machine-learning', 'neuralnetwork', 'feature-selection', 'feature-extraction']",What features from sound waves to use for an AI song composer?,"<p>First off, ignore the haters.  I started working on ML in Music a long time ago and got several degrees using that work. When I started I was asking people the same kind of questions you are. It is a fascinating field and there is always room for someone new. We all have to start somewhere.</p>

<p>The areas of study you are inquiring about are Music Information Retrieval (<a href=""https://en.wikipedia.org/wiki/Music_information_retrieval"" rel=""nofollow"">Wiki Link</a>) and Computer Music (<a href=""https://en.wikipedia.org/wiki/Computer_music"" rel=""nofollow"">Wiki Link</a>) .  You have made a good choice in narrowing your problem to a single instrument (monophonic music) as polyphonic music increases the difficulty greatly.  </p>

<p>You're trying to solve two problems really:</p>

<p>1) Automatic Transcription of Monophonic Music (<a href=""https://scholar.google.com/scholar?q=Automatic+Transcription+of+Monophonic+Music"" rel=""nofollow"">More Readings</a>) which is the problem of extracting the notes from a single instrument musical piece.</p>

<p>2) Algorithmic Composition (<a href=""https://scholar.google.com/scholar?q=Algorithmic%20Composition"" rel=""nofollow"">More Readings</a>) which is the problem of generating new music using a corpus of transcribed music.</p>

<p>To answer your questions directly:</p>

<blockquote>
  <p>I think that this would be an unsupervised learning problem, but I am
  not really sure.</p>
</blockquote>

<p>Since there are two learning problems here there are two answers. For the Automatic Transcription you will probably want to follow a supervised learning approach, where your classification are the notes you are trying to extract.  For the Algorithmic Composition problem it can actually go either way. Some reading in both areas will clear this up a lot.</p>

<blockquote>
  <p>What features from the sound wave I should extract so that the output music is melodious?</p>
</blockquote>

<p>There are a lot of features used commonly in MIR. @abhnj listed MFCC's in his answer but there are a lot more.  Feature analysis in MIR takes place in several domains and there are features for each.  Some Domains are:</p>

<ol>
<li>The Frequency Domain (these are the values we hear played through a speaker)</li>
<li>The Spectral Domain (This domain is calculated via the Fourier function (<a href=""https://en.wikipedia.org/wiki/Fast_Fourier_transform"" rel=""nofollow"">Read about the Fast Fourier Transform</a>) and can be transformed using several functions (Magnitude, Power, Log Magnitude, Log Power)</li>
<li>The Peak Domain (A domain of amplitude and spectral peaks over the spectral domain)</li>
<li>The Harmonic Domain</li>
</ol>

<p>One of the first problems you will face is how to segment or ""cut up"" your music signal so that you can extract features.  This is the problem of Segmentation (<a href=""https://scholar.google.com/scholar?hl=en&amp;q=music%20segmentation"" rel=""nofollow"">Some Readings</a>) which is complex in itself.  Once you have cut your sound source up you can apply various functions to your segments before extracting features from them. Some of these functions (called window functions) are the: Rectangular, Hamming, Hann, Bartlett, Triangular, Bartlett_hann, Blackman, and Blackman_harris.</p>

<p>Once you have your segments cut from your domain you can then extract features to represent those segments.  Some of these will depend on the domain you selected.  A few example of features are: Your normal statistical features (Mean, Variance, Skewness, etc.), ZCR, RMS, Spectral Centroid, Spectral Irregularity, Spectral Flatness, Spectral Tonality, Spectral Crest, Spectral Slope, Spectral Rolloff, Spectral Loudness, Spectral Pitch, Harmonic Odd Even Ratio, MFCC's and Bark Scale. There are many more but these are some good basics.</p>

<blockquote>
  <p>Is it possible, with recurrent neural networks, to output a vector of sequenced musical notes (ABCDEF)?</p>
</blockquote>

<p>Yes it is. There have been several works to do this already. (<a href=""https://scholar.google.com/scholar?q=Algorithmic%20Composition%20with%20Neural%20Networks"" rel=""nofollow"">Here are several readings</a>)</p>

<blockquote>
  <p>Any smart way I can feed in the features of the soundwaves as well as sequence of musical notes?</p>
</blockquote>

<p>The standard method is to use the explanation I made above (Domain, Segment, Feature Extract) etc.  To save yourself some work I highly recommend starting with a MIR framework such as MARSYAS (<a href=""http://marsyas.info/"" rel=""nofollow"">Marsyas</a>).  They will provide you with all the basics of feature extraction.  There are many frameworks so just find one that uses a language you are comfortable in.</p>
"
"['dataset', 'predictive-modeling', 'data-cleaning', 'linear-regression']","How to start analysing and modelling data for an academic project, when not a statistician or data scientist","<p>Typically, quantitative analysis is planned and performed, based on research study's goals. Focusing on research goals and corresponding research questions, researcher would propose a model (or several models) and a set of hypotheses, associated with the model(s). Model(s) and its/their elements' types usually dictate (suggest) quantitative approaches that would make sense in a particular situation. For example, if your model includes latent variables, you would have to use appropriate methods to perform data analysis (i.e., structural equation modeling). Otherwise, you can apply a variety of other methods, such as time series analysis or, as you mentioned, multiple regression and machine learning. For more details on research workflow with latent variables, also see section #3 in <a href=""http://datascience.stackexchange.com/a/1006/2452"">my relevant answer</a>.</p>

<p>One last note: whatever methods you use, pay enough attention to the following two very important aspects - performing full-scale exploratory data analysis (EDA) (see <a href=""http://datascience.stackexchange.com/a/5095/2452"">my relevant answer</a>) and trying to design and perform your analysis in the reproducible research fashion (see <a href=""http://datascience.stackexchange.com/a/759/2452"">my relevant answer</a>).</p>
"
"['machine-learning', 'markov']",How scientists come up with the correct Hidden Markov Model to use?,"<p>I'm familiar with three main approaches:</p>

<ol>
<li><p>A priori. You might know that there are four base pairs to pick from, and so allow the HMM to have four states. Or you might know that English has 44 phonemes, and so have 44 states for the hidden phoneme layer in a voice recognition model.</p></li>
<li><p>Estimation. The number of states can often be estimated beforehand, perhaps by simple clustering on the observed features of the HMM. If the HMM transition matrix is triangular (which is often the case in failure prediction), the number of states determines the shape of the distribution of total time from the start state to the end state.</p></li>
<li><p>Optimization. Like you suggest, either many models are created and fit and the best model selected. One could also adapt the methodology that learns the HMM to allow the model to add or discard states as needed.</p></li>
</ol>
"
"['bigdata', 'databases']","is there big difference between data Science , big Data and database?","<p>Well, they are absolutely different things but that are somehow linked. I gonna go through each of them.</p>

<p><strong>Data Base</strong></p>

<p>Think of a data base (DB from now) like a computer which only purpose is to store data accesable to be read. By data, and focusing only in SQL-like DB, I mean basically tables of information like and excel file with columns and rows. You can think of a SQL DB like an ecosystem of excel tables which share some common field. So basically a DB is a hardware infrastructure which allows to write and read a given amount of information within it (in the very beggining they were plain computers, of course with the rise of internet specialized hardware appeared). You can build your own DB in your personal computer.</p>

<p><strong>Big Data</strong></p>

<p>"" An intellect which at a certain moment would know all forces that set nature in motion, and all positions of all items of which nature is composed ... for such an intellect nothing would be uncertain and the future just like the past would be present before its eyes."" P.S.Laplace</p>

<p>Laplace did not think over it very deeply before formulate his sentence, obviusly if god had come to him and give what he wanted soon or later he would have realized all that information was indeed useless for him. Where could him store all that? From where should him start to read? What can he do with such amount of information he could never end to compute? In case he could read everything what should he calculate first?</p>

<p>These all are questions Big Data tries to answer and find a solution to. Big data appeared together with the huge websites internet gave birth, such as Amazon or Google. At some point they need to store so many information that it was imposible to store it in a single computer, not even in a big one, so they need to use a set of computers for which previous standard technologies for DB didnt work anymore. This fact was also the seed for No-SQL DB.</p>

<p>More about Big Data and non-sql here: 
<a href=""http://www.kdnuggets.com/2016/07/seven-steps-understanding-nosql-databases.html"" rel=""nofollow"">http://www.kdnuggets.com/2016/07/seven-steps-understanding-nosql-databases.html</a></p>

<p><strong>Data Science</strong></p>

<p>Finally Data Science is a statistical science, which aim is to extract order out of chaos as any other science, however meanwhile the rest of the sciences are focused in a single ""narrow"" piece of knowledge, such as biology, chemistry, etc; data science is on the other hand multidisciplinar science that could face problems from a broaden origin. Examples would be marketing or business oriented, cosmology, etc.</p>

<p>So data science uses mathematical and computer science algorithms to provide some useful information out of a disordered set of data. And here is where the link with Big Data comes, actually is in the question is set before: What can he do with such amount of information he could never end to compute? So Data Science and Big data are a very usual marriage in most IT companies nowadays and in more specific fields day after day. But, data science is apply a set of mathematical algorithms to data (like apply a calculation in an excel file to create a new row) and big data is the technology to have a huge amount of excel files (I use the word ""excel files"" here is just to make it easier to grasp).</p>
"
['career'],Preparation for Career in Data Analysis Without College,"<p>At least based on what I and other data analysts/scientists do in my company, your technical topics list seems sufficient. But I would also add:</p>

<ul>
<li>Visualization (ggplot2 in R, matplotlib in Python, d3.js for really cool stuff)</li>
<li>Design of experiments</li>
</ul>

<p>Communication skills are also quite important.</p>

<p>For more inspiration, here's a good ""curriculum"" represented as a metro map: <a href=""http://nirvacana.com/thoughts/becoming-a-data-scientist/"" rel=""nofollow"">http://nirvacana.com/thoughts/becoming-a-data-scientist/</a></p>

<p>Let me also recommend building up a portfolio of data science projects. This could consist of your analyses of data sets you find online (e.g. on <a href=""https://archive.ics.uci.edu/ml/"" rel=""nofollow"">UCI</a>), Kaggle competitions, or class projects (e.g. via Udacity or Coursera). That way, you can give direct proof of your technical skills, your communication in the form of reports or graphics, and your ability to extract insight.</p>
"
"['data-mining', 'social-network-analysis', 'crawling', 'scraping']",LinkedIn web scraping,"<p>Beautiful Soup is specifically designed for web crawling and scraping, but is written for python and not R:</p>

<p><a href=""http://www.crummy.com/software/BeautifulSoup/bs4/doc/"">http://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></p>
"
"['machine-learning', 'linear-regression']",How exactly does a validation data-set work work in machine learning?,"<p>Machine learning models output some sort of function; for example, a decision tree is a series of comparisons that results in a leaf node, and that leaf node has some associated value. This decision tree predicts survival chance on the Titanic, for example:</p>

<p><a href=""http://i.stack.imgur.com/7OqZ2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7OqZ2.png"" alt=""Titanic decision tree from Wikipedia""></a></p>

<p>(Image by Steven Milborrow, from <a href=""https://commons.wikimedia.org/wiki/File:CART_tree_titanic_survivors.png"" rel=""nofollow"">Wikipedia</a>)</p>

<p>This can be used exactly like the equation generated by linear regression. You feed the new data points into the model, you get predictions out, and then you compare those predictions to the actual values.</p>

<p>The comparison you do is applying a ""cost function,"" and what cost function you use is determined by your application. For example, linear regression can be done with different cost functions; typically, people use ""ordinary least squares"" which tries to minimize mean squared error, but other possibilities exist, like minimizing the absolute value of the error. When the result is a percentage, then something like a log probability loss function is typically a better choice than mean squared difference between the actual and predicted probability.</p>

<p>Even k-nearest neighbors generates a function; it splits the space into regions where the k-nearest neighbors are the same set, and then has a flat value for that region. It ends up very similar to the result generated by a decision-tree based method.</p>

<hr>

<p>There's also a terminology point here: typically, people talk about the training set and the test set, or the training set, the validation set, and the test set. The point of the validation set is different from that of the test set; the test set is used to determine out-of-sample error (that is, how much the model has overfit and what its real world error is likely to be like) whereas the validation set is used to determine what hyperparameters minimize the expected test error. </p>

<p>One could fit ten different models on 80% of the data, determine which one has the lowest error on another 10% of the data, and then finally estimate real-world error on the last 10% of the data.</p>
"
['orange'],Import Orange 2.7 canvas in Orange 3,"<p>Not only the naming has changed, also the individual widget settings and layouts are completely different. Some Orange 2 widgets are not even available in Orange 3 yet, or have been consolidated into other ones. I'm afraid Orange 2 workflows are just not compatible with Orange 3.</p>
"
['nlp'],Coreference Resolution for German Texts,"<p>Here is a couple of tools that may be worth a look:</p>

<ul>
<li>Bart, an open source tool that have been used for several languages, including German. <a href=""http://www.bart-coref.org/"" rel=""nofollow"">Available from the website</a></li>
<li>Sucre is a tool developed at the University of Stuttgart. I don't know if it's available easily. <a href=""http://hnk.ffzg.hr/bibl/acl2011/CoNLL-ST/pdf/CoNLL-ST3.pdf"" rel=""nofollow"">You can see this paper about it</a>.</li>
</ul>
"
"['python', 'optimization']",Optimisation strategy webstores with shipping costs,"<p>I have done the opposite of your problem - I have written code to <em>implement</em> shipping costs for e-commerce sites which runs on their sites.</p>

<p>Shipping cost rules can be almost completely arbitrary logic. In the general case, you have no good choice but to implement them as logic that is assessed per order. That means your optimiser will have to run that logic for every combination of purchases it considers. Which makes this a planning/combinatorics problem.</p>

<p>This may not be so bad - e.g. for purchasing 10 items across 5 stores you have $5^{10}$ combinations which is 9765625. That is possible by brute force.</p>

<p>For larger orders, or more choices of store, you may want to look at dynamic solutions to optimise cost, such as simulated annealing, genetic algorithms. Reinforcement learning run per order may also work.</p>
"
"['hadoop', 'databases', 'sql', 'mongodb']",Storing Sensor Data for Analysis of the Office,"<p>I have been doing similar project in my college. I have classroom and I'm supposed to collect data like temp, humidity, light, occupancy, etc. Assuming that you have worked with sensors and motes to use, I'm going to explain rest of the structure.</p>

<p>You need sensor network setup and like you said you have done it. These sensor networks generally do not send data directly over internet so you need a Gateway that can collect data from sensors and send it over internet to local server.</p>

<p>On server side you need REST API and you could use any language to develop it and I use PHP. I find it very easy to use and develop using PHP. This REST API shall receive data from Gateway and store it into database. I use mysql database because amount of data is not so big for us. But if your data is big enough you can use big data Nosql tool like mongoDB or so. Whatever type of database you use structure remains same. </p>

<p>For sending data from Gateway to server you can use protocols like HTTP or MQTT whichever you feel comfortable. What I do is I have WSN controller that sends data over USB to Gateway then Gateway sends data to server over Ethernet. So I had to develop USB to Ethernet Gateway. If you can just take two UART terminals out of your controller you can build UART to Ethernet Gateway using any microcontroller or even Arduino Ethernet shield would work in that case.</p>

<p>In my case data is sensed periodically but as you said you are sensing data when event of interest occurs then you can use poisson distribution method over periodically collected data to predict what is average number of events per day and then you can decide if your data is big or not.</p>
"
"['machine-learning', 'parsing']",Is parsing files an application of machine learning?,"<p>I would answer the question at two levels. The first level is ""<strong>can it be done using machine learning</strong>?"" I would say that machine learning is essentially about learning. So given that you prepare sufficient examples of sample documents and the output to expect from those documents, you can train a network to learn the structure of documents and extract the relevant information. The more general form of extracting information from documents is a well-researched problem and is more commonly known as <a href=""https://en.wikipedia.org/wiki/Information_retrieval"">Information Retrieval</a>. And it is not limited to just machine learning techniques, you can use Natural Language Processing tools as well. So, in its general form, it is actually being done in practice. Coming to the second level, ""<strong>should you be doing it using machine learning</strong>?"". I would agree to what @NeilSlater said. The better and more feasible approach would be to use good programming practices so that you can reuse parts of your parser as your dataset evolves.</p>
"
"['machine-learning', 'neuralnetwork']",How exactly does adding a new unit work in Cascade Correlation?,"<p>I've been reading up on cascade correlation quite a bit recently and made a python implementation <a href=""https://github.com/DanielSlater/CascadeCorrelation"" rel=""nofollow"">https://github.com/DanielSlater/CascadeCorrelation</a> (though it still needs a bit of cleaning up/extra work and has a bunch of me mucking around with using Particle Swarm Optimization for selecting candidates, definitely not production ready).</p>

<p>To try and explain step 3.</p>

<ul>
<li>Start by creating a number of candidate hidden nodes with random weights. These have incoming connection from all existing hidden nodes and input nodes.</li>
<li>We then use that equation $$S=\sum_o \left | \sum_p(V_p - \overline{V})(E_{p,o} - \overline{E_o}) \right |$$ to train the candidate nodes. </li>
<li>The residual output error is the difference between the output of the network and the target value(think sum of squared error without the square).</li>
<li>$S$ is the correlation between the activation of our candidate node and the the residual error.</li>
<li>$V_p$ is the activation of the candidate node given input $p$.</li>
<li>After a bit of backprop training against $S$ we choose our best candidate. This becomes a new hidden node.</li>
<li>This is when it's weights are frozen. That is to say after random initialization then back-prop training, then selecting the best one.</li>
</ul>

<p>Hope this helps :)</p>
"
"['python', 'hadoop', 'k-means', 'map-reduce', 'distributed']",How to make k-means distributed?,"<p>Unless you are trying to do this as a learning exercise, just use Spark which has ML libraries made for distributed computing.  See <a href=""http://spark.apache.org/docs/latest/mllib-clustering.html"" rel=""nofollow"">http://spark.apache.org/docs/latest/mllib-clustering.html</a></p>
"
"['python', 'bigdata', 'nlp', 'scikit', 'dimensionality-reduction']",Improve the speed of t-sne implementation in python for huge data,"<p>Since, there are no answers in SO, I have asked myself in github page and the issue has been closed by stating the following reply by GaelVaroquaux..</p>

<blockquote>
  <p>If you only want to parallelise vector operation, then you should use a
  build of numpy compiled with MKL (don't attempt to do it yourself, it's
  challenging).</p>
  
  <p>There could be approaches to high-level parallelism in the algorithm
  itself, which would probably lead to larger gains. However, after a quick
  look at the code, I didn't see any clear way of doing that.</p>
  
  <p>I am going to ahead and close this issue, as it is more of a blue-sky
  whish list. I completely agree, I would like TSNE to go faster, and it
  would be great is parallelism was easy. But in the current state of
  affairs, more work is required to be in a state where we can tackle such
  wish list.</p>
</blockquote>
"
"['machine-learning', 'neuralnetwork', 'julia']",Best Julia library for neural networks,"<p><a href=""https://github.com/pluskid/Mocha.jl"">Mocha.jl</a> - Mocha is a Deep Learning framework for Julia, inspired by the C++ framework Caffe.</p>

<p>Project with good <a href=""http://mochajl.readthedocs.org/en/latest/"">documentation</a> and examples.
Can be run on CPU and GPU backend.</p>
"
"['r', 'dataset', 'data-cleaning', 'data-wrangling']",Technical name for this data wrangling process? Multiple columns into multi-factor single column,"<p>It is usually called reshaping! For a great description of the process, see <a href=""http://www.statmethods.net/management/reshape.html"" rel=""nofollow"">this walkthrough</a>, or read up on Hadley Wickham's documentation for <a href=""https://github.com/hadley/reshape"" rel=""nofollow"">the <code>reshape</code> package</a>!</p>
"
"['machine-learning', 'data-mining', 'algorithms', 'neuralnetwork']",How to use neural networks with large and variable number of inputs?,"<p>I had almost the same problem: 'restoring' age, gender, location for social network users. But I used users' ego-networks, not visited sites statistics. And I faced with two almost independent tasks:</p>

<ol>
<li>'Restoring' or 'predicting' data. You can use a bunch of different technics to complete this task, but my vote is for simplest ones (KISS, yes). E.g., in my case, for age prediction, mean of ego-network users' ages gave satisfactory results (for about 70% of users error was less than +/-3 years, in my case it was enough). It's just an idea, but you can try to use for age prediction weighted average, defining weight as similarity measure between visited sites sets of current user and others.</li>
<li>Evaluating prediction quality. Algorithm from task-1 will produce prediction almost in all cases. And second task is to determine, if prediction is reliable. E.g., in case of ego network and age prediction: can we trust in prediction, if a user has only one 'friend' in his ego network? This task is more about machine-learning: it's a binary classification problem. You need to compose features set, form training and test samples from your data with both right and wrong predictions. Creating appropriate classifier will help you to filter out unpredictable users. But you need to determine, what are your features set. I used a number of network metrics, and summary statistics on feature of interest distribution among ego-network.</li>
</ol>

<p>This approach wouldn't populate all the gaps, but only predictable ones.</p>
"
"['dataset', 'graphs']",Network analysis classic datasets,"<p>What you are looking for can be found in <a href=""http://konect.uni-koblenz.de/networks"" rel=""nofollow"">KONECT</a> (the website is down as I'm writing this but it should be fixed soon!). It's almost the most comprehensive data collection for network analysis. But the question is which one is more <strong>standard</strong> to use?</p>

<p>Well, there is no clear answer except of Zachary's Karate Club!</p>

<p>If you do a literature review in Community Detection algorithms you'll see that almost all shining papers use different networks. My suggestion is going through what Andrea Lancichinetti and Santo Fortunato did for benchmarking graphs. They proposed some benchmark graph generation algorithms e.g. <a href=""http://arxiv.org/abs/0904.3940"" rel=""nofollow"">this one</a>. </p>

<p>Hope it helps :) </p>
"
"['machine-learning', 'computer-vision']",How to create and format an image dataset from scratch for machine learning?,"<p>Looking at existing challenges around and their data format (for example <a href=""http://www.kaggle.com/c/datasciencebowl/data"" rel=""nofollow"">http://www.kaggle.com/c/datasciencebowl/data</a>) I would say put the images in a folder per class. You can use the file names for the index.</p>
"
['reinforcement-learning'],AlphaGo (and other game programs using reinforcement-learning) without human database,"<p>The same question has been asked to the author of the AlphaGo paper and his answer was that we dont know what would happen if AlphaGo would learn from scratch (they haven't tested it).</p>

<p>However, given the complexity of the game, it would be a difficult task to train an algorithm from scratch without prior knowledge. Thus, it is reasonable at the beginning to start building such a system by upgrading it to a Master level using knowledge acquired by humans.</p>

<p>It is worth noting that, although the human moves bias the action selection at the tree nodes (states), this prior has a decay factor. This means that increased visitations to a specific state, reduce the strength of the prior to encourage the algorithm to explore.</p>

<p>The current level of Mastery of AlphaGo is unknown how close or far it is to a human's way of playing (in the tournament it did one move that a human had almost zero probability to perform!- but equally did some really bad moves as well). Possibly it remains for all these questions to be answered by actually implementing the corresponding testing algorithms.</p>
"
"['classification', 'logistic-regression', 'scikit', 'parameter', 'kaggle']",Finding parameters with extreme values (classification with scikit-learn),"<p>If by ""parameters"" you mean features (called ""Data Fields"" at Kaggle), then, yes, you can log-scale those. To visualize them you can just use histograms.
To do it for all features in python, for example, you can put your data in pandas DataFrame (let us call it ""data"") and then use data.hist()
This has nothing to do with the regularization in any model.</p>

<p>If by ""parameters"" you mean the <em>coefficients</em> obtained after fitting the logistic regression, then one uses regularization. This has, however, is not directly related to log-transform. How you list/visualize your coefficients depends on the programming tool you use for logistic regression (or other model)</p>
"
"['python', 'pandas']",Histogram of some values only,"<p>Ok, after some digging around I found that I can pass a range = (1,100) and that does the trick.</p>
"
['education'],What do you think of Data Science certifications?,"<p>I did the first 2 courses and I'm planning to do all the others too.  If you don't know R, it's a really good program. There are assignments and quizzes every week. Many people find some courses very difficult. You are going to have hard time if you don't have any programming experience (even if they say it's not required). </p>

<p>Just remember.. it's not because you can drive a car that you are a F1 pilot ;) </p>
"
"['neuralnetwork', 'convnet']",What are deconvolutional layers?,"<p><em>Deconvolution layer</em> is a very unfortunate name and should rather be called a <a href=""https://github.com/tensorflow/tensorflow/issues/256#issuecomment-162257789""><em>transposed convolutional layer</em></a>.</p>

<p>Visually, for a transposed convolution with stride one and no padding, we just pad the original input (blue entries) with zeroes (white entries) (Figure 1).</p>

<p><a href=""http://i.stack.imgur.com/YyCu2.gif""><img src=""http://i.stack.imgur.com/YyCu2.gif"" alt=""Figure 1""></a></p>

<p>In case of stride two and padding, the transposed convolution would look like this (Figure 2): </p>

<p><a href=""http://i.stack.imgur.com/f2RiP.gif""><img src=""http://i.stack.imgur.com/f2RiP.gif"" alt=""Figure 2""></a></p>

<p>You can find more (great) visualisations of convolutional arithmetics <a href=""https://github.com/vdumoulin/conv_arithmetic"">here</a>.</p>
"
['recommendation'],Price optimization for tiered and seasonal products,"<p>You should be able to use <a href=""https://en.wikipedia.org/wiki/Linear_regression"" rel=""nofollow"">linear regression</a> to find correlation between the factors which cause your products to sell better (or worse).</p>

<p>There are many correlations you can test against in this data set. Some examples are:</p>

<ol>
<li>If a product has been marketed aggressively, does it sell more quickly?</li>
<li>If a low tier item is available, do fewer high-tier items sell?</li>
<li>If multiple high-tier items are available, are fewer sold of each item?</li>
</ol>

<p>Keep in mind that correlation does not necessarily imply causation. Always think about other factors which may cause sales to go up and down. For example, you may sell more high tier items in a season one year than another year. But, this could be due to changes in the overall economy, rather than changes in your pricing.</p>

<p>The second thing you can do is perform <a href=""https://en.wikipedia.org/wiki/A/B_testing"" rel=""nofollow"">A/B tests</a> on your product sales pages. This gives you clear feedback right away. Some example tests could be:</p>

<ol>
<li>Show the user one high-tier product and one low-tier product (A). Show the user two high-tier products and no low-tier products(B). Which page generates more revenue?</li>
<li>Send out marketing emails for a seasonal sale 5 days in advance to one group of users (A). Send the same email to a different set of users 1 day in advance (B).</li>
</ol>

<p>There are many possibilities. Use your intuition and think about previous knowledge you have about your products.</p>
"
"['machine-learning', 'reinforcement-learning']",Difference between AlphaGo's policy network and value network,"<p>Although my answer is a bit late I hope it helps. I will try not to get into lots of details, as people covered pretty much of everything in technical terms, but I will give you a slightly different perspective. In brief each net has a different purpose as you mentioned:</p>

<ul>
<li>The value network was used at the leaf nodes to reduce the depth of the tree search.</li>
<li>The policy network was used to reduce the breadth of the search from a node (guiding towards promising immediate actions).</li>
</ul>

<p>In general, you can use value function methods to find an optimal policy or directly search in the policy space to optimize a parametrized policy function (of course there are pros and cons). You can use function approximators (e.g Deep Nets) in each case. I see that mainly you are confused about the policy net so I focus my answer into this.</p>

<p>The policy net was first:</p>

<p>trained to do the moves that most likely a human would do given a board state (so input is a board state and output is a histogram that shows the probability of each action given that state). The net can approximate the probability function underlying the mapping from states to actions. It is reasonable to think to start building your policy from available data after all. After supervised training using experts moves the policy net could play the game sufficient (although far from a Master's level). Simply, you attempted to capture the general pattern of action selection of professional players.</p>

<p>Then,</p>

<p>it was trained in games with opponent itself, in order to optimize the previous-learned policy. This time its weights were updated using the REINFORCE algorithm. By doing this, you update the net parameters towards maximization of expected reward. Eventually you have a net that not only selects the actions like a professional player but also towards winning the game (However it cannot plan!).</p>

<p>After this step, they approximated the value function of a bit more noisy version of the learned policy, by regression (input is the state board and target the result of the game). You can use this network to affect the leaf node evaluation.</p>

<p>Conceptually speaking, the policy net gives you a probability over actions, but this doesn't indicate that you will end up in a good, for winning the game, state. AlphaGo had some ""blind spots"" and during the tournament did some really bad moves but also one exceptional move that a human could never had thought.</p>

<p>Finally you can use your planning algorithm (MCTS) in combination with these nets. Why we took all these steps? Briefly, the simple MCTS without any ""intuition"" would have failed.</p>

<p>Hope it helps you understand better (if it is still unclear) the differences and similarities of the two nets.</p>
"
"['data-mining', 'classification', 'data-cleaning']",Use test data as train: does it make sense?,"<p>This idea will most likely increase the bias in the model. Let's assume that the model has non-zero bias in the model. In this case, when it assumes its predictions to be true, without confirmation from an Oracle as in <a href=""https://en.wikipedia.org/wiki/Active_learning_(machine_learning)"" rel=""nofollow"">active learning</a>, the bias of the model increases. In common terms, if the model has some amount of bias in its predictions, and it uses its predictions to learn on, the bias in the model can only increase. This issue does not arise when there is 0 bias in the model to begin with, however, in that case, there is no need to learn any further!  </p>

<p>Note that this is a highly intuitive answer but I cannot think of an argument against the intuition :-) I will appreciate any feedback on this. </p>
"
['graphs'],Hub removal from graphs,"<p>Depends on how you define ""hubs"". In Network Science, a hub is simply a node with high degree i.e. those node who contribute to the power-law nature of the degree distribution the most. But you can also find other definitions for instance according to the information flow in the network where hubs are defined as those node that are critical in the process information flow (also called central nodes). </p>

<h2>My Suggestions</h2>

<ol>
<li><p><strong>Degree Distribution:</strong> The simplest approach would be to choose high degree nodes as hubs. To ensure your results a bit more I recommend to calculate the summation of all correlations correspond to each node and have a look at these numbers as well. In this case you are looking for nodes which have high degrees and contain a larger value of correlation.</p></li>
<li><p><strong>Centrality Measure:</strong> or <em>Betweenness</em> introduced by Linton Freeman which again somehow measures the influence of a node in the process of information flow over the network. Calculating the centrality for a vertex $v$ in a graph has basically 3 steps:</p>

<ul>
<li>For each pair of vertices $(s,t)$, compute the shortest paths between them.</li>
<li>For each pair of vertices $(s,t)$, determine the fraction of shortest paths that pass through the vertex in question (here, vertex $v$).</li>
<li>Sum this fraction over all pairs of vertices $(s,t)$.</li>
</ul></li>
</ol>

<p>For more information read <a href=""https://en.wikipedia.org/wiki/Centrality"" rel=""nofollow"">this</a> carefully and in case you need any help (specially for implementation) just drop me a line in the comments :)</p>

<p>Good Luck!</p>
"
"['scikit', 'categorical-data', 'feature-engineering']",When to use One Hot Encoding vs LabelEncoder vs DictVectorizor?,"<p>There are some cases where LabelEncoder or DictVectorizor are useful, but these are quite limited in my opinion due to ordinality.</p>

<p>LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat.  Still there are algorithms like decision trees and random forests that can work with categorical variables just fine and LabelEncoder can be used to store values using less disk space.</p>

<p>One-Hot-Encoding has a the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space.  The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality. In these cases, I typically employ one-hot-encoding followed by PCA for dimensionality reduction.  I find that the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes.  PCA finds the linear overlap, so will naturally tend to group similar features into the same feature.</p>

<p>Hope this helps!</p>
"
"['neuralnetwork', 'deep-learning', 'rnn']",LSTMs: what is Wx & Uz in φ(Wx + Uz + b)?,"<p>If you look deeper in LSTMs or GRUs, we observe that the gates(input, output, cell or forget based on the RNN) are calculated using an equation like you specified.</p>

<p>Example, according to <a href=""http://deeplearning.net/tutorial/lstm.html"" rel=""nofollow"">deep learning tutorial of lstm</a>, <strong>i<sub>t</sub>=sigma(W<sub>i</sub> x<sub>t</sub> + U<sub>i</sub> h<sub>t-1</sub> + b<sub>i</sub>)</strong> In this, h is the hidden state vector and x is the input state vector as specified and W and U are the corresponding weights for the input gate i<sub>t</sub>. SImilarly, there are gates for output and forget. So in the paper, they recall a gist of RNNs and sum it up as a general equation. It is a common computational block in RNNs despite their minor differences.</p>

<p>Refer <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow"">Colah's blog</a> or <a href=""http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"" rel=""nofollow"">wildml</a>, I think they are one of the best to understand RNNs.</p>
"
"['machine-learning', 'python', 'r']","Where can I find a software library for pairwise matching (ideally, Python, R, Java)?","<p>If you can transform those sentences into number vectors (e.g. into a <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow"">bag of words</a> or <a href=""https://en.wikipedia.org/wiki/Tf-idf"" rel=""nofollow"">tf-idf</a> representation), I guess you could use k-Means or hierarchical clustering functionality from <a href=""http://orange.biolab.si/"" rel=""nofollow"">Orange</a>, a GUI and machine learning library written in Python.</p>

<p>It also has an <a href=""https://github.com/biolab/orange3-text/"" rel=""nofollow"">add-on for text mining</a> specifically, but I cannot attest to it as I haven't tried it yet.</p>
"
"['time-series', 'deep-learning', 'rnn', 'prediction']",Using RNN (LSTM) for predicting one feature value of a time series,"<ol>
<li>The way you are doing it is just fine. The idea in time series
prediction is to do regression basically. Probably what you have
seen other places in case of vector, it is about the size of the
input or basically it means feature vector. Now, assuming that you
have <code>t</code> timesteps and you want to predict time <code>t+1</code>, the best way
of doing it using either time series analysis methods or RNN models
like LSTM, is to train your model on data up to time <code>t</code> to predict
<code>t+1</code>. Then <code>t+1</code> would be the input for the next prediction and so
on. There is a good example <a href=""http://stackoverflow.com/questions/25967922/pybrain-time-series-prediction-using-lstm-recurrent-nets"">here</a>. It is based on LSTM using the pybrain framework.</li>
<li>Regarding your question on <code>batch_size</code>, at first you need to understand the difference between batch learning versus online learning. Batch size basically indicates a subset of samples that your algorithms is going to use in gradient descent optimization and it has nothing to do with the way you input the data or what you expect your output to be. For more information on that I suggest you read <a href=""https://www.kaggle.com/c/datasciencebowl/forums/t/12634/can-someone-explain-what-batch-size-is-doing-in-convolutional-nns/100344"" rel=""nofollow"">this kaggle</a> post.</li>
</ol>
"
"['machine-learning', 'text-mining', 'multiclass-classification']",how to generate sample dataset for classification problem,"<p>I know this isn't answering the question that you actually asked, but I suggest that you <strong>NOT</strong> generate data for your 'short text' categorization problem.</p>

<p>Generated data can work for certain cases when data scientists who are very familiar with an algorithm want to demonstrate a specific feature, but there is a hokeyness that may lead you astray as someone new to data science and machine learning.</p>

<p>I suggest you avail yourself of some <a href=""http://datascience.stackexchange.com/questions/155/publicly-available-datasets/259#259"">free and open data here</a>, or an <a href=""http://opendata.stackexchange.com/"">entire stack exchange dedicated to open data</a>.</p>

<p>Real data will give you experience with less contrived problems and you can often try out similar ML models on very different data sets in order to gain experience with the variations and pitfalls of data science. Be creative... you might not find exactly what you want, but you can subset the data by projecting out a few text fields and using them to classify another field.</p>

<p><strong>Addendum update:</strong></p>

<p>With your recent edit, it is now apparent that you are seeking to turn your unsupervised training data into supervised training data in order to train a supervised learning classification model.  The method that you suggested, ""classify some dataset using regex or lucene rule based matches and manually verify them"", is a deterministic unsupervised method without the human verification step. </p>

<p><strong>Without the human verification of the target data</strong> that you have created, I would <strong>not</strong> consider feeding the derived targets back into the classification algortihm as your results will only be as strong as the derived target data and predictions will show similar errors. Instead, you should think about a semi-supervised learning method where you perhaps employ a clustering algorithm and then label the clusters with the target variable.</p>

<p><strong>With human verification of the target data</strong> that you have created, this will work just fine as training data for a classification model. The only issue is that this can become tedious. There are <a href=""https://www.mturk.com/mturk/welcome"" rel=""nofollow"">mechanical turks</a> (humans paid to perform repetitive tasks) that you can hire to perhaps perform the labeling for you, which may be a more scalable option.</p>

<p>Hope this helps!</p>
"
"['time-series', 'online-learning']",online detection of plateaus in time series,"<p>I found a good solution for my problem here: <a href=""http://stats.stackexchange.com/a/201315/101744"">http://stats.stackexchange.com/a/201315/101744</a></p>

<p>Thanks to everyone!</p>
"
"['machine-learning', 'dimensionality-reduction', 'visualization']",Purpose of visualizing high dimensional data?,"<p>I take Natural Language Processing as an example because that's the field that I have more experience in so I encourage others to share their insights in other fields like in Computer Vision, Biostatistics, time series, etc. I'm sure in those fields there are similar examples.</p>

<p>I agree that sometimes model visualizations can be meaningless but I think the main purpose of visualizations of this kind are to help us check if the model actually relates to human intuition or some other (non-computational) model. Additionally, Exploratory Data Analysis can be performed on the data.</p>

<p>Let's assume we have a word embedding model built from Wikipedia's corpus using <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">Gensim</a></p>

<pre><code>model = gensim.models.Word2Vec(sentences, min_count=2)
</code></pre>

<p>We would then have a 100 dimension vector for each word represented in that corpus that's present at least twice. So if we wanted to visualize these words we would have to reduce them to 2 or 3 dimensions using the t-sne algorithm. Here is where very interesting characteristics arise.</p>

<p>Take the example:</p>

<p>vector(""king"") + vector(""man"") - vector(""woman"") = vector(""queen"")</p>

<p><img src=""http://multithreaded.stitchfix.com/assets/images/blog/vectors.gif"" alt=""http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/""></p>

<p>Here each direction encode certain semantic features. The same can be done in 3d</p>

<p><img src=""http://www.tensorflow.org/versions/master/images/linear-relationships.png"" alt=""http://www.tensorflow.org/versions/master/images/linear-relationships.png""></p>

<p>See how in this example past tense is located in a certain position respective to its participle. The same for gender. Same with countries and capitals.</p>

<p>In the word embedding world, older and more naive models, didn't have this property. </p>

<p>See this Stanford lecture for more details.
<a href=""https://www.youtube.com/watch?v=T8tQZChniMk"" rel=""nofollow"">Simple Word Vector representations: word2vec, GloVe</a></p>

<p>They only were limited to clustering similar words together without regard for semantics (gender or verb tense weren't encoded as directions). Unsurprisingly models which have a semantic encoding as directions in lower dimensions are more accurate. And more importantly, they can be used to explore each data point in a more appropriate way. </p>

<p>In this particular case, I don't think t-SNE is used to aid classification per se, it's more like a sanity check for your model and sometimes to find insight in the particular corpus you are using. As for the problem of the vectors not being in original feature space anymore. Richard Socher explains in the lecture (link above) that low dimensional vectors share statistical distributions with its own larger representation as well as other statistical properties which make plausible visually analyse in lower dimensions embedding vectors.</p>

<p><strong>Additional resources &amp; Image Sources:</strong></p>

<ol>
<li><p><a href=""http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/"" rel=""nofollow"">http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/</a></p></li>
<li><p><a href=""http://www.tensorflow.org/tutorials/word2vec/index.html#motivation_why_learn_word_embeddings%3F"" rel=""nofollow"">http://www.tensorflow.org/tutorials/word2vec/index.html#motivation_why_learn_word_embeddings%3F</a></p></li>
<li><p><a href=""http://deeplearning4j.org/word2vec.html"" rel=""nofollow"">http://deeplearning4j.org/word2vec.html</a></p></li>
<li><p><a href=""http://www.tensorflow.org/tutorials/word2vec/index.html#motivation_why_learn_word_embeddings%3F"" rel=""nofollow"">http://www.tensorflow.org/tutorials/word2vec/index.html#motivation_why_learn_word_embeddings%3F</a></p></li>
</ol>
"
"['r', 'data-cleaning']",Code Vectorization of gsub in R,"<p>You have plenty of alternatives for this problem</p>

<ul>
<li><p><strong>Using sapply</strong></p>

<pre><code>data$abstract &lt;- sapply(data$abstract, 
                        function(x){gsub(pattern = ""no abstract available"",
                        replacement = "" "", x)})
</code></pre></li>
<li><p><strong>Using mapply</strong></p>

<pre><code>data$abstract &lt;- mapply(gsub, pattern = ""no abstract available"",
                        replacement = "" "", data$abstract)
</code></pre></li>
<li><p><strong>Using the stringr package</strong></p>

<pre><code>library(stringr)
data$abstract &lt;- str_replace(data$abstract, ""no abstract available"", 
                             "" "")
</code></pre></li>
</ul>

<p>Also, check this <a href=""https://stackoverflow.com/questions/19424709/r-gsub-pattern-vector-and-replacement-vector"">this question on StackOverflow</a> for more information, like solutions with match and the qdap package.</p>
"
"['python', 'predictive-modeling', 'scikit', 'sklearn', 'categorical-data']",Scikit Learn OneHotEncoded Features causing error in classifier,"<pre><code>scores = cross_val_score(mnb, Y, chk, cv=10, scoring='accuracy')
</code></pre>

<p>You have your <code>Y</code> and <code>chk</code> switched. That's it. :)</p>

<p>The signature of <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html"" rel=""nofollow""><code>cross_val_score</code></a> is <code>sklearn.cross_validation.cross_val_score(estimator, X, y)</code>.</p>

<p><code>X</code> is a matrix and <code>y</code> is a 1D vector with your class labels.</p>

<p>unlike in R, most (or all?) sklearn models do not support categorical variables. Most of the time, encoding your feature matrix <code>X</code> into what is called <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow"">one-hot encoding</a> is good enough.</p>

<p>Notice that, in some models, this hack is not the same as true native categorical support, and the performance of the model will be worse.</p>

<p><strong>Invert One-Hot Encoding</strong></p>

<p>Sklearn does not seem to have an easy method to invert the one-hot encoding.</p>

<p>It is not trivial how to do this. I found <a href=""http://stackoverflow.com/a/35898452/2680707"">this suggestion</a>:</p>

<pre><code>def inverse(enc, out, shape):
    return np.array([enc.active_features_[col] for col in out.sorted_indices().indices]).reshape(shape) - enc.feature_indices_[:-1]
</code></pre>

<p>Example:</p>

<pre><code>import numpy as np
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
X = np.array([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]))
Z = enc.fit_transform(X)
print(inverse(enc, Z, X.shape))
# [[0 0 3]
#  [1 1 0]
#  [0 2 1]
#  [1 0 2]]
print(X)
# [[0 0 3]
#  [1 1 0]
#  [0 2 1]
#  [1 0 2]]
</code></pre>

<p>Notice:</p>

<ul>
<li>This only works when <code>HotOneEncoding(sparse=True)</code> (default) because it uses scipy sparse matrix methods (this could be changed by making the code only use numpy methods), but this is probably what you want since working with a dense matrix will kill your memory anyhow</li>
<li>I think this will only work if your variables are within the range [0,something] because you lose that information in the transformation (no work-around for this other than you using something like <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"" rel=""nofollow"">DictVectorizer</a> which offers you more control over the transformation.</li>
</ul>
"
"['algorithms', 'clustering']",K-means vs. online K-means,"<p>Online k-means (more commonly known as <a href=""http://stackoverflow.com/questions/3698532/online-k-means-clustering"">sequential k-means</a>) and traditional k-means are very similar.  The difference is that online k-means allows you to update the model as new data is received.</p>

<p>Online k-means should be used when you expect the data to be received one by one (or maybe in chunks).  This allows you to update your model as you get more information about it.  The drawback of this method is that it is dependent on the order in which the data is received (<a href=""http://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/C/sk_means.htm"">ref</a>).  </p>
"
"['efficiency', 'distributed', 'performance']",How to speedup message passing between computing nodes,"<p>Firstly, I would generally agree with everything that AirThomas suggested. Caching things is generally good if you can, but I find it slightly brittle since that's very dependent on exactly what your application is. Data compression is another very solid suggestion, but my impression on both of these is that the speedups you're looking at are going to be relatively marginal. Maybe as high as 2-5x, but I would be very surprised if they were any faster than that.</p>

<p>Under the assumption that pure I/O (writing to/reading from memory) is <em>not</em> your limiting factor (if it is, you're probably not going to get a lot faster), I would make a strong plug for <a href=""http://zeromq.org/"">zeromq</a>. In the words of the creators:</p>

<blockquote>
  <p>We took a normal TCP socket, injected it with a mix of radioactive
  isotopes stolen from a secret Soviet atomic research project,
  bombarded it with 1950-era cosmic rays, and put it into the hands of a
  drug-addled comic book author with a badly-disguised fetish for
  bulging muscles clad in spandex. Yes, ØMQ sockets are the world-saving
  superheroes of the networking world.</p>
</blockquote>

<p>While that may be a little dramatic, <code>zeromq</code> sockets in my opinion are one of the most amazing pieces of software that the world of computer networks has put together in several years. I'm not sure what you're using for your message-passing layer right now, but if you're using something traditional like <code>rabbitmq</code>, you're liable to see speedups of multiple orders of magnitude (personally noticed about 500x, but depends a lot of architecture)</p>

<p>Check out some basic benchmarks <a href=""http://blog.x-aeon.com/2013/04/10/a-quick-message-queue-benchmark-activemq-rabbitmq-hornetq-qpid-apollo/"">here.</a></p>
"
"['machine-learning', 'feature-selection', 'unbalanced-classes']",What are the implications for training a Tree Ensemble with highly biased datasets?,"<p>Yes, it's problematic.  If you oversample the minority, you risk overfitting.  If you undersample the majority, you risk missing aspects of the majority class.  Stratified sampling, btw, is the equivalent to assigning non-uniform misclassification costs.  </p>

<p>Alternatives:</p>

<p>(1) Independently sampling several subsets from the majority class and making multiple classifiers by combining each subset with all the minority class data, as suggested in the answer from @Debasis and described in this <a href=""http://cse.seu.edu.cn/people/xyliu/publication/tsmcb09.pdf"">EasyEnsemble paper</a>, </p>

<p>(2) <a href=""http://arxiv.org/pdf/1106.1813.pdf"">SMOTE (Synthetic Minority Oversampling Technique)</a> or <a href=""http://www3.nd.edu/~nchawla/papers/ECML03.pdf"">SMOTEBoost, (combining SMOTE with boosting)</a> to create synthetic instances of the minority class by making nearest neighbors in the feature space.  SMOTE is implemented in R in <a href=""http://cran.r-project.org/web/packages/DMwR/index.html"">the DMwR package</a>.</p>
"
"['machine-learning', 'classification', 'clustering']",Classification problem where one attribute is a vector,"<p>1) If you want to build a model with:</p>

<pre><code>Input: Items bought
Output: Win/Loss
</code></pre>

<p>then you will probably want to learn a non-linear combination of the inputs to represent a build.  For example <code>item_X</code> may have very different purpose when paired with <code>item_Y</code> than with <code>item_Z</code>.</p>

<p>For the input format, you may consider creating a binary vector from the item list.  For example if there were only ten items, a game in which the champion purchased <code>items 1,4,5,9</code> (in any order) would look like row 1; a game where he also purchased <code>item 2</code> and <code>7</code> would look like row 2:</p>

<pre><code>item_ID   | 0  1  2  3  4  5  6  7  8  9 
________________________________________    
champion_1| 0  1  0  0  1  1  0  0  0  1
champion_1| 0  1  1  0  1  1  0  1  0  1
</code></pre>

<p>There are a variety of models that might suit this task.  You might use <a href=""https://en.wikipedia.org/wiki/Decision_tree_learning"" rel=""nofollow"">decision trees</a> for interpretability.  A simple <a href=""https://en.wikipedia.org/wiki/Artificial_neural_network"" rel=""nofollow"">neural net</a> or <a href=""https://en.wikipedia.org/wiki/Support_vector_machine"" rel=""nofollow"">SVM</a> would likely also do a good job.  These should all be found in most basic ML packages.</p>

<p>2) The win rates of various items are directly computable.  Simply count the number of times a champion used the items in question and won and divide by the total number of times a champion used that item combination.  You can do this for any given group size (1 to 6)</p>
"
"['python', 'neuralnetwork', 'regression']",SKNN regression problem,"<p>My best guess here is that your learning rate is <em>way</em> too high for the problem. You also probably have far more neurons in your hidden network than you need, seeing as you're using just one feature. </p>

<p>Recall that learning rate is controlling the ""step size"" in gradient descent and that for your dataset, it is likely far too high. I made some minor changes to your code and got better results than linear regression. Notice the use of 2 hidden neurons, a 0.001 learning rate, and 20 iterations.</p>

<pre><code># Now using the sknn regressor
# http://scikit-neuralnetwork.readthedocs.org/en/latest/guide_beginners.html


from sknn.mlp import Regressor, Layer

nn = Regressor(
    layers=[
        Layer(""Rectifier"", units=2),
        Layer(""Linear"")],
    learning_rate=0.001,
    n_iter=20)

nn.fit(diabetes_X_train, diabetes_y_train)
print(""Results of SKNN Regression...."")


# The coefficients
print('Coefficients: ', regr.coef_)
# The mean square error
print(""Residual sum of squares: %.2f""
      % np.mean((nn.predict(diabetes_X_test) - diabetes_y_test) ** 2))
# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % nn.score(diabetes_X_test, diabetes_y_test))

# Plot outputs
plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')
plt.plot(diabetes_X_test, nn.predict(diabetes_X_test), color='blue',
         linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()
</code></pre>

<p>SKNN regression: </p>

<pre><code>Results of SKNN Regression....
Coefficients:  [ 938.23786125]
Residual sum of squares: 6123.67
Variance score: 0.50
</code></pre>
"
"['neuralnetwork', 'deep-learning', 'gradient-descent']",Understanding dropout and gradient descent,"<p>In dropout as described in <a href=""http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"">http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a>, weights are <em>not</em> masked. Instead, the neuron activations are masked, <em>per example as it is presented for training</em> (i.e. the mask is randomised for each run forward and gradient backprop, not ever repeated).</p>

<p>The activations are masked during forward pass, and gradient calculations use the same mask during back-propagation of that example. This can be implemented as a modifier within a layer description, or as a separate dropout layer.</p>

<p>During weight update phase, typically applied on a mini-batch (where each example would have had different mask applied) there is no further use of dropout masks. The gradient values used for update have already been affected by masks applied during back propagation.</p>

<p>I found a useful reference for learning how dropout works, for maybe implementing yourself, is the <a href=""https://github.com/rasmusbergpalm/DeepLearnToolbox"">Deep Learn Toolbox</a> for Matlab/Octave.</p>
"
"['classification', 'scikit']",Classifying text documents using linear/incremental topics,"<p>If you want these output dimensions to be continuous, simply convert your size and relevance metrics to real-valued targets.  Then you can perform <a href=""https://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow"">regression</a> instead of classification, using any of a variety of models.  You could even attempt to train a multi target neural net to predict all of these outputs at once.</p>

<p>Additionally, you might consider first using  a <a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow"">topic model</a> such as <a href=""https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"" rel=""nofollow"">LDA</a> as your feature space.</p>

<p>Based on the values, it sounds like the ""relevance"" might be a variable best captured by techniques from <a href=""https://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""nofollow"">sentiment analysis</a>.  </p>
"
"['deep-learning', 'tensorflow']",Noisy behavior in deep learning feed-forward net,"<p>The overal logarithmic shape of your f1 score graph indicates that learning is effective and cost is heading towards some minimum. That's good. I'm assuming that the noise you're referring to is the instability of the graph after approximately 3k iterations: cost dropping and rising in a zig-zag manner.</p>

<p>This often hints at the learning rate being too large. Back propagation finds the right gradient but you take too big step and end up climbing rather than descending along the edge of the cost function. It's especially evident when a graph seems to oscillate around some middle value. You haven't mentioned what exact value of learning rate you're using but try to reduce it. A good starting point would be 0.01 but it depends on many factors so try to experiment.</p>

<p>Another issue might be a batch size: that is, how many examples contribute to the calculation of a gradient. If it's too large you might end up with an average gradient pointing in the wrong direction. And then even a small step (i.e., low learning rate) won't help. And it might again manifest itself in a zig-zag pattern. If batch size is one of the parameters try to decrease it.</p>

<p>The least likely issue might be the encoding architecture of your network. And especially the modest number of 8 neurons on the last layers. In this case individual neurons might have a considerable impact on the final output. And even little adjustments resulting from a single step of back propagation could potentially flip the sign of that neuron's activation value, impacting the results of other examples. Try increasing the number of neurons on the last layers. I'd personally suggest trying an architecture of 50x50x50.</p>

<p>Hope this helps!</p>
"
['r'],survey data analysis (discrete data),"<p>Here are some things to try.</p>

<ol>
<li>Plot a bar graph. The bar graph will clearly show jobless people are often choosing NO. Try an 1-way ANOVA test. If the p &lt; delta (i.e. delta=0.05), try a post-hoc test (i.e. Tukey's HSD) to do a pairwise comparison.</li>
<li>Like I said earlier, try a multiple comparison test (1-way ANOVA) first, if there is a statistically significant difference, you can try a pairwise comparison test (post-hoc test).</li>
<li>Maybe try a clustering algorithm? Be careful, because the marginal sums (by rows or columns) are not equal. Maybe create a similarity matrix by profession? To me, it seems that Employees and Businessmen are in one group (very similar), while Workers and Jobless are each in their own group. If you turn those frequencies into proportions, then you might just have 2 groups; one for employees + workers + businessmen, and one for jobless.</li>
<li>Use contingency table analysis to see if the responses (yes/no/don't know) are associated with profession. </li>
</ol>
"
"['feature-selection', 'random-forest', 'linear-regression']",feature importance via random forest and linear regression are different,"<p>So your query is a comparison of linear regression vs. random forest's model-derived importance of variables.</p>

<p>The lasso finds linear regression model coefficients by applying regularization. A popular approach to rank a variable's importance in a linear regression model is to decompose $R^2$ into contributions attributed to each variable. But variable importance is not straightforward in linear regression due to correlations between variables. Refer to the document describing the PMD method (Feldman, 2005) in the references below.</p>

<p>Another popular approach is averaging over orderings (LMG, 1980). The LMG works like this:</p>

<ul>
<li>Find the semi-partial correlation of each predictor in the model, e.g. for variable a we have: $SS_a/SS_{total}$. It implies how much would $R^2$ increase if variable $a$ were added to the model.</li>
<li>Calculate this value for each variable for each order in which the variable gets introduced into the model, i.e. {$a,b,c$} ; {$b,a,c$} ; {$b,c,a$}</li>
<li>Find the average of the semi-partial correlations for each of these orders. This is the average over orderings.</li>
</ul>

<p>The random forest algorithm fits multiple trees, each tree in the forest is built by randomly selecting different features from the dataset. The nodes of each tree are built up by choosing and splitting to achieve maximum variance reduction. While predicting on the test dataset, the individual trees output is averaged to obtain the final output. Each variable is permuted among all trees and the difference in out of sample error of before and after permutation is calculated. The variables with highest difference are considered most important, and ones with lower values are less important.</p>

<p>The method by which the model is fit on the training data is very different for a linear regression model as compared to random forest model.  But both models don't contain any structural relationships between the variables.</p>

<p>Regarding your query about non-linearity of the dependent variable: The lasso is essentially a linear model which will not be able to give good predictions for an underlying non-linear processes, as compared to tree based models. You should be able to check this by verifying the models performance over a set-aside test set, if the random forest performs better, the underlying process may be non-linear.  Alternatively, you could include variable interaction effects and higher order variables created using a, b, and c in the lasso model and verify if this model performs better as compared to a lasso with only a linear combination of a, b and c. If it does, then the underlying process might be non-linear.</p>

<p>References:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Decision_tree_learning"" rel=""nofollow"">https://en.wikipedia.org/wiki/Decision_tree_learning</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow"">https://en.wikipedia.org/wiki/Random_forest</a></li>
<li><a href=""https://www.jstatsoft.org/article/view/v017i01"" rel=""nofollow"" title=""Relative importance of Linear Regressors in R"">Relative importance of Linear Regressors in R</a></li>
<li><a href=""http://www.gtcenter.org/Archive/Conf05/Downloads/Conf/Feldman60.pdf"" rel=""nofollow"" title=""Relative Importance and Value"">Relative Importance and Value, Barry Feldman (PMD method)</a></li>
</ul>
"
"['python', 'bigdata']",Writing custom data analysis program,"<blockquote>
  <p>1) Store the data. For the moment I have plain text files (each file
  has rows of a fixed number of columns - most of the data are
  categorical). Would it make sense to save those in some database (SQL)
  or hdf5? Any hints on which is preferable?</p>
</blockquote>

<p><strong>Yes,</strong> it would make sense to store in a local database, rather than using large csv/text files. As you say that the data is derived from a NoSQL source, I assume unstructured data. So, using a SQL/relational store is out of question. As you say you are using Python, I would suggest you use <a href=""http://tinydb.readthedocs.org/en/latest/"" rel=""nofollow"">TinyDB</a>, which is both light-weight and easy to handle.</p>

<blockquote>
  <p>2) Which plotting library would you propose for the graphs? I have
  seen about bookeh and matplotlib supports interactive widgets but I
  don't know what people normally use.</p>
</blockquote>

<p>Matplotlib would be good enough. Actually, this question is more opinion-based than anything else. There are a lot of visualization libraries you can use, like Bokeh, Seaborn, etc.</p>

<blockquote>
  <p>3) Could I export the analysis results in an IPython notebook and then
  in html programmatically?</p>
</blockquote>

<p><strong>Yes</strong>, you can do the analytics directly in an Ipython notebook(Jupyter), which also supports Markdown and HTML cells.</p>

<p>In addition, you can also use <a href=""https://github.com/ipython/ipywidgets"" rel=""nofollow"">widgets and interactive visualization with Jupyter Ipy notebooks</a> and Matplotlib.  <a href=""https://github.com/ipython/ipywidgets/blob/master/examples/notebooks/Index.ipynb"" rel=""nofollow"">Tutorials for the same</a></p>
"
"['machine-learning', 'visualization', 'categorical-data', 'linear-regression']",How to visualize (make plot) of regression output against categorical input variable?,"<p>One possible first step is to convert the data back to the original coding.
This is called in SQL <em>unpivot</em>, in R <em>melt</em>.</p>

<p>Here an R example</p>

<pre><code>&gt; my.df &lt;- read.table(
+ text = ""DistrictA     DistrictB    DistrictC    DistrictD     DistrictE     Price
+         1             0            0            0             0             10000
+         0             1            0            0             0             20000
+         0             0            1            0             0             30000
+         0             0            0            1             0             40000
+         0             0            0            0             1             50000""
+      , header = TRUE)
&gt; my.df
  DistrictA DistrictB DistrictC DistrictD DistrictE Price
1         1         0         0         0         0 10000
2         0         1         0         0         0 20000
3         0         0         1         0         0 30000
4         0         0         0         1         0 40000
5         0         0         0         0         1 50000

&gt; library(reshape)
&gt; subset(melt(my.df, id=""Price"", variable = ""District""),value == 1)[,c(1,2)]
   Price  District
1  10000 DistrictA
7  20000 DistrictB
13 30000 DistrictC
19 40000 DistrictD
25 50000 DistrictE
</code></pre>

<p>After that you plot the Price dependent on a factor variable. You may additionally consider to order the factor based on the predicted price.</p>

<p>I provide no details, as you don't tagged your tool, but I would recommend additional to a scatter plot to consider a box plot and/or density plot - always combined with the prediction value from the model for each factor level.</p>
"
['statistics'],Standardize numbers for ranking ratios,"<p>This is relatively simple to do mathematically.  First, fit a regression line to the scatter plot of ""total graduates"" (y) vs. ""total students"" (x).  You will probably see a downward sloping line if your assertion is correct (smaller schools graduate a higher %).</p>

<p>You can identify the slope and y-intercept for this line to convert it into an equation y = mx + b, and then do a little algebra to convert the equation  into normalized form: ""y / x = m + b / x""</p>

<p>Then, with all the ratios in your data , you should <em>subtract</em> this RHS:  </p>

<p>normalized ratio = (total grads / total students) - (m + b / total students)</p>

<p>If the result is postive, then the ratio is above normal for that size (i.e. above the regression line) and if it is negative it is below the regression line.  If you want all positive numbers, you can add a positive constant to move all results above zero.</p>

<hr>

<p>This is how to do it mathematically, but I suggest that you consider whether it is wise, from a data analysis point of view, to normalize by school size.  This depends on the purpose of your analysis and specifically how this ratio is being analyzed in relation to other data.</p>
"
"['text-mining', 'neuralnetwork', 'svm']",What circumstances causes two different classifiers to classify data exactly like one another,"<p>Your results are reasonable. Your data brings several ideas to mind: </p>

<p>1) It is quite reasonable that as you change the available features, this will change the relative performance of machine learning methods. This happens quite a lot. Which machine learning method performs best often depends on the features, so as you change the features the best method changes.</p>

<p>2) It is reasonable that in some cases, disparate models will reach the exact same results. This is most likely in the case where the number of data points is low enough or the data is separable enough that both models reach the exact same conclusions for all test points. </p>
"
['data-mining'],how to modify sparse survey dataset with empty data points?,"<p>I would consider approaching this situation from the following two <strong>perspectives</strong>:</p>

<ul>
<li><p><strong>Missing data analysis</strong>. Despite formally the values in question are empty and not NA, I think that effectively <em>incomplete</em> data can (and should) be considered as <em>missing</em>. If that is the case, you need to automatically recode those values and then apply standard <em>missing data handling</em> approaches, such as <em>multiple imputation</em>. If you use <code>R</code>, you can use packages <code>Amelia</code> (if the data is multivariate normal), <code>mice</code> (supports non-normal data) or some others. For a nice <strong>overview</strong> of <em>approaches</em>, <em>methods</em> and <em>software</em> for multiple imputation of data with missing values, see the 2007 excellent article by Nicholas Horton and Ken Kleinman <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1839993"" rel=""nofollow"">""Much ado about nothing: A comparison of missing data methods and software to fit incomplete data regression models""</a>.</p></li>
<li><p><strong>Sparse data analysis</strong>, such as <em>sparse regression</em>. I'm not too sure how well this approach would work for variables with high levels of sparsity, but you can find a lot of corresponding information in <a href=""http://datascience.stackexchange.com/a/918/2452"">my relevant answer</a>.</p></li>
</ul>
"
"['machine-learning', 'terminology']",What is the term for when a model acts on the thing being modeled and thus changes the concept?,"<p>There are three terms from social science that apply to your situation:</p>

<ol>
<li><p><a href=""http://en.wikipedia.org/wiki/Reflexivity_%28social_theory%29"">Reflexivity</a> - refers to circular relationships between cause and effect.  In particular, you could use the definition of the term adopted by George Soros to refer to reverse causal loop between share prices (i.e. present value of fundamentals) and business fundamentals.  In a way, the share price is a ""model"" of the fundamental business processes.  Usually, people assume that causality is one-way, from fundamentals to share price.</p></li>
<li><p><a href=""http://en.wikipedia.org/wiki/Performativity"">Performativity</a> - As used by Donald MacKenzie (e.g. <a href=""http://www.lse.ac.uk/accounting/CARR/pdf/MacKenzie.pdf"">here</a>), many economic models are not ""cameras"" -- taking pictures of economic reality -- but in fact are ""engines"" -- an integral part of the construction of economic reality. He has a book of that title: <em><a href=""http://mitpress.mit.edu/books/engine-not-camera"">An Engine, Not a Camera</a></em>.</p></li>
<li><p><a href=""http://en.wikipedia.org/wiki/Self-fulfilling_prophecy"">Self-fulfilling Prophecy</a> - a prediction that directly or indirectly causes itself to become true, by the very terms of the prophecy itself, due to positive feedback between belief and behavior. This is the broadest term, and least specific to the situation you describe.</p></li>
</ol>

<p>Of the three terms, I suggest that MacKenzie's ""performativity"" is the best fit to your situation.  He claims, among other things, that the validity of the economic models (e.g. Black-Scholes option pricing) has been improved by its very use by market participants, and therefore how it reflects in options pricing and trading patterns.</p>
"
['machine-learning'],percentage of confidance on desion trees results,"<p>Some classification algorithms can indeed return a probability distribution over the considered classes (see Wikipedia on <a href=""http://en.wikipedia.org/wiki/Probabilistic_classification"" rel=""nofollow"">probabilistic classification</a>).</p>

<p>In the topic of your question you're asking about Decision Trees. Well, these have their limitations in terms of providing probability estimates (see this <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.324.132&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">paper on probability estimates from decision trees</a>).</p>

<p>In case you would like to play around with this, it's very easy to start with scikit-learn:</p>

<pre><code>import numpy as np
from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier()

# training samples
X = np.array([[1, 1, 0],
              [1, 1, 0],
              [0, 0, 1],
              [0, 0, 1]])

# target values for training samples
y = np.array([0, 0, 0, 1])

dtc.fit(X, y)

print 'Class probabilities for training samples:'
print dtc.predict_proba(X)
print 'Probabilities for previously unseen samples:'
for sample in ((1, 0, 1), (0, 0, 1), (1, 1, 1), (0, 0, 0)):
     print 'Sample {}. Result: {}'.format(sample, dtc.predict_proba(sample))
</code></pre>

<p>This code returns following results:</p>

<pre><code>Class probabilities for training samples:
[[ 1.   0. ]
 [ 1.   0. ]
 [ 0.5  0.5]
 [ 0.5  0.5]]
Probabilities for previously unseen samples:
Sample (1, 0, 1). Result: [[ 0.5  0.5]]
Sample (0, 0, 1). Result: [[ 0.5  0.5]]
Sample (1, 1, 1). Result: [[ 0.5  0.5]]
Sample (0, 0, 0). Result: [[ 1.  0.]]
</code></pre>

<p>At this scale the results are easily interpretable:</p>

<ul>
<li>a sample of features (1, 1, 0) is classified as 0 with 100% probability</li>
<li>a sample of features (0, 0, 1) is classified 50/50 as 0 or 1. etc.</li>
</ul>

<p>This brings us to an important question of <a href=""http://stats.stackexchange.com/a/34936/41317"">how accurate is your model</a>?</p>
"
"['machine-learning', 'time-series']","Finding frequencies in a noisy, ""uneven"" dataset","<p>Just to point you in one possible direction: you could treat this problem as one of probabilistic mixture modeling.</p>

<p>Imagine that each person's drink ordering is governed by a probability distribution. That distribution may be characterized by the time since their last drink order.  As time passes, the probability that they will order another drink increases until eventually they order another drink and the time resets.</p>

<p>One possible model for the time between drink orders for a single person is the <a href=""http://en.wikipedia.org/wiki/Exponential_distribution"" rel=""nofollow"">exponential distribution</a>.  If you consider many people ordering, the resultant times would likely be a mixture of exponentials. The problem then comes down to fitting a mixture of exponentials to the table's drink data.  You would likely have to supply some additional prior data as well to get a meaningful model (otherwise it's difficult to tell the difference between 5 people ordering 2 drinks a piece or 1 person ordering 10 drinks).</p>
"
"['r', 'dataset', 'data-cleaning', 'data']",Add new factor across multiple groups,"<p>Several possibilities. For example, to add <code>c</code> for existing <code>mon</code>-<code>year</code> combinations in your data frame:</p>

<pre><code>rbind(df, transform(df[!duplicated(df[, 3:4]), ], col1=""c"", col2=NA))
#     col1 col2 mon year
# 1      a    1 Jan 2016
# 2      a    2 Feb 2016
# 3      a    3 Mar 2016
# 4      a    4 Apr 2016
# 5      b    1 Jan 2016
# 6      b    2 Feb 2016
# 7      b    3 Mar 2016
# 8      b    4 Apr 2016
# 9      a    1 Jan 2015
# 10     a    2 Feb 2015
# 11     a    3 Mar 2015
# 12     a    4 Apr 2015
# 13     b    1 Jan 2015
# 14     b    2 Feb 2015
# 15     b    3 Mar 2015
# 16     b    4 Apr 2015
# 17     c &lt;NA&gt; Jan 2016
# 21     c &lt;NA&gt; Feb 2016
# 31     c &lt;NA&gt; Mar 2016
# 41     c &lt;NA&gt; Apr 2016
# 91     c &lt;NA&gt; Jan 2015
# 101    c &lt;NA&gt; Feb 2015
# 111    c &lt;NA&gt; Mar 2015
# 121    c &lt;NA&gt; Apr 2015
</code></pre>

<p>To add <code>c</code> for all possible combinations of existing <code>mon</code> values and existing <code>year</code> values: </p>

<pre><code>rbind(df, data.frame(col1=""c"", col2=NA, expand.grid(mon=levels(df$mon), year=levels(df$year))))
</code></pre>

<p>To add <code>c</code> for all possible combinations of all months names and existing <code>year</code> values:</p>

<pre><code>rbind(df, data.frame(col1=""c"", col2=NA, expand.grid(mon=month.abb, year=levels(df$year))))
</code></pre>

<p>and so on.</p>
"
"['machine-learning', 'neuralnetwork', 'feature-selection', 'visualization', 'preprocessing']",How to visualize data of a multidimensional dataset (TIMIT),"<p>Like I said in the comment, you'll need to perform dimension reduction, otherwise you'll not be able to visualize the $\mathbb{R}^n$ vector space and this is why :</p>

<p>Visualization of high-dimensional data sets is one of the traditional applications of dimensionality reduction methods such as PCA (Principal components analysis).</p>

<p>In high-dimensional data, such as experimental data where each dimension corresponds to a different measured variable, dependencies between different dimensions often restrict the data points to a manifold whose dimensionality is much lower than the dimensionality of the data space. </p>

<p>Many methods are designed for manifold learning, that is, to find and unfold the lower-dimensional manifold. There has been a research boom in manifold learning since 2000, and there now exist many methods that are known to unfold at least certain kinds of manifolds successfully.</p>

<p>One of the most used methods for dimension reduction is called PCA or Principal component analysis. PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. You can read more on this topics <a href=""http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf"" rel=""nofollow"">here</a>.</p>

<p>So once you reduce your high dimensional space into a ${\mathbb{R}^3}$ or ${\mathbb{R}^2}$ space you will able to project it using your adequate visualization method.
References :</p>

<ul>
<li><a href=""http://jmlr.csail.mit.edu/papers/volume11/venna10a/venna10a.pdf"" rel=""nofollow"">Information Retrieval Perspective to Nonlinear Dimensionality
Reduction for Data Visualization</a> - Jarkko Venna</li>
</ul>

<p><strong>EDIT:</strong> <em>To avoid confusion for some concerning PCA and Dimension Reduction</em>, I add the following details :</p>

<p>PCA will allow you compute the principal components of your vector model, so the information are not lost but ""synthesized"".</p>

<p>Unfortunately there is no other imaginable way to display 39 dimensions on a 2/3 dimension screen. If you wish to analyze correlations between your 39 features, maybe you should consider another visualization technique. </p>

<p>I would recommend a scatter plot matrix in this case.</p>
"
"['data-mining', 'categorical-data', 'linear-regression', 'numerical']",Steps in exploratory methods for mild-sized data with mixed categorical and numerical values?,"<p>You can get a reasonably good approximation of steps for <em>exploratory data analysis (EDA)</em> by reviewing the <a href=""http://www.itl.nist.gov/div898/handbook/eda/eda.htm"" rel=""nofollow"">EDA section</a> of the <em>NIST Engineering Statistics Handbook</em>. Additionally, you might find helpful parts of <a href=""http://datascience.stackexchange.com/a/1006/2452"">my related answer</a> here on <em>Data Science SE</em>.</p>

<p>Methods, related to EDA, are too diverse that it is not feasible to discuss them in a single answer. I will just mention several approaches. If you are interested in applying <em>classification</em> to your data set, you might find information, mentioned in <a href=""http://stats.stackexchange.com/a/135842/31372"">my other answer</a> helpful. In order to <em>detect structures</em> in a data set, you can try to apply <a href=""https://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow"">principal component analysis (PCA)</a>. If, on the other hand, you are interested in exploring <em>latent structures</em> in data, consider using <a href=""https://en.wikipedia.org/wiki/Exploratory_factor_analysis"" rel=""nofollow"">exploratory factor analysis (EFA)</a>.</p>
"
"['machine-learning', 'javascript']",How do I normalize an array of positive and negative numbers so they are between 0 and 1?,"<p>This is called unity-based normalization. If you have a vector $X$, you can obtain a normalized version of it, say $Z$, by doing:</p>

<p>$$Z = \frac{X - \min(X)}{\max(X) - \min(X)}$$ </p>
"
"['machine-learning', 'data-mining']",What is a good non cryptographic Hash for string feature translation?,"<p>See also: <a href=""http://datascience.stackexchange.com/questions/869/neural-network-parse-string-data"">Neural Network parse string data?</a></p>

<p>I do not see a problem with using MurMur3 per se.</p>

<p>For the categorical labels, you can use one-hot encoding / one-of-k encoding.</p>

<p>For the strings, it's an application-specific question.  Presumably if you use exactly those strings as features, it will be very sparse.  The effect of this will depend on the algorithm that you are using, and how the training data compare to the data you see in practice.  You are running the risk that you will effectively either only create a traditional IP/email whitelist/blacklist OR throw out the feature altogether.</p>

<p>You must decide what you want (eg should a certain email address <em>always</em> get a certain output label?) and have some intuition about the application so as to generate more features from <em>IP address</em> and <em>email address</em>.  For example, from <em>email address</em> you can extract the <em>local part</em> (eg ""john1972"") and <em>domain</em>, and from each of those you can extract:<br>
- <em>length</em><br>
- <em>character tri-grams</em><br>
- <em>count/proportion of numbers to alphachars</em><br>
- <em>number of hyphens</em><br>
- <em>dictionary validity</em><br>
...<br>
(From <em>domain</em> you can also extract TLD and possibly subdomains.)
You can try to tokenise .  You can even hit external services to get information like <em>number of Google hits</em>, <em>detected language</em>, <em>spam score</em> etc.</p>
"
"['machine-learning', 'classification', 'python', 'predictive-modeling', 'scikit']",How should I convert Logistic Regression's coefs into action strategy?,"<p>To understand the coefficients you just need to understand how the logistic regression model that you fit uses the coefficients to make predictions. No, it does not work like a decision tree. It's a linear model.</p>

<p>Really, predictions are based on the dot product of the coefficients and the values from some new instance to predict. This is just the sum of their products. The higher the dot product, the more positive the prediction.</p>

<p>So you can understand it as computing something like <code>-2.40477246e-10 * MATCH_HOME + -5.57611571e-02 * MATCH_AWAY + ...</code> (I don't know what coefficients go with what feature in your model.)</p>

<p>That generally means that inputs with bigger coefficients matter more, and inputs with positive coefficients correlate positively with a positive prediction. That's most of what you can interpret here.</p>

<p>The first of those conclusions is only really valid if inputs have been normalized to be on the same scale though. I'm not clear that you've done that here. You should also in general use L1 regularization if you intend to interpret the coefficients this way.</p>
"
"['machine-learning', 'data-mining', 'classification', 'google-prediction-api']",Analyzing customer response,"<p><a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></p>

<p>You can use the above tutorial to get acquaintance with text classification. Afterwards it should be easier to formulate nontrivial questions to move even further.</p>
"
"['machine-learning', 'classification']",Building Customers/Patient Profiles,"<p>Maybe I'm being a bit too simplistic, but I would build a set of <strong>training</strong> data that looks like this (Good=1 means patient showed up for appt and is <strong>good</strong> by your definition, 0 = <strong>bad</strong>)</p>

<pre><code>Recid, PatId, SurgeryId, DrId, DateAppt, TimeAppt, Gender, Age, Good
1, 1, 100, 10, 01jan16, 10:30, M, 31, 1
2, 1, 100, 12, 05jan16, 15:20, M, 31, 1
3, 1, 100, 10, 06mar16, 11:45, M, 31, 0
4, 2, 101, 15, 02Feb16, 12:35, F, 75, 1
....
</code></pre>

<p>I would then use one of the machine learning tools in R - there are a variety of them, to train a model of your data.</p>

<p>Then with another set of data, I would test the model you have just built to see how correct it is. If you don't have a second set of data, then randomly partition your original training set and only train with half of the data.</p>

<p>Some further suggestions to make your model more powerful, is create additional variables with the following information</p>

<ul>
<li>a Flag to indicate if a person has missed a previous appointment with any doctor.</li>
<li>a Flag to indicate if a person has missed a previous appointment with the particular doctor they are going to be visiting - I know from person experience there are some doctors in my surgery that I have a preference to see.</li>
<li>The number of days since the last appointment</li>
<li>Day of week of appointment</li>
</ul>

<p>Sounds like a nice dataset to be working with!</p>
"
"['machine-learning', 'neuralnetwork', 'svm']",Where to start on neural networks,"<p>No, you should go ahead and learn the maths on your own. You will ""only"" need to learn calculus, statistics, and linear algebra (like the rest of machine learning). The theory of neural networks is pretty primitive at this point -- it more of an art than a science -- so I think you can understand it if you try. Ipso facto, there are a lot of tricks that you need practical experience to learn. There are lot of complicated extensions, but you can worry about them once you get that far.</p>

<p>Once you can understand the Coursera classes on ML and neural networks (Hinton's), I suggest getting some practice. You might like <a href=""http://karpathy.github.io/neuralnets/"" rel=""nofollow"">this</a> introduction.</p>
"
"['r', 'ggplot2']",R: lattice equivalent of density2d in ggplot?,"<p>Use a custom panel function in which the density is estimated (e.g. using <code>MASS::kde2d()</code>):</p>

<pre><code>library(""lattice"")
library(""MASS"")

set.seed(1);
dat &lt;- data.frame(x=rnorm(100), y=rt(100, 5))

xyplot(y~x, data=dat,
       panel=function(x, y, ...) {
         dens &lt;- kde2d(x=dat$x, y=dat$y, n=50)
         tmp &lt;- data.frame(x=dens$x,
                           y=rep(dens$y, each=length(dens$x)),
                           z=as.vector(dens$z))
         panel.levelplot(tmp$x, tmp$y, tmp$z, contour=TRUE,
                         subscripts=1:nrow(tmp), region=FALSE, ...)
         panel.xyplot(x, y, ...)
       })
</code></pre>

<p><a href=""http://i.stack.imgur.com/d2rSi.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d2rSi.png"" alt=""lattice plot""></a></p>
"
['pandas'],Pandas: how can I create multi-level columns,"<p>Finally, I found a solution.</p>

<p>You can find the example script below.</p>

<pre><code>#!/usr/bin/env python3
import pickle
import pandas as pd
import itertools
import numpy as np

data = pd.DataFrame(np.random.randn(10, 5), columns=('0_n', '1_n', '0_p', '1_p', 'x'))

indices = set()
groups = set()
others = set()
for c in data.columns:
    if '_' in c:
        (i, g) = c.split('_')
        c2 = pd.MultiIndex.from_tuples((i, g),)
        indices.add(int(i))
        groups.add(g)
    else:
        others.add(c)
columns = list(itertools.product(groups, indices))
columns = pd.MultiIndex.from_tuples(columns)
ret = pd.DataFrame(columns=columns)
for c in columns:
    ret[c] = data['%d_%s' % (int(c[1]), c[0])]
for c in others:
    ret[c] = data['%s' % c]
ret.rename(columns={'total': 'total_indices'}, inplace=True)

print(""Before:"")
print(data)
print("""")
print(""After:"")
print(ret)
</code></pre>

<p>Sorry for this...</p>
"
['neuralnetwork'],Using Neural Networks To Predict Sets,"<p>The <code>75^6</code> option is not only bad for speed, but it is a very difficult representation to train, because the NN doesn't ""understand"" that any of the output categories are related. You would need an immense amount of data to train such a network, because ideally you need at least a few examples in any category that you expect the network to predict. Unless you had literally billions of examples to train from, the chances are certain combinations will never occur in your training set, thus could never be predicted with any confidence.</p>

<p>Therefore I would probably use 75 outputs, one for each object representing the probability that it would be chosen. This is easy to create training data for, if you have training examples with the 6 favoured objects - just a 1 for the objects chosen and 0 for all others as a 75-wide label.</p>

<p>For prediction, select the 6 objects with the highest probabilities. If these choices are part of a recommender system (i.e. may be presented to same person as being predicted for), then you can select items randomly using the outputs as weights. You may even find that this weighted Monte Carlo selection works well for predicting bulk user behaviour as well (e.g. for predictions fed into stock purchases). In addition, this stochastic approach <em>can</em> be made to predict duplicates (but not accurately, except perhaps averaged over many predictions).</p>

<p>A sigmoid transfer function on the output layer is good for representing non-exclusive probability. The logloss objective function can be used to generate the error values and train the network.</p>

<p>If you want to accurately predict duplicate choices out of the 6 items chosen, then you will need plenty of examples where duplicates happened and have some way to represent that in the output layer. For example, you could have double the number of output neurons, with two assigned to each object. The first probability would then be probability of selecting the item once, and the second probability would be for selecting it twice.</p>

<hr>

<p>The question has since been updated, and it appears there are strong relationships between items making the choice of a set of items potentially very recipe-like. That may reduce the effectiveness of the ideas outlined above in this answer.</p>

<p>However, using 75 outputs may still work better than other approaches, and is maybe the simplest setup, so I suggest still giving it a try, even if just to establish a benchmark for other ideas. This will work best when decisions are driven heavily by the feature data available, and when in practice there are lots of valid choices for combining items so there is a strong element of player preference. It will work less well if there is a large element of game mastery and logic in player decisions in order to combine items.</p>
"
"['machine-learning', 'python', 'scikit']",Running examples from scikit-learn tutorials,"<p>I installed older version of numpy,that's the problem. If you have installed scikit-learn using windows binaries, then you must first install numpy+mkl from the windows binaries site. It is a prerequisite for scikit-learn.</p>
"
"['classification', 'nlp', 'algorithms']",How to define person's gender from the fullname?,"<p>That dataset looks like a good starting point. Keep in mind that when you make your own dataset from those datasets you'll want to keep the male to female ratio balanced if you want it to predict both well. </p>

<p>It should not matter what machine learning software you use (Apache Mahout, scikit-learn, weka, etc.). Pick one that fits your language of choice since speed will probably not be too much of a concern with the smallish dataset size. As for features, you'd generally use ngrams as your baseline for NLP classification tasks. If you use ngrams here you won't end up with anything very interesting because the model won't generalize to any unseen names. I'd suggest as a feature baseline that you try character ngrams, and maybe something like syllable ngrams for something slightly more advanced (for syllable tokenization see <a href=""http://stackoverflow.com/questions/405161/detecting-syllables-in-a-word"">http://stackoverflow.com/questions/405161/detecting-syllables-in-a-word</a>).</p>
"
"['time-series', 'feature-extraction', 'multiclass-classification']",Feature Extraction - calculate slope,"<p>There are several ways to do this, here are a couple of options:</p>

<ul>
<li>Calculate different lag values (difference between now and <strong>t</strong> time units)</li>
<li>Calculate a linear regression for different time windows and store the slope and the bias</li>
<li>You can also involve higher order models to describe what is happening, if you think for example that the acceleration also matters you could use a 2nd or 3rd degree polynomial over the past couple of observations.</li>
</ul>
"
['predictive-modeling'],Predict which user will buy with an offer - discount,"<p>You can use Decision trees for a single model prediction for both the set of users.</p>

<p>A good start would be to first <a href=""https://en.wikipedia.org/wiki/Decision_tree"" rel=""nofollow"">read up</a> on Decision Trees and their applications.</p>

<p>You can include the offer as a decision(as a boolean in this case). </p>

<p><code>buying with offer</code> and <code>buying without offer</code> can be the decision criterion.</p>

<p>You can, in fact go ahead and put in the offer values also. For example, </p>

<p><code>offer&gt;10%</code> and <code>offer &lt;10%</code></p>
"
"['machine-learning', 'reference-request']",Sentiment Analysis Tutorial,"<p>The <a href=""https://www.coursera.org/course/nlp"" rel=""nofollow"">Stanford NLP course on Coursera</a> covers Sentiment Analysis in <a href=""https://class.coursera.org/nlp/lecture/preview"" rel=""nofollow"">week 3</a>:<br>
- <a href=""https://class.coursera.org/nlp/lecture/31"" rel=""nofollow"">What is Sentiment Analysis?</a><br>
- <a href=""https://class.coursera.org/nlp/lecture/145"" rel=""nofollow"">Sentiment Analysis: A baseline algorithm</a><br>
- <a href=""https://class.coursera.org/nlp/lecture/35"" rel=""nofollow"">Sentiment Lexicons</a><br>
- <a href=""https://class.coursera.org/nlp/lecture/144"" rel=""nofollow"">Learning Sentiment Lexicons</a><br>
- <a href=""https://class.coursera.org/nlp/lecture/33"" rel=""nofollow"">Other Sentiment Tasks</a></p>

<p>For coding tutorials see:  </p>

<ul>
<li><p><a href=""http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/"" rel=""nofollow"">Stream Hacker's NLP tutorials</a> </p></li>
<li><p><a href=""http://fjavieralba.com/basic-sentiment-analysis-with-python.html"" rel=""nofollow"">Basic Sentiment Analysis with Python</a>  </p></li>
<li><p><a href=""http://andybromberg.com/sentiment-analysis-python/"" rel=""nofollow"">Andy Bromberg's Sentiment Analysis tutorials</a></p></li>
<li><p><a href=""http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"" rel=""nofollow"">Laurent Luce's Sentiment Analysis tutorials</a> </p></li>
</ul>

<p>These are really basic, so their performance will not be great in all cases.</p>
"
"['algorithms', 'logistic-regression']",Is logistic regression actually a regression algorithm?,"<p>Logistic regression is regression, first and foremost. It becomes a classifier by adding a decision rule. I will give an example that goes backwards. That is, instead of taking data and fitting a model, I'm going to start with the model in order to show how this is truly a regression problem.</p>

<p>In logistic regression, we are modeling the log odds, or logit, that an event occurs, which is a continuous quantity. If the probability that event $A$ occurs is $P(A)$, the odds are:</p>

<p>$$\frac{P(A)}{1 - P(A)}$$</p>

<p>The log odds, then, are:</p>

<p>$$\log \left( \frac{P(A)}{1 - P(A)}\right)$$</p>

<p>As in linear regression, we model this with a linear combination of coefficients and predictors:</p>

<p>$$\operatorname{logit} = b_0 + b_1x_1 + b_2x_2 + \cdots$$</p>

<p>Imagine we are given a model of whether a person has gray hair. Our model uses age as the only predictor. Here, our event A = a person has gray hair:</p>

<p>log odds of gray hair = -10 + 0.25 * age</p>

<p>...Regression! Here is some Python code and a plot:</p>

<pre><code>%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

x = np.linspace(0, 100, 100)

def log_odds(x):
    return -10 + .25 * x

plt.plot(x, log_odds(x))
plt.xlabel(""age"")
plt.ylabel(""log odds of gray hair"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xR0OT.png"" alt=""plot of the log odds for our toy example""></p>

<p>Now, let's make it a classifier. First, we need to transform the log odds to get out our probability $P(A)$. We can use the sigmoid function:</p>

<p>$$P(A) = \frac1{1 + \exp(-\text{log odds}))}$$</p>

<p>Here's the code:</p>

<pre><code>plt.plot(x, 1 / (1 + np.exp(-log_odds(x))))
plt.xlabel(""age"")
plt.ylabel(""probability of gray hair"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/hSpCa.png"" alt=""plot of the probability of gray hair for our toy example""></p>

<p>The last thing we need to make this a classifier is to add a decision rule. One very common rule is to classify a success whenever $P(A) &gt; 0.5$. We will adopt that rule, which implies that our classifier will predict gray hair whenever a person is older than 40 and will predict non-gray hair whenever a person is under 40.</p>

<p>Logistic regression works great as a classifier in more realistic examples too, but before it can be a classifier, it must be a regression technique!</p>
"
"['definitions', 'social-network-analysis', 'graphs']",Network structure: k-cliques vs. p-cliques,"<p>In graph theory a clique indicates a fully connected set of nodes: <a href=""http://books.google.com/books?id=E3-OSVSPbU0C&amp;pg=PA40&amp;lpg=PA40&amp;dq=%22graph%20theory%22,%20%22p-clique%22&amp;source=bl&amp;ots=smbhcK-9AC&amp;sig=X0v_EbqSqB4WBbudgPbqo_j1pvk&amp;hl=en&amp;sa=X&amp;ei=VNWiU8KaA9WxsQSQhYGIAg&amp;ved=0CDEQ6AEwBA#v=onepage&amp;q=%22graph%20theory%22,%20%22p-clique%22&amp;f=false"" rel=""nofollow"">as noted here</a>, a p-clique simply indicates a clique comoprised of p nodes. A k-clique is an undirected graph and a number k, and the output is a clique of size k if one exists.</p>

<p><a href=""http://en.wikipedia.org/wiki/Clique_problem"" rel=""nofollow"">Clique Problem</a></p>
"
"['apache-spark', 'scala', 'market-basket-analysis']",Market Basket Analysis - Data Modelling,"<p>You may use groupByKey or combineByKey in Spark</p>
"
"['logistic-regression', 'java']",How can I create a custom tag in JPMML?,"<p>You can/should use a generic Java Architecture for XML Binding (JAXB) approach.</p>

<p>Simply put, call <code>Row#withContent(Object...)</code> with instances of <code>org.w3c.dom.Element</code> that represent the desired XML content.</p>

<p>For example:</p>

<pre><code>Document document = documentBuilder.newDocument();
Element shortForm = document.createElement(""shortForm"");
shortForm.setTextContent(""m"");
Element longForm = document.createElement(""longForm"");
longForm.setTextContent(""male"");
row = row.withContent(shortForm, longForm);
</code></pre>
"
"['r', 'regression', 'linear-regression']",Interpreting the evaluation result of multiple linear regression,"<p>I am going to answer your questions one after another. </p>

<p><strong>First, what do the 44 degrees of freedom mean?</strong> </p>

<p>It simply means that the model you built is constructed by using 44 independent variables. For example a model that looks like y = a<em>x + b has 1 independent variable (i.e. a) and thus 1 degree of freedom. A model that looks like y= a</em>x1 + b*x2 + c would have 2 independent variables (i.e. a and b) and thus 2 degrees of freedom. </p>

<p><strong>Second, what is the Multiple R-squared?</strong></p>

<p>Here for the purpose of interpretating it Multiple R-squared is equivalent to the (simple) R-squared you would have for a linear regression model with 1 degree of freedom. Multiple R-squared tells us the share of the observed variance that is explained by the model. For example if you have a Multiple R-squared of 0.79 it means that your model explains 79% of the observed variance in your data. </p>

<p><strong>Third, what is the Adjusted R-squared and why do we need it?</strong></p>

<p>There are several problems with Multiple R-squared.</p>

<p><strong>Problem 1:</strong> Every time you add a predictor to a model, the R-squared increases, even if due to chance alone. It never decreases. Consequently, a model with more independent variables (more degrees of freedom) may appear to have a better fit simply because it has more independent variables.</p>

<p><strong>Problem 2:</strong> If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data. This condition is known as overfitting the model and misleadingly high R-squared values and a lessened ability to make predictions.</p>

<p>Problem 1 is caused by Problem 2. And this is where Adjusted R-squared is coming in handy. Adjusted R-squared is an attempt at fixing these problems by factoring in the number of independent variables. The adjusted R-squared tells you the percentage of variation explained by only the independent variables that actually affect the dependent variable.</p>

<p><a href=""http://i.stack.imgur.com/oyBLe.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oyBLe.jpg"" alt=""enter image description here""></a></p>

<p>where:</p>

<ul>
<li>n is the number of data points you have,  </li>
<li>and k is the number of independent variables used to explain their distribution, excluding the constant</li>
</ul>

<p>If you add more and more useless variables to a model, adjusted r-squared will decrease. If you add more useful variables, adjusted r-squared will increase.
Adjusted R-squared  will always be less than or equal to R-squared . You only need R-squared when working with samples. In other words, R-squared  isn’t necessary when you have data from an entire population.</p>

<hr>

<p>Here is an interesting series of articles which will help you understand how to use R-squared to interpret the results of your model even better. </p>

<ul>
<li><a href=""http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit"" rel=""nofollow"">Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?</a> </li>
<li><a href=""http://blog.minitab.com/blog/adventures-in-statistics/multiple-regession-analysis-use-adjusted-r-squared-and-predicted-r-squared-to-include-the-correct-number-of-variables"" rel=""nofollow"">Multiple Regression Analysis: Use Adjusted R-Squared and Predicted R-Squared to Include the Correct Number of Variables</a></li>
</ul>
"
['orange'],Orange 3 Heatmap clustering under the hood,"<p>It appears the widget uses hierarchical clustering. I guess the metric is Euclidean distance by default and there doesn't seem to be a way to specify another one (except by using <em>Distances</em> widget and connecting it into the <em>Distance Map</em> widget).</p>

<p>I don't think it is possible to export the widget's workflow as pure code, but you can look at what the widget does in <a href=""https://github.com/biolab/orange3/blob/master/Orange/widgets/visualize/owheatmap.py"" rel=""nofollow"">the source code</a> (seems pretty low-level, though). What you <em>can</em> do, however, is select subsets of data (can be saved with <em>Save Data</em> widget) for further analysis if that's of any help.</p>

<p><a href=""http://i.stack.imgur.com/YXUzC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YXUzC.png"" alt=""Orange Heatmap widget screenshot""></a></p>
"
"['machine-learning', 'nlp']",Stanford NER - Increase probability for a certain class,"<p>Your data set will influence the labeling results. If it is focused on organizations, the NER should favor them simply by virtue of the data it's fed. So you might need to do anything. But if you do observe undesirable behavior in the resulting NER, you can adjust the weights.</p>

<p>It's the same with any machine learning algorithm. A humorous demonstration was provided by Google's Deep Dream: <a href=""http://rhizome.org/editorial/2015/jul/10/deep-dream-doggy-monster/?ref=tags_dogs_post_readbtn"" rel=""nofollow"">it kept seeing dogs every where</a>. Why? Because the data set they used for training had an abundance of dogs.</p>

<p>(And Carl Zeis should be labeled as a person. The company is Carl Zeiss.)</p>
"
"['data-cleaning', 'gradient-descent', 'linear-regression']",Feature Scaling and Mean Normalization,"<p>I agree with the existing answer that <em>feature scaling</em> is a superset into which techniques like mean normalization, residual normalization, etc falls under.</p>

<p>So, assuming that by <em>feature scaling</em>, you mean the techniques other than mean normalization, I would attempt to answer your questions:</p>

<blockquote>
  <p>1) Can I mix and match these two approaches? e.g. Feature Scale x1 and
  Mean Normalize x2?</p>
</blockquote>

<p>In most cases <strong>No</strong>. Generally, only one normalization technique is used and it pretty much suffices the need. In addition to that argument, it should also be noted that any normalization technique introduces duplication in the data records (not necessarily redundant duplication).</p>

<p>So, pretty much a single normalization technique would suffice most of the times.</p>

<blockquote>
  <p>2) How do you determine which of these options to apply? It seems that
  either could accomplish the task of increasing your convergence
  rates... I suppose you just need to know your data set to understand
  which will reliably reduce your values while leaving as few outliers
  as possible?</p>
</blockquote>

<p><strong>Yes</strong>, you are right. The selection of the technique depends on the data. And the feature scaling (and normalization) process comes under the process of data cleaning. So, it is done immediately after the selection of the relevant data for the analytics process.</p>
"
['machine-learning'],Detecting cats visually by means of anomaly detection,"<p>You could simplify your problem significantly by using a motion/change detection approach. For example, you could compare each image/frame with one from an early time (e.g., a minute earlier), then only consider pixels that have changed since the earlier time. You could then extract the rectangular region of change and use that as the basis for your classification or anomaly detection.</p>

<p>Taking this type of approach can significantly simplify your classifier and reduce your false target rate because you can ignore anything that is not roughly the size of a cat (e.g., a person or bird). You would then use the extracted change regions that were not filtered out to form the training set for your classifier (or anomaly detector).</p>

<p>Just be sure to get your false target rate sufficiently low before mounting a laser turret to your feline intrusion detection system.</p>
"
['classification'],How to classify test objects?,"<p>Suppose your test object is <code>(sunny, hot, normal, TRUE)</code>. Look through the rules top to bottom and see if any of the conditions are matched. The first rule for example tests the <code>outlook</code> feature. The value doesn't match, so the rule isn't matched. Move on to the next rule. And so on. In this case, rule 5 matches the test case and the classification for the p lay variable is ""yes"".</p>

<p>More generally, for any test case, look at the values its features take and find the first rule that those values satisfy. The implication of that rule will be its classification.</p>
"
"['python', 'data-cleaning', 'pandas']",Ignoring symbols and select only numerical values with pandas,"<p>Use <code>str.strip</code> if the prefix is fixed or <code>str.replace</code> if not:</p>

<pre><code>data = pandas.Series([""U$ 192.0""])
data.str.replace('^[^\d]*', '').astype(float)
</code></pre>

<p>This removes all the non-numeric characters to the left of the number, and casts to float.</p>
"
['r'],Finding predictions using biglm without finding errors,"<p>biglm calls model.frame which ""all the variables in the formula are included in the data frame"" see documentation for model.frame. This is the issue that comes up when predict is called on the biglm class. It looks for those values in the predict function. To get around this you can just create a variable and encode it with 0. See below...</p>

<pre><code>data(trees)
ff&lt;-log(Volume)~log(Girth)+log(Height)

chunk1&lt;-trees[1:10,]
chunk2&lt;-trees[11:20,]
chunk3&lt;-trees[21:31,]

a &lt;- biglm(ff,chunk1)
summary(a)

#produces same error
chunk2 &lt;- select(chunk2, -Volume)
predict(a, chunk2)

#Fixed
chunk2$Volume &lt;- 0
predict(a, chunk2)
</code></pre>
"
['data-mining'],Non-parametric approach to healthcare dataset?,"<p>They are not specifically referring to a plot based approach.  They are referring to a class of methods that must be employed when the data is not normal enough or not well-powered enough to use regular statistics.</p>

<p>Parametric and nonparametric are two broad classifications of statistical procedures with loose definitions separating them:</p>

<ul>
<li>Parametric tests usually assume that the data are approximately normally distributed.</li>
<li>Nonparametric tests do not rely on a normally distributed data assumption.</li>
<li>Using parametric statistics on non-normal data could lead to incorrect results.</li>
<li>If you are not sure that your data is normal enough or that your sample size is big enough (n &lt; 30), use nonparametric procedures rather than parametric procedures.</li>
<li>Nonparametric procedures generally have less power for the same sample
size than the corresponding parametric procedure if the data truly are normal.</li>
</ul>

<p>Take a look at some examples of parametric and analogous nonparametric tests from <a href=""http://www.mayo.edu/mayo-edu-docs/center-for-translational-science-activities-documents/berd-5-6.pdf"" rel=""nofollow"">Tanya Hoskin's Demystifying Summary</a>:</p>

<p><a href=""http://i.stack.imgur.com/Q5JE7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q5JE7.png"" alt=""enter image description here""></a></p>

<p>Here are some summary references:</p>

<ul>
<li><a href=""http://changingminds.org/explanations/research/analysis/parametric_non-parametric.htm"" rel=""nofollow"">Another general table with some different information</a></li>
<li><a href=""http://www.statoek.wiso.uni-goettingen.de/veranstaltungen/graduateseminar/Nonpar2006.pdf"" rel=""nofollow"">Nonparametric Statistics</a></li>
<li><a href=""http://ksu.edu.sa/sites/py/ar/mpy/departments/math/learnResources/ResourceCenter/Documents/All%20of%20Nonparametric%20Statistics.pdf"" rel=""nofollow"">All of Nonparametric Statistics, by Larry Wasserman</a></li>
<li><a href=""http://www.r-tutor.com/elementary-statistics/non-parametric-methods"" rel=""nofollow"">R tutorial</a></li>
<li><a href=""http://statsmodels-np.blogspot.ca/"" rel=""nofollow"">Nonparametric Econometrics with Python</a></li>
</ul>
"
['r'],how to perform Step calcuations in R,"<p>Try a simple equation like this:
\begin{eqnarray}
&amp;&amp;\max(x-20,0)*0.1 + \max(0,x-25)*(-.047) + \\
&amp;&amp;\max(0, x-125)*(-.011) + \max(0,x-1000)*(-.003)
\end{eqnarray}</p>

<p>This should take the usage $(x)$ and walk it through steps, initially charging 0.1 for each Kwh over 20: $\max(x-20,0)*0.1$</p>

<p>Then, if $x$ is above 20, it takes away 0.047 (0.1-0.053) for every Kwh between 20 and 25: $\max(0, x-25)*(-0.047)$</p>

<p>Then, if $x$ is above 125, it takes away 0.011 (0.053-0.042) for every Kwh between 25 and 125: $\max(0, x-125)*(-0.011)$</p>
"
['nlp'],How to process natural language queries?,"<p>Natural language querying poses very many intricacies which can be very difficult to generalize.  From a high level, I would start with trying to think of things in terms of nouns and verbs.</p>

<p>So for the sentence: How many books were sold last month?</p>

<p>You would start by breaking the sentence down with a parser which will return a tree format similar to this:</p>

<p><img src=""http://i.stack.imgur.com/ogoiY.png"" alt=""enter image description here""></p>

<p>You can see that there is a subject books, a compound verbal phrase indicating the past action of sell, and then a noun phrase where you have the time focus of a month.</p>

<p>We can further break down the subject for modifiers: ""how many"" for books, and ""last"" for month.</p>

<p>Once you have broken the sentence down you need to map those elements to sql language e.g.: how many => count, books => book, sold => sales, month => sales_date (interval), and so on.  </p>

<p>Finally, once you have the elements of the language you just need to come up with a set of rules for how different entities interact with each other, which leaves you with:</p>

<p>Select count(*) 
  from sales 
  where 
   item_type='book' and 
   sales_date >= '5/1/2014' and 
   sales_date &lt;= '5/31/2014'</p>

<p>This is at a high level how I would begin, while almost every step I have mentioned is non-trivial and really the rabbit hole can be endless, this should give you many of the dots to connect.  </p>
"
"['statistics', 'education', 'knowledge-base', 'definitions']",Data Science oriented dataset/research question for Statistics MSc thesis,"<p>Just head to kaggle.com; it'll keep you busy for a long time. For open data there's the <a href=""http://archive.ics.uci.edu/ml/"">UC Irvine Machine Learning Repository</a>. In fact, there's a whole <a href=""http://opendata.stackexchange.com/"">Stackexchange site</a> devoted to this; look there.</p>
"
['bigdata'],Amazon S3 vs Google Drive,"<p>Personally, we use S3 on top of GCE and really love it. Depending on how much data you're dealing with, Google Drive just doesn't quite match the 5 TB max that S3 gives you. Also, if you're using python, <code>boto</code> does a pretty fantastic job of making most aws services pretty accessible regardless of what stack you're dealing with. Even if you're not using python, they've got a pretty straightforward API that generally is more accessible than Google Drive.</p>

<p>Instead of google drive, though google did recently release a cloud storage service, apart from drive, that lets you more closely integrate your storage with any gce instance you've got, <a href=""https://cloud.google.com/products/cloud-storage/"">https://cloud.google.com/products/cloud-storage/</a></p>

<p>They've got an API which seems to be pretty comparable to S3's, but I can't profess to having really played around with it much. Pricing-wise the two are identical, but I think that the large community and experience with aws in general still puts S3 squarely above both google's cloud storage and google drive.</p>
"
"['dataset', 'excel']",Working with text files in Excel,"<p>I'm no Excel expert, as I generally use Python or R instead, but this might get you started until an Excel expert comes along.  In the meantime, it would help if you clarified your question.  And you should be aware that <code>search</code> will only find you the index of the <strong>first</strong> match, not all matches in the string.  If you only need the first hit, you can use</p>

<pre><code>=MID(A1,SEARCH(""popul"",A1,1),IFERROR(FIND("" "",A1,SEARCH(""popul"",A1,1)),LEN(A1)+1)-SEARCH(""popul"",A1,1))
</code></pre>

<p>although I cannot claim this is the <em>best</em> way to do this.  You really didn't specify where you want the results to appear, how they should look, or if you only have one cell you need to search in.  It would also help to know the version of Excel you have.  I'll also present a crude way to return all the hits in the string:</p>

<p><img src=""http://i.stack.imgur.com/MrYrb.png"" alt=""cells""></p>

<p>Cell A1 contains the string, B1 has no formula, and if you run out of ""n/a""s you can extend columns B, C, and D by filling down.  The formulas are as follows:</p>

<p><strong>B3 and below use</strong></p>

<pre><code>=IF(C2+1&lt;LEN($A$1),C2+1,""n/a"")
</code></pre>

<p><strong>C2 and below use</strong></p>

<pre><code>=IFERROR(FIND("" "",$A$1,SEARCH(""popul"",$A$1,B2)),LEN($A$1)+1)
</code></pre>

<p><strong>D2 and below use</strong></p>

<pre><code>=IFERROR(MID($A$1,SEARCH(""popul"",$A$1,B2),C2-SEARCH(""popul"",$A$1,B2)),"""")
</code></pre>

<p>As you can see, there's little to no error checking except to deal with the match at the end of the string.  In the end though, if you're going to use Excel for this you should probably create a user defined function or utilize VBA instead of in-cell formulas.</p>
"
"['python', 'statistics', 'feature-scaling']",Sensitivity to scaling of features in a multivariate gaussians,"<p>There are many pitfalls to not scaling your data and it is generally very advisable to scale it. It is so easy to do, it is reversible, and it useful in other operations like removing outliers.</p>

<p>Upon further analysis of the specifics of Hidden Markov Models using Multivariate Gaussians the theoretical accuracy should not suffer as a result of drastic differences in the scales of your features.  But, an important operation involving multivariate Gaussian distribution is matrix inversion.</p>

<p>Though the theoretical invertibility won't change, the practical numerical solution to your problem will likely suffer inaccuracy due to the difference in scales.  Iterative methods will have issues with convergence and direct solve methods will suffer from stiffness.  This is especially true when common complications like linear dependence are present.</p>

<p>Here is a set of 3 lectures on HMM with some specifics on multivariate Gaussians (<a href=""http://www.cs.berkeley.edu/~jordan/courses/281A-fall04/lectures/lec-10-21.pdf"" rel=""nofollow"">1</a>-<a href=""http://www.cs.berkeley.edu/~jordan/courses/281A-fall04/lectures/lec-10-26.pdf"" rel=""nofollow"">2</a>-<a href=""http://www.cs.berkeley.edu/~jordan/courses/281A-fall04/lectures/lec-10-28.pdf"" rel=""nofollow"">3</a>)</p>

<p>I know we've already discussed this in the comments, but I wanted to close out this question, so have added it as an answer.</p>

<p>I hope this helps! </p>
"
['text-mining'],TF-IDF not a strong measure in this senario?,"<p>Please have a look at the weighting scheme 2 in table <strong>Recommended TF-IDF weighting schemes</strong> in <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a>. That should solve your problem.</p>
"
"['clustering', 'regression', 'correlation']",What is the best Data Mining algorithm for prediction based on a single variable?,"<p>Common rule in machine learning is to <strong>try simple things first</strong>. For predicting continuous variables there's nothing more basic than <strong>simple linear regression</strong>. ""Simple"" in the name means that there's only one predictor variable used (+ intercept, of course): </p>

<pre><code>y = b0 + x*b1
</code></pre>

<p>where <code>b0</code> is an intercept and <code>b1</code> is a slope. For example, you may want to predict lemonade consumption in a park based on temperature:</p>

<pre><code>cons = b0 + temp * b1
</code></pre>

<p>Temperature is in well-defined <strong>continuous</strong> variable. But if we talk about something more abstract like ""weather"", then it's harder to understand how we measure and encode it. It's ok if we say that the weather takes values <code>{terrible, bad, normal, good, excellent}</code> and assign values numbers from -2 to +2 (implying that ""excellent"" weather is twice as good as ""good""). But what if the weather is given by words <code>{shiny, rainy, cool, ...}</code>? We can't give an order to these variables. We call such variables <strong>categorical</strong>. Since there's no natural order between different categories, we can't encode them as a single numerical variable (and linear regression expects numbers only), but we can use so-called <strong>dummy encoding</strong>:  instead of a single variable <code>weather</code> we use 3 variables - <code>[weather_shiny, weather_rainy, weather_cool]</code>, only one of which can take value 1, and others should take value 0. In fact, we will have to drop one variable because of <a href=""http://en.wikipedia.org/wiki/Multicollinearity"">collinearity</a>. So model for predicting traffic from weather may look like this: </p>

<pre><code>traffic = b0 + weather_shiny * b1 + weather_rainy * b2  # weather_cool dropped
</code></pre>

<p>where either <code>b1</code> or <code>b2</code> is 1, or both are 0. </p>

<p>Note that you can also encounter non-linear dependency between predictor and predicted variables (you can easily check it by plotting <code>(x,y)</code> pairs). Simplest way to deal with it without refusing linear model is to use polynomial features - simply add polynomials of your feature as new features. E.g. for temperature example (for dummy variables it doesn't make sense, cause <code>1^n</code> and <code>0^n</code> are still 1 and 0 for any <code>n</code>):</p>

<pre><code>traffic = b0 + temp * b1 + temp^2 * b2 [+ temp^3 * b3 + ...]
</code></pre>
"
"['predictive-modeling', 'word-embeddings', 'word2vec', 'gensim']",Does traning the Word2Vec model multiple times affect `min_count` parameter?,"<p>The question has been answered in google groups by Gordon mohr.</p>

<blockquote>
  <p>Normally there's one read of the corpus to build the vocabulary (which includes initializing the model based on the learned vocabulary size), then any number of extra passes for training. It's only after the one vocabulary-learning scan that word counts are looked at (and compared to <code>min_count</code> for trimming). </p>
  
  <p>If you supply a corpus (as a restartable iterator) as one of the arguments to the initial creation of the Word2Vec model, all these steps are done automatically: one read of the corpus (through the <code>build_vocab()</code> method) to collect words/counts, then one or more passes (as controlled by the 'iter' parameter and done through the <code>train()</code> method) for training. Still, only the count for the single pass over the supplied corpus matters for frequency decisions. </p>
  
  <p>If you don't supply a corpus at model-initialization, you can then call <code>build_vocab(…)</code> and <code>train(…)</code> yourself. It's only what's passed to <code>build_vocab()</code> that matters for retained frequency counts (and the estimate of corpus size). You can then call <code>train(…)</code> in other ways, or repeatedly – it just keeps using the vocabulary from the one earlier <code>build_vocab(…)</code> call. </p>
  
  <p>(Note that <code>train(…)</code> does try to reuse the single-pass corpus size, remembered from the vocab-scanning pass, to give accurate progress-estimates and schedule the decay of the training-rate <code>alpha</code>. So if you give a different-sized corpus to <code>train(…)</code>, you should also use its other optional parameters to give it a hint of the size.)</p>
</blockquote>
"
"['decision-trees', 'performance']",What can I do with a Decision Tree with poor ROC,"<p>Decision trees has one big quality and one big drawback. Their big quality is that they are what is known as glass-box models. What I mean by that is that they expose what they learn in a very clear and intuitive way. The name comes from the fact that you can see through a glass box. So, because of that, decision trees are very useful for analysis, they are a nice support to understand the relations between variables, which variables are important, in which way they are important. And even if they does not provide crystal clear information, they can bring you ideas about that. This might be very helpful especially if you have domain expert knowledge and you can put things together in a meaningful manner. </p>

<p>Their main drawback is their high variability. This problem is mainly caused by their greedy approach. Each decision in the first level nodes shape differently the tree. You can even go further and see that a single additional data point is enough in many cases to get a totally different tree, especially if the sample is small or if the data is noisy. </p>

<p>There are two types of approaches to solve this issue. The first type of approach tries to improve the single tree you built. This kind of approaches are known as pruning. A simple example would be reduced error pruning. This approach is simple and produce good results. You train your tree on a sample of data. Then you take another sample of data, you fit the new data on the tree, and then evaluate again the nodes of the tree from the perspective of the new data. If a non-leaf node get at least the same error if would not be split than if it would be split, then you can decide to cut the child nodes and transform that node into a leaf node. There are however much nicer pruning strategies, which are stronger, perhaps based on cross validation or some other criteria, mostly statistical based. Notice however that for reduced error pruning you need additional data, or to split your original sample in two, one for training, the other for pruning. If you go further to estimate the prediction error you need a third sample.</p>

<p>The second approach would be either to build multiple times some trees and chose some based on cross validation, bootstrapping or whatever method you you, or use a tree ensembles like bagging or boosting algorithms. Note that for boosting and bagging you loose glass box property. </p>

<p>Ultimately you have to choose between understanding and performance, having as a decent compromise the pruning procedure. </p>
"
"['machine-learning', 'classification', 'data-mining', 'predictive-modeling', 'time-series']",Compute Baseline/Representative of Time-Series Data,"<p>Ad 1. Assuming the measurements at any given time are normally distributed (they shape approximately a bell curve), you could use simple standard deviation to detect outliers. Specifically, for any given time, you can calculate the mean and standard error. Then you calculate the mean sans outliers by taking into account only the measurements that fall at most some pre-set distance from the mean (e.g. given normal distribution, <a href=""https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule"" rel=""nofollow"">68% of measurements fall within one standard deviation from the mean</a>).</p>

<p>Pseudo-code example:</p>

<pre><code># Measurements at time t0 for all 10 days
t0 = np.array([0.1, 0.1, 1.4, .9, 1.25, 1.25, 1.5, 0.1, 0.3, 1.75])

# Get mean and standard error
mean0, std0 = t0.mean(), t0.std()

# Inliers are within one sigma from the mean
inliers = np.logical_and(mean0 - std0 &lt; t0,
                         mean0 + std0 &gt; t0)
# ==&gt; [0, 0, 1, 1, 1, 1, 0, 0, 1, 0]

# And the baseline mean at time t0 is
baseline0 = t0[inliers].mean()
# ==&gt; 1.02
</code></pre>

<p>Ad 2. You can find the most similar days to the baseline by using any appropriate distance measure (i.e. for time series: Euclidean or <a href=""https://en.wikipedia.org/wiki/Dynamic_time_warping"" rel=""nofollow"">dynamic time warping</a>). The result, then, consists of those days where distance is the least.</p>
"
['r'],Avoid recycling when merging with R Data Table,"<p>What you want to do is <code>join</code>-like behaviour that is among others documented here: 
<a href=""https://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html"" rel=""nofollow"">joins</a> and <a href=""https://jangorecki.github.io/blog/2015-12-11/Solve-common-R-problems-efficiently-with-data.table.html"" rel=""nofollow"">others</a></p>

<p>To make things readable here a version with <code>dplyr</code></p>

<pre><code>ex1 &lt;- data.frame(id = 1:10, name = replicate(10, paste(sample(letters[1:5], size = 2), collapse = """")))

ex2 &lt;- data.frame(otherinfo = (1:5)+50, othername = replicate(5, paste(sample(letters[1:5], size = 2), collapse = """")))

dplyr::left_join(ex1, ex2, by = c(name = ""othername""))

   id name otherinfo
1   1   ac        NA
2   2   ac        NA
3   3   ab        NA
4   4   da        53
5   5   ab        NA
6   6   cb        NA
7   7   eb        NA
8   8   de        NA
9   9   ad        NA
10 10   ec        52
</code></pre>
"
"['machine-learning', 'predictive-modeling']",Machine Learning Steps,"<p>I found both of your options slightly faulty. So, this is generally (very broadly) how a predictive modelling workflow looks like:</p>

<ul>
<li><strong>Data Cleaning</strong>: Takes the most time, but every second spent here is worth it. The cleaner your data gets through this step, the lesser would your total time spent would be. </li>
<li><strong>Splitting the data set</strong>: The data set would be splitted into training and testing sets, which would be used for the modelling and prediction purposes respectively. In addition, an additional split as a cross-validation set would also need to be done.</li>
<li><strong>Transformation and Reduction:</strong> Involves processes like transformations, mean and median scaling, etc.</li>
<li><strong>Feature Selection:</strong> This can be done in a lot of ways like threshold selection, subset selection, etc.</li>
<li><strong>Designing predictive model:</strong> Design the predictive model on the training data depending on the features you have at hand.</li>
<li><strong>Cross Validation:</strong></li>
<li><strong>Final Prediction, Validation</strong></li>
</ul>
"
"['machine-learning', 'statistics', 'career']",Statistics + Computer Science = Data Science?,"<p>I think that you're on the right track toward becoming an <strong>expert</strong> <em>data scientist</em>. Recently I have answered related question here on Data Science StackExchange: <a href=""http://datascience.stackexchange.com/a/742/2452"">http://datascience.stackexchange.com/a/742/2452</a> (pay attention to the <em>definition</em> I mention there, as it essentially answers your question by itself, as well as to aspects of <strong>practicing</strong> <em>software engineering</em> and <strong>applying</strong> knowledge to solving <em>real-world</em> problems). I hope that you will find all that useful. Good luck in your career!</p>
"
"['classification', 'clustering', 'statistics', 'missing-data']",Cluster analysis as an associative model?,"<p>I think the question you're wrestling with is essentially this: Is there a way to use information that may be present in the data without A as part of your strategy for predicting A?  There is actually a name for the set of methods that do exactly that: <a href=""https://en.wikipedia.org/wiki/Semi-supervised_learning"" rel=""nofollow"">semi-supervised learning</a>.  </p>

<p>While there are multiple techniques, a method analogous to what you suggest in your question would be something like the following.  Here I refer to the set of data with A as labeled and the rest as unlabeled.</p>

<ol>
<li><p>Apply an unsupervised learning technique (such as clustering) to the full data set (both labelled and unlabelled).</p></li>
<li><p>Transform the labelled data based on the result of your unsupervised learning technique.</p></li>
<li><p>Apply a supervised learning technique with the transformed set (possibly combined with the original set) of labelled data.</p></li>
</ol>

<p>Generally you would want to apply a cross-validation strategy as well to detect overfitting.</p>
"
"['data-cleaning', 'preprocessing']",Dealing with training set of questionable quality,"<p>Welcome to the real world of data science. Here, the data sets are not as clean as you thought while doing those courses/tutorials online.  Those are super polished and refined.  But, the real world data is not so.</p>

<p>The step where one does the cleaning and scrubbing is called the data pre-processing step.</p>

<p>So, some nice data cleaning techniques, in addition to @jknappen's excellent answer are:</p>

<ol>
<li><strong>Elimination of zero variance columns/predictors:</strong> These columns are not important, and they cause the model and the fit to crash and leak errors. So, eliminating them would make complete sense.</li>
<li><strong>Correlated Predictors:</strong> Reducing the level of correlation between the predictors would be a very nice step in the pre-processing process.</li>
<li><strong>Scaling:</strong> You must be knowing why scaling is important during pre-process. </li>
<li><strong>Predictor Transformations</strong></li>
</ol>

<p>A <a href=""https://www.kaggle.com/c/MerckActivity/forums/t/2900/what-do-you-use-to-preprocess-the-data/26554"" rel=""nofollow"">nice reference from Kaggle forums</a> where the pre-processing and cleaning of data sets is discussed.</p>
"
"['machine-learning', 'classification']",StackOverflow Tags Predictor...Suggest an Machine Learning Approach please?,"<p>This exact problem was a kaggle competition sponsored by Facebook. The particular forum thread of interest for you is the one where many of the top competitors explained their methodology, this should provide you with more information than you were probably looking for: <a href=""https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/forums/t/6650/share-your-approach"" rel=""nofollow"">https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/forums/t/6650/share-your-approach</a> </p>

<p>In general, it appears that most people treated the problem as a supervised one. Their primary feature was a tf-idf, or unweighted BOW, representations of the text and they ensembled 1000s of single-tag models. Owen, the winner of the competition, noted that the title text was a more powerful feature than the content of the body of the post.</p>
"
"['r', 'clustering']",How to create clusters of position data?,"<p>I doubt any of the clustering algorithms will work well.</p>

<p>Instead, you should look into:</p>

<ul>
<li>segmentation (yes, this is something different), specifically time series segmentation</li>
<li>change detection (as you said, there is a rather constant distribution first, then a change, then a rather constant distribution again</li>
<li>segment-wise regression may also work: try to find the best fit that is constant, linearly changing, and constant again. It's essentially four parameters to optimize in this restricted model: average before and after + beginning and end of transition.</li>
</ul>
"
"['machine-learning', 'neuralnetwork', 'algorithms', 'optimization', 'hyperparameter']",Choosing a learning rate,"<ul>
<li><p><strong>Is the learning rate related to the shape of the error gradient, as
it dictates the rate of descent?</strong></p>

<ul>
<li>In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.</li>
</ul></li>
<li><p><strong>If so, how do you use this information to inform your decision about a value?</strong></p>

<ul>
<li><p>Adagrad is the most widely known of these and scales a global learning rate <em>η</em> on each dimension based on l2 norm of the history of the error gradient <em>gt</em> on each dimension:</p>

<p><img src=""http://i.stack.imgur.com/aP96K.png"" alt=""enter image description here""></p></li>
<li><p><a href=""http://arxiv.org/pdf/1212.5701v1.pdf"">Adadelta</a> is another such training algorithm which uses both the error gradient history like adagrad and the weight update history and has the advantage of <em>not having to set a learning rate at all</em>.</p></li>
</ul></li>
<li><p><strong>If it's not what sort of values should I choose, and how should I choose them?</strong></p>

<ul>
<li>Setting learning rates for plain SGD in neural nets is usually a
process of starting with a sane value such as 0.01 and then doing cross validation
to find an optimal value. Typical values range over a few orders of
magnitude from 0.0001 up to 1.</li>
</ul></li>
<li><p><strong>It seems like you would want small values to avoid overshooting, but
how do you choose one such that you don't get stuck in local minima
or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?</strong></p>

<ul>
<li>Usually the value that's best is near the highest stable learning
rate and learning rate decay/annealing (either linear or
exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.</li>
</ul></li>
</ul>
"
"['efficiency', 'statistics', 'tools', 'knowledge-base']",Best languages for scientific computing,"<p>This is a pretty massive question, so this is not intended to be a full answer, but hopefully this can help to inform general practice around determining the best tool for the job when it comes to data science. Generally, I have a relatively short list of qualifications I look for when it comes to any tool in this space. In no particular order they are:</p>

<ul>
<li><strong>Performance</strong>: Basically boils down to how quickly the language does matrix multiplication, as that is more or less the most important task in data science.</li>
<li><strong>Scalability</strong>: At least for me personally, this comes down to ease of building a distributed system. This is somewhere where languages like <code>Julia</code> really shine.</li>
<li><strong>Community</strong>: With any language, you're really looking for an active community that can help you when you get stuck using whichever tool you're using. This is where <code>python</code> pulls very far ahead of most other languages. </li>
<li><strong>Flexibility</strong>: Nothing is worse than being limited by the language that you use. It doesn't happen very often, but trying to represent graph structures in <code>haskell</code> is a notorious pain, and <code>Julia</code> is filled with a lot of code architectures pains as a result of being such a young language.</li>
<li><strong>Ease of Use</strong>: If you want to use something in a larger environment, you want to make sure that setup is a straightforward and it can be automated. Nothing is worse than having to set up a finnicky build on half a dozen machines.</li>
</ul>

<p>There are a ton of articles out there about performance and scalability, but in general you're going to be looking at a performance differential of maybe 5-10x between languages, which may or may not matter depending on your specific application. As far as GPU acceleration goes, <code>cudamat</code> is a really seamless way of getting it working with <code>python</code>, and the <code>cuda</code> library in general has made GPU acceleration far more accessible than it used to be.</p>

<p>The two primary metrics I use for both community and flexibility are to look at the language's package manager, and the language questions on a site like SO. If there are a large number of high-quality questions and answers, it's a good sign that the community is active. Number of packages and the general activity on those packages can also be a good proxy for this metric.</p>

<p>As far as ease of use goes, I am a firm believer that the only way to actually know is to actually set it up yourself. There's a lot of superstition around a lot of Data Science tools, specifically things like databases and distributed computing architecture, but there's no way to really know if something is easy or hard to setup up and deploy without just building it yourself.</p>
"
"['data-cleaning', 'anonymization']","How can I transform names in a confidential data set to make it anonymous, but preserve some of the characteristics of the names?","<p>One of the references I mentioned in the OP led me to a potential solution that seems quite powerful, described in ""Privacy-preserving record linkage using Bloom filters"" (<a href=""http://www.biomedcentral.com/1472-6947/9/41"">doi:10.1186/1472-6947-9-41</a>):</p>

<blockquote>
  <p>A new protocol for privacy-preserving record linkage with encrypted identifiers allowing for errors in identifiers has been developed. The protocol is based on Bloom filters on q-grams of identifiers.</p>
</blockquote>

<p>The article goes into detail about the method, which I will summarize here to the best of my ability.</p>

<p>A Bloom filter is a fixed-length series of bits storing the results of a fixed set of independent hash functions, each computed on the same input value. The output of each hash function should be an index value from among the possible indexes in the filter; i.e., if you have a 0-indexed series of 10 bits, hash functions should return (or be mapped to) values from 0 to 9. </p>

<p>The filter starts with each bit set to 0. After hashing the input value with each function from the set of hash functions, each bit corresponding to an index value returned by any hash function is set to 1. If the same index is returned by more than one hash function, the bit at that index is only set once. You could consider the Bloom filter to be a superposition of the set of hashes onto the fixed range of bits.</p>

<p>The protocol described in the above-linked article divides strings into n-grams, which are in this case sets of characters. As an example, <code>""hello""</code> might yield the following set of 2-grams:</p>

<pre><code>[""_h"", ""he"", ""el"", ""ll"", ""lo"", ""o_""]
</code></pre>

<p>Padding the front and back with spaces seems to be generally optional when constructing n-grams; the examples given in the paper that proposes this method use such padding.</p>

<p>Each n-gram can be hashed to produce a Bloom filter, and this set of Bloom filters can be superimposed on itself (bitwise OR operation) to produce the Bloom filter for the string.</p>

<p>If the filter contains many more bits than there are hash functions or n-grams, arbitrary strings are relatively unlikely to produce exactly the same filter. However, the more n-grams two strings have in common, the more bits their filters will ultimately share. You can then compare any two filters <code>A, B</code> by means of their Dice coefficient:</p>

<blockquote>
  <p>D<sub>A, B</sub> = 2h / (a + b)</p>
</blockquote>

<p>Where <code>h</code> is the number of bits that are set to 1 in both filters, <code>a</code> is the number of bits set to 1 in <em>only</em> filter A, and <code>b</code> is the number of bits set to 1 in <em>only</em> filter B. If the strings are exactly the same, the Dice coefficient will be 1; the more they differ, the closer the coefficient will be to <code>0</code>.</p>

<p>Because the hash functions are mapping an indeterminate number of unique inputs to a small number of possible bit indexes, different inputs may produce the same filter, so the coefficient indicates only a <em>probability</em> that the strings are the same or similar. The number of different hash functions and the number of bits in the filter are important parameters for determining the likelihood of false positives - pairs of inputs that are much less similar than the Dice coefficient produced by this method predicts.</p>

<p>I found <a href=""http://billmill.org/bloomfilter-tutorial/"">this tutorial</a> to be very helpful for understanding the Bloom filter.</p>

<p>There is some flexibility in the implementation of this method; see also <a href=""https://www.uni-due.de/~hq0215/documents/2010/Bachteler_2010_An_Empirical_Comparison_Of_Approaches_To_Approximate_String_Matching_In_Private_Record_Linkage.pdf"">this 2010 paper</a> (also linked at the end of the question) for some indications of how performant it is in relation to other methods, and with various parameters.</p>
"
"['predictive-modeling', 'scikit', 'pandas', 'marketing', 'scoring']",Prediction model for marketing to prospective customers (using pandas),"<p>First, I would ask the company if there is more information about the customer. You mentioned you have 10 original columns, which might not be enough to make a good prediction. Same goes for number of rows. Usually, more data, better the model, up to a certain limit.</p>

<p>Second, encode categorical features (demographical data in your case) is good thing to do. The increased number of columns dont have to bother you in your case.</p>

<p>For the task itself, yes, it is doable. Start easy, simply check importance of each feature (I would let PCA for later), pick few models and test them.</p>

<p>Also consider train simple decision tree. Your results can be easily visualized in way the business people understand. As oposed to black-box methods as K-Means.</p>
"
"['machine-learning', 'dataset', 'algorithms']",Sequence of numbers as single feature,"<p>2 solutions:</p>

<ul>
<li>You aggregate each sequence of numbers into a single number, which use as a feature. There exist plenty of aggregation functions, such as some derived from descriptive statistics root-mean-square, kurtosis, skewness, max, min, duration, standard deviation, crest factor, mean, or more specific aggregation such as fourier transforms or wavelet transforms.</li>
<li>You use some model that accepts sequences as input: all such models I have in mind would require your sequences of numbers to be of same length though. Example of such model: <a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network"" rel=""nofollow"">recurrent neural networks</a>, <a href=""https://en.wikipedia.org/wiki/Dynamic_Bayesian_network"" rel=""nofollow"">Dynamic Bayesian networks</a>.</li>
</ul>
"
['octave'],Clarification about Octave data size limit,"<p>You have about 4GB of RAM on your machine and Octave is an in memory application. </p>

<p>If you want to work with 515345 instances and 4275 features, assuming that you are using double precision (i.e. 8 bytes), you would need a memory of 515345*4275*8/1000000/1024 bytes ~ 17.6 GB. Even if you were using 4 bytes for each data point, you would require at least 9 GB for the computation to go through. </p>

<p>This issue might not be the Octave memory restriction in this case. See <a href=""http://stackoverflow.com/questions/565806/is-anyone-using-64-bit-build-of-octave"">here</a> for further details on Octave's memory usage. </p>
"
"['statistics', 'marketing']",How can I use data on customer interactions to drive communication strategy?,"<p>I would do the following:</p>

<p>First, cluster your customers into several groups, based on age, location, gender etc. Second, for each group, use different combination of interaction as features to predict 'yes/no', and select which combination of interaction performs best.</p>
"
"['machine-learning', 'neuralnetwork']",How do AI's learn to act when the problem space is too big,"<p>That is a good question and many scientists around the world are asking the same. Well, first a game like Age of Empires is not considered to have a really big solution space, there are not so many things you can do. It's the same in games like Mario Bros. The problem of learning in easy games like Atari games was solve by the guys of DeepMind (here the <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow"">paper</a>), that was acquired by Google. They used an implementation of Reinforcement Learning with Deep Learning.</p>

<p>Going back to your question. A really big problem is how to imitate the amount of decisions a human being takes every day. Wake up, have breakfast, take a shower, leave your house... All these action need a really high level of intelligence and many actions to develop. </p>

<p>There are many people working on this problem, I'm one of them. I don't know the solution but I can tell you in which way I'm looking. I follow the theories of Marvin Minsky, he is one of the fathers of AI. This book, the Emotion Machine, tells a very good view of the problem. He suggested that the way to create a machine that imitates the human behavior is not by constructing an unified compact theory of artificial intelligence. On the contrary, he argues that our brain contains resources that compete
between each other to satisfy different goals at the same moment. They called this <strong>Ways to Think</strong>.</p>
"
"['python', 'clustering', 'parameter-estimation', 'dbscan']",Knn distance plot for determining eps of DBSCAN,"<p>You </p>

<ol>
<li>take the last column of that matrix</li>
<li>sort descending</li>
<li>plot index, distance</li>
<li>hope to see a knee (if the distance does not work well. there might be none)</li>
</ol>
"
"['r', 'predictive-modeling', 'csv']",Reading a wide dataset in R,"<p>Refer to my comment, I believe what you need is  </p>

<pre><code>df &lt;- read.csv(""./yourpath/yourfile"", sep = "";"", header = TRUE) # play around with the arguments per your file.
</code></pre>
"
"['feature-selection', 'svm', 'matlab', 'overfitting']",Is there a problem of over fitting in my dataset?,"<p>It is not possible to tell whether a machine learning algorithm is overfitting based purely on the training set accuracy. </p>

<p>You could be right, that using more features with a small data set increases sampling error and reduces the generalisation of the SVM model you are building. It is a valid concern, but you cannot say that for sure with only this worry and the training accuracy to look at.</p>

<p>The usual solution to this is to keep some data aside to test your model. When you see a high training accuracy, but a low test accuracy, that is a classic sign of over-fitting. </p>

<p>Often you are searching for the best hyper-parameters to your model. In your case you are trying to discover the best number of features to use. When you start to do that, you will need to make multiple tests in order to pick the best hyper-parameter values. At that point, a single test set becomes weaker measure of true generalisation (because you have had several attempts and picked best value - just by selection process you will tend to over-estimate the generalisation). So it is common practice to split the data three ways - training set, cross-validation set and test set. The cross-validation set is used to check accuracy as you change the parameters of your model, you pick the best results and then finally use the test set to measure accuracy of your best model. A common split ratio for this purpose is 60/20/20.</p>

<p>Taking a pragmatic approach when using the train/cv/test split, it matters less that you are over or under fitting than simply getting the best result you can with your data and model class. You can use the feedback on whether you are over-fitting (high training accuracy, low cv accuracy) in order to change model parameters  - increase regularisation when you are over-fitting for example.</p>

<p>When there are a small number of examples, as in your case, then the cv accuracy measure is going to vary a lot depending on which items are in the cv set. This makes it hard to pick best hyper-params, because it may just be noise in the data that makes one choice better than another. To reduce the impact of this, you can use <a href=""https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation"" rel=""nofollow"">k-fold cross-validation</a> - splitting your train/cv data multiple times and taking an average measure of the accuracy (or whatever metric you want to maximise).</p>

<hr>

<p>In your confusion matrices, there is no evidence of over-fitting. A training accuracy of 100%* and testing accuracy of 93.8% are suggestive of some degree of over-fit, but the sample size is too low to read anything into it. You should bear in mind that balance between over- and under- fit is very narrow and most models will do one or the other to some degree. </p>

<p>* A training accuracy of 100% is nearly always <em>suggestive</em> of overfit. Matched with e.g. 99% test accuracy, you may not be too concerned. The question is at worst ""could I do better by increasing regularisation a little""? However, matched with ~60% test accuracy it is clear you have actually overfit - even then you might be forced to accept the situation if that's the best you could achieve after trying many different hyperparameter values (including some attempts with increased regularisation).</p>
"
"['regression', 'linear-regression', 'decision-trees']",Using Decision Tree methodology to identify Independent Variables for Multiple Regression,"<p>Decision trees are useful for determining nested/interactive relationships between combinations of IVs and a DV. </p>

<p>The model you specified, a multiple regression, presupposes a relationship between the IVs and the DV (e.g. linear). </p>

<p>As you are aware, these models are different. So using a decision tree coupled with some importance measure to find predictive variables won't necessarily provide you with an optimal set of IVs in a regression model.</p>

<p>That being said, it can be a helpful exercise to inform you of non-linear relationships or interaction terms that could be predictive, and which may not be captured by specifying a model such as a multiple regression. </p>

<p>If I were you, I wouldn't solely rely on using decision trees to determine a  set of IVs for a regression model. I would investigate penalized regression methods such as LASSO or ridge regression to help take you from a reduced candidate set of IVs to your final IVs. In addition, you might want to explore associative metrics related to your model specification that might be useful in exploring the relationships in your data, such as information values, chi-square tests, correlations, etc.</p>

<p>This may be helpful:
<a href=""http://stats.stackexchange.com/questions/47367/decision-tree-as-variable-selection-for-logistic-regression"">http://stats.stackexchange.com/questions/47367/decision-tree-as-variable-selection-for-logistic-regression</a></p>
"
"['machine-learning', 'python', 'data-mining', 'clustering', 'decision-trees']",Choosing the correct learning algorithm,"<p>It seems you have a data set for one component where the component suffered a fixed number of failure modes. You want to find out which data (let's assume continuous in time, so, what time) correspond to what failure mode. In other words, you are doing ""pattern recognition"" in your failure data.</p>

<p>Have you thought of using Self-Organizing Maps (SOM)? They are a sub-branch of artificial neural networks and have great capability in such problems. </p>

<p>You should also consider that not all failure modes appear in shape of a ""peak"" value. So, only looking at peaks is not a very smart way. It most probably will cover most of the failures, though, there will be moments that you miss. SOM could take care of this too. </p>

<p>Data pre-processing is done before the analysis. Be careful in normalization. You could miss the peaks or valley points easily if you don't pay enough attention in normalization. Don't just use any code or normalization method you find online. Test and check it with you data. For instance, some normalization could make all negative values positive which a negative value may have an important meaning in your work.</p>

<p>I assume you have another variable called ""failure"". Another approach I suggest is building a Neural Network (NN) model, which is very common. You have three input data that you mentioned, consider the ""failure"" variable as your target variable. Build the neural network and apply it to your data again. If the numbers of failures are little, the NN will be able to rebuild the normal behavior of your data (NN here is called a normal behavior model). When you apply it to your input data, the NN model will detect any deviation which is not expected. </p>

<p>MATLAB has a very good support for both of these approaches. </p>
"
"['apache-spark', 'recommender-system', 'word2vec']",solution for in Time/Space Complexity challenge in Recommendation System?,"<p>Apache Spark is a great solution for such problems. </p>

<p>But, first let's be clear about the definition of <strong>real time processing</strong>. It's the type of processing that must guarantee response within specified time which on an interactive business site is actually very low. You can read about those kind of specifications in <a href=""http://stackoverflow.com/questions/164175/what-is-considered-a-good-response-time-for-a-dynamic-personalized-web-applicat"">this answer</a>.</p>

<p>Spark doesn't provide such luxury in predicting under 0.1 sec and I'm citing </p>

<blockquote>
  <p>Excerpt from Chapter 5 in my book Usability Engineering, from 1993:</p>
  
  <ul>
  <li><strong>0.1 second</strong> is about the limit for having the user feel that the system is <strong>reacting instantaneously</strong>, meaning that no special feedback is necessary except to display the result.</li>
  </ul>
</blockquote>

<p>Having an <strong>interactive business site</strong> on which you'd want to display predictions doesn't mean that your predictions has to be in real time.</p>

<p>So the obvious is, actually, the following : </p>

<p><strong>Q:</strong> <em>Now that I have computed recommendations for my users, what should I do ?</em> </p>

<p><strong>A:</strong> Let's define a <strong>serving layer</strong> for our systems fast enough to query when recommendations are needed.</p>

<p>It can be anything fast enough to answer your calls e.g Elasticsearch, Solr, HBase, Redis. Whatever flavor suits you.</p>

<p>On other hands, well </p>

<p><strong>Q:</strong> <em>I don't want my system to be static, I need to recompute my predictions every T hours/days/etc</em></p>

<p><strong>A:</strong> Spark can do a scheduled job perfectly here. (a simple cron would do)</p>

<p><strong>Q:</strong> <em>But when do I retrain my recommender system ?</em> </p>

<p><strong>A:</strong> I would say it actually depends on so many stuff a bit too broad to discuss here. You can read about the topic <a href=""https://www.quora.com/How-often-do-big-companies-refit-their-learning-algorithms-to-new-data"" rel=""nofollow"">here</a> if you wish.</p>

<p>Ok, so we defined now our <strong>batch layer</strong>.</p>

<p><strong>Q:</strong> <em>And what about data coming in real time, through Kafka, Rabbit, etc. ?</em></p>

<p><strong>A:</strong> This is actually when it can get more complicated, because the method that you'll use to compute distances, approximations, new recommendations will depends on what type of recommender systems you are building and what technologies you are using.</p>

<p>Spark streaming can fit very well to apply ""simple"" computations on ""window"" based micro batches. This can be our <strong>speed layer</strong>.</p>

<p>To conclude, all of the above defines what is called a <a href=""http://lambda-architecture.net/"" rel=""nofollow"">lambda architecture</a>. And one of the best framework that follows this design is <a href=""https://github.com/OryxProject/oryx"" rel=""nofollow"">Oryx</a> (personal opinion).
It's quite interesting, you ought taking a look at it.</p>

<p>I also believe that it's quite possible to have a RT set-up for a recommendation system without the <strong>speed layer</strong>.</p>

<p>I hope that this answers your question.</p>
"
['r'],Building a static local website using Rmarkdown: step by step procedure,"<p>In most things, related to <strong>R</strong>, there are <strong>many approaches</strong> to solve a problem, sometimes too many, I would say. The task of building a static website, using <em>RMarkdown</em>, is not an exception.</p>

<p>One of the best, albeit somewhat brief, sets of workflows on the topic include the following one by Daniel Wollschlaeger, which includes <a href=""http://www.uni-kiel.de/psychologie/rexrepos/posts/rerWorkflowJN.html"" rel=""nofollow"">this workflow</a>, based on <em>R</em>, <em>nanoc</em> and <em>Jekyll</em>, as well as <a href=""http://www.uni-kiel.de/psychologie/rexrepos/posts/rerWorkflowWP.html"" rel=""nofollow"">this workflow</a>, based on <em>R</em> and <em>WordPress</em>. Another good workflow is <a href=""http://jason.bryer.org/posts/2012-12-10/Markdown_Jekyll_R_for_Blogging.html"" rel=""nofollow"">this one</a> by Jason Bryer, which is focused on <em>R(Markdown)</em>, <em>Jekyll</em> and <em>GitHub Pages</em>.</p>

<p>Not everyone likes <em>GitHub Pages</em>, <em>Jekyll</em>, <em>Octopress</em> and <em>Ruby</em>, so some people came up with <em>alternative solutions</em>. For example, <a href=""http://www.znmeb.mobi/stories/blogging-with-rstudio-and-nikola.html"" rel=""nofollow"">this workflow</a> by Edward Borasky is based on <em>R</em> and, for a static website generator, on <em>Python</em>-based <em>Nicola</em> (instead of <em>Ruby</em>-based <em>Jekyll</em> or <em>nanoc</em>). Speaking about <strong>static website generators</strong>, there are tons of them, in various programming languages, so, if you want to experiment, check <a href=""http://staticsitegenerators.net"" rel=""nofollow"">this amazing website</a>, listing almost all of them. Almost, because some are missing - for example, <em>Samantha</em> and <em>Ghost</em>, listed <a href=""http://ropensci.github.io/reproducibility-guide/sections/tools"" rel=""nofollow"">here</a>.</p>

<p>Some other interesting workflows include <a href=""http://joshualande.com/jekyll-github-pages-poole"" rel=""nofollow"">this one</a> by Joshua Lande, which is based on Jekyll and GitHub Pages, but includes some nice examples of customization for integrating a website with Disqus, Google Analytics and Twitter as well as getting custom URL for the site and more.</p>

<p>Those who want a <strong>pure R-based static site solution</strong>, now have some options, including <code>rsmith</code> (<a href=""https://github.com/hadley/rsmith"" rel=""nofollow"">https://github.com/hadley/rsmith</a>), a static site generator by Hadley Wickham, and <code>Poirot</code> (<a href=""https://github.com/ramnathv/poirot"" rel=""nofollow"">https://github.com/ramnathv/poirot</a>), a static site generator by Ramnath Vaidyanathan.</p>

<p>Finally, I would like to mention an <a href=""http://notebook.madsenlab.org/labnotebook.html"" rel=""nofollow"">interesting project</a> (from an <strong>open science</strong> perspective) that I recently ran across - an open source software by Mark Madsen for a <em>lab notebook static site</em>, which is based on <em>GitHub Pages</em> and <em>Jekyll</em>, but also supports <em>pandoc</em>, <em>R</em>, <em>RMarkdown</em> and <em>knitr</em>.</p>
"
"['data-mining', 'classification', 'clustering']",about predict the class for a new datapoint,"<p>I think you need to take a step back and figure out what you're trying to do at a higher level.</p>

<p>How were the existing classes built? If they were built by clustering unlabeled data, then with this new data point you're continuing with the clustering process.</p>

<p>If the existing classes are labeled data, then k-NN is one possible classification method, and there are plenty more (decision trees, naive bayes, neural networks, etc.). </p>

<p>If you're doing clustering, then there are several ways of assigning a point to a cluster, among measuring the distances from the point to cluster centroids is one. There's also single-linkage (distance is min of distances from point to points of cluster), complete-linkage (max of distances). These different methods will give clusters with different shapes and there's no universally best approach. You could test them with points that are already in clusters... but then if you're certain of what classes they re in, then you have a classification problem.</p>

<p>So... if it's classification, then you can use k-NN, that's similar to the idea of assigning a point to a cluster according to distance. But it's not defined as finding the nearest cluster, it's defined as finding the classes of the k nearest points, then applying a vote or something. 1-NN is basically like single-linkage clustering. kNN does require finding the most similar (training) data points to your new data point. Sampling is definitely sub-optimal, but it may be good enough if you classes are well separated. If the cost of calculating distances is high, then one way of reducing the cost of calculation is the idea of skyline clustering: use a cheap distance metric to determine a subset of points that are likely to be among the k nearest neighbours, then compute these neighbours using the more expensive distance metric.</p>

<p>Finally, if you will be classifying many points and not updating your model, it may be worth training a model (e.g. a decision tree) on the existing classes.</p>
"
['processing'],SAS PROC means (two variants together),"<p>Just run </p>

<p>PROC means data=d mean; var a; run;</p>

<p>for overall data</p>
"
"['machine-learning', 'python', 'classification', 'logistic-regression', 'topic-model']",Binary classification model for sparse / biased data,"<ol>
<li><p>Since you are doing binary classification, have you tried adjusting the classification threshold? Since your algorithm seems rather insensitive, I would try lowering it and check if there is an improvement.</p></li>
<li><p>You can always use <a href=""http://stackoverflow.com/questions/4617365/what-is-a-learning-curve-in-machine-learning"">Learning Curves</a>, or a plot of one model parameter vs. Training and Validation error to determine whether your model is overfitting. It seems it is under fitting in your case, but that's just intuition.</p></li>
<li><p>Well, ultimately it depends on your dataset, and the different models you have tried. At this point, and without further testing, there can not be a definite answer.</p></li>
<li><p>Without claiming to be an expert on the topic, there are a number of different techniques you may follow (hint: <a href=""http://florianhartl.com/thoughts-on-machine-learning-dealing-with-skewed-classes.html"">first link on google</a>), but in my opinion you should first make sure you choose your cost function carefully, so that it represents what you are actually looking for. </p></li>
<li><p>Not sure what you mean by pattern intuition, can you elaborate?</p></li>
</ol>

<p>By the way, what were your results with the different algorithms you tried? Were they any different?</p>
"
['machine-learning'],How to select algorithms for ensemble methods?,"<p>In general in an ensemble you try to combine the opinions of multiple classifiers. The idea is like asking a bunch of experts on the same thing. You get multiple opinions and you later have to combine their answers (e.g. by a voting scheme). For this trick to work you want the classifiers to be different from each other, that is you don't want to ask the same ""expert"" twice for the same thing.</p>

<p>In practice, the classifiers do not have to be different in the sense of a different algorithm. What you can do is train the same algorithm with different subset of the data or a different subset of features (or both). If you use different training sets you end up with different models and different ""independent"" classifiers. </p>

<p>There is no golden rule on what works best in general. You have to try to see if there is an improvement for your specific problem. </p>
"
"['machine-learning', 'dataset']",Combining Datasets with Different Features,"<p>You can use <a href=""http://www.r-project.org/"" rel=""nofollow"">R</a> to do that. </p>

<p><a href=""http://www.inside-r.org/packages/cran/gtools/docs/smartbind"" rel=""nofollow"">The smartbind function</a> is the perfect way to combine datsets in the way you are asking for:</p>

<pre><code>library(gtools)

d1&lt;-as.data.frame(rbind(c(1,7,3),c(4,8,4))))
names(d1)&lt;-c(""featureA"",""featureB"",""featureC"")

d2&lt;-as.data.frame(rbind(c(3,4,5,6),c(9,8,4,6)))
names(d2)&lt;-c(""featureA"",""featureC"",""featureD"",""featureE"")

d3&lt;-smartbind(d1,d2)
</code></pre>
"
['linear-regression'],Are there any interesting application of linear regression,"<p>The way that you have phrased this question makes it tough for people to answer without first offering you some background on linear regression (LR).  Its great that you are interested in learning some ML and LR is a great place to start.</p>

<p>Linear regression is really just finding a line (or plain or hyperplain) that maps a relationship between two or more variables or features.  The important requirement is that the target variable be a continuous numerical value.  So its less about finding a problem for which linear regression ""works"" and more about finding some data that interests you and playing with it using linear regression.</p>

<p>I suggest you download some open data in the realms that you have mentioned.  There is plenty of open data in every topic that you mentioned that contain continuous numerical values that you can predict using other features of the data.</p>

<ol>
<li><a href=""https://www.quandl.com/data/WIKI"" rel=""nofollow"">stock trading</a></li>
<li><a href=""http://www.giantbomb.com/api/"" rel=""nofollow"">video games</a></li>
<li><a href=""http://sdo.gsfc.nasa.gov/data/dataaccess.php"" rel=""nofollow"">astronomy</a></li>
<li><a href=""http://www.opensourcesports.com/"" rel=""nofollow"">sports betting</a></li>
<li><a href=""https://www.firebase.com/docs/open-data/airports.html"" rel=""nofollow"">flight time prediction</a></li>
</ol>

<p>Also think about taking some sort of online MOOC as this will help you gain some footing in the subject.  <a href=""https://www.coursera.org/learn/machine-learning/home/info"" rel=""nofollow"">Andrew Ng's Coursera on Machine Learning</a> is highly recommended as a starting point and include some linear regression during the first portion and <a href=""http://scikit-learn.org/"" rel=""nofollow"">scikit-learn</a> is a great Python based library.</p>
"
"['sql', 'hive']",How to extract a column that has the highest value within row in Hive?,"<p>I can't test in hive, but a possible SQL query is as follows (greatest returns the maximum value from the list):</p>

<pre><code>select 
  case 
   when col1 = greatest(col1,col2,col3) then 'col1' 
   when col2 = greatest(col1,col2,col3) then 'col2'  
   when col3 = greatest(col1,col2,col3) then 'col3'    
  end as c1 
from test;
</code></pre>

<p>Additional note:
you should check, how ties are to be handled, in my solution I simple take the first column.</p>
"
"['python', 'data-mining', 'databases', 'orange']",Overcome memory limitation when downloading from database into Orange,"<p>You can use ""download data to local memory"" checkbox in SQL Table widget, it should allow you to work with up to 1000000 rows.</p>

<p><a href=""http://i.stack.imgur.com/8Vskn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8Vskn.png"" alt=""enter image description here""></a></p>
"
"['machine-learning', 'neuralnetwork']",normalize identification values properly,"<p>It is possible that the other variables you're feeding into the NN are simply bad at predicting sales.  Sell prediction is a notoriously hard problem.</p>

<p>Specifically the addressing of mapping a multi-state categorical variable to the NN's {0,1} input range:  Another idea is to change that one, 5-state variable into five boolean variables.  Rather than {0,0.25,0.5,0.75,1.0} on your one variable, make each of the five boolean variables represent a single day and make [1,0,0,0,0] equal Monday, [0,1,0,0,0] equal Tuesday, etc.  I've personally had more success both with training good networks and introspecting the network itself when spreading out states of classes like that.</p>

<p>Other hacks you can try:<br>
 * Take out the the 'day' column all together and see if any of the other variables get used.<br>
 * Plot the distribution of spend as a function of day.  Even if nothing else comes of this current model, it sounds like you've found one interesting insight already.<br>
 * Consider also trying different models.</p>
"
"['python', 'apache-spark', 'pyspark', 'ipython']",How to run a pyspark application in windows 8 command prompt,"<p>Finally, I resolved the issue. I had to set the pyspark location in PATH variable and py4j-0.8.2.1-src.zip location in PYTHONPATH variable. </p>
"
"['machine-learning', 'classification', 'python', 'naive-bayes-classifier']",Implementing Complementary Naive Bayes in python?,"<p>Naive Bayes should be able to handle imbalanced datasets. Recall that the Bayes formula is</p>

<p>$$P(y \mid x) = \cfrac{P(x \mid y) \, P(y)}{P(x)} \propto P(x \mid y) \, P(y)$$</p>

<p>So $P(x \mid y) \, P(y)$ takes the prior $P(y)$ into account.</p>

<p>In your case maybe you overfit and need some smoothing? You can start with +1 smoothing and see if it gives any improvements. In python, when using numpy, I'd implement the smoothing this way:</p>

<pre><code>table = # counts for each feature 
PT = (table + 1) / (table + 1).sum(axis=1, keepdims=1)
</code></pre>

<p>Note that this is gives you Multinomial Naive Bayes - which applies only to categorical data. </p>

<p>I can also suggest the following link: <a href=""http://www.itshared.org/2015/03/naive-bayes-on-apache-flink.html"" rel=""nofollow"">http://www.itshared.org/2015/03/naive-bayes-on-apache-flink.html</a>. It's about implementing Naive Bayes on Apache Flink. While it's Java, maybe it'll give you some theory you need to understand the algorithm better.</p>
"
['neuralnetwork'],Neural Networks : Can I use both sigmoid and tanh as activation functions?,"<p>Yes you can. There are no hard rules against having different activation functions in any layer, and combining these two types should give no numerical difficulties.</p>

<p>In fact it can be a good choice to have tanh in hidden layers and sigmoid on the last layer, if your goal is to predict membership of a single class or non-exclusive multiple class probabilities. The sigmoid output lends itself well to predicting an independent probability (using e.g. a logloss (aka cross-entropy) objective function).</p>

<p>Whether or not it is better than using sigmoid on all layers will depend on other features of your network, the data and the problem you are trying solve. Usually the best way to find out which is better - at least in terms of accuracy - is to try out some variations and see which scores best on a cross-validation data set. In my experience, there often is a small difference between using tanh or sigmoid in the hidden layers.</p>
"
"['machine-learning', 'predictive-modeling', 'optimization', 'training']",Should a model be re-trained if new observations are available?,"<ol>
<li>Once a model is trained and you get new data which can be used for training, you can load the previous model and train onto it. For example, you can save your model as a <code>.pickle</code> file and load it and train further onto it when new data is available. Do note that for the model to predict correctly, <code>the new training data should have a similar distribution as the past data</code>.</li>
<li>Predictions tend to degrade based on the dataset you are using. For example, if you are trying to train using twitter data and you have collected data regarding a product which is widely tweeted that day. But if you use use tweets after some days when that product is not even discussed, it might be biased. <code>The frequency will be dependent on dataset</code> and there is no specific time to state as such. <code>If you observe that your new incoming data is deviating vastly, then it is a good practise to retrain the model</code>.</li>
<li>Optimizing parameters on the aggregated data is not overfitting. Large data doesn't imply overfitting. Use cross validation to check for over-fitting.</li>
</ol>
"
"['python', 'scikit', 'decision-trees', 'error-handling']",First steps with Python and scikit-learn,"<p>In python 3 the <code>print</code> function <strong>must</strong> have parenthesis, so <code>print(clf.predict([[150, 0]]))</code> will work</p>
"
"['neuralnetwork', 'regression']","What is the simplest neural network for the simplest nonlinear function $f(x,y) =xy$","<p>Probably you need to do one or more of:</p>

<ul>
<li><p>Decrease learning rate. Diverging loss is often a symptom of learning rate too high.</p></li>
<li><p>Increase number of hidden neurons. The output function will be the combination of many ""patches"" each created by a neuron that has learnt a different bias. The shape and quality of each patch is determined by the activation functions, but almost any nonlinear activation function used in an NN library should work to make a universal function approximator.</p></li>
<li><p>Normalise inputs. If you are training with high values for $x_1$ or $x_2$, this could make it harder to train unless you normalise your training data.</p></li>
</ul>

<p>For your purposes, it might be an idea to skip the need for normalisation by training with $x_1$ and $x_2$ in range -2.0 to 2.0. It doesn't change your goal much to do this, and removes one potential problem.</p>

<p>You should note that a network trained against an open-ended function like this (where there are no logical bounds on the input params) will not learn to extrapolate. It never ""learns"" the function itself, but an approximation of it close to the supplied examples. When you supply  a $(x_1, x_2)$ input far from the training examples, the output is likely to completely mismatch your original function.</p>
"
"['svm', 'logistic-regression', 'hinge-loss']",What's the relationship between an SVM and hinge loss?,"<p>They are both discriminative models, yes. The logistic regression loss function is conceptually a function of all points. Correctly classified points add very little to the loss function, adding more if they are close to the boundary. The points near the boundary are therefore more important to the loss and therefore deciding how good the boundary is.</p>

<p>SVM uses a hinge loss, which conceptually puts the emphasis on the boundary points. Anything farther than the closest points contributes nothing to the loss because of the ""hinge"" (the max) in the function. Those closest points are the support vectors, simply. Therefore it actually reduces to picking a boundary that creates the largest margin -- distance to closest point. The theory is that the boundary case is all that really matters to generalization.</p>

<p>The downside is that hinge loss is not differentiable, but that just means it takes more math to discover how to optimize it via Lagrange multipliers. It doesn't really handle the case where data isn't linearly separable. Slack variables are a trick that lets this possibility be incorporated cleanly into the optimization problem.</p>

<p>You can use hinge loss with ""deep learning"", e.g. <a href=""http://arxiv.org/pdf/1306.0239.pdf"" rel=""nofollow"">http://arxiv.org/pdf/1306.0239.pdf</a></p>
"
"['neuralnetwork', 'regularization']",Choosing regularization method in neural networks,"<p>There are not any strong, well-documented principles to help you decide between types of regularisation in neural networks. You can even combine regularisation techniques, you don't have to choose just one.</p>

<p>A workable approach can be based on experience, and following literature and other people's results to see what gave good results in different problem domains. Bearing this in mind, dropout has proved very successful for a broad range of problems, and you can probably consider it a good first choice almost regardless of what you are attempting.</p>

<p>Also sometimes just picking a option you are familiar with can help - working with techniques you understand and have experience with may get you better results than trying a whole grab bag of different options where you are not sure what order of magnitude to try for a parameter. A key issue is that the techniques can interplay with other network parameters - for instance, you may want to increase size of layers with dropout depending on the dropout percentage.</p>

<p>Finally, it may not matter hugely <em>which</em> regularisation techniques you are using, just that you understand your problem and model well enough to spot when it is overfitting and could do with more regularisation. Or vice-versa, spot when it is underfitting and that you should scale back the regularisation.</p>
"
['regression'],Chose the right regression analysis,"<p>Since I cannot comment because I don't have enough reputation, I will post this as an answer.</p>

<p>If your goal is to ""find a point where the increase of promotion don't increase profit or new_users"", I won't do a simple regression, since the regression will tell you that if you do more promotions, you will <strong>always inscrease</strong> the profits. I would say that, in reality, the relationship between promotions and profits or new users is <strong>not linear</strong>. Because the number of new users is limited and the promotions are not.</p>

<p>A better model is to say that there is a <strong>optimal promotion</strong> that will give you the best increase of profits and new users.</p>

<p>(if you have a real business to optimise, I would introduce the <strong>Customer Lifetime Value</strong> of new users. Because generally, the new users you get when doing huge promotions will not come back...)</p>
"
"['machine-learning', 'data-mining', 'predictive-modeling']",Why are ensembles so unreasonably effective,"<p>For a specific model you feed it data, choose the features, choose hyperparameters etcetera. Compared to the reality it makes a three types of mistakes: </p>

<ul>
<li>Bias (due to too low model complexity, a sampling bias in your data)</li>
<li>Variance (due to noise in your data, overfitting of your data)</li>
<li>Randomness of the reality you are trying to predict (or lack of predictive features in your dataset)</li>
</ul>

<p>Ensembles average out a number of these models. The bias due to sampling bias will not be fixed for obvious reasons, it can fix some of the model complexity bias, however the variance mistakes that are made are very different over your different models. Especially low correlated models make very different mistakes in this areas, certain models perform well in certain parts of your feature space. By averaging out these models you reduce this variance quite a bit. This is why ensembles shine.</p>
"
"['python', 'pandas']",pandas count values for last 7 days from each date,"<p>The main obstacle is figuring out whether a date is within the last 7 days of the month. I'd recommend something hacky like the following:</p>

<pre><code>from datetime import datetime, date, timedelta
def last7(datestr):
    orig = datetime.strptime(datestr,'%Y-%m-%d')
    plus7 = orig+timedelta(7)
    return plus7.month != orig.month
</code></pre>

<p>Once you have that, it's relatively simple to adapt your previous code:</p>

<pre><code>df3 = df1[df1['is_buy'] == 1 &amp;&amp; last7(df1['date'])].groupby(['id', 'month']).agg({'is_buy': np.sum})
</code></pre>

<p>Now we just join together <code>df2</code> and <code>df3</code> and we're done.</p>
"
"['python', 'text-mining', 'hadoop', 'k-means', 'distance']",Improve k-means accuracy,"<p>Great question, @gsamaras! The way you've set up this experiment makes a lot of sense to me, from a design point of view, but I think there are a couple aspects you can still examine. </p>

<p>First, it's possible that uninformative features are distracting your classifier, leading to poorer results. In text analytics, we often talk about <a href=""https://en.wikipedia.org/wiki/Stop_words"" rel=""nofollow"">stop word</a> filtering, which is just the process of removing such text (e.g., the, and, or, etc.). There are standard stop word lists you can easily find online (e.g., <a href=""http://www.ranks.nl/stopwords"" rel=""nofollow"">this one</a>), but they can sometimes be heavy-handed. The best approach is to build a table relating feature frequency to class, as this will get at domain-specific features that you won't likely find in such look-up tables. There is varying evidence as to the efficacy of stop word removal in the literature, but I think these findings mostly have to do with classifier-specific (for example, support vector machines tend to be less affected by uninformative features than does a naive bayes classifier. I suspect k-means falls into the latter category).</p>

<p>Second, you might consider a different feature modeling approach, rather than tf-idf. Nothing against tf-idf--it works fine for many problems--but I like to start with binary feature modeling, unless I have experimental evidence showing a more complex approach leads to better results. That said, it's possible that k-means could respond strangely to the switch from a floating-point feature space to a binary one. It's certainly an easily-testable hypothesis!</p>

<p>Finally, you might look at the expected class distribution in your data set. Are all classes equally likely? If not, you may get better results from either a sampling approach, or using a different distance metric. <a href=""https://www.erpublication.org/admin/vol_issue1/upload%20Image/IJETR021251.pdf"" rel=""nofollow"">k-means is known to respond poorly in skewed class situations</a>, so this is something to consider as well! There is probably research available in your specific domain describing how others have handled this situation.</p>
"
"['machine-learning', 'neuralnetwork']",Should I take random elements for mini-batch gradient descent?,"<p>It should be enough to shuffle the elements at the beginning of the training and then to read them sequentially. This really achieves the same objective as taking random elements every time, which is to break any sort of predefined structure that may exist in your original dataset (e.g. all positives in the beginning, sequential images, etc).</p>

<p>While it would work to fetch random elements every time, this operation is typically not optimal performance-wise. Datasets are usually large and are not saved in your memory with fast random access, but rather in your slow HDD. This means sequential reads are pretty much the only option you have for good performance.</p>

<p>Caffe for example uses LevelDB, which does not support efficient random seeking. See <a href=""https://github.com/BVLC/caffe/issues/1087"" rel=""nofollow"">https://github.com/BVLC/caffe/issues/1087</a>, which confirms that the dataset is trained with images always in the same order.</p>
"
"['classification', 'image-classification']",Python script/GUI to generate positive/negative images for CascadeClassifier?,"<p>So I wrote the script. It gave me an excuse to learn Tkinter. It's pasted below. Note this is a one-off, not a model of good programming practice! If anyone uses this and has bugs or suggestions, let me know. Here's the <a href=""https://github.com/sunnysideprodcorp/CascadeImagesorter/"" rel=""nofollow"">git link</a> and model code is pasted below:</p>

<pre><code>import Tkinter
import Image, ImageTk
from Tkinter import Tk, BOTH
from ttk import Frame, Button, Style
import cv2
import os
import time
import itertools

IMAGE_DIRECTORY =            # Directory of existing files
POSITIVE_DIRECTORY =         # Where to store 'positive' cropped images
NEGATIVE_DIRECTORY =         # Where to store 'negative' cropped images (generated automatically based on 'positive' image cropping
IMAGE_RESIZE_FACTOR =        # How much to scale images for display purposes. Images are not scaled when saved.

# Everything stuffed into one class, not exactly model programming but it works for now
class Example(Frame):

    def __init__(self, parent, list_of_files, write_file):
        Frame.__init__(self, parent)           
        self.parent = parent
        self.list_of_files = list_of_files
        self.write_file = write_file
        self.image = None
        self.canvas = None
        self.corners = []
        self.index = -1
        self.loadImage()
        self.initUI()
        self.resetCanvas()


    def loadImage(self):
        self.index += 1
        img = cv2.imread(self.list_of_files[self.index])            
        print(self.list_of_files[self.index])
        while not img.shape[0]:
            self.index += 1
            img = cv2.imread(self.list_of_files[self.index])            
        self.cv_img = img
        img_small = cv2.resize(img, (0,0), fx = IMAGE_RESIZE_FACTOR, fy = IMAGE_RESIZE_FACTOR)
        b, g, r = cv2.split(img_small)
        img_small = cv2.merge((r,g,b))
        im = Image.fromarray(img_small)
        self.image = ImageTk.PhotoImage(image=im)       

    def resetCanvas(self):        
        self.canvas.create_image(0, 0, image=self.image, anchor=""nw"")
        self.canvas.configure(height = self.image.height(), width = self.image.width())
        self.canvas.place(x = 0, y = 0, height = self.image.height(), width = self.image.width())

    def initUI(self):
        self.style = Style()
        self.style.theme_use(""default"")
        self.pack(fill=BOTH, expand=1)

        print ""width and height of image should be "", self.image.width(), self.image.height()
        self.canvas = Tkinter.Canvas(self, width = self.image.width(), height = self.image.height())       
        self.canvas.bind(""&lt;Button-1&gt;"", self.OnMouseDown)
        self.canvas.pack()

        nextButton = Button(self, text=""Next"", command=self.nextButton)
        nextButton.place(x=0, y=0)

        restartButton = Button(self, text=""Restart"", command=self.restart)
        restartButton.place(x=0, y=22)       

    def nextButton(self):
        new_img = self.cv_img[self.corners[0][1]/IMAGE_RESIZE_FACTOR:self.corners[1][1]/IMAGE_RESIZE_FACTOR, self.corners[0][0]/IMAGE_RESIZE_FACTOR:self.corners[1][0]/IMAGE_RESIZE_FACTOR]
        files = self.list_of_files[self.index].split(""/"")
        try:
            os.stat(POSITIVE_DIRECTORY+files[-2])
        except:
            os.mkdir(POSITIVE_DIRECTORY+files[-2])
        print(""saving to "", ""{}{}/{}"".format(POSITIVE_DIRECTORY, files[-2], files[-1]))
        cv2.imwrite(""{}{}/{}"".format(POSITIVE_DIRECTORY, files[-2], files[-1]), new_img)
        self.saveNegatives(files)
        self.restart()
        self.loadImage()
        self.resetCanvas()

    def saveNegatives(self, files):
        low_x = min(self.corners[0][0], self.corners[1][0])/IMAGE_RESIZE_FACTOR
        high_x = max(self.corners[0][0], self.corners[1][0])/IMAGE_RESIZE_FACTOR
        low_y = min(self.corners[0][1], self.corners[1][1])/IMAGE_RESIZE_FACTOR
        high_y = max(self.corners[0][1], self.corners[1][1])/IMAGE_RESIZE_FACTOR

        try:
            os.stat(NEGATIVE_DIRECTORY+files[-2])
        except:
            os.mkdir(NEGATIVE_DIRECTORY+files[-2])

        new_img = self.cv_img[ :low_y, :]
        cv2.imwrite(""{}{}/{}{}"".format(NEGATIVE_DIRECTORY, files[-2], ""LY"", files[-1]), new_img)
        new_img = self.cv_img[ high_y: , :]
        cv2.imwrite(""{}{}/{}{}"".format(NEGATIVE_DIRECTORY, files[-2], ""HY"", files[-1]), new_img)

        new_img = self.cv_img[ :, :low_x ]
        cv2.imwrite(""{}{}/{}{}"".format(NEGATIVE_DIRECTORY, files[-2], ""LX"", files[-1]), new_img)
        new_img = self.cv_img[:,  high_x: ]
        cv2.imwrite(""{}{}/{}{}"".format(NEGATIVE_DIRECTORY, files[-2], ""HX"", files[-1]), new_img)


    def restart(self):
        self.corners = []
        self.index -=1
        self.canvas.delete(""all"")
        self.loadImage()
        self.resetCanvas()


    def OnMouseDown(self, event):
        print(event.x, event.y)
        self.corners.append([event.x, event.y])
        if len(self.corners) == 2:
            self.canvas.create_rectangle(self.corners[0][0], self.corners[0][1], self.corners[1][0], self.corners[1][1], outline ='cyan', width = 2)


def main():

    root = Tk()
    root.geometry(""250x150+300+300"")

    list_of_files = []
    file_names = []
    walker = iter(os.walk(IMAGE_DIRECTORY))
    next(walker)
    for dir, _, _ in walker:
        files = [dir + ""/"" +  file for file in os.listdir(dir)]            
        list_of_files.extend(files)
        file_names.extend(os.listdir(dir))

    list_of_processed_files = []
    processed_file_names = []
    walker = iter(os.walk(POSITIVE_DIRECTORY))
    next(walker)
    for dir, _, _ in walker:
        files = [dir + ""/"" +  file for file in os.listdir(dir)]            
        list_of_processed_files.extend(files)
        processed_file_names.extend(os.listdir(dir))

    good_names = set(file_names) - set(processed_file_names)
    list_of_files = [f for i, f in enumerate(list_of_files) if file_names[i] in good_names] 

    app = Example(root, list_of_files, IMAGE_DIRECTORY+""positives"")
    root.mainloop()  


if __name__ == '__main__':
    main()  
</code></pre>
"
"['machine-learning', 'data-mining']",Connection between Regularization and Gradient Descent,"<p>The fitting procedure is the one that actually finds the coefficients of the model. The regularization term is used to indirectly find the coefficients by penalizing big coefficients <em>during the fitting procedure</em>. A simple (albeit somewhat biased/naive) example might help illustrate this difference between regularization and gradient descent:</p>

<pre><code>X, y &lt;- read input data
for different values of lambda L
    for each fold of cross-validation using X,y,L
        theta &lt;- minimize (RSS + regularization using L) via MLE/GD
        score &lt;- calculate performance of model using theta on the validation set 
    if average score across folds for L is better than the current best average score 
        L_best &lt;- L
</code></pre>

<p>As you can see, the fitting procedure (MLE or GD in our case) finds the best coefficients given the specific value of lambda.</p>

<p>As a side note, I would look at this answer <a href=""http://stats.stackexchange.com/questions/137481/how-bad-is-hyperparameter-tuning-outside-cross-validation"">here</a> about tuning the regularization parameter, because it tends a little bit murky in terms of bias. </p>
"
"['r', 'python', 'optimization']",What are some nice algorithms/techniques for optimizing and predicting Click Through Rates (CTR)?,"<p>There are a group of algorithms (or techniques) called the <a href=""https://en.wikipedia.org/wiki/Multi-armed_bandit"" rel=""nofollow"">Bandit algorithms</a>, which deal especially with the problem statement, which is the optimization of Click-through rates of advertisements.</p>

<p>The problem is framed in a setting of multiple bandits with vending machines. There are various strategies which can be implemented:</p>

<ul>
<li>Epsilon-greedy strategy</li>
<li>Epsilon-first strategy</li>
<li>Epsilon-decreasing strategy</li>
<li>Contextual Epsilon strategy</li>
</ul>

<p><a href=""https://www.chrisstucchio.com/blog/2012/bandit_algorithms_vs_ab.html"" rel=""nofollow"">Reference on why Bandit algorithms are better than A/B testing frameworks.</a></p>
"
"['statistics', 'data']",how often should one sample a dataset?,"<p>In order to show a correlation between X and Y they should be aggregated over the same time period. Otherwise, there there may be seasonality or other features in Y that may have been averaged out in X. Is it possible re-index X to get monthly aggregates? I would not sub-sample Y. Find a common time period for both X and Y to make your comparisons.</p>
"
"['optimization', 'consumerweb']",Using Heuristic Methods for AB Testing,"<p>If I understand you question correctly, there are two reasons why genetic algorithm might not a good idea for optimizing website features:</p>

<p>1) Feedback data is coming in too slow, say once a day, genetic algorithm might take a while to converge.</p>

<p>2) In the process of testing genetic algorithm will probably come up with combinations that are 'strange' and that might not be the risk the company wants to take.</p>
"
"['data-mining', 'statistics', 'education']",What are good sources to learn about Bootstrap?,"<p>A classic book is by B. Efron who created the technique:</p>

<ul>
<li>Bradley Efron; Robert Tibshirani (1994). An Introduction to the Bootstrap. Chapman &amp; Hall/CRC. ISBN 978-0-412-04231-7.</li>
</ul>
"
"['machine-learning', 'bigdata', 'efficiency', 'scalability', 'distributed']",Looking for example infrastructure stacks/workflows/pipelines,"<p>In order to understand the variety of ways machine learning can be integrated into production applications, I think it is useful to look at open source projects and papers/blog posts from companies describing their infrastructure.</p>

<p>The common theme that these systems have is the separation of model training from model application. In production systems, model application needs to be fast, on the order of 100s of ms, but there is more freedom in how frequently fitted model parameters (or equivalent) need to be updated.</p>

<p>People use a wide range of solutions for model training and deployment:</p>

<ul>
<li><p>Build a model, then export and deploy it with PMML</p>

<blockquote>
  <ul>
  <li><p><a href=""http://nerds.airbnb.com/architecting-machine-learning-system-risk/"">AirBnB describes their model training</a> in R/Python and deployment of PMML models via OpenScoring.</p></li>
  <li><p><a href=""http://www.cascading.org/projects/pattern/"">Pattern</a> is project related to <a href=""http://www.cascading.org/"">Cascading</a> that can consume PMML and deploy predictive models.</p></li>
  </ul>
</blockquote></li>
<li><p>Build a model in MapReduce and access values in a custom system</p>

<blockquote>
  <ul>
  <li><p><a href=""http://codeascraft.com/2014/06/18/conjecture-scalable-machine-learning-in-hadoop-with-scalding/"">Conjecture is an open source project from Etsy</a> that allows for model training with <a href=""https://github.com/twitter/scalding"">Scalding</a>, an easier to use scala wrapper around MapReduce, and  deployment via Php.</p></li>
  <li><p><a href=""http://www.kiji.org/"">Kiji is an open source project from WibiData</a> that allows for real-time model scoring (application) as well as functioanlity for persisting user data and training models on that data via <a href=""https://github.com/twitter/scalding"">Scalding.</a></p></li>
  </ul>
</blockquote></li>
<li><p>Use an online system that allows for continuously updating model parameters.</p>

<blockquote>
  <ul>
  <li><a href=""http://dl.acm.org/citation.cfm?id=1242610"">Google released a great paper about an online collaborative filtering</a> they implemented to deal with recommendations in Google News.</li>
  </ul>
</blockquote></li>
</ul>
"
"['python', 'statistics', 'pandas', 'ipython']",How to group identical values and count their frequency in Python?,"<blockquote>
  <p>From this sample set I would expect a histogram of receipt that shows
  two occurrences of receipt 102857 (since that person bought two items
  in one transaction) and one occurrence respectively of receipt 102856
  and of receipt 102858.</p>
</blockquote>

<p>Then you want:</p>

<blockquote>
  <p>df.groupby('receipt').receipt.count()</p>
</blockquote>

<pre><code>receipt
102856    1
102857    2
102858    1
Name: receipt, dtype: int64
</code></pre>
"
"['machine-learning', 'neuralnetwork']","What is the ""dying ReLU"" problem in neural networks?","<p>A ""dead"" ReLU always outputs the same value (zero as it happens, but that is not important) for any input. Probably this is arrived at by learning a large negative bias term for its weights.</p>

<p>In turn, that means that it takes no role in discriminating between inputs. For classification, you could visualise this as a decision plane <em>outside</em> of all possible input data.</p>

<p>Once a ReLU ends up in this state, it is unlikely to recover, because the function gradient at 0 is also 0, so gradient descent learning will not alter the weights. ""Leaky"" ReLUs with a small positive gradient for negative inputs (<code>y=0.01x</code> when x &lt; 0 say) are one attempt to address this issue and give a chance to recover.</p>

<p>The sigmoid and tanh neurons can suffer from similar problems as their values saturate, but there is always at least a small gradient allowing them to recover in the long term.</p>
"
['tools'],Lightweight data provenance tool,"<p>Yes, you should save result files before you make major mods to the code.  Disk space is cheap, so you're unlikely to run into issues unless your results sets are prolific.  I would suggest storing old results sets with folder names that include a time stamp of when they were generated.</p>

<p>As far as time shots of your code, using github (or some other code repository tool) is as easy as can be, will save version information, allows for collaboration, and is an all around great way to backup and version your code. </p>

<p>The combination of these two things, you'll effectively have an easy way of mapping a result set to a specific version of the code.</p>
"
"['scikit', 'feature-selection', 'bioinformatics']",Feature selection for gene expression dataset,"<p>It would be helpful if you described your dataset more.  Gene expression datasets seem to often have very high dimensionality and Lasso regularized logistic regression is a popular method to approach this problem.  This paper takes it a little further and might help you out:
<a href=""http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-198"" rel=""nofollow"">http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-198</a></p>

<p>Random forest can generally certainly provide a meaningful importance ranking, but it also depends on what your dataset looks like.</p>

<p>mRMR sounds like it is specifically designed for identifying gene characteristics, so definitely give it a try.</p>

<p>There's also Principle Component Analysis which is also used for gene expression data.</p>

<p>Lots of options, but your questions is not detailed enough to go any further, and providing code as a solution at this point isn't realistic. The documentation for Python scikit-learn has many good explanations and examples.</p>
"
"['machine-learning', 'algorithms']","Machine Learning Identification and Classification, based on string contents: General advice","<p>If the content/information is lengthy, I'd suggest you to use some NLP tasks for starters. I would suggest you to use some basic NLP based preprocessing because it makes our model perform better. So, the basic feature extraction can be used for this. Example, using Porter <a href=""http://www.nltk.org/howto/stem.html"" rel=""nofollow"">Stemmer</a>, <a href=""http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization"" rel=""nofollow"">Lemmatizer</a> to clean the data or removing stop words and then using ngrams for features seem to be a basic idea and a good start. There are various vectorizers which can be used to extract features the documents. For example, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow"">TfidfVectorizer</a> calculates the frequency of a word in a document and also frequency across documents. This can be more useful than a naive <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow"">Bag of words</a> approach. Then, on top of this there are various classifiers which can be used like <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html"" rel=""nofollow"">OneVsRestClassifier</a> or <a href=""http://scikit-learn.org/stable/modules/classes.html#module-sklearn.multiclass"" rel=""nofollow"">others</a>.</p>

<p>A simple approach could be selecting the input and target first. Select the parameters which are to be passed as input and the desired output. Then, decide to clean the input or not based on some NLP APIs(you can use <a href=""http://www.nltk.org/"" rel=""nofollow"">nltk</a>). Then decide on a classifier. You can then predict the values. Test on <a href=""http://datascience.stackexchange.com/questions/12626/what-is-the-difference-between-the-training-and-testdata-set/12628#12628"">validation set</a> and 
try various classifiers for starters.</p>

<p>As for terminology, I can think of <a href=""https://en.wikipedia.org/wiki/Multiclass_classification"" rel=""nofollow"">Multiclass Classification</a> only now.</p>
"
"['machine-learning', 'data-mining', 'bigdata']",Tools for ML on csv files and jsons,"<p>In <a href=""https://www.r-project.org/"" rel=""nofollow"">R</a> you can load the csv file easily by using the method <a href=""http://www.cookbook-r.com/Data_input_and_output/Loading_data_from_a_file/"" rel=""nofollow"">read.csv</a> and get the summary of the data using the method <code>summary</code>, you will get the most of the basic statistics of each columns like <code>Min</code>, <code>1st Qu</code>, <code>Median</code>, <code>Mean</code>, <code>3rd Qu</code>, <code>Max</code> and <code>NA</code> counts for numeric/integer rows and for string columns you will also get the some statics of count of different type of string if it has repeated multiple i.e factors. </p>

<p>Sample code:</p>

<ul>
<li><p>Loading the csv file and check the few datasets:</p>

<pre><code>&gt; df &lt;- read.csv(""SampleData.csv"")
&gt; head(df)
  name age       Type
1    A  34     Active
2    B  56 Cancelled 
3    C  12     Active
4    D  32 Cancelled 
5   Z   34     Active
</code></pre></li>
<li><p>Basic statistics if numeric types: avg, stdev, mode, median</p>

<pre><code>&gt; summary(df)
 name        age               Type  
 A :1   Min.   :12.0   Active    :3  
 B :1   1st Qu.:32.0   Cancelled :2  
 C :1   Median :34.0                 
 D :1   Mean   :33.6                 
 Z :1   3rd Qu.:34.0                 
        Max.   :56.0                 
&gt; 
</code></pre></li>
<li><p>Column type and Cardinality detection</p>

<pre><code>&gt; sapply(df, class)
   name       age      Type 
  ""factor"" ""integer""  ""factor"" 
&gt;
&gt; nrow(df)
[1] 5
&gt; ncol(df)
[1] 3
&gt;
</code></pre></li>
<li><p>Find relationships between columns, if column A is X, then column B is always Y</p></li>
</ul>

<p>For this you have to do some Basic calculation and you can go through this cookbook: <a href=""http://www.cookbook-r.com/"" rel=""nofollow"">http://www.cookbook-r.com/</a> </p>

<p><a href=""http://i.stack.imgur.com/qEAqC.png"" rel=""nofollow"">I have attached Sample CSV Image here</a></p>
"
"['deep-learning', 'convnet']",Is there a known convolutional net architecture to calculate object masks for images?,"<p>I'm surprised nobody mention <a href=""https://arxiv.org/abs/1411.4038"" rel=""nofollow"">fully convolutional neural networks (FCNs) for semantic segmentation</a>.</p>

<p>They are inspired by the original AlexNet style convnets that ended with one or two densely connected layers and softmax classifier. But FCNs dispense with the dense layers and stay <em>fully convolutional</em> all the way to the end.</p>

<p><a href=""http://i.stack.imgur.com/k4WlS.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/k4WlS.jpg"" alt=""from the linked article https://arxiv.org/abs/1411.4038""></a></p>

<p>Here's the basic principle. Take AlexNet, or VGG or something like that. But instead of using the parameters in the classifier to compute a scalar for each category, use them to compute a whole array (i.e. image) using a  <code>1x1xNUM_CATEGORIES</code> convolution. The output will be <code>NUM_CATEGORIES</code> feature maps, each representing a coarse-grained ""heat map"" for that category. A map of dogness, a map of catness. It can be sharpened by including information from earlier layers with ""skip connections"".</p>

<p>EDIT: Just one further bit of good news: the authors of that paper provide implementations of their nets in <a href=""https://github.com/shelhamer/fcn.berkeleyvision.org"" rel=""nofollow"">Caffe's Model Zoo</a>. Tweak away!</p>
"
"['algorithms', 'scalability', 'social-network-analysis', 'library']",Geospatial Social Network Analysis Visualization,"<p>Your dataset can be viewed as a directed graph. The party's location (latitude and longitude) can be denoted as a node and the directed edge can denote who referred whom. Once the dataset can be viewed as this, the problem boils down to joining co-ordinates with lines. </p>
"
"['machine-learning', 'bigdata', 'statistics', 'programming']",Data Science in C (or C++),"<blockquote>
  <p>Or must I loose most of the efficiency gained by programming in C by calling on R scripts or other languages?</p>
</blockquote>

<p>Do the opposite: learn C/C++ to write R extensions. Use C/C++ only for the performance critical sections of your new algorithms, use R to build your analysis, import data, make plots etc.</p>

<p>If you want to go beyond R, I'd recommend learning python. There are many libraries available such as <a href=""http://scikit-learn.org/stable/"">scikit-learn</a> for machine learning algorithms or <a href=""http://pybrain.org/"">PyBrain</a> for building Neural Networks etc. (and use pylab/<a href=""http://matplotlib.org/"">matplotlib</a> for plotting and <a href=""http://ipython.org/notebook.html"">iPython notebooks</a> to develop your analyses). Again, C/C++ is useful to implement time critical algorithms as python extensions.</p>
"
"['r', 'python', 'xgboost']",Hypertuning XGBoost parameters,"<p>Whenever I work with xgboost I often make my own homebrew parameter search but you can do it with the caret package as well like KrisP just mentioned.</p>

<ol>
<li><strong>Caret</strong></li>
</ol>

<p>See this answer on Cross Validated for a thorough explanation on how to use the caret package for hyperparameter search on xgboost.
<a href=""https://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees"">How to tune hyperparameters of xgboost trees?</a></p>

<ol start=""2"">
<li><strong>Custom Grid Search</strong></li>
</ol>

<p>I often begin with a few assumptions based on <a href=""https://www.kaggle.com/owenzhang1"" rel=""nofollow"">Owen Zhang</a>'s slides on <a href=""http://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions"" rel=""nofollow"">tips for data science</a> P. 14</p>

<p><a href=""http://i.stack.imgur.com/9GgQK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9GgQK.jpg"" alt=""enter image description here""></a></p>

<p>Here you can see that you'll mostly need to tune row sampling, column sampling and maybe maximum tree depth. This is how I do a custom row sampling and column sampling search for a problem I am working on at the moment:</p>

<pre><code>searchGridSubCol &lt;- expand.grid(subsample = c(0.5, 0.75, 1), 
                                colsample_bytree = c(0.6, 0.8, 1))
ntrees &lt;- 100

#Build a xgb.DMatrix object
DMMatrixTrain &lt;- xgb.DMatrix(data = yourMatrix, label = yourTarget)

rmseErrorsHyperparameters &lt;- apply(searchGridSubCol, 1, function(parameterList){

    #Extract Parameters to test
    currentSubsampleRate &lt;- parameterList[[""subsample""]]
    currentColsampleRate &lt;- parameterList[[""colsample_bytree""]]

    xgboostModelCV &lt;- xgb.cv(data =  DMMatrixTrain, nrounds = ntrees, nfold = 5, showsd = TRUE, 
                           metrics = ""rmse"", verbose = TRUE, ""eval_metric"" = ""rmse"",
                           ""objective"" = ""reg:linear"", ""max.depth"" = 15, ""eta"" = 2/ntrees,                               
                           ""subsample"" = currentSubsampleRate, ""colsample_bytree"" = currentColsampleRate)

    xvalidationScores &lt;- as.data.frame(xgboostModelCV)
    #Save rmse of the last iteration
    rmse &lt;- tail(xvalidationScores$test.rmse.mean, 1)

    return(c(rmse, currentSubsampleRate, currentColsampleRate))

})
</code></pre>

<p>And combined with some ggplot2 magic using the results of that apply function you can plot a graphical representation of the search.<a href=""http://i.stack.imgur.com/jaAuy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jaAuy.png"" alt=""My xgboost hyperparameter search""></a> </p>

<p>In this plot lighter colors represent lower error and each block represents a unique combination of column sampling and row sampling. So if you want to perform an additional search of say eta (or tree depth) you will end up with one of these plots for each eta parameters tested.</p>

<p>I see you have a different evaluation metric (RMPSE), just plug that in the cross validation function and you'll get the desired result. Besides that I wouldn't worry too much about fine tuning the other parameters because doing so won't improve performance too much, at least not so much compared to spending more time engineering features or cleaning the data.</p>

<ol start=""3"">
<li><strong>Others</strong></li>
</ol>

<p>Random search and Bayesian parameter selection are also possible but I haven't made/found an implementation of them yet.</p>

<p>Here is a good primer on bayesian Optimization of hyperparameters by Max Kuhn creator of caret.</p>

<p><a href=""http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html"" rel=""nofollow"">http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html</a></p>
"
"['machine-learning', 'statistics', 'feature-extraction', 'feature-engineering', 'probability']",Which feature can help to differentiate these two density?,"<ol>
<li>Regarding two probability distributions $A,B$ on the line, that have densities, the following are equivalent:   

<ul>
<li>$ A = B$ (i.e.: given any Borel set $\mathscr S$, the probability assign to $\mathscr S$ by A equals the probability assign to $\mathscr S$ by B)</li>
<li>$A(-\infty,a] = B(-\infty,a] $, for any $a \in \mathbb R$, i.e.: $A,B$ have the same cumulative distribution function </li>
<li>$A,B$ have the same densities.<br>
In the case of your question, the two densities are clearly different, so the two probabilities are different.  </li>
</ul></li>
</ol>

<p>From your question, it seems you want something you can measure (perhaps on sample from these distributions). There are infinitely many quantities that you can measure that when apply to $A$ and $B$ produce different results. The statement above gives some hints for a possible solution.</p>

<p>From your graphs, calling $A$ the first pdf, and $B$ the second:
$$ F_A(0.35) = A(-\infty, 0.35] = \text{ probability that an observation from the first distribution is &lt; 0.35 } = 0 $$
since the density is 0 to the left of 0.35. On the other hand, 
$$ F_B(0.35) = B(-\infty, 0.35] = \text{ probability that an observation from the second distribution is &lt; 0.35 } &gt; 0 $$
since the density is positive from -0.2 to 0.35.</p>

<p>We can a little to the right of 0.35 and find a number $a_0$ such that $0 &lt; F_A(a_0) &lt; F_B(a_0) $.  </p>

<p>Being less than $a_0$ is something one can measure on samples.  </p>

<p>Let $A_1, .., A_n$ be an iid sample from the first distribution, and let $B_1,...,B_m$ be an iid sample from the second distribution. Then $x_j = I(A_j \le a_0)$ is a measure (Note: I am using the notation $I(S)$ = indicator function of a set $S$, or, in more typical notation in the context of probabilitie, $I(S)$ = indicator function of event $S$ = 1 if $S$ is true, 0 if $S$ is false).  </p>

<p>Likewise, one can measure $y_j = I(B_j \le a_0).$   </p>

<p>$\{x_j\}$ are iid Bernoulli, with $P(x_j=1) = P(A_j \le a_0) = F_A(a_0).$<br>
$ \frac{1}{n} \sum_1^n x_j $ is an estimator of $F_A(a_0)$</p>

<p>$\{y_j\}$ are iid Bernoulli, with $P(y_j=1) = P(B_j \le a_0) = F_B(a_0).$<br>
$ \frac{1}{m} \sum_1^m y_j $ is an estimator of $F_B(a_0)$</p>

<p>Now one can run a rest of the hyposesis that the tw means are the same.</p>

<p>Before proceeding, does this construction answer your question?</p>
"
"['machine-learning', 'feature-selection', 'feature-extraction', 'matlab']",How many features should I take out of 515 features?,"<p><strong>Disclaimer:</strong>  I do not know enough about MATLAB to answer the full question but I'll have a go at answering the rest</p>

<h1>1 - How many features should you take?</h1>

<p>If this is a predictive problem you're trying to solve, how about you try and assess the accuracy of your predictions by using features sets of 7 and 10 respectively? You might find that there is very little difference in predictive power so taking 7 might be OK.</p>

<p><strong>NB:</strong> I am quite surprised that your feature selection algo selected such a small proportion of your available features - you're right to want to dig a little deeper on what's happening under the hood..</p>

<h1>2 - Will any ML algorithm be able to capture the characteristics of the data with just these 7 features?</h1>

<p>If your feature selection algo is working fine then there's all the likelihood that a ML would be able to characterise the underlying structure of your dataset relatively easily.</p>

<p>There's a number of problems out there that have a large amount of explanatory variables but are best characterised by a select few (for instance GDP is usually relatively well explained by what it was last quarter, with all other related variables having small impacts on prediction accuracy).</p>

<h1>3 - MATLAB QUESTIONS: TBC</h1>

<p>Sorry I can't answer all your questions - maybe someone will come by with MATLAB knowledge and we can roll our answers together. Otherwise you might be better splitting this into two questions to get more detailed answers on the MATLAB algo.</p>
"
"['cross-validation', 'sampling']",Cross-validation: K-fold vs Repeated random sub-sampling,"<p>If you have an adequate number of samples and want to use all the data, then k-fold cross-validation is the way to go. Having ~1,500 seems like a lot but whether it is adequate for k-fold cross-validation also depends on the dimensionality of the data (number of attributes and number of attribute values). For example, if each observation has 100 attributes, then 1,500 observations is low.</p>

<p>Another potential downside to k-fold cross-validation is the possibility of a single, extreme outlier skewing the results. For example, if you have one extreme outlier that can heavily bias your classifier, then in a 10-fold cross-validation, 9 of the 10 partitions will be affected (though for random forests, I don't think you would have that problem).</p>

<p>Random subsampling (e.g., bootstrap sampling) is preferable when you are either undersampled or when you have the situation above, where you don't want each observation to appear in k-1 folds.</p>
"
"['efficiency', 'performance', 'sklearn']",Scikit Learn Logistic Regression Memory Leak,"<p>Ok, this ended up being an RTFM situation, although in this case it was RTF error message.</p>

<p>While running this, I kept getting the following error:</p>

<pre><code>DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
</code></pre>

<p>I assumed that, since this had to do with the target vector, and since it was a warning only, that it would just silently change my target vector to 1-D.</p>

<p>However, when I explicitly converted my target vector to 1-D, my memory problems went away. Apparently having the target vector in an incorrect form caused it to convert my input vectors into dense vectors from sparse vectors.</p>

<p>Lesson learned: <strong>follow the recommendations</strong> when sklearn 'suggests' you do something.</p>
"
"['machine-learning', 'recommendation']",Cosine Similarity for Ratings Recommendations? Why use it?,"<p>Rating bias and scale can easily be accounted for by standardization. The point of using  Euclidean similarity metrics in vector space co-embeddings is that it reduces the recommendation problem to one of finding the nearest neighbors, which can be done efficiently both exactly and approximately. What you don't want to do in real-life settings is to have to compare every item/user pair and sort them according to some expensive metric. That just doesn't scale.</p>

<p>One trick is to use an approximation to cull the herd to a managable size of tentative recommendations, then to run your expensive ranking on top of that.</p>

<p>edit: Microsoft Research is presenting a paper that covers this very topic at RecSys right now: <a href=""http://www.ulrichpaquet.com/Papers/SpeedUp.pdf"" rel=""nofollow"">Speeding Up the Xbox Recommender System Using a
Euclidean Transformation for Inner-Product Spaces</a></p>
"
"['machine-learning', 'recommendation', 'beginner']",Correctly interpreting Cosine Angular Distance Similarity & Euclidean Distance Similarity,"<p>If you look at the definitions of the two distances, cosine distance is the normalized dot product of the two vectors and euclidian is the square root of the sum of the squared elements of the difference vector.</p>

<p>The cosine distance between M and J is smaller than between M and G because the normalization factor of M's vector still includes the numbers for which J didn't have any ratings. Even if you make J's vector more similar, like you did, the remaining numbers of M (2 and 5) get you the number you get.
The number for M and G is this high because they both have non-zeroes for all the books. Even though they seem quite different, the normalization factors in the cosine are more ""neutralized"" by the non-zeroes for corresponding entries in the dot product. Maths don't lie.</p>

<p>The books J didn't rate will be ignored if you make their numbers zero in the computation of the normalization factor for M. Maybe the fault in your thinking is that the books J didn't rate should be 0 while they shouldn't be any number.</p>

<p>Finally, for recommendation systems, I would like to refer to matrix factorization.</p>
"
['nlp'],Detecting if a sentence contains a numeric series,"<p>So there are many ways to denote a series.  How are you going to parse the series down to determine the values if you don't know the format?  </p>

<p>Determining if the label has a series does not get you to the specific numbers in the series. </p>

<p>2,3,5,7  parses out to 4 numbers</p>

<p>Is 6 in 1996?  I assume that is one number and 1996 != 6</p>

<p>""55,56,57"" is series with a 6 but but not the number 6 </p>

<p>Does 7-9 parse out to 2 numbers or 3 number<br>
Is 6 in 7-9?<br>
If 6 is in 7-9 identifying that as a series does not answer that question.   </p>

<p>How many ways can there be represent a series that regex got out of hand?  For each format of series you also need to parse the values.  You need to know the format of the series to parse out the numbers.  I would have a set of regex mapped to set of parsers.  </p>

<p>Maybe use machine learning to identity new series formats but you are still going to need to parse out the series. </p>
"
"['neuralnetwork', 'gradient-descent', 'supervised-learning']",What is conjugate gradient descent?,"<blockquote>
  <p>What does this sentence mean? </p>
</blockquote>

<p>It means that the next vector should be perpendicular to all the previous ones with respect to a matrix. It's like how the <a href=""https://en.wikipedia.org/wiki/Standard_basis"" rel=""nofollow"">natural basis</a> vectors are <a href=""https://en.wikipedia.org/wiki/Orthogonality"" rel=""nofollow"">perpendicular</a> to each other, with the added twist of a matrix:</p>

<p>$\mathrm {x^T A y} = 0$ instead of $\mathrm{x^T y} = 0$</p>

<blockquote>
  <p>And what is line search mentioned in the webpage?</p>
</blockquote>

<p><a href=""https://en.wikipedia.org/wiki/Line_search"" rel=""nofollow"">Line search</a> is an optimization method that involves guessing how far along a given direction (i.e., along a line) one should move to best reach the local minimum.</p>
"
"['r', 'data-cleaning']",R: Revalue multiple special characters in a data.frame,"<p>That seems pretty straight forward to me. I'm pretty new too but in general I'm not sure if you can get better than one command. Though you could have combined all that:</p>

<pre><code>&gt; newValueVector &lt;- c(""+""=""5"", ""?""=""2"", ""K""=""3"", ""k""=""3"")
&gt; data$Multiplier &lt;-  revalue(data$Multiplier, newValueVector)
</code></pre>
"
"['python', 'pandas']",Pandas: access fields within field in a DataFrame,"<p>Multi-indexing <em>might</em> be helpful. See this:</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/advanced.html"" rel=""nofollow"">http://pandas.pydata.org/pandas-docs/stable/advanced.html</a> </p>

<p>But the below was the immediate solution that came to mind. I think it's a little more elegant than what you came up with (fewer obscure numbers, more interpretable natural language):</p>

<pre><code>import pandas as pd
df = pd.read_json('test_file.json')
df = df.append(df) # just to give us an extra row to loop through below
df.reset_index(inplace=True) # not really important except to distinguish your rows
for _ , row in df.iterrows():
    currNumbDict = len(row['analytics'])
    print(currNumbDict)
</code></pre>
"
"['data-mining', 'social-network-analysis']",Web services to mine the social web?,"<p>Twitter's API is one of the best sources of social network data. You can extract off twitter pretty much everything you can imagine, you just need an account and a developer ID. The documentation is rather big so I will let you navigate it.
<a href=""https://dev.twitter.com/overview/documentation"" rel=""nofollow"">https://dev.twitter.com/overview/documentation</a></p>

<p>As usual there are wrappers that make your life easier.</p>

<ul>
<li><a href=""https://github.com/bear/python-twitter"" rel=""nofollow"">python-twitter</a></li>
<li><a href=""https://cran.r-project.org/web/packages/twitteR/index.html"" rel=""nofollow"">twitteR</a></li>
</ul>

<p>There are also companies who offer detailed twitter analytics and historic datasets for a fee.</p>

<ul>
<li><a href=""https://gnip.com/historical/"" rel=""nofollow"">Gnip</a></li>
<li><a href=""http://datasift.com/products/stream-for-social-data/"" rel=""nofollow"">Datasift</a></li>
</ul>

<p>Check them out!</p>
"
"['python', 'svm', 'forecast']",Support vector regression and paremeters,"<p>It looks like that you are using scikit-learn. In this case use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html"" rel=""nofollow"">Grid Search Cross Validation</a>  or <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn.grid_search.RandomizedSearchCV"" rel=""nofollow"">Randomized Search Cross Validation</a> to find the best parameters. </p>

<pre><code>sklearn.grid_search.GridSearchCV(estimator, param_grid, scoring=None, cv=None, ...)
</code></pre>

<p>In these approaches you basically loop over possible sets of your parameters, specified via <code>param_grid</code>. For each set you perform a cross-validation (default is 3-fold, but you can specify it via <code>cv</code> parameter). Cross-validation gives you the mean value and deviation of your 'scoring' parameter (e.g. 'accuracy' for classification, 'r2' for regression). The set of parameters with the best 'scoring' is the winner. </p>

<p>See example <a href=""http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#example-model-selection-randomized-search-py"" rel=""nofollow"">here</a>. It also show how to output not only <em>the</em> best set of parameters, but also ""top-N"". I find it very useful in building intuition about my model. </p>
"
"['hadoop', 'java', 'hive']","getting error:-Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/io/Writable","<p>I got the answer. One jar file was missing now it is solved. 
This file was missing.
hadoop-common-2.1.0-beta.jar</p>
"
"['machine-learning', 'neuralnetwork']",What is affine transformation in regards to Neural Networks?,"<p>It's a linear transformation, for example lines that were parallel before the transformation are still parallel. Scaling, rotation, reflection etcetera. With regards to neural networks it's usually just the input matrix multiplied by the weight matrix.</p>
"
"['career', 'reference-request', 'self-study']",How to understand equations in research papers?,"<p>Study multiclass logistic regression; it's similar and has many tutorials. Good books include Larry Wasserman's <em>All of Statistics</em>, and <a href=""http://www-bcf.usc.edu/~gareth/ISL/"" rel=""nofollow"">An Introduction to Statistical Learning</a>.</p>

<p>The way to understand research papers is simply to read more of them. Whenever you encounter something you don't understand follow the references or look it in one of the aforementioned books. </p>
"
"['python', 'categorical-data']",How to assign a new level to many levels of a categorical variable,"<p>Using the notation you gave (i.e. <code>data</code> is the dataframe and <code>data.class</code> is the categorical variable you want to process), this can be done this way:</p>

<pre><code>def cut_levels(x, threshold, new_value):
    value_counts = x.value_counts()
    labels = value_counts.index[value_counts &lt; threshold]
    x[np.in1d(x, labels)] = new_value

cut_levels(data.class, 30, 'others')
</code></pre>

<p>Note that this modifies the original data, thus if you want to keep it, do a copy of it before or change the code to this:</p>

<pre><code>def cut_levels(x, threshold, new_value):
    x = x.copy()
    value_counts = x.value_counts()
    labels = value_counts.index[value_counts &lt; threshold]
    x[np.in1d(x, labels)] = new_value
    return x

new_class = cut_levels(data.class, 30, 'others')
</code></pre>
"
"['machine-learning', 'python', 'classification', 'regression', 'matlab']",Obtain a model where each feature vector is past few samples and labels are future few samples?,"<p>I'm not sure why your data set looks like this (having different steps of future result as a single label) so provide some info about the context of your data. </p>

<p>But you can use any Neural Network, Fuzzy Network and so on. you just need to consider your labels as continuous variables and then in final step round you network outputs. You probably don't need any recurrent or dynamic network, because your data already has lots of delays in it which is possibly able to represent dynamic behavior of your system behind data.</p>

<p>You can also use Multi-Step ahead prediction instead. for example using your Instant1_feature predict or estimate [s21_var1 s21_var2]. then use a feature vector which looks like Instant2_feature but with your estimations of [s21_var1 s21_var2] replaced with real values of [s21_var1 s21_var2].</p>

<p>I hope this helps.</p>
"
"['python', 'pandas', 'performance']",python pandas optimization: filtering on text index values,"<p>I've found a solution: instead of filtering through all the values, I compute a hash table (in fact a DataFrame) of the possible values and filter on those values:</p>

<ul>
<li>Step 0: 0:00:00.014251</li>
<li>Step 1: 0:00:00.010097</li>
<li>Init fast filter: 0:00:00.353645</li>
<li>Step 2: 0:00:00.027462</li>
<li>Step 3: 0:00:00.002550</li>
<li>Step 4: 0:00:00.027452</li>
<li>Globally: 0:00:00.435839</li>
</ul>

<p>It runs faster because there are lots of duplicated values.</p>

<p>Here's a class which is doing that stuff. It works for indexes and normal columns:</p>

<pre><code>import pandas as pd
import numpy as np


class DataFrameFastFilterText(object):
    """"""This class allows fast filtering of text column or index of a data frame
    Instead of filtering directy the values, it computes a hash of the values
    and applies the filtering to it.
    Then, it filters the values based on the hash mapping.
    This is faster than direct filtering if there are lots of duplicates values.
    Please note that missing values will be replaced by """" and every value will
    be converted to string.
    Input data won't be changed (working on a copy).
    """"""

    def __init__(self, data, column, isIndex):
        """"""Constructor
        - data: pandas Data Frame
        - column: name of the column - the column can be an index
        - isIndex: True if the column is an index
        """"""
        self._data = data.copy()  # working with a copy because we will make some changes
        self._column = column
        self._isIndex = isIndex
        self._hashedValues = None
        self._initHashAndData()

    def _initHashAndData(self):
        """"""Initialize the hash and transforms the data
        1. data: a. adds an order column
                 b. creates the column from the index if needed,
                 c. rename the column as 'value' and
                 d. drops all the other columns
                 e. fill na with """"
                 f. convert to strings
        2. hash: data frame of 'hashId', 'hashValue'
        3. data: a. sort by initial order
                 b. delete order
        """"""
        self._data['order'] = np.arange(self._data.shape[0])
        if self._isIndex:
            self._data[self._column] = self._data.index.get_level_values(self._column)
        self._data = self._data.loc[:, [self._column, 'order']]
        self._data.rename(columns={self._column: 'value'}, inplace=True)
        self._data['value'] = self._data['value'].fillna("""", inplace=False)
        self._data['value'] = self._data['value'].astype(str)
        self._hash = pd.DataFrame({'value': pd.Series(self._data['value'].unique())})
        self._hash['hashId'] = self._hash.index
        self._data = pd.merge(
            left=self._data,
            right=self._hash,
            on='value',
            how='inner',
        )
        self._data.sort_values(by='order', inplace=True)
        del self._data['order']

    def getFilter(self, *args, **kwargs):
        """"""Returns the filter as a boolean array. It doesn't applies it.
        - args: positional arguments to pass to pd.Series.str.contains
        - kwargs: named arguments to pass to pd.Series.str.contains
        """"""
        assert ""na"" not kwargs.keys()  # useless in this context because all the NaN values are already replaced by empty strings
        h = self._hash.copy()
        h = h.loc[h['value'].str.contains(*args, **kwargs), :].copy()
        d = self._data.copy()
        d['keep'] = d['hashId'].isin(h['hashId'])
        return np.array(
            d['keep'],
            dtype=np.bool
        ).copy()
</code></pre>

<p>And the example code is adapted as follows:</p>

<pre><code>...
tic = datetime.datetime.now()
fastFilter = DataFrameFastFilterText(data, 'identifier')
print(""- Init fast filter: %s"" % str(datetime.datetime.now() - tic))

tic = datetime.datetime.now()
filt = filt | fastFilter.getFilter(rexpPlus)
print(""- Step 2: %s"" % str(datetime.datetime.now() - tic))

tic = datetime.datetime.now()
filt = filt &amp; (~fastFilter.getFilter(rexpMinus))
print(""- Step 3: %s"" % str(datetime.datetime.now() - tic))
...
</code></pre>
"
"['data-mining', 'statistics', 'pca']",What statistical method can be applied in my case?,"<p>There are a number of methods for this. Here's a list:</p>

<ol>
<li>You can build a Regression model and observe the p-values of the coefficients of each variable.</li>
<li>Pearson Correlation</li>
<li>Spearman Correlation</li>
<li>Kendall Correlation</li>
<li>Mutual Information</li>
<li>RReliefF algorithm</li>
<li>Decision trees</li>
<li>Principal Component Analysis (which you have tried)</li>
</ol>

<p>etc. </p>

<p>You can search for other methods with these keywords:</p>

<p>feature selection, variable importance, variable ranking, parameter selection</p>

<p>They are all almost the same, just different terminology in different fields.</p>

<p>However, the issue that you MUST pay attention here is ""10"" observations are almost nothing. It will be extremely difficult to trust any outcome, regardless of the method you use. </p>
"
"['machine-learning', 'image-recognition']",Dimension-Hopping in Machine Learning,"<p>Welcome to DataScience.SE! I'd never heard of this problem so I looked it up. It is explained on the third slide of <a href=""http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec5.pdf"" rel=""nofollow"">this</a> presentation by Geoff Hinton:</p>

<blockquote>
  <p>More things that make it hard to recognize objects</p>
  
  <p>• Changes in viewpoint cause changes in images that standard learning
  methods cannot cope with.</p>
  
  <p>–  <strong>Information hops between input dimensions</strong> (i.e. pixels)</p>
  
  <p>• Imagine a medical database in which the age of a patient sometimes
  hops to the input dimension that normally codes for weight!</p>
  
  <p>–  To apply machine learning we would first want to eliminate this
  dimension-hopping.</p>
</blockquote>

<p>In other words, it is about conceptual features migrating or hopping from one input feature dimension to another while still representing the same thing. One would like to be able to capture or extract the essence of the feature while being invariant to which input dimension it is encoded on.</p>
"
"['python', 'visualization', 'ggplot2']",Python: ggplot geom_histogram() binwidth or bin parameter missing?,"<p>bins=10 parameter is actually added in latest version (ggplot 0.11) <a href=""https://github.com/yhat/ggplot/issues/513"" rel=""nofollow"">https://github.com/yhat/ggplot/issues/513</a></p>
"
['machine-learning'],Is Tensor Flow a complete Machine Learning Library?,"<p>This is a big oversimplification, but there are essentially two types of machine learning libraries available today: </p>

<ol>
<li><strong>Deep learning</strong> (CNN,RNN, fully connected nets, linear models)</li>
<li><strong>Everything else</strong> (SVM, GBMs, Random Forests, Naive Bayes, K-NN, etc)</li>
</ol>

<p>The reason for this is that deep learning is much more computationally intensive than other more traditional training methods, and therefore requires intense specialization of the library (e.g., using a GPU and distributed capabilities). If you're using Python and are looking for a package with the greatest breadth of algorithms, try scikit-learn. In reality, if you want to use deep learning and more traditional methods you'll need to use more than one library. There is no ""complete"" package. </p>
"
"['machine-learning', 'data-mining', 'cross-validation', 'evaluation']",When do I have to use aucPR instead of auROC? (and vice versa),"<p>Yes, you are correct that the dominant difference between the area under the curve of a receiver operator characteristic curve (<a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic"">ROC-AUC</a>) and the area under the curve of a Precision-Recall curve (<a href=""http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"">PR-AUC</a>) lies in its tractability for unbalanced classes.  They are very similar and have been shown to contain essentially the same information, however PR curves are slightly more finicky, but a well drawn curve gives a more complete picture.  The issue with PR-AUC is that its difficult to interpolate between points in the PR curve and thus numerical integration to achieve an area under the curve becomes more difficult.</p>

<p><a href=""http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf"">Check out this discussion of the differences and similarities.</a></p>

<p>Quoting Davis' 2006 abstract:</p>

<blockquote>
  <p>Receiver Operator Characteristic (ROC)
  curves are commonly used to present results
  for binary decision problems in machine
  learning. However, when dealing
  with highly skewed datasets, Precision-Recall
  (PR) curves give a more informative picture
  of an algorithm’s performance. We show that
  a deep connection exists between ROC space
  and PR space, such that a curve dominates
  in ROC space if and only if it dominates
  in PR space. A corollary is the notion of
  an achievable PR curve, which has properties
  much like the convex hull in ROC space;
  we show an efficient algorithm for computing
  this curve. Finally, we also note differences
  in the two types of curves are significant for
  algorithm design. For example, in PR space
  it is incorrect to linearly interpolate between
  points. Furthermore, algorithms that optimize
  the area under the ROC curve are not
  guaranteed to optimize the area under the
  PR curve.</p>
</blockquote>

<p><a href=""https://www.kaggle.com/forums/f/15/kaggle-forum/t/7517/precision-recall-auc-vs-roc-auc-for-class-imbalance-problems/41179"">This was also discussed on Kaggle recently.</a></p>

<p><a href=""http://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves"">There is also some useful discussion on Cross Validated.</a></p>
"
"['algorithms', 'outlier']",Which algorithms or methods can be used to detect an outlier from this data set?,"<p>You can use BoxPlot for outlier analysis. I would show you how to do that in Python:</p>

<p>Consider your data as an array:</p>

<pre><code>a = [100, 50, 150, 200, 35, 60 ,50, 20, 500]
</code></pre>

<p>Now, use seaborn to plot the boxplot:</p>

<pre><code>import seaborn as sn
sn.boxplot(a)
</code></pre>

<p>So, you would get a plot which looks somewhat like this:</p>

<p><a href=""http://i.stack.imgur.com/cQv7b.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cQv7b.png"" alt=""enter image description here""></a></p>

<p>Seems like 500 is the only outlier to me.  But, it all depends on the analysis and the tolerance level of the analyst or the statistician and also the problem statement.</p>

<p>You can have a look at <a href=""http://stats.stackexchange.com/a/178106/84191"">one of my answers</a> on the CrossValidated SE for more tests.</p>

<p>And there are several nice questions on outliers and the algorithms and techniques for detecting them.</p>

<p>My personal favourite is the <a href=""http://stackoverflow.com/questions/9935271/one-dimensional-mahalanobis-distance-in-python"">Mahalanobis distance technique</a>. </p>
"
"['machine-learning', 'classification', 'nlp', 'text-mining', 'similarity']",Ranking skills depending on similarity,"<p>Pretty late but I'm surprised this wasn't answered more.  ""Cosine similarity"" is a great technique to try, though simply letting users search with a hard string and then ranking by popularity isn't so bad (e.g. ""dutch"" brings up everything with ""dutch"" in it, though I would discard mid-word matches, so ""ball"" wouldn't return ""football"", but would return ""ball room dancing"").  </p>

<p>I'd say in any approach a main issue will be deduplicating previous (non-standardized) skills input by users that weren't quite standardized.  You could also try replacing the candidate skills with versions that have different synonyms substituted at search time, e.g. ""soccer coaching"" might be stored also as ""football coaching"" if most of your content is from Europeans.</p>

<p>Sometimes extreme accuracy might not be the best goal, though... You may want to encourage users to explore new skills that they never knew existed!  Not sure what your needs are...</p>

<p>Whatever you settle on, it might be worth building a semi-hand-crafted test set of queries and relevant results so that you can see if the performance is terrible (Google precision and recall in the context of search results).</p>
"
"['sklearn', 'gbm']",Can a Gradient Boosting Regressor be tuned on a subset of the data and achieve the same result?,"<p>You should never train or do grid search on your entire data set, since it will lead to overfitting and reduce the accuracy of your model on new data. What you have described is actually the ideal approach: do grid search / training on a subset of your data. Yes, your model will get different results vs if you used the entire set of data, but your model will be much stronger because of it. </p>

<p>For more details on why you would want to split up / sample your data, see this quesiton: <a href=""http://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set"">http://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set</a></p>
"
['machine-learning'],"In Weka, how to draw learning curve evaluated on both test and training set?","<p>This is only possible with KnowledgeFlow. In WekaManual.pdf (which is included in Weka package) for version 3.7.12 there is an example in Chapter 7.4.2 ""Plotting multiple ROC curves"" with picture and step-by-step instructions. For other Weka versions it is the same, just find the appropriate chapter.</p>

<p>To give an impression on how it goes, I extracted the picture from the manual. It will draw two curves for two classifiers. For your question it is very similar. You use one classifier and then connect trainingSet to one ClassifierPerformanceEvaluator and testSet to another.
<img src=""http://i.stack.imgur.com/ZzEKV.png"" alt=""enter image description here""></p>
"
"['python', 'optimization']",Solve a pair of coupled nonlinear equations within certain limits,"<p>As a workaround, you could minimize another function that includes both the objective and the constraints, then check if sol.fun is (numerically) equal to zero.</p>

<pre><code>from scipy.optimize import minimize
import numpy as np

f = lambda x: np.sin(x).sum() #the function to find roots of
L = np.array([-1,-1]) #lower bound for each coordinate
U = np.array([1, 1])  #upper bound for each coordinate
g = lambda x: f(x) **2 +  max(0, (L - x).max(), (x - U).max())
sol = minimize(g, [0.5,0.5])
</code></pre>

<p>Also, scipy.optimize seems to have some optimisers that support rectangular bounds, i.e. <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution"" rel=""nofollow"">differential_evolution</a> (since version 0.15.0).</p>
"
"['r', 'ggplot2']",Manage x-axis using ggplot(),"<p>Change <code>Year</code> to a factor and add <code>group=1</code>:</p>

<pre><code>g &lt;- ggplot(Total_Emmisssions,aes(x=factor(Year), y=Emissions, colour=Type, group=1))
</code></pre>

<p>you can leave the rest the same (you'll also prbly want to change the <code>xlab</code>).</p>

<p><a href=""http://i.stack.imgur.com/1Xbqs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Xbqs.png"" alt=""enter image description here""></a></p>
"
"['machine-learning', 'nlp']",Contributions of each feature in classification?,"<p>This is called feature ranking, which is closely related to <a href=""https://en.wikipedia.org/wiki/Feature_selection"" rel=""nofollow"">feature selection</a>.</p>

<ul>
<li>feature ranking = determining the importance of any individual feature</li>
<li>feature selection = selecting a subset of relevant features for use in model construction. </li>
</ul>

<p>So if you are able to ranked features, you can use it to select features, and if you can select a subset of useful features, you've done at least a partial ranking by removing the useless ones. </p>

<p>This <a href=""https://en.wikipedia.org/wiki/Feature_selection"" rel=""nofollow"">Wikipedia page</a> and this <a href=""https://www.quora.com/How-do-I-perform-feature-selection?share=1"" rel=""nofollow"">Quora post</a> should give some ideas. The distinction filter methods vs. wrapper based methods vs. embedded methods is the most common one.</p>

<hr>

<p>One straightforward approximate way is to use <a href=""http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"" rel=""nofollow"">feature importance with forests of trees</a>:</p>

<p><a href=""http://i.stack.imgur.com/r7Io5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/r7Io5.png"" alt=""enter image description here""></a></p>

<p>Other common ways:</p>

<ul>
<li><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"" rel=""nofollow"">recursive feature elimination</a>.</li>
<li>stepwise regression (or <a href=""http://scikit-learn.org/stable/modules/linear_model.html#lars-lasso"" rel=""nofollow"">LARS Lasso</a>).</li>
</ul>

<p>If you use scikit-learn, check out <a href=""http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection"" rel=""nofollow"">module-sklearn.feature_selection</a>. I'd guess Weka has some similar functions.</p>
"
"['machine-learning', 'r', 'predictive-modeling', 'azure-ml']",Custom trained model in Azure ML,"<p>No you cannot. I had a discussion with someone in their development/support team on the MSDN forums and currently they don't support 'drag and drop' type of functionality. However you CAN serialize the model output and then de-serialize them in Azure. <a href=""http://i.stack.imgur.com/QIILN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QIILN.png"" alt=""enter image description here""></a></p>

<p><strong>Note that the answer in the image is a bit outdated</strong> and there is the 'Create R Script' module to replace the serialization-deserialization steps <em>within</em> Azure. However I believe you can still serialize outside Asure (in your Desktop version of R) and deserialize them in Azure.</p>

<p>Link to the conversation in Image:
<a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/5944c342-79ac-4ada-8006-8edf40f36ee1/r-script-as-a-trained-model?forum=MachineLearning"" rel=""nofollow"">https://social.msdn.microsoft.com/Forums/azure/en-US/5944c342-79ac-4ada-8006-8edf40f36ee1/r-script-as-a-trained-model?forum=MachineLearning</a></p>
"
['deep-learning'],Do I need to buy a NVIDIA graphic card to run deep learning algorithm?,"<p>I would recommend familiarizing yourself with <a href=""https://www.youtube.com/watch?v=Py0VInjRSBE"" rel=""nofollow"">AWS spot instances</a>. It's the most practical solution I can think of for your problem, and <a href=""https://www.youtube.com/watch?v=NdR03RpCpac"" rel=""nofollow"">it works your computer too</a>. So, no you don't have to <em>buy</em> an Nvidia card, but as of today you will want to use one since almost all the solutions rely on them.</p>
"
"['machine-learning', 'r', 'neuralnetwork', 'predictive-modeling', 'forecast']",R - Interpreting neural networks plot,"<p>As David states in the comments if you want to interpret a model you likely want to explore something besides neural nets.  That said it you want to intuitively understand the network plot it is best to think of it with respect to images (something neural networks are very good at).</p>

<ol>
<li>The left-most nodes (i.e. input nodes) are your raw data variables.</li>
<li>The arrows in black (and associated numbers) are the <strong>weights</strong> which you can think of as <strong>how much that variable contributes to the next node.</strong>  The blue lines are the bias weights.  You can find the purpose of these weights in the excellent answer <a href=""http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks"">here</a>.</li>
<li>The middle nodes (i.e. anything between the input and output nodes) are your hidden nodes.  This is where the image analogy helps.  <strong>Each of these nodes constitute a component that the network is learning to recognize.</strong>  For example a nose, mouth, or eye.  This is not easily determined and is far more abstract when you are dealing with non-image data.</li>
<li>The far-right (output node(s)) node is the final output of your neural network.</li>
</ol>

<p>Note that this all is omitting the activation function that would be applied at each layer of the network as well.</p>
"
"['data-formats', 'geospatial']",Best format for recording time stamp and GPS,"<p>As Spacedman put it, ""best"" is pretty subjective. However, as we have found, a good format for time is <a href=""http://en.wikipedia.org/wiki/Unix_time"" rel=""nofollow"">Unix time</a> (aka POSIX time, aka Epoch time). Most databases support it and it is still pretty human readable.</p>

<p>For location, we like <a href=""http://en.wikipedia.org/wiki/Decimal_degrees"" rel=""nofollow"">decimal degrees</a> as it is easy to read and stored and is compatible with Google Maps API. It's also easy to convert to other formats if needed.</p>
"
"['machine-learning', 'python']",Python Machine Learning Experts,"<p>You could try some competitions from <a href=""http://kaggle.com"" rel=""nofollow"">kaggle</a>.</p>

<p>Data Science courses from Coursera, edX, etc also provide forums for discussion.</p>

<p>Linkedin or freelance sites could be other possibilities.</p>
"
"['text-mining', 'feature-extraction', 'text']",Choosing the right parameters to train a Tf-Idf vectoriser,"<p>There are no silver bullets. But here are some suggests:</p>

<ol>
<li><p>Use a better stopwords vocabulary. If you still have words like ""to"" and ""at"", then you are either not removing stopwords or using a lousy vocabulary. Try using the Spanish stopwords from <code>nltk</code>:</p>

<pre><code>from nltk.corpus import stopwords
stopwords.words('spanish')
</code></pre></li>
<li><p>Use <code>max_df &lt; 1</code>. This will truncate words that appear in more than that percentage number of documents.</p></li>
<li><p>The TF-IDF part that punishes common words (transversal to all documents) is the IDF part of TF-IDF, which means inverse document transform. Several functions may be used as your <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency_2"" rel=""nofollow"">IDF function</a>. Usually, IDF=$\log\frac{\text{#documents}}{\text{#documents where word appears}}$ is used. You could try a more punitive IDF function. sklearn does not seem to allow to specify it, but you can use <code>nltk</code> or <code>gensim</code> or easily implement your own TF-IDF vectorization. It needs no more than five lines of code.</p></li>
</ol>

<p>I would try each of these suggestions in this order, and stop when it is good enough. It sounds like using a better stopwords vocabulary will be good enough for you.</p>

<p><strong>EDIT:</strong> I forgot to mention, but of course you can add more uninteresting words to the stopwords list, if you have the need to do so.</p>

<p>You may also want to first evaluate the impact TF-IDF is having in your counts. I think this will work:</p>

<pre><code>m = TfidfVectorizer().fit(docs)
c = [m.transform([word]) for word in m.vocabulary_]
for i in np.argsort(c)[::-1][:20]:
    print(m.vocabulary_[i], c[i])
</code></pre>

<p>You may have to adjust the code. Try to use with IDF disabled and enabled, and other tweaks.</p>
"
['bigdata'],What are the differences between Apache Spark and Apache Flink?,"<p>Flink is the Apache renaming of the <a href=""http://stratosphere.eu/"" rel=""nofollow"">Stratosphere project from several universities in Berlin</a>. It doesn't have the same industrial foothold and momentum that the Spark project has, but it seems nice, and more mature than, say, Dryad. I'd say it's worth investigating, at least for personal or academic use, but for industrial deployment I'd still prefer Spark, which at this point is battle tested. For a more technical discussion, see <a href=""https://www.quora.com/Are-Spark-and-Stratosphere-competitors-Do-they-cover-the-same-set-of-use-cases"" rel=""nofollow"">this Quora post by committers on both projects</a>.</p>
"
['bigdata'],What is advantage of using Dryad instead of Spark?,"<p>Dryad is an academic project, whereas Spark is widely deployed in production, and now has a company behind it for support. Just focus on Spark.</p>
"
"['r', 'statistics']",R Programing beginner,"<p>I second saq7 and Gopinath, the R courses on Coursera are excellent. I really rate the Johns Hopkins ones: <a href=""https://www.coursera.org/specialization/jhudatascience/1/courses"" rel=""nofollow"">https://www.coursera.org/specialization/jhudatascience/1/courses</a>. You should also keep an eye on the <a href=""http://software-carpentry.org/"" rel=""nofollow"">software carpentry</a> site for courses they run in your area. If you can't wait, all the software carpentry <a href=""http://swcarpentry.github.io/r-novice-inflammation/"" rel=""nofollow"">learning material is online</a> so you can follow it yourself.</p>
"
['similarity'],Applications and differences for Jaccard similarity and Cosine Similarity,"<p>Jaccard Similarity is given by 
$s_{ij} = \frac{p}{p+q+r}$</p>

<p>where,</p>

<p>p = # of attributes positive for both objects <br>
q = # of attributes 1 for i and 0 for j <br>
r = # of attributes 0 for i and 1 for j <br></p>

<p>Whereas, cosine similarity = $\frac{A \cdot B}{\|A\|\|B\|}$ where A and B are object vectors.</p>

<p>Simply put, in cosine similarity, the number of common attributes is divided by the total number of possible attributes. Whereas in Jaccard Similarity, the number of common attributes is divided by the number of attributes that exists in at least one of the two objects. </p>

<p>And there are many other measures of similarity, each with its own eccentricities. When deciding which one to use, try to think of a few representative cases and work out which index would give the most usable results to achieve your objective.</p>

<p>For example, if you have two objects both with 10 attributes, out of a possible 100 attributes. Further they have all 10 attributes in common. In this case, the Jaccard index will be 1 and the cosine index will be 0.001. Now consider another scenario where object A has 10 attributes, and object B has 50 attributes, but B has all 10 attributes that A has. Here, Jaccard index will be 0.2 and cosine index will still be 0.001. So the key question is if that extra bit of information reflected, in this case, in the Jaccard index useful or does it hurt or does it not matter. Your choice will depend on which is the best solution for your problem. </p>

<p>The Cosine index could be used to identify plagiarism, but will not be a good index to identify mirror sites on the internet. Whereas the Jaccard index, will be a good index to identify mirror sites, but not so great at catching copy pasta plagiarism (within a larger document). </p>

<p>Of course, these are toy examples to illustrate a point. When applying these indices, you must think about your problem thoroughly and figure out how to define similarity. Once you have a definition in mind, you can go about shopping for an index.    </p>
"
"['python', 'statistics']",Best Python library for statistical inference,"<p><a href=""http://statsmodels.sourceforge.net/devel/"" rel=""nofollow"">statsmodels</a> is a good, and fairly standard, package to statistics.</p>

<p>For Bayesian interference you can go with <a href=""http://pymc-devs.github.io/pymc/"" rel=""nofollow"">PyMC</a> - see as in <a href=""http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/"" rel=""nofollow"">Cam Davidson-Pilon, Probabilistic Programming &amp; Bayesian Methods for Hackers</a>.</p>
"
"['r', 'apache-spark', 'parallel']",How to reduce time R takes for model building,"<p>Depends on the models you are trying to run. Your data isn't that big. For example using a support vector model from the <code>kernlab</code> package, you run into problems. Not every model is fast or has a fast implementation. </p>

<p>Without more information on what you are doing it is difficult to say what causes the bottleneck. But if you just want a speed boost in running models, have a look at the <code>xgboost</code> package, the <code>h2o</code> package (GLM, GBM, rf, deeplearning), <code>ranger</code> for a faster implementation of a randomforest model. </p>
"
"['r', 'regression']",Comparing training and validation data set Root MSE for a best subset regression?,"<p>Im a little late, but better late than never.  It looks like your line where you find the coefficients: </p>

<pre><code>regfit.exh=regsubsets(HPV~. -Model.Types..code.-Year..code.,data=Mydata, nvmax=NULL,force.in = NULL, force.out = NULL, method=""exhaustive"")
</code></pre>

<p>should have <code>data=Mydata[train,]</code> instead of <code>data=Mydata</code>.</p>

<p>Your model had already seen your test samples, so the validation error is not an accurate assessment.</p>
"
['r'],Find the column(s) name where the value of the variable matches a regex,"<p>Would this work for you,</p>

<pre><code> names(iris)[apply(iris, 2, function(x) any(grepl(""rsi"", x)))]
</code></pre>
"
['tools'],Do you need a virtual machine as an instrument for your data science practice?,"<h1>Do you need a VM?</h1>

<p>You need to keep in mind that a virtual machine is a software emulation of your own or another machine hardware configuration that can run an operating systems. In most basic terms, it acts as a layer interfacing between the virtual OS, and your own OS which then communicates with the lower level hardware to provide support to the virtual OS. What this means for you is:</p>

<h1>Cons</h1>

<h3>Hardware Support</h3>

<p>A drawback of virtual machine technology is that it supports only the hardware that both the virtual machine hypervisor and the guest operating system support. Even if the guest operating system supports the physical hardware, it sees only the virtual hardware presented by the virtual machine. 
The second aspect of virtual machine hardware support is the hardware presented to the guest operating system. No matter the hardware in the host, the hardware presented to the guest environment is usually the same (with the exception of the CPU, which shows through). For example, VMware GSX Server presents an AMD PCnet32 Fast Ethernet card or an optimized VMware-proprietary network card, depending on which you choose. The network card in the host machine does not matter. VMware GSX Server performs the translation between the guest environment's network card and the host environment's network card. This is great for standardization, but it also means that host hardware that VMware does not understand will not be present in the guest environment.</p>

<h3>Performance Penalty</h3>

<p>Virtual machine technology imposes a performance penalty from running an additional layer above the physical hardware but beneath the guest operating system. The performance penalty varies based on the virtualization software used and the guest software being run. This is significant.</p>

<h1>Pros</h1>

<h3>Isolation</h3>

<blockquote>
  <p>One of the key reasons to employ virtualization is to isolate applications from each other. Running everything on one machine would be great if it all worked, but many times it results in undesirable interactions or even outright conflicts. The cause often is software problems or business requirements, such as the need for isolated security. Virtual machines allow you to isolate each application (or group of applications) in its own sandbox environment. The virtual machines can run on the same physical machine (simplifying IT hardware management), yet appear as independent machines to the software you are running. For all intents and purposes—except performance, the virtual machines are independent machines. If one virtual machine goes down due to application or operating system error, the others continue running, providing services your business needs to function smoothly.</p>
</blockquote>

<h3>Standardization</h3>

<blockquote>
  <p>Another key benefit virtual machines provide is standardization. The hardware that is presented to the guest operating system is uniform for the most part, usually with the CPU being the only component that is ""pass-through"" in the sense that the guest sees what is on the host. A standardized hardware platform reduces support costs and increases the share of IT resources that you can devote to accomplishing goals that give your business a competitive advantage. The host machines can be different (as indeed they often are when hardware is acquired at different times), but the virtual machines will appear to be the same across all of them.</p>
</blockquote>

<h3>Ease of Testing</h3>

<blockquote>
  <p>Virtual machines let you test scenarios easily. Most virtual machine software today provides snapshot and rollback capabilities. This means you can stop a virtual machine, create a snapshot, perform more operations in the virtual machine, and then roll back again and again until you have finished your testing. This is very handy for software development, but it is also useful for system administration. Admins can snapshot a system and install some software or make some configuration changes that they suspect may destabilize the system. If the software installs or changes work, then the admin can commit the updates. If the updates damage or destroy the system, the admin can roll them back.
  Virtual machines also facilitate scenario testing by enabling virtual networks. In VMware Workstation, for example, you can set up multiple virtual machines on a virtual network with configurable parameters, such as packet loss from congestion and latency. You can thus test timing-sensitive or load-sensitive applications to see how they perform under the stress of a simulated heavy workload.</p>
</blockquote>

<h3>Mobility</h3>

<blockquote>
  <p>Virtual machines are easy to move between physical machines. Most of the virtual machine software on the market today stores a whole disk in the guest environment as a single file in the host environment. Snapshot and rollback capabilities are implemented by storing the change in state in a separate file in the host information. Having a single file represent an entire guest environment disk promotes the mobility of virtual machines. Transferring the virtual machine to another physical machine is as easy as moving the virtual disk file and some configuration files to the other physical machine. Deploying another copy of a virtual machine is the same as transferring a virtual machine, except that instead of moving the files, you copy them.</p>
</blockquote>

<h1>Which VM should I use if I am starting out?</h1>

<p>The Data Science Box or the Data Science Toolbox are your best bets if you just getting into data science. They have the basic software that you will need, with the primary difference being the virtual environment in which each of these can run. The DSB can run on AWS while the DST can run on Virtual Box (which is the most common tool used for VMs).</p>

<h1>Sources</h1>

<ul>
<li><a href=""http://www.devx.com/vmspecialreport/Article/30383"" rel=""nofollow"">http://www.devx.com/vmspecialreport/Article/30383</a></li>
<li><a href=""http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html"" rel=""nofollow"">http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html</a></li>
</ul>
"
['recommendation'],Recommended Language/Framework for Building a New Recommendation Engine,"<p>If you merely want to scale up a simple collaborative filter (low rank matrix factorization), I'd suggest looking at <a href=""http://docs.graphlab.org/collaborative_filtering.html"" rel=""nofollow"">graphlab</a>. Another graph-based (or should I say <a href=""http://giraph.apache.org/"" rel=""nofollow"">Giraph</a>?) solution is <a href=""http://grafos.ml/okapi.html#collaborative"" rel=""nofollow"">Okapi</a>. Spark's <a href=""http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html"" rel=""nofollow"">MLLib</a> is another option (<a href=""https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html"" rel=""nofollow"">details</a>), and it also supports implicit feedback out of the box. Mahout's behind the curve today; I wouldn't bother with it until it is <a href=""https://www.mapr.com/blog/mahout-spark-what%E2%80%99s-new-recommenders"" rel=""nofollow"">migrated to Spark</a>.</p>

<p>If you want to do something that the libraries don't do, say with regularization, you'll have to roll your own solution in your general purpose programming language of choice. It's not hard to get a prototype running, but you might run into scale problems in production; that's why I recommended solutions that scale easily.</p>

<p>There are also black box recommender systems from commercial vendors but I have no experience with those.</p>
"
['neuralnetwork'],Any differences in regularisation in MLP between batch and individual updates?,"<p>Regularization is relevant in per-item learning as well. I would suggest to start with a basic validation approach for finding out lambda, whether you are doing batch or per-item learning. This is the easiest and safest approach. Try manually with a number of different values. e.g. 0.001. 0.003, 0.01, 0.03, 0.1 etc. and see how your validation set behaves. Later on you may automate this process by introducing a linear or local search method. </p>

<p>As a side note, I believe the value of lambda should be considered in relation to the updates of the parameter vector, rather than the training set size. For batch training you have one parameter update <em>per dataset pass</em>, while for online one update <em>per sample</em> (regardless of the training set size).</p>

<p>I recently stumbled upon this <a href=""http://stats.stackexchange.com/questions/64224/regularization-and-feature-scaling-in-online-learning"">Crossvalidated Question</a>, which seems quite similar to yours. There is a link to a paper about <a href=""http://leon.bottou.org/publications/pdf/jmlr-2009.pdf"" rel=""nofollow"">a new SGD algorithm</a>, with some relevant content. It might be useful to take a look (especially pages 1742-1743).</p>
"
"['machine-learning', 'algorithms']",Credit card fraud detection - anomaly detection based on amount of money to be withdrawn?,"<blockquote>
  <p>I was thinking to calculate the average amount of money, say for the
  last ten transactions, and check how far is the next transaction
  amount from the average. Too much deviation would signal an anomaly.
  But this does not sound much, does it?</p>
</blockquote>

<p>A typical outlier detection approach. This would work in most cases. But, as the problem statement deals with credit card fraud detection, the detection technique/algorithm/implementation should be more robust.</p>

<p>You might want to have a look at the <a href=""https://en.wikipedia.org/wiki/Mahalanobis_distance"" rel=""nofollow""><strong>Mahalanobis Distance</strong></a> metric for this type of outlier detection.</p>

<p>Coming to the algorithms for fraud detection, I would point out to the standards used in the industry (as I have no experience in this, but felt these resources would be useful to you).</p>

<p>Check <a href=""http://datascience.stackexchange.com/questions/8099/classifying-transactions-as-malicious/8100#8100"">my answer</a> for this question. It contains the popular approaches and algorithms used in the domain of fraud detection. The <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">Genetic Algorithm</a> is the most popular amongst them.</p>
"
"['statistics', 'unsupervised-learning']",Identifying baseline consumption,"<p>You can try and model your data as having trend/seasonal/cyclic components if you want something a little more sophisticated. Here is an intro <a href=""https://www.otexts.org/fpp/6/1"" rel=""nofollow"">reference</a> to get you started.</p>
"
"['python', 'sklearn']",How does SelectKBest work?,"<p>The SelectKBest class just scores the features using a function (in this case f_classif but could be others) and then ""removes all but the k highest scoring features"". <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest</a></p>

<p>So its kind of a wrapper, the important thing here is the function you use to score the features.</p>

<p>For other feature selection techniques in sklearn read: <a href=""http://scikit-learn.org/stable/modules/feature_selection.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/feature_selection.html</a></p>

<p>And yes, f_classif and chi2 are independent of the predictive method you use. </p>
"
"['machine-learning', 'predictive-modeling', 'supervised-learning', 'unsupervised-learning']",Ideas for prospect scoring model,"<p>I faced almost exactly the same scenario a year and a half ago -- basically what you have is a variation of the <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow"">one-class classification</a> (OCC) problem, specifically <a href=""https://www.cs.uic.edu/~liub/NSF/PSC-IIS-0307239.html"" rel=""nofollow"">PU-learning</a> (learning from Positive and Unlabelled data). You have your known, labelled positive dataset (<strong>clients</strong>) and an un-labelled dataset of <strong>prospects</strong> (<em>some of which are client-like and some of which are not client-like</em>). Your task is to identify the most client like of the prospects and target them... this hinges on the assumption that <em>prospects that look most like clients</em> are <strong>more likely to convert</strong> than <em>prospects that look less like clients</em>.</p>

<p>The approach we settled upon used a procedure called the <a href=""https://www.cs.uic.edu/~liub/S-EM/unlabelled.pdf"" rel=""nofollow"">Spy-technique</a>. The basic idea is that you take a sample from your known positive class and inject them into your unlabelled set. You  then train a classifier on this combined data and then run the unlabelled set back through the trained classifier assigning each instance a probability of being a positive class member. The intuition is that the injected positives (<em>so-called spies</em>) should behave similarly to the positive instances (as reflected by their posterior probabilities). By setting a threshold this allows you to extract reliable negative instances from the unlabelled set. Now, having both positive and negative labelled data, and you can build a classifier using any standard classification algorithm you choose. In essence, with the spy technique, you boot-strap your data to provide you with the needed negative instances for proper training.</p>

<p><a href=""http://i.stack.imgur.com/ZlLJG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZlLJG.png"" alt=""spy technique""></a></p>

<p>For starters you should look into the work of Li and Liu who have a number of papers exploring the topic of OCC and PU-learning.</p>

<p><a href=""http://i.stack.imgur.com/1lWd4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1lWd4.png"" alt=""OCC and PU-learning Papers""></a></p>
"
"['machine-learning', 'feature-selection', 'sklearn']","How does SelectKBest(f_classif, k) perform feature selection?","<p>Your question is really more about <code>f_classif</code> than <code>SelectKBest</code>. It's to <a href=""https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/feature_selection/univariate_selection.py#L148"" rel=""nofollow"">drop duplicate labels</a>; note the <code>np.unique(y)</code>:</p>

<pre><code>X, y = check_X_y(X, y, ['csr', 'csc', 'coo'])
args = [X[safe_mask(X, y == k)] for k in np.unique(y)]
return f_oneway(*args)
</code></pre>

<p><code>f_oneway</code> still only gets passed the feature matrix, but a subset of it.</p>
"
['predictive-modeling'],How to take advantage of variables whose values are available in the past but not in the future?,"<p>My generic answer to the title is to use the extra data for <a href=""https://en.wikipedia.org/wiki/Regularization_(mathematics)"" rel=""nofollow"">regularization</a> in <a href=""https://en.wikipedia.org/wiki/Feature_learning"" rel=""nofollow"">representation learning</a>; a transformation of your features into a space conducive to your main task: regression (prediction, forecasting). <a href=""http://dx.doi.org/10.1109/TPAMI.2013.50"" rel=""nofollow"">Here's</a> a survey [<a href=""http://www.cifasis-conicet.gov.ar/granitto/ECI2014/Bengio13.pdf"" rel=""nofollow"">PDF</a>].</p>

<p>For your example, you could build a model that takes the delay of the target time from the present as an input, so you can predict arbitrarily far into the future, though it probably would not predict as well as a simple regressor that has a fixed horizon since it is trying to learn a more complex function.</p>
"
['r'],How to install rattle in centos,"<p>I got the answer.
I have to install some of the packages in terminal . I installed it and it works.</p>

<pre><code>sudo yum install gtk+-devel gtk2-devel
</code></pre>
"
"['machine-learning', 'clustering', 'scikit']",How to cluster a link traversal dataset,"<p>@Shagun's answer is right actually. I just expand it!</p>

<p>There are 2 different approaches to your problem:</p>

<h2>Graph Approach</h2>

<ul>
<li>As stated in @Shagun's answer you have a weighted directed graph and you want to cluster the paths. I mention again because it's important to know that your problem is not a <em>Graph Clustering</em> or <em>Community Detection</em> problem where vertices are clustered!</li>
<li>Cunstructing a Graph in networkx using the last two column of the data, you can add time spent as weight and users who passed that link as an edge attribute. After all you'll have different features for clustering: the set of all vertices an individual ever met in the graph, total, mean and std of time spent, shortest path distribution parameters, ... which can be used for clustering the user behaviors.</li>
</ul>

<h2>Standard Data</h2>

<ul>
<li>All above can be done by reading data efficiently in a matrix. If you consider each edge for a specified user as a single row (i.e. you'll have <strong>M</strong>x<strong>N</strong> rows where <strong>M</strong> is the number of users and <strong>N</strong> the number of edges in case you stick with 100 case!) and add properties as columns you'll probably able to cluster behaviors. if a user passed an edge <em>n</em> times, in the row corresponding to that user and that edge add a count column with value <em>n</em> and same for time spend, etc. Starting and ending edges are also informative. Be careful that node names are categorical variables.</li>
</ul>

<p>Regarding clustering algorithms you can find enough if you have a quick look at SKlearn.</p>

<p>Hope it helped.
Good Luck :)</p>
"
['keras'],What is the significance of model merging in Keras?,"<p>It is used for several reasons, basically it's used to join multiple networks together. A good example would be where you have two types of input, for example tags and an image. You could build a network that for example has:</p>

<p>IMAGE -> Conv -> Max Pooling -> Conv -> Max Pooling -> Dense</p>

<p>TAG -> Embedding -> Dense layer</p>

<p>To combine these networks into one prediction and train them together you could merge these Dense layers before the final classification.</p>

<p>Networks where you have multiple inputs are the most 'obvious' use of them, here is a picture that combines words with images inside a RNN, the Multimodal part is where the two inputs are merged:</p>

<p><a href=""http://i.stack.imgur.com/nKKzB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nKKzB.png"" alt=""Multimodal Neural Network""></a></p>

<p>Another example is Google's Inception layer where you have different convolutions that are added back together before getting to the next layer.</p>

<p>To feed multiple inputs to Keras you can pass a list of arrays. In the word/image example you would have two lists:</p>

<pre><code>x_input_image = [image1, image2, image3]
x_input_word = ['Feline', 'Dog', 'TV']
y_output = [1, 0, 0]
</code></pre>

<p>Then you can fit as follows:</p>

<pre><code>model.fit(x=[x_input_image, x_input_word], y=y_output]
</code></pre>
"
"['machine-learning', 'feature-scaling']",Most important part of feature standardization and how is standardization affected by sparsity?,"<p>No, you are misinterpreting his comments.  If you have data that has some outliers in it then the outliers will extend beyond 3 standard deviations.  Then if you standardize the data some will extend beyond the [-3,3] region.  </p>

<p>He is simply saying that you need to remove your outliers so the outliers don't reap havoc on your stochastic gradient descent algorithm. He is NOT saying that you need to use some weird scaling algorithm.</p>

<p>You should standardize your data by subtracting the mean and dividing by the standard deviation, and then remove any points that extend beyond [-3,3], which are the outliers.</p>

<p>In stochastic gradient descent, the presence of outliers could increase the instability of the minimization and make it thrash around excessively, so its best to remove them.  </p>

<p>If the sparseness of the data prevents removal then... Do you need to use stochastic gradient descent, or can you just use gradient descent?  Gradient descent (GD) might help to alleviate some of the problems relating to convergence.  Finally, if GD is having trouble converging, you could always do an direct solve (e.g. direct matrix inversion) rather than an iterative solve.</p>

<p>Hope this helps!</p>
"
"['text-mining', 'scraping']",Ethically and Cost-effectively Scaling Data Scrapes,"<p>For many APIs (most I've seen) ratelimiting is a function of your API Key or OAuth credentials. (Google, Twitter, NOAA, Yahoo, Facebook, etc.) The good news is you won't need to spoof your IP, you just need to swap out credentials as they hit there rate limit. </p>

<p>A bit of shameless self promotion here but I wrote a python package specifically for handling this problem.</p>

<p><a href=""https://github.com/rawkintrevo/angemilner"" rel=""nofollow"">https://github.com/rawkintrevo/angemilner</a></p>

<p><a href=""https://pypi.python.org/pypi/angemilner/0.2.0"" rel=""nofollow"">https://pypi.python.org/pypi/angemilner/0.2.0</a></p>

<p>It requires a mongodb daemon and basically you make a page for each one of your keys. So you have 4 email addresses each with a separate key assigned. When you load the key in you specify the maximum calls per day and minimum time between uses. </p>

<p>Load keys:</p>

<pre><code>from angemilner import APIKeyLibrarian
l= APIKeyLibrarian()
l.new_api_key(""your_assigned_key1"", 'noaa', 1000, .2)
l.new_api_key(""your_assigned_key2"", 'noaa', 1000, .2)
</code></pre>

<p>Then when you run your scraper for instance the NOAA api:</p>

<pre><code>url= 'http://www.ncdc.noaa.gov/cdo-web/api/v2/stations' 
payload= {  'limit': 1000,
        'datasetid':  'GHCND', 
        'startdate': '1999-01-01' }

r = requests.get(url, params=payload, headers= {'token': 'your_assigned_key'})
</code></pre>

<p>becomes:</p>

<pre><code>url= 'http://www.ncdc.noaa.gov/cdo-web/api/v2/stations'
payload= {  'limit': 1000,
            'datasetid':  'GHCND',
            'startdate': '1999-01-01' }

r = requests.get(url, params=payload, headers= {'token': l.check_out_api_key('noaa')['key']})
</code></pre>

<p>so if you have 5 keys, <code>l.check_out_api_key</code> returns the key that has the least uses and waits until enough time has elapsed for it to be used again. </p>

<p>Finally to see how often your keys have been used / remaining useage available:</p>

<pre><code>pprint(l.summary())
</code></pre>

<p>I didn't write this for R because most scraping is done in python (most of MY scraping).  It could be easily ported. </p>

<p>Thats how you can <em>technically</em> get around rate limiting.  <em>Ethically</em> ...</p>

<p><strong>UPDATE</strong> The example uses Google Places API <a href=""https://github.com/rawkintrevo/angemilner/blob/master/example2.py"" rel=""nofollow"">here</a></p>
"
['time-series'],ARIMAX v. ARX Time Series Modeling,"<p>Yes, there are characteristics. You can use a correlogram to inform you as to the error structure in your data. That will tell you whether or not your data needs to account for AR and/or MA terms. Also check for unit roots to tell you whether you need to difference the time series.</p>

<p>This <a href=""https://onlinecourses.science.psu.edu/stat510/node/60"" rel=""nofollow"">link</a> has a good introduction to analyzing the correlogram that shows the pattern for AR(1) process. It is charecterized by a declining or oscillating but declining values. Depending on whether or not you have positive or negative autocorrelation. </p>

<p>The ACF for an MA(1) process is below. It is characterized by one significant value and then non-significant oscillating values thereafter.</p>

<p><a href=""http://i.stack.imgur.com/YJrrw.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YJrrw.jpg"" alt=""MA(1) Process""></a></p>

<p>To tell if you need to do differencing you should use a test like the augmented dickey-fuller test to check for the unit roots (i.e. your data is integrated).</p>
"
"['data-mining', 'predictive-modeling']",How to make data predictions,"<p>Regression will work well if your data set is large, but only for predicting current house prices (say, for example, estimating the value of your house). That's what people generally mean when they talk about predicting house prices from current house sales data.</p>

<p>The question of how house prices will behave in the next year is much, but much more complicated, and would not depend simply on the data you currently have. You would need to involve other information and a much more complex model, which would need to involve things like current level of household debt, inflation rate, economic outlook, etc. Daunting.</p>

<p>Generally, speculative prices follow some stochastic process. They depend on the current value, but diverge more and more the farther you go in the future.</p>
"
"['machine-learning', 'data-mining', 'classification', 'statistics', 'predictive-modeling']",How do I perform Naive Bayes Classification with a Bayesian Belief Network?,"<p>After reading some more papers, I realize that I misunderstood how the graphs work. The graphs are supposed to contain the conditional probabilities based on their parent(s). </p>

<p>This clears up the doubts that I had before.</p>

<p>For more information, see <a href=""http://www.csse.monash.edu.au/bai/book/BAI_Chapter2.pdf"" rel=""nofollow"">this</a> book chapter.</p>
"
"['machine-learning', 'data-mining', 'supervised-learning', 'unsupervised-learning']",Theoretical treatment of unlabeled samples,"<p>In a neural network model, you can use <a href=""https://en.wikipedia.org/wiki/Autoencoder"" rel=""nofollow"">autoencoders</a>.</p>

<p>The basic idea of an autoencoder is to learn a hidden layer of features by creating a network that simply copies the input vector at the output. So the training features and training ""labels"" are initially identical, no supervised labels are required. This can work using a classic <em>triangular</em> network architecture with progressively smaller layers that capture a compressed and hopefully useful set of derived features. The network's hidden layers learn representations based on the larger unsupervised data set. These layers can then be used to initialise a regular supervised learning network to be trained using the actual labels. </p>

<p>A similar idea is pre-training layers using a <a href=""https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine"" rel=""nofollow"">Restricted Boltzmann Machine</a>, which can be used in a very similar way, although based on different principles. </p>
"
"['machine-learning', 'dataset', 'data-cleaning', 'data']",Do I have to standardize my new polynomial features?,"<p>It's better to do the square computing on the values of the non standardized features, and then standardize these new features. </p>

<p>The point of standardizing features is to put features in a similar scale, e.g. 0-1. If you didn't standardize these new features after the computing, although your new features are between 0-1, your new features will be one or more magnitude smaller than your old features. </p>
"
['classification'],Cosine similarity versus dot product as distance metrics,"<p>Think geometrically. Cosine similarity only cares about angle difference, while dot product cares about angle and magnitude. If you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. Note that neither of them is a ""distance metric"".</p>
"
"['classification', 'r', 'graphs']",Libraries for (label propagation algorithms/frequent subgraph mining) for graphs in R,"<p>This is an old post, but there is a subgraph package and accompanying book/documentation for doing this in R:
<a href=""https://www.csc.ncsu.edu/faculty/samatova/practical-graph-mining-with-R/PracticalGraphMiningWithR.html"" rel=""nofollow"">https://www.csc.ncsu.edu/faculty/samatova/practical-graph-mining-with-R/PracticalGraphMiningWithR.html</a></p>

<p>Although I personally don't get the connection between subgraph mining and label propagation in this case. SVD++ might be closer to what you're looking for (supported by GraphX of Spark, which I think also supports label propagation).
<a href=""http://spark.apache.org/graphx/"" rel=""nofollow"">http://spark.apache.org/graphx/</a></p>
"
"['bigdata', 'research']",How much data space is used by all scientific articles?,"<p>Perhaps you are looking to quantify the amount of filespace used by a specific subset of data that we will label as ""academic publications."" </p>

<p>Well, to estimate, you could find stats on how many publications are housed at all the leading libraries (JSTOR, EBSCO, AcademicHost, etc) and then get the mean average size of each. Multiply that by the number of articles and whamo, you've got yourself an estimate.</p>

<p>Here's the problem, though: PDF files store the text from string <code>s</code> differently (in size) than, say, a text document stores that same string. Likewise, a compressed JPEG will store an amount of information <code>i</code> differently than a non-compressed JPEG. So you see we could have two of the same articles containing the same information <code>i</code> but taking up different amounts of memory <code>m</code>.</p>

<p>Are you looking to get a wordcount on the amount of scientific literature?</p>

<p>Are you looking to get an approximation of file system space used to store all academically published content in the world? </p>
"
['probability'],crow probability problem,"<p>This is a <a href=""https://en.wikipedia.org/wiki/Random_walk"" rel=""nofollow"">random walk</a> problem.  You could write a fairly simple random walk simulation which ends as soon as the tourist is 60 city blocks away from the starting location.  At any given time you only need to know the tourist's x,y coordinates.  Then each time-step, you pick a direction and increment either x or y.  You then need to keep track of distance from the origin (0,0).  Presumably this should be Euclidean distance, since it's all as the crow flies.</p>

<ul>
<li>decide direction to move</li>
<li>update x,y and distance</li>
<li>check if any of the distance requirements mentioned are met and store the timestep at which it occured</li>
<li>if distance > 60 blocks, terminate</li>
</ul>
"
"['machine-learning', 'algorithms', 'predictive-modeling', 'random-forest', 'k-means']",Predictive analysis of rare events,"<p>When you have an unbalanced data set, the algorithm is going to weight its success on each data point equally, meaning the majority class comes out as much more important than the minority class. The typical solution is to sample down the majority class until it's the same size as the minority class, and an alternate (similar) solution is to adjust the cost function so that the minority class is weighted appropriately.</p>

<p>See these similar questions for more:</p>

<ul>
<li><a href=""http://datascience.stackexchange.com/questions/810/should-i-go-for-a-balanced-dataset-or-a-representative-dataset/"">Should I go for a 'balanced' dataset or a 'representative' dataset?</a></li>
<li><a href=""http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets/"">Quick Guide into training highly imbalanced data sets</a></li>
<li><a href=""http://datascience.stackexchange.com/questions/454/what-are-the-implications-for-training-a-tree-ensemble-with-highly-biased-datase"">What are the implications for training a tree ensemble with highly biased datasets?</a></li>
<li><a href=""http://datascience.stackexchange.com/questions/736/skewed-multi-class-data/"">Skewed multi-class data</a></li>
<li><a href=""http://datascience.stackexchange.com/questions/6939/ratio-of-positive-to-negative-sample-in-data-set-for-best-classification/"">Ratio of positive to negative sample in data set for best classification</a></li>
</ul>
"
"['python', 'barplot']",Python Seaborn: how are error bars computed in barplots?,"<p>Looking at the source (seaborn/seaborn/categorical.py, line 2166), we find</p>

<pre><code>def barplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None,
        estimator=np.mean, ci=95, n_boot=1000, units=None,
        orient=None, color=None, palette=None, saturation=.75,
        errcolor="".26"", ax=None, **kwargs):
</code></pre>

<p>so the default value is, indeed, .95, as you guessed.</p>

<p>EDIT: How CI is calculated: <code>barplot</code> calls <code>utils.ci()</code> which has</p>

<p><strong>seaborn/seaborn/utils.py</strong></p>

<pre><code>def ci(a, which=95, axis=None):
    """"""Return a percentile range from an array of values.""""""
    p = 50 - which / 2, 50 + which / 2
    return percentiles(a, p, axis)
</code></pre>

<p>and this call to <code>percentiles()</code> is calling:</p>

<pre><code>def percentiles(a, pcts, axis=None):
    """"""Like scoreatpercentile but can take and return array of percentiles.
    Parameters
    ----------
    a : array
        data
    pcts : sequence of percentile values
        percentile or percentiles to find score at
    axis : int or None
        if not None, computes scores over this axis
    Returns
    -------
    scores: array
        array of scores at requested percentiles
        first dimension is length of object passed to ``pcts``
    """"""
    scores = []
    try:
        n = len(pcts)
    except TypeError:
        pcts = [pcts]
        n = 0
    for i, p in enumerate(pcts):
        if axis is None:
            score = stats.scoreatpercentile(a.ravel(), p)
        else:
            score = np.apply_along_axis(stats.scoreatpercentile, axis, a, p)
        scores.append(score)
    scores = np.asarray(scores)
    if not n:
        scores = scores.squeeze()
    return scores
</code></pre>

<p><code>axis=None</code> so <code>score = stats.scoreatpercentile(a.ravel(), p)</code> which is</p>

<pre><code>scipy.stats.scoreatpercentile(a, per, limit=(), interpolation_method='fraction', axis=None)[source]
Calculate the score at a given percentile of the input sequence.
</code></pre>

<blockquote>
  <p>For example, the score at per=50 is the median. If the desired quantile lies between two data points, we interpolate between them, according to the value of interpolation. If the parameter limit is provided, it should be a tuple (lower, upper) of two values.</p>
</blockquote>

<pre><code>Parameters: 
a : array_like
A 1-D array of values from which to extract score.
per : array_like
Percentile(s) at which to extract score. Values should be in range [0,100].
limit : tuple, optional
Tuple of two scalars, the lower and upper limits within which to compute the percentile. Values of a outside this (closed) interval will be ignored.
interpolation_method : {‘fraction’, ‘lower’, ‘higher’}, optional
This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j
fraction: i + (j - i) * fraction where fraction is the fractional part of the index surrounded by i and j.
lower: i.
higher: j.
axis : int, optional
Axis along which the percentiles are computed. Default is None. If None, compute over the whole array a.
Returns:    
score : float or ndarray
Score at percentile(s).
</code></pre>

<p>and looking in the source for <strong>scipy.stats.stats.py</strong> we see the signature</p>

<pre><code>def scoreatpercentile(a, per, limit=(), interpolation_method='fraction',
                      axis=None):
</code></pre>

<p>so since seaboard calls it with no param for <code>interpolation</code> it is using <code>fraction</code>.</p>

<p>On a side note, there is a warning of future obsolescence in <code>stats.scoreatpercentile()</code>, namely </p>

<blockquote>
  <p>This function will become obsolete in the future. For Numpy 1.9 and higher, numpy.percentile provides all the functionality that scoreatpercentile provides. And it’s significantly faster. Therefore it’s recommended to use numpy.percentile for users that have numpy >= 1.9.</p>
</blockquote>
"
"['clustering', 'k-means']",Fast k-means like algorithm for 10^10 points?,"<p>k-means is based on <strong>averages</strong>.</p>

<p>It models clusters using means, and thus <strong>the improvement by adding more data is marginal.</strong> The error of the average estimation reduces with 1/sqrt(n); so adding more data pays off less and less...</p>

<p>Strategies for such large data always revolve around sampling:</p>

<p><em>If you want sublinear runtime, you have to do sampling!</em></p>

<p>In fact, Mini-Batch-Kmeans etc. do exactly this: repeatedly sample from the data set.</p>

<p>However, sampling (in particular unbiased sampling) isn't exactly free either... usually, you will have to read your data linearly to sample, because you don't get random access to individual records.</p>

<p>I'd go with MacQueen's algorithm. It's online; by default it does a single pass over your data (although it is popular to iterate this). It's not easy to distribute, but I guess you can afford to linearly read your data say 10 times from a SSD?</p>
"
"['machine-learning', 'python', 'clustering']",Hierarchical Clustering customized Linkage function,"<p>Fork sklearn and implement it yourself! The linkage function is referenced in <a href=""https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cluster/hierarchical.py"" rel=""nofollow"">cluster/hierarchical.py</a> as</p>

<blockquote>
  <p><a href=""https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/hierarchical.py#L396"" rel=""nofollow"">join_func = linkage_choices[linkage]</a> </p>
</blockquote>

<p>and </p>

<blockquote>
  <p><a href=""https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/hierarchical.py#L520"" rel=""nofollow"">coord_col = join_func(A[i], A[j], used_node, n_i, n_j)</a></p>
</blockquote>

<p>If you have time, <a href=""http://scikit-learn.org/stable/developers/contributing.html#contributing-code"" rel=""nofollow"">polish your code</a> and submit a pull request when you're done.</p>
"
"['clustering', 'parallel']",How to optimize cohort sizes to reduce pair-wise comparisons?,"<p>There is a better way if I understand your question correctly. Here is the algorithm I propose:</p>

<ul>
<li>Initialize a 'window' list and a 'pairs' list</li>
<li>Sort your data on birthday from old to young (or the other way around)</li>
<li>Loop over your rows and keep track of all the rows that are still in the last three years since your current row. When you get to a new row, throw out rows that are now more than three years apart, add the current row together with the rows in your 'window' to your pairs set and add current row to your 'window'.</li>
</ul>

<p>This means you only iterate over your main list once, and if you implement your 'window' list properly (like a linked list for example) you don't need to do a lot of looping in that regard either. You also get all the pairs only once as a by product. Plus you actually get all the pairs, while with your binning approach you get missing pairs around the bin edges (if you don't overlap).</p>

<p>If you use the overlapping binning approach I think you should have bins of 6 years width and the borders shift 3 years each time, that way all true pairs are in at least 1 bin together, and there is the least amount of unwanted pairs. </p>
"
"['deep-learning', 'keras']",Does `batch_size` in Keras have any effects in results' quality?,"<p>No, the batch_size on average only influences the speed of your learning, not the quality of learning. The batch_sizes also don't need to be powers of 2, although I understand that certain packages only allow powers of 2. You should try to get your batch_size the highest you can that still fits the memory of your GPU to get the maximum speed possible.</p>

<p>Edit: I just found out that powers of 2 have some advantages with regards to vectorized operations in certain packages, so if it's close it might be faster to keep your batch_size in a power of 2.</p>
"
"['databases', 'word2vec']",word2vec -storing a word and its vector yet be able to efficiently run k-nearest neighbour,"<p>A most popular way of obtaining the <em>approximate</em> nearest neighbors is the <a href=""https://en.wikipedia.org/wiki/Locality-sensitive_hashing#LSH_algorithm_for_nearest_neighbor_search"" rel=""nofollow"">locality sensitive hash</a>. And <a href=""https://erikbern.com/2015/07/04/benchmark-of-approximate-nearest-neighbor-libraries/"" rel=""nofollow"">here</a> are some practical results. Then once you have the neighboring keys, it's straightforward to use a key-value store to retrieve the corresponding words.</p>
"
"['education', 'open-source']",What open-source books (or other materials) provide a relatively thorough overview of data science?,"<p>One book that's freely available is ""The Elements of Statistical Learning"" by Hastie, Tibshirani, and Friedman (published by Springer): <a href=""http://statweb.stanford.edu/~tibs/ElemStatLearn/"">see Tibshirani's website</a>.</p>

<p>Another fantastic source, although it isn't a book, is Andrew Ng's Machine Learning course on Coursera. This has a much more applied-focus than the above book, and Prof. Ng does a great job of explaining the thinking behind several different machine learning algorithms/situations.</p>
"
"['visualization', 'data', 'csv']",Creating queries on the fly and general manipulation for dataset of half a million data records,"<p>You can do this in pandas since your data set is small. For ""big"" data that does not fit in memory you would want to use a database; <a href=""https://en.wikipedia.org/wiki/PostgreSQL"" rel=""nofollow"">PostgreSQL</a> with the <a href=""https://en.wikipedia.org/wiki/PostGIS"" rel=""nofollow"">PostGIS extension</a> would be ideal, since it handles the nearest neighbor part, which is the most challenging aspect. Here are some sample queries, in python.</p>

<h3>Show me the Globs where average of each Glob in file Efficiency is greater than 50%</h3>

<pre><code>import pandas

efficiency = pandas.read_csv('efficiency.csv', sep=',', index_col=0)
structure = pandas.read_csv('structure.cv', sep=',', index_col=0)

efficiency[efficiency.mean(1) &gt; 0.5]
</code></pre>

<h3>Sort the Structure list descending on Run3, then ascending on Run2.</h3>

<pre><code>structure.sort_values(by=[""Run3"", ""Run2""], ascending=[False, True])
</code></pre>

<p><hr>
I'm not sure how you're defining the distance so I am unable to demonstrate the last part.</p>
"
['machine-learning'],Detecting Spam using Machine Learning,"<p>Strictly speaking, ""one class classification"" does not make sense as an idea. If there is only one possible state of a predicted value, then there is no prediction problem. The answer is always the single class.</p>

<p>Concretely, if you only have spam examples, you would always achieve 100% accuracy by classifying all email as spam. This is clearly wrong, and the only way to know how it is wrong is to know where the classification is wrong -- where emails are not in the spam class.</p>

<p>So-called <a href=""http://en.wikipedia.org/wiki/One-class_classification"">one-class classification</a> techniques are really anomaly detection approaches. They have an implicit assumption that things unlike the examples are not part of the single class, but, this is just an assumption about data being probably <em>not</em> within the class. There's a binary classification problem lurking in there.</p>

<p>What is wrong with a binary classifier?</p>
"
"['dataset', 'search', 'google']",Where did this NY Times op-ed get his Google Search data?,"<p>Google AdWords.  That has absolute search volumes. </p>
"
"['machine-learning', 'bigdata', 'finance']",Machine Learning on financial big data,"<p>There are tons of materials on financial (big) data analysis that you can read and peruse. I'm not an expert in finance, but am curious about the field, especially in the context of data science and R. Therefore, the following are selected relevant resource suggestions that I have for you. I hope that they will be useful.</p>

<p><strong>Books: Financial analysis (general / non-R)</strong></p>

<ul>
<li><p><a href=""http://www.springer.com/statistics/business,+economics+%26+finance/book/978-0-387-20270-9"" rel=""nofollow"">Statistics and Finance: An Introduction</a>;</p></li>
<li><p><a href=""http://www.springer.com/economics/public+finance/book/978-0-387-77826-6"" rel=""nofollow"">Statistical Models and Methods for Financial Markets</a>.</p></li>
</ul>

<p><strong>Books: Machine Learning in Finance</strong></p>

<ul>
<li><p><a href=""http://www.worldscientific.com/worldscibooks/10.1142/p818"" rel=""nofollow"">Machine Learning for Financial Engineering</a> (!) - seems to be an edited collection of papers;</p></li>
<li><p><a href=""http://www.elsevier.com/books/neural-networks-in-finance/mcnelis/978-0-12-485967-8#description"" rel=""nofollow"">Neural Networks in Finance: Gaining Predictive Edge in the Market</a>.</p></li>
</ul>

<p><strong>Books: Financial analysis with R</strong></p>

<ul>
<li><p><a href=""http://www.springer.com/statistics/business,+economics+%26+finance/book/978-1-4614-8787-6"" rel=""nofollow"">Statistical Analysis of Financial Data in R</a>;</p></li>
<li><p><a href=""http://www.springer.com/statistics/business,+economics+%26+finance/book/978-1-4419-7786-1"" rel=""nofollow"">Statistics and Data Analysis for Financial Engineering</a>;</p></li>
<li><p><a href=""http://rads.stackoverflow.com/amzn/click/0470978708"" rel=""nofollow"">Financial Risk Modelling and Portfolio Optimization with R</a></p></li>
<li><p><a href=""http://www.springer.com/statistics/business%2C+economics+%26+finance/book/978-3-642-16520-7"" rel=""nofollow"">Statistics of Financial Markets: An Introduction</a> (code in R and MATLAB).</p></li>
</ul>

<p><strong>Academic Journals</strong></p>

<ul>
<li><a href=""http://algorithmicfinance.org"" rel=""nofollow"">Algorithmic Finance</a> (open access)</li>
</ul>

<p><strong>Web sites</strong></p>

<ul>
<li><p><a href=""https://www.rmetrics.org"" rel=""nofollow"">RMetrics</a></p></li>
<li><p><a href=""http://quant.stackexchange.com"">Quantitative Finance on StackExchange</a></p></li>
</ul>

<p><strong>R Packages</strong></p>

<ul>
<li><p>the above-mentioned <em>RMetrics</em> site (see <a href=""https://www.rmetrics.org/about"" rel=""nofollow"">this page</a> for general description);</p></li>
<li><p><em>CRAN Task Views</em>, including <a href=""http://cran.r-project.org/web/views/Finance.html"" rel=""nofollow"">Finance</a>, <a href=""http://cran.r-project.org/web/views/Econometrics.html"" rel=""nofollow"">Econometrics</a> and several other Task Views.</p></li>
</ul>

<p><strong>Competitions</strong></p>

<ul>
<li><a href=""http://www.modeloff.com"" rel=""nofollow"">MODELOFF (The Financial Modeling World Championships)</a></li>
</ul>

<p><strong>Educational Programs</strong></p>

<ul>
<li><p><a href=""http://ieor.columbia.edu/ms-financial-engineering"" rel=""nofollow"">MS in Financial Engineering - Columbia University</a>;</p></li>
<li><p><a href=""http://www3.cs.stonybrook.edu/~skiena/691"" rel=""nofollow"">Computational Finance - Hong Kong University</a>.</p></li>
</ul>

<p><strong>Blogs (Finance/R)</strong></p>

<ul>
<li><p><a href=""http://timelyportfolio.blogspot.com"" rel=""nofollow"">Timely Portfolio</a>;</p></li>
<li><p><a href=""https://systematicinvestor.wordpress.com"" rel=""nofollow"">Systematic Investor</a>;</p></li>
<li><p><a href=""https://moneymakingmankind.wordpress.com"" rel=""nofollow"">Money-making Mankind</a>.</p></li>
</ul>
"
"['predictive-modeling', 'actuarial']","Are there pitfalls in using the output of machine learning model, such as a neural net as the input to a traditional GLM or similar?","<p>Theoretically there is no problem. I've seen tree models put as predictors in logistic models. NN as input into a GLM moodel makes sense. The ultimate decition should be made based on the predictability of the NN.   </p>

<p>You have to mind a few issues:  </p>

<ol>
<li><p>model mantenance and deployment. The NN model would probably have more parameters than your vanilla GLM deployment might be more involved. You could live it frozen and never update it, and let the GLM model use the NN score as long as it adds to the GLM prediction.  </p></li>
<li><p>interpretability. There might be managers whose acountability is to review risk, and they may not be comfortable with a NN. In that case feeding the NN results into a GLM might make it more acceptable, much as credit scores are used in some risk models. </p></li>
</ol>
"
"['python', 'pandas']",How can I calculate a rolling window sum in pandas across this MultiIndex dataframe?,"<p>If anyone else comes looking, this was my solution:</p>

<pre><code># find last column
last_column = df.shape[1]-1

# grab the previous 11 columns (also works if there aren't that many in the df)
wanted = df.iloc[:, (last_column-11):last_column]

# calculate the rounded moving annual total 
mat_calc = round(wanted.sum(axis=1)/len(wanted.columns), 2)
</code></pre>

<p>Probably not the most pandastic solution, but it works well.</p>
"
['data-cleaning'],convert some observations into variables,"<p>Assume your table can be put into a pandas DataFrame object <b>data</b> with 4 columns as above. One way to achieve what you want is to do a GROUPBY using ID and Location. Then gradually assign values to each row of new table:</p>

<pre><code>    newdata = pd.DataFrame(columns=['ID', 'Location', 'Feat1', 'Feat2', 'Feat3', 'Feat4', 'Feat5', 'Feat6'])
    grouped = data.groupby(['ID', 'Location'])

    for index, (group_name, d) in enumerate(grouped):
        newdata.loc[index, 'ID'] = group_name[0]
        newdata.loc[index, 'Location'] = group_name[1]
        for feature, amount in zip(d['Feature'], d['amount']):
            newdata.loc[index, feature] = amount
</code></pre>
"
"['machine-learning', 'python', 'classification']",How to store and analyze classification results with Python?,"<p><a href=""http://sacred.readthedocs.org/"" rel=""nofollow"">Sacred</a> is a <a href=""https://github.com/IDSIA/sacred"" rel=""nofollow"">python library</a> developed by the <a href=""http://www.idsia.ch/"" rel=""nofollow"">IDSIA lab</a> that ""facilitates automated and reproducible experimental research"". It is available through pip as <code>sacred</code>.</p>

<p>For a related discussion see <a href=""https://www.reddit.com/r/MachineLearning/comments/3npg0d"" rel=""nofollow"">reddit</a>.</p>
"
"['r', 'dataset', 'beginner']",R aggregate() with dates,"<p>I am going to agree with @user1683454's comment. After importing, your DATE column is of either <code>character</code>, or <code>factor</code> class (depending on your settings for <code>stringsAsFactors</code>). Therefore, I think that you can solve this issue in at least several ways, as follows:</p>

<p>1) <strong>Convert data</strong> to correct type <strong>during import</strong>. To do this, just use the following options of <code>read.csv()</code>: <code>stringsAsFactors</code> (or <code>as.is</code>) and <code>colClasses</code>. By default, you can specify conversion to <code>Date</code> or <code>POSIXct</code> classes. If you need a non-standard format, you have two options. First, if you have a single Date column, you can use <code>as.Date.character()</code> to pass the desired format to <code>colClasses</code>. Second, if you have multiple Date columns, you can write a function for that and pass it to <code>colClasses</code> via <code>setAs()</code>. Both options are discussed here: <a href=""http://stackoverflow.com/questions/13022299/specify-date-format-for-colclasses-argument-in-read-table-read-csv"">http://stackoverflow.com/questions/13022299/specify-date-format-for-colclasses-argument-in-read-table-read-csv</a>.</p>

<p>2) <strong>Convert data</strong> to correct format <strong>after import</strong>. Thus, after calling <code>read.csv()</code>, you would have to execute the following code: <code>dateColumn &lt;- as.Date(dateColumn, ""%m/%d/%Y"")</code> or <code>dateColumn &lt;- strptime(dateColumn, ""%m/%d/%Y"")</code> (adjust the format to whatever Date format you need).</p>
"
"['data-cleaning', 'evaluation']",Modelling on one Population and Evaluating on another Population,"<p>If the users who you are getting client-side data from are from the same population of users who you would get server-side data from. If that is true, then you aren't really training on one population and applying to another. The main difference is that the client side data happened in the past (by necessity unless you are constantly refitting your model) and the server side data will come in the future.</p>

<p>Let's reformulate the question in terms of models rather than web clients and servers.</p>

<p>You are fitting a model on one dataset and applying it to another. That is the classic use of predictive modeling/machine learning. Models use features from the data to make estimates of some parameter or parameters. Once you have a fitted (and tested) model, all that you need is the same set of features to feed into the model to get your estimates.</p>

<p>Just make sure to model on a set of features (aka variables) that are available on the client-side and server-side. If that isn't possible, ask that question separately.</p>
"
['statistics'],Data Science as a Social Scientist?,"<p>The downvotes are because of the topic, but I'll attempt to answer your question as best I can since it's here.</p>

<p>Data science is a term that is thrown around as loosely as Big Data.  Everyone has a rough idea of what they mean by the term, but when you look at the actual work tasks, a data scientist's responsibilities will vary greatly from company to company.</p>

<p>Statistical analysis could encompass the entirety of the workload in one job, and not even be a consideration for another.</p>

<p>I wouldn't chase after a job title per se.  If you are interested in the field, network (like you are doing now) and find a good fit.  If you are perusing job ads, just look for the ones that stress statistical and informatics backgrounds.  Hadoop and SQL are both easy to become familiar with given the time and motivation, but I would stick with the areas you are strongest in and go from there.</p>
"
"['machine-learning', 'preprocessing']",Redundancy - is it a big problem?,"<p>Assuming you want to learn sentiment this is a problem. What happens when you feed this to a Machine Learning algorithm is that it will give more weight to the tweets that are in there multiple times, while not learning more information. It's unlikely that a tweet that has been retweeted 10 times carries more significant information about sentiment than one that hasn't been retweeted. What you could do is add the number of times the tweet was in the set as a feature to your sentiment model, it's possible something can be learned from that fact (maybe positive tweets are retweeted more often), but you should keep it at 1 row for every distinct tweet.</p>

<p>Getting rid of these redundant records should not be difficult programmatically though. I don't know what language you are using but if you only consider the body of the tweet (the content) you could iterate over your tweets, keep a list of all the unique bodies, combined with other meta information (like user, labeled sentiment) and if the content of the next tweet is already in there, just do not add it. Look for 'distinct' functionality as opposed to redundant, enough information out there.</p>
"
['knowledge-base'],Example tasks of a data scientist and the necessary knowledge,"<p><a href=""http://nirvacana.com/thoughts/becoming-a-data-scientist/"" rel=""nofollow"">Becoming a Data Scientist – Curriculum via Metromap</a> is a popular reference for this kind of question.</p>
"
"['r', 'logistic-regression', 'sas']",Logistic Regression in R models for 1 or 0?,"<p>I always learned to model for 1. See below for the impact of switching the encoding,</p>

<pre><code>model &lt;- glm(ans ~ x, data = simulated_data, family = binomial)
summary(model)

# 
# Call:
# glm(formula = ans ~ x, family = binomial, data = simulated_data)
# 
# Deviance Residuals: 
#       1        2        3        4        5  
#  1.6388  -0.6249  -1.2146  -0.8083   0.7389  
# 
# Coefficients:
#             Estimate Std. Error z value Pr(&gt;|z|)
# (Intercept)  -2.4753     2.5006   -0.99    0.322
# x             0.5957     0.6543    0.91    0.363
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
#     Null deviance: 6.7301  on 4  degrees of freedom
# Residual deviance: 5.7505  on 3  degrees of freedom
# AIC: 9.7505
# 
# Number of Fisher Scoring iterations: 4
# 

simulated_data$ans &lt;- !simulated_data$ans
model_opp &lt;- glm(ans ~ x, data = simulated_data, family = binomial)
summary(model_opp)

# 
# Call:
# glm(formula = ans ~ x, family = binomial, data = simulated_data)
# 
# Deviance Residuals: 
#       1        2        3        4        5  
# -1.6388   0.6249   1.2146   0.8083  -0.7389  
# 
# Coefficients:
#             Estimate Std. Error z value Pr(&gt;|z|)
# (Intercept)   2.4753     2.5006    0.99    0.322
# x            -0.5957     0.6543   -0.91    0.363
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
#     Null deviance: 6.7301  on 4  degrees of freedom
# Residual deviance: 5.7505  on 3  degrees of freedom
# AIC: 9.7505
# 
# Number of Fisher Scoring iterations: 4
</code></pre>

<p>Hope this helps.</p>
"
"['neuralnetwork', 'preprocessing']",Denoising Autoenoders with Variable Length Input,"<p>Recurrent Neural Networks can deal with variable length data. You might want to have a look at:</p>

<ul>
<li>Andrej Karpathy: <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">The Unreasonable Effectiveness of Recurrent Neural Networks</a>.</li>
<li>Christopher Olah: <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow"">Understanding LSTM Networks</a>.</li>
<li>Hochreiter, Schmidthuber: <a href=""http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=6795963"" rel=""nofollow"">Long short-term memory</a>.</li>
</ul>

<p>Another idea (which I have not tested so far and  just came to my mind) is using a histogram approach: You could probably make fixed-size windows, get the data from those windows and make it discrete (e.g. vector quantization, k-means). After that, you can make a histogram of how often those vectors appeared.</p>

<p>You could also use <a href=""https://en.wikipedia.org/wiki/Hidden_Markov_model"" rel=""nofollow"">HMMs</a> for recognition of variable length data.</p>

<p>Transformations (e.g. <a href=""https://en.wikipedia.org/wiki/Fourier_transform"" rel=""nofollow"">Fourier transform</a> from the time domain in the <a href=""https://en.wikipedia.org/wiki/Frequency_domain"" rel=""nofollow"">frequency domain</a>) might also come in handy.</p>
"
['neuralnetwork'],How to draw a hyperplane using the weights calculated,"<p>If you have only an input layer, one set of weights, and an output layer, you can solve this directly with $$ X \cdot w = threshold $$</p>

<p>However if you add in hidden layers, you no longer necessarily have a <a href=""https://en.wikipedia.org/wiki/Hyperplane"" rel=""nofollow"">hyperplane</a>, as in order to be a hyperplane it must be able to be expressed as the ""solution of a single algebraic equation of degree 1.""</p>

<p>Even if you can't solve directly, you can still get a sense for the response surface by evaluating your network's output over a wide range of inputs. </p>
"
"['data-mining', 'classification', 'decision-trees', 'preprocessing']",What is a benchmark model?,"<p>Benchmarking is the process of comparing your result to existing methods. You may compare to published results using another paper, for example. If there is no other obvious methodology against which you can benchmark, you might compare to the best naive solution (guessing the mean, guessing the majority class, etc) or a very simple model (a simple regression, K Nearest Neighbors).  If the field is well studied, you should probably benchmark against the current published state of the art (and possibly against human performance when relevant).</p>
"
"['machine-learning', 'classification', 'multiclass-classification', 'multilabel-classification']",how to make new class from the test data,"<p>I think you need to read about <a href=""https://en.wikipedia.org/wiki/Online_machine_learning"" rel=""nofollow"">Online learning</a>, it refers to learning when new data is being constantly added. In these cases you need an algorithm that can update itself as new data arrives (i.e. it doesn't need to recalculate itself from scratch). In other words, <em>incrementally</em>.</p>

<p>There are incremental versions for support vector machines (SVMs) and for neural networks. Also, bayesian networks can be made to work incrementally.</p>
"
"['machine-learning', 'r', 'programming']",How can I vectorize this code in R? Maybe with the apply() function?,"<p>First of all it should be noted that the code you posted does not actually replicate the output of the <code>dist</code> function, because the line:</p>

<pre><code>distancematrix[i, j] &lt;- sum(abs(myMatrix[i,] - myMatrix[j,]))
</code></pre>

<p>does not calculate the Euclidean distance; it should be:</p>

<pre><code>distancematrix[i, j] &lt;- sqrt(sum((myMatrix[i,] - myMatrix[j,]) ^ 2))
</code></pre>

<p>Here are two solutions that rely on <code>apply</code>. They are simplified, and in particular do not take advantage of the symmetry of the distance matrix (which, if considered, would lead to a 2-fold speedup). First, generate some test data:</p>

<pre><code># Number of data points
N &lt;- 2000
# Dimensionality
d &lt;- 10
# Generate data
myMatrix = matrix(rnorm(N * d), nrow = N)
</code></pre>

<p>For convenience, define:</p>

<pre><code># Wrapper for the distance function
d_fun &lt;- function(x_1, x_2) sqrt(sum((x_1 - x_2) ^ 2))
</code></pre>

<p>The first approach is a combination of <code>apply</code> and <code>sapply</code>:</p>

<pre><code>system.time(
    D_1 &lt;- 
        apply(myMatrix, 1, function(x_i) 
            sapply(1:nrow(myMatrix), function(j) d_fun(x_i, myMatrix[j, ]))
    )
)

   user  system elapsed 
 14.041   0.100  14.001 
</code></pre>

<p>while the second uses only <code>apply</code> (but going over the indices, which are paired using <code>expand.grid</code>):</p>

<pre><code>system.time(
    D_2 &lt;- 
        matrix(apply(expand.grid(i = 1:nrow(myMatrix), j = 1:nrow(myMatrix)), 1, function(I) 
            d_fun(myMatrix[I[[""i""]], ], myMatrix[I[[""j""]], ])
        )
    )
)

   user  system elapsed 
 39.313   0.498  39.561 
</code></pre>

<p>However, as expected both are much slower than <code>dist</code>:</p>

<pre><code>system.time(
    distancematrix &lt;- as.matrix(
        dist(myMatrix, method = ""euclidean"", diag = T, upper = T)
    )
)

   user  system elapsed 
  0.337   0.054   0.388
</code></pre>
"
['text-mining'],How to approach automated text writing?,"<p>You should probably do some reading in the field of ""<a href=""http://en.wikipedia.org/wiki/Natural_language_generation"">Natural Language Generation</a>"", since this seems to relate most directly to your question. </p>

<p>But the way you have described the process -- ""text mining...text building"" -- leads me to wonder if you are aiming for something much more ambitious.  It seems as though you aim to automate the process of 1) reading natural language texts, 2) <strong><em>understanding the meaning</em></strong>, and then 3) generate new texts based on that semantic knowledge.  I'm not aware of any general-purpose end-to-end systems that can do that, not even specialized systems by the likes of <a href=""http://www.palantir.com/"">Palantir</a>.  What you are aiming for would probably pass the Turing Test for fully capable Artificial Intelligence.</p>
"
"['machine-learning', 'classification', 'text-mining']",How to learn spam email detection?,"<p>First of all check <a href=""https://archive.ics.uci.edu/ml/datasets/Spambase"">this</a> carefully. You'll find a simple dataset and some papers to review.</p>

<p><strong>BUT</strong> as you want to start a simple learning project I recommend to not going through papers (which are obviously not <em>basic</em>) but try to build your own bayesian learner which is not so difficult.</p>

<p>I personally suggest <a href=""http://www.cs.cmu.edu/~guestrin/Class/10708-F08/schedule.html"">Andrew Moore</a>'s lecture slides on Probabilistic Graphical Models which are freely available and you can learn from them simply and step by step.</p>

<p>If you need more detailed help just comment on this answer and I'll be glad to help :)</p>

<p>Enjoy baysian learning!</p>
"
"['bigdata', 'career']",Can data analytics be a basis for artificial intelligence?,"<p>Classes related to Artificial Intelligence are typically taught in Computer Science departments. Looking at the <a href=""https://my.feit.uts.edu.au/pages/course/undergraduate/it_project_subjects"" rel=""nofollow"">IT Project Subjects offered by your university</a>, I suspect Data Analytics would indeed be more relevant to AI than Internetworking and Applications.</p>

<p>Looking at the courses offered by your department, the following likely involve aspects of AI:</p>

<ul>
<li>Image Processing and Pattern Recognition</li>
<li>Intelligent Agents</li>
<li>Building Intelligent Agents</li>
</ul>

<p>For self-directed study in AI, I recommend starting with Russell &amp; Norvig's essential textbook <a href=""http://aima.cs.berkeley.edu/"" rel=""nofollow"">Artificial Intelligence: A Modern Approach</a>.</p>

<p>As to what it will take to create a human-like strong AI, I recommend this collection of essays: <a href=""http://rads.stackoverflow.com/amzn/click/0198248547"" rel=""nofollow"">The Philosophy of Artificial Intelligence</a>... even though the material is getting a bit out-of-date by now.</p>

<p>Good luck!</p>
"
"['machine-learning', 'svm', 'feature-scaling']",Consequence of Feature Scaling,"<p>Within each class, you'll have distributions of values for the features. That in itself is not a reason for concern. </p>

<p>From a slightly theoretical point of view, you can ask yourself why you should scale your features and why you should scale them in exactly the chosen way.<br>
One reason may be that your particular training algorithm is known to converge faster (better) with values around 0 - 1 than with features which cover other orders of magnitude. In that case, you're probably fine. My guess is that your SVM is fine: you want to avoid too large numbers because of the inner product, but a max of 1.2 vs. a max of 1.0 won't make much of a difference.<br>
(OTOH, if you e.g. knew your algorithm to not accept negative values you'd obviously be in trouble. )</p>

<p>The practical question is whether your model performs well for cases that are slightly out of the range covered by training. This I believe can best and possibly only be answered by testing with such cases / inspecting test results for performance drop for cases outside the training domain. It is a valid concern and looking into this would be part of the validation of your model. </p>

<p>Observing differences of the size you describe is IMHO a reason to have a pretty close look at model stability. </p>
"
"['machine-learning', 'java']",Do I need an Artificial Intelligence API?,"<p>One needs to use an <em>artificial intelligence (AI)</em> API, if there is a need to add <em>AI functionality</em> to a software application - this is pretty obvious. Traditionally, my advice on <em>machine learning (ML)</em> software includes the following two excellent <strong>curated lists</strong> of resources: <a href=""https://github.com/onurakpolat/awesome-bigdata"" rel=""nofollow"">this one</a> and <a href=""https://github.com/josephmisiti/awesome-machine-learning"" rel=""nofollow"">this one</a>.</p>

<p>However, keep in mind that ML is just a <strong>subset</strong> of AI domain, so if your tasks involve AI areas <em>beyond</em> ML, you need more <em>AI-focused</em> tools or platforms. For example, you can take a look at <a href=""http://www.ai-one.com"" rel=""nofollow"">ai-one</a>'s AI platforms and APIs as well as interesting <em>general AI</em> open source project <a href=""http://opencog.org"" rel=""nofollow"">OpenCog</a>.</p>

<p>In addition to the above-mentioned AI-focused platforms, IBM's <a href=""http://www.ibm.com/smarterplanet/us/en/ibmwatson"" rel=""nofollow"">Watson</a> AI system deserves a separate mention, as quite cool and promising. It offers its own <em>ecosystem</em> for developers, called <a href=""http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud"" rel=""nofollow"">IBM Watson Developer Cloud</a>, based on IBM's <a href=""https://ace.ng.bluemix.net"" rel=""nofollow"">BlueMix</a> cloud computing <em>platform-as-a-service (PaaS)</em>. However, at the present time, I find this offering to be quite expensive as well as limiting, especially for individual developers, small startups and other small businesses, due to its tight integration with and reliance only on a single PaaS (Blue Mix). It will be interesting to watch this space as competition in AI domain and marketplace IMHO will surely intensify in the future.</p>
"
"['python', 'nlp', 'text-mining']","How to recognize a two part term when the space is removed? (""bigdata"" and ""big data"")","<p>There is a nice implementation of this in gensim: <a href=""http://radimrehurek.com/gensim/models/phrases.html"">http://radimrehurek.com/gensim/models/phrases.html</a></p>

<p>Basically, it uses a data-driven approach to detect phrases, ie. common collocations. So if you feed the Phrase class a bunch of sentences, and the phrase ""big data"" comes up a lot, then the class will learn to combine ""big data"" into a single token ""big_data"". There is a more complete tutorial-style blog post about it here: <a href=""http://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/"">http://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</a></p>
"
"['python', 'scikit', 'pandas']",Building a machine learning model to predict crop yields based on environmental data,"<p>For starters, you can predict the yield for the upcoming year based on the daily data for the previous year. You can estimate the model parameters by considering each year's worth of data as one ""point"", then validate the model using cross-validation. You can extend this model by considering more than the past year, but look back too far and you'll have trouble validating your model and overfit.</p>
"
"['machine-learning', 'data-mining', 'decision-trees']",Decision tree vs. KNN,"<p>They serve different purposes. </p>

<p>KNN is unsupervised, Decision Tree (DT) supervised.</p>

<p>KNN is used for clustering, DT for classification.</p>

<p>KNN determines neighborhoods, so there must be a distance metric. This implies that all features must be numeric. Distance metrics may be effected by varying scales between attributes and also high-dimensional space.</p>

<p>DT on the other hand predicts a class for a given input vector. The attributes may be numeric or nominal.</p>

<p>So, if you want to find similar examples you could use KNN. If you want to classify examples you could use DT.</p>
"
['r'],"Spike Slab in r, bad output","<p>There seems to be no <code>summary</code> method for objects of class <code>spikeslab</code>, so the default summary method is used which prints the internal list elements.</p>

<p>Just print the object to get a nice summary - maybe that's what you want? Is this the output you expected?</p>

<pre><code>&gt; model1_ss
------------------------------------------------------------------- 
Variable selection method     : AIC 
Big p small n                 : FALSE 
Screen variables              : FALSE 
Fast processing               : TRUE 
Sample size                   : 7 
No. predictors                : 3 
No. burn-in values            : 500 
No. sampled values            : 500 
Estimated mse                 : 7.5765 
Model size                    : 2 


---&gt; Top variables:
                            bma  gnet bma.scale gnet.scale
Mentally.Unhealthy.Days   2.290 2.331    13.003     13.236
Physically.Unhealthy.Days 0.926 0.951     1.084      1.113
------------------------------------------------------------------- 
</code></pre>
"
"['machine-learning', 'data-mining', 'classification', 'algorithms']",Machine learning algorithm to classify blog posts,"<h1>Question 1: Category prediction</h1>

<p>To predict the category of a new blog post, you could do the following:</p>

<ul>
<li>Build a MLP (multilayer Perceptron, a very simple neural network). Each category gets an output node, each tag is a binary input node. However, this will only work if the number of tags is very little. As soon as you add new tags, you will have to retrain the network.</li>
<li>Build a MLP with ""important words"" as features.</li>
<li>If you have internal links, you might want to have a look at ""On Node Classification in Dynamic Content-based Networks"". In case you're German, you might also like <a href=""http://arxiv.org/abs/1512.04469"" rel=""nofollow"">Über die Klassifizierung von Knoten in dynamischen Netzwerken mit Inhalt</a></li>
<li>You could take all words you currently have, see those as a vector space. Fix that vocabulary (and probably remove some meaningless words like ""with"", ""a"", ""an"" - this is commonly called a ""stopword""). For each text, you count the different words you have in your vocabulary. A new blog post is point in this space. Use $k$ nearest neighbor for classification.</li>
<li>Use combinations of different predictiors by letting each predictor give a vote for a classification.</li>
</ul>

<h2>See also</h2>

<ul>
<li>Yiming Yang, Jan O. Pedersen: <a href=""http://www.surdeanu.info/mihai/teaching/ista555-spring15/readings/yang97comparative.pdf"" rel=""nofollow"">A Comparative Study on Feature Selection in Text Categorization</a>, 1997.</li>
<li>Scikit-learn: <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">Working With Text Data</a></li>
</ul>

<h1>Question 2: Tagging texts</h1>

<p>This can be treated the same way like question 1.</p>

<h1>Question 3: Finding locations</h1>

<p>Download a database of countries / cities (e.g. <a href=""https://www.maxmind.com/de/free-world-cities-database"" rel=""nofollow"">maxmind</a>) and just search for a match.</p>
"
"['hadoop', 'map-reduce']",Suggestions on what patterns/analysis to derive from Airlines Big Data,"<p>There really is no wrong answer here, but I recommend predicting flight cancellations (#22) and/or delays (25-29), since this is how I often see this data set being used. It could also have practical significance to you if you should ever find yourself flying to or departing from one of the worst offending airports/airlines. </p>

<p>I'm not sure if you have a choice (perhaps your employer requires it), but don't use Map Reduce -- it's incredibly difficult to learn/maintain, it's slow, and on top of that it has become obsolete. Use something like Spark's ML lib (<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"" rel=""nofollow"">http://spark.apache.org/docs/latest/mllib-guide.html</a>). It's much easier to use and is much more current. </p>
"
"['machine-learning', 'svm', 'sklearn']","Support Vector Classification kernels ‘linear’, ‘poly’, ‘rbf’ has all same score","<p><strong>You will love the answer to this one...</strong></p>

<p>Take a look at your code and notice that you are calling the scoring function and each time you are passing in the exact same values i.e. they are all spitting out the lin_svc.score().  Try interweaving the four scoring calls below the four respective fit calls and you should see the desired variation in the result.</p>

<pre><code># we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
#rbf is for gaussian
C = 1.0  # SVM regularization parameter

svc = svm.SVC(kernel='linear', C=C).fit(X_train, y_train)
print svc.score(X_test, y_test)

rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X_train, y_train)
print rbf_svc.score(X_test, y_test)

poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X_train, y_train)
print poly_svc.score(X_test, y_test)

lin_svc = svm.LinearSVC(C=C).fit(X_train, y_train)
print lin_svc.score(X_test, y_test)
</code></pre>

<p>Similarly, you are doing the same thing below also.</p>

<p>Hope this helps! </p>
"
"['machine-learning', 'python']",t-SNE Python implementation: Kullback-Leibler divergence,"<p>The TSNE source in scikit-learn is in pure Python. Fit <code>fit_transform()</code> method is actually calling a private <code>_fit()</code> function which then calls a private <code>_tsne()</code> function. That <code>_tsne()</code> function has a local variable <code>error</code> which is printed out at the end of the fit. Seems like you could pretty easily change one or two lines of source code to have that value returned to <code>fit_transform()</code>.</p>
"
"['machine-learning', 'python', 'clustering', 'k-means', 'clusters']","Clustering geo location coordinates (lat,long pairs)","<p>K-means should be right in this case. Since k-means tries to group based solely on euclidean distance between objects you will get back clusters of locations that are close to each other. </p>

<p>To find the optimal number of clusters you can try making an 'elbow' plot of the within group sum of square distance. This may be helpful (<a href=""http://nbviewer.ipython.org/github/nborwankar/LearnDataScience/blob/master/notebooks/D3.%20K-Means%20Clustering%20Analysis.ipynb"" rel=""nofollow"">http://nbviewer.ipython.org/github/nborwankar/LearnDataScience/blob/master/notebooks/D3.%20K-Means%20Clustering%20Analysis.ipynb</a>)</p>
"
['jupyter'],Is it possible to (de)activate a specific set of cells in jupyter?,"<p>Welcome to DataScience.SE! </p>

<p>This is not currently possible. You could change the cells to <em>Raw</em>.</p>
"
"['machine-learning', 'classification', 'feature-selection']",Expand or compact features?,"<p>The way I see it is that your 10 sources of data, they all refer to the same set of people. Depending on the attributes, some can be expanded, some can be merged ...</p>

<p>Attributes such as age should be unique, so it doesn't make sense to expand it to Profile 1 age, profile 2 age ... One simple way is merge them is by using the average or use max. Expanding age only add redundant data to your feature matrix, and increase its dimensionality, in most cases, this doesn't help generalization performance of your model. </p>

<p>On the other hand, number of followers can be expanded. Depending on the data source, a guy has 10 followers on Twitter but 1000 followers on Google+ might simply mean that he barely uses Twitter. </p>

<p>That being said, the way you pick your features or engineer new features should increase your model performance, so if expanding number of followers actually decrease Cross Validation or Test performance, compared to the one using sum of followers then you can simply use sum of followers. </p>
"
"['machine-learning', 'statistics', 'education']",PhD program in statistics,"<p>Certain ingredients are needed to give you the best chance of a successful PhD. One of the important ones is that you and your supervisor have mutual interests.</p>

<p>A second important ingredient, in my opinion, is that you immerse yourself in that environment. It's important to develop a network of colleagues. It helps to spread ideas, start collaborations, get help when needed, and to explore unthought of opportunities.</p>

<p>From what you have said, I think you will be missing out on these two important ingredients if you continue in the same place or if you work remotely.</p>

<p>What is also important is what you to do <em>after</em> the PhD. PhD is required for academic position. But I think you will be in a weak position to get to the next step (fellowships, faculty positions, etc.) if you do what you proposed. In certain industrial positions it can be looked on favourably, not necessarily for the topic you pursued, but because it says something about your personally. Basically that you can get things gone, rise to a challenge, work independently, work as a team, communicate difficult topics and can bring creativity to solving a problems. </p>

<p>My advice would be to find a machine learning research group and apply for PhDs. If this is not possible why not consider following the topic of your supervisor and keep machine learning as a hobby? You will become and expert in statistic and so you will find manny concepts will translate between the various statistical disciplines. But only do this if you get along with her/him, and you can see yourself studying this topic.</p>

<p>Finally you could try a compromise? Are there applications for ""mixing statistics"" in machine learning? Can you find one? Is there an unexplored opportunity to do something new?</p>

<hr>

<p>As a side note I find it ridiculous that PhD supervisor ask the student for topics. This always leads to problems because the student doesn't really have a clue about the research field. There is room for flexibility but often this hides supervisor laziness.</p>
"
['regression'],What does the Ip mean in the Bayesian Ridge Regression formula?,"<p>$\mathcal{N}$ does indeed denote a (multivariate) normal / Gaussian distribution. $I_p$ is just an identity matrix of dimension $p$. So this a matrix with $\lambda^{-1}$ along the diagonal. Read this as the covariance matrix, so this is a spherical Gaussian (0 covariance between different dimensions) where each variable has variance $\lambda^{-1}$.</p>
"
"['data-mining', 'predictive-modeling', 'decision-trees']",How does QUEST compare to other decision tree algorithms?,"<p>QUEST stands for Quick, Unbiased and Efficient Statistical Tree.</p>

<p>It uses ANOVA F and contingency table Chi Square tests to select variables for splitting. Variables with multiple classes are merged into two super-classes to get binary splits which are determined using QDA (Quadratic Discriminant analysis). The tree can be pruned using the CART algorithm. It can be used for both classification and regression tasks.</p>

<p>Quest first transforms categorical (symbolic) variables into continuous variables by assigning discriminant coordinates to categories of the predictor. Then it applies quadratic discriminant analysis (QDA) to determine the split point. Notice that QDA usually produces two cut-off points—choose the one that is closer to the sample mean of the first superclass.</p>

<p>An advantage of the QUEST tree algorithm is that it is not biased in split-variable selection, unlike CART which is biased towards selecting split-variables which allow more splits, and those which have more missing values.</p>

<p>Not sure why it is not used as widely as CART or C5.0. It might be due to greater coverage of CART/C5.0 in literature than others.</p>

<p>References:</p>

<ol>
<li>Quest reference manual: <a href=""http://www.stat.wisc.edu/~loh/treeprogs/guide/guideman.pdf"" rel=""nofollow"">http://www.stat.wisc.edu/~loh/treeprogs/guide/guideman.pdf</a></li>
<li><a href=""http://www.stat.wisc.edu/~loh/quest.html"" rel=""nofollow"">http://www.stat.wisc.edu/~loh/quest.html</a></li>
</ol>
"
"['python', 'pandas', 'scraping']",How to scrape a table from a webpage?,"<p>This will give you all the  values under <code>&lt;tr&gt;</code>:</p>

<pre><code>bs=BeautifulSoup(data, ""lxml"")
table_body=bs.find('tbody')
rows = table_body.find_all('tr')
for row in rows:
    cols=row.find_all('td')
    cols=[x.text.strip() for x in cols]
    print cols
</code></pre>
"
"['machine-learning', 'predictive-modeling', 'ranking']",Best way to format data for supervised machine learning ranking predictions,"<p>This problem looks like a lot with the problem of ranking college football teams. I have never worked on this ranking problem, but I believe you can borrow some tools used there to build your model.</p>

<p>Here goes a couple of references:</p>

<p><a href=""http://www.colleyrankings.com/matrate.pdf"" rel=""nofollow"">Colley Matrix Rankings</a> - This was one of computer rankings used by the BCS. It is also the only one that shared their methodology.</p>

<p><a href=""http://public.gettysburg.edu/~cwessell/RankingPage/colley.pdf"" rel=""nofollow"">An example of the Colley Matrix Rankings</a> - An easy example to follow.</p>

<p><a href=""https://umdrive.memphis.edu/ccrousse/public/MATH%207375/PERRON.pdf"" rel=""nofollow"">The Perron-Frobenius Theorem and the Ranking of Football Teams</a> - A well known reference that presents some ranking methods. This paper also shows how to assess the probability of winning a game based on the rankings of the teams.</p>
"
['graphs'],"Graphlab, plots are invisible","<p>I have found some solution and will post it here, because somebody, who works with graphlab, can have the same question.</p>

<p>We can look at the example here: <a href=""http://graphlab.com/learn/gallery/notebooks/graph_analytics_movies.html"" rel=""nofollow"">Six degrees of Kevin Bacon</a></p>

<p>At te beginning of the program execution you need to run next command:</p>

<pre><code>graphlab.canvas.set_target('ipynb')
</code></pre>

<p>Exactly this is a key of the whole problem (at least by me:-)</p>

<p>At the beginning it is important to know, which parameter of <code>set_target()</code> command you want to execute. You can use two options for argument of this command:  <code>'ipynb'</code> (which is executed direct in iPython Notebook, like in example) or <code>'browser'</code> (which open the new window with the plots)
On my machine 64-bit, Ubuntu, I can not use the command <code>'ipynb'</code>. Only the <code>'browser'</code>-command get me the plot back. I don't think, it is necessary to change https to http, but you can do it anyway. We have also the other machine by us (32-bit, Ubuntu) and it executes the other command <code>'ipynb'</code>, but not <code>'browser'</code> (without to change https to http)</p>
"
['usecase'],Do you have any real example of Data Science reports?,"<p>I think you can better go through various university thesis reports and data science related journal papers to get more details on ""Including hypothesis, testing, reports, conclusions"" of the above mentioned Data science related problems.</p>

<p>Check these links from Stanford university :</p>

<p><a href=""http://cs229.stanford.edu/projects2014.html"" rel=""nofollow"">http://cs229.stanford.edu/projects2014.html</a></p>

<p><a href=""http://cs229.stanford.edu/projects2013.html"" rel=""nofollow"">http://cs229.stanford.edu/projects2013.html</a></p>

<p><a href=""http://cs229.stanford.edu/projects2012.html"" rel=""nofollow"">http://cs229.stanford.edu/projects2012.html</a></p>
"
['optimization'],Which Optimization method to use?,"<p><a href=""http://en.wikipedia.org/wiki/Bayesian_optimization"" rel=""nofollow"">Bayesian optimization</a> is a principled way of sequentially finding the extremum of black-box functions. What's more, there a numerous software packages that make it easy, such as <a href=""http://rmcantin.bitbucket.org/html/"" rel=""nofollow"">BayesOpt</a> and <a href=""http://engineeringblog.yelp.com/2014/07/introducing-moe-metric-optimization-engine-a-new-open-source-machine-learning-service-for-optimal-ex.html"" rel=""nofollow"">MOE</a>. Another flexible Bayesian framework that you can use for optimization is <a href=""http://en.wikipedia.org/wiki/Gaussian_process"" rel=""nofollow"">Gaussian processes</a>: <a href=""http://ml.dcs.shef.ac.uk/gpss/gpss13/talks/Sheffield-GPSS2013-Osborne.pdf"" rel=""nofollow"">Global Optimisation with Gaussian Processes</a></p>
"
"['machine-learning', 'data-mining', 'bigdata', 'predictive-modeling']","Could someone please offer me some guidance on some kind of particular, SPECIFIC project that I could attemp, to ""get my feet wet, so to speak""","<p>I would suggest Kaggle learning projects - <a href=""http://www.kaggle.com/competitions"" rel=""nofollow"">http://www.kaggle.com/competitions</a></p>

<p>Look for the ones in the 101 section that offer knowledge. There's many pre-made solutions ready, which you can ingest and try variations of.</p>

<p>Also, I have bookmarked a <a href=""http://www.analyticsvidhya.com/blog/learning-path-data-science-python/"" rel=""nofollow"">Comprehensive learning path – Data Science in Python</a>, which among other things gives a few answers to your specific question.</p>
"
"['dataset', 'programming']",Which programming languages should I use to integrate different analysis software packages?,"<p>If you are mostly stitching up together calls into other software, like unix utilities (awk, grep, sed ...), python, and matlab scripts, bash <em>is just fine</em> or possibly even best for the job to construct simple pipelines and workflows.</p>

<p>It's easy in bash to read user input, store it in variables, then launch other software depending on the set variables. It's perfectly fast enough for that, and nothing else gets any easier.</p>

<p>If you, however, were to use bash for preprocessing itself, like looping through files line by line, packing and unpacking tab-separated values into arrays etc., that would be excruciatingly slow and not recommended.</p>
"
"['machine-learning', 'neuralnetwork', 'predictive-modeling', 'time-series', 'deep-learning']",Predicting next action to take to reach a final state,"<p>The problem you've described can be formalized as a <a href=""https://en.wikipedia.org/wiki/Markov_decision_process"" rel=""nofollow"">Markov decision process</a>, the foundational problem of <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow"">reinforcement learning</a>. In broad strokes, reinforcement learning is concerned with how agents (robot) in a given environment (room) out to take actions (movements from one state to another) to maximize some notion of reward. </p>

<p>Formalizing your problem requires defining a few parts of an MDP model:</p>

<ul>
<li>Set of states $S$</li>
<li>Set of actions $A$</li>
<li>A reward function $R(s)$, defining the reward of arriving in a given state. In your case, a simple scheme of $R(s) = \mathbb{1}(s = s_{goal})$ is one option.</li>
<li>A transition function $T(s,a,s')$ giving the probability of winding up in state $s'$ having taken action $a$ from state $s$.(Note that you can model deterministic transitions by returning a value of $1$ for a single $s'$.)</li>
</ul>

<p>If the problem goes on infinitely, you'll also need a discount factor $\gamma$.</p>

<p>In reinforcement learning, the term <strong>optimal policy</strong> describes a function that returns the best action to take from a given state. I.e., this gives the recommendation you're looking for.</p>

<p>If you <em>know</em> all of the model components described above—or can at least derive ones that match your problem—you can use a variety of planning algorithms to find the optimal policy, e.g. <a href=""https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/node19.html"" rel=""nofollow"">value iteration</a> or <a href=""https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/node20.html"" rel=""nofollow"">policy iteration</a>. </p>

<p>If you don't know the rewards and transitions—say, for example, that the robot is seeking a sensor attached to a charging station and you don't know where it is or the size of the enclosing room—you'll likely need to explore algorithms that observe action-outcome pairs and try to learn optimal policy from these learning episodes. </p>

<p>A full description of these is beyond the scope of your question, but a good place to start is Sutton &amp; Barto's <em>Reinforcement Learning: An Introduction</em>. An <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html"" rel=""nofollow"">html version</a> is freely available. Another resource is <a href=""https://www.udacity.com/course/reinforcement-learning--ud600"" rel=""nofollow"">RL udacity course</a> produced by Georgia Tech.</p>

<p>In your example, you may also want to research potential-based reward functions. Very loosely, one potential might be the robot's distance to goal state, and the reward would be based on changes to this potential value. (This is described in a <a href=""http://www.cs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf"" rel=""nofollow"">paper</a> by Ng, Harada and Russell, and in unit 6 of the GA Tech course mentioned above.)</p>
"
"['statistics', 'social-network-analysis']",Where the following propagation model arise in practice,"<p>Imagine each node in your graph as a user, and each edge as an action such as 'share'. It may also be a bidirectional relationship 'share' and 'view'.</p>

<p>Some social scientists and engineers estimate the probability that a message is 'shared' and 'viewed' given that a particular user decides to share it. This process is called ""information diffusion"", ""information propagation"", ""cascading"", etc.</p>

<p>If you are interested in the details of how these calculations are performed, check out these papers:</p>

<p><a href=""http://cs.stanford.edu/people/jure/pubs/netrate-netsci14.pdf"" rel=""nofollow"">http://cs.stanford.edu/people/jure/pubs/netrate-netsci14.pdf</a></p>

<p><a href=""http://cs.stanford.edu/people/jure/pubs/cascades-www14.pdf"" rel=""nofollow"">http://cs.stanford.edu/people/jure/pubs/cascades-www14.pdf</a></p>

<p><a href=""http://cs.stanford.edu/people/jure/pubs/infopath-wsdm13.pdf"" rel=""nofollow"">http://cs.stanford.edu/people/jure/pubs/infopath-wsdm13.pdf</a></p>

<p>Similar topics:
<a href=""http://snap.stanford.edu/papers.html"" rel=""nofollow"">http://snap.stanford.edu/papers.html</a></p>
"
"['bigdata', 'hadoop', 'efficiency', 'scalability', 'performance']",When it is time to use Hadoop?,"<p>It's an economic calculation, really.  When you have a computing ""problem"" (in the most general possible sense) that you can't solve with one computer, it makes sense to use a cluster of commodity machines when doing so A. allows you to solve the problem, and B. is cheaper than a forklift upgrade to a bigger computer, or upgrading to specialized hardware.</p>

<p>When those things are true, and you are going the ""commodity cluster"" route, Hadoop makes a lot of sense, especially if the nature of the problem maps (no pun intended) well to MapReduce.  If it doesn't, one shouldn't be scared to consider ""older"" cluster approaches like a Beowulf cluster using MPI or OpenMP.  </p>

<p>That said, the newer YARN based Hadoop does support a form of MPI, so those worlds are starting to move closer together.</p>
"
"['machine-learning', 'dataset', 'neuralnetwork', 'time-series', 'regression']",training neural net with multiple sets of time-series data,"<p>Yes, this is a straightforward application for neural networks.  In this case yk are the outputs of the last layer (""classifier""); xk is a feature vector and yk is what it gets classified into.  For simplicity prepare your data so that N is the same for all.  The problem you have is perhaps that in the case of time series you won't have <em>enough</em> data: you need (ideally) many 1000's of examples to train a network, which in this case means time series, not points.  Look at the specialized literature on neural networks for time series prediction for ideas on network architecture.</p>

<p>Library: try Pylearn2 at <a href=""http://deeplearning.net/software/pylearn2/"" rel=""nofollow"">http://deeplearning.net/software/pylearn2/</a>  It's not the only good option but it should serve you well.</p>
"
['markov'],How can I predict the post popularity of reddit.com with hidden markov model(HMM)?,"<p>An HMM doesn't really make sense (echoing what Dries said). If you want to use an HMM, you would have to justify it by asking ""Can Reddit posts be represented by a Markov process?"" I can't think of a way to make that sentence true and still take advantage of the features related to a popular post.</p>

<p>Consider the possible feature set: the time it was posted, the user posting it, the type of post (link / image / text), the subreddit, the number of subscribers to that subreddit, a score of positivity / negativity, number of words in the title etc. Don't count out these features.</p>
"
"['classification', 'sklearn', 'multiclass-classification', 'unbalanced-classes']",Balanced Linear SVM wins every class except One vs All,"<p>I think the following synthetic example explains what is going on:</p>

<pre><code>from sklearn.base import clone
from sklearn.svm import SVC
import numpy as np
from sklearn.datasets import make_blobs

X, y = make_blobs(1000, centers=[[1, 1], [-1, -1], [1, -1]], cluster_std=0.8)

models = [
    ('normal', SVC()),
    ('balanced', SVC(class_weight='balanced')),
]

import matplotlib.pyplot as plot
plot.ioff()

for i, (name, m) in enumerate(models):
    for k in np.unique(y):
        m = clone(m).fit(X, (y == k).astype(int))

        xx, yy = np.meshgrid(np.arange(-3, 3, 0.1), np.arange(-3, 3, 0.1))
        z = m.predict(np.c_[xx.ravel(), yy.ravel()])
        z = z.reshape(xx.shape)
        colors = ['blue', 'green', 'red']
        plot.contour(xx, yy, z, colors=colors[k])
    plot.scatter(X[:, 0], X[:, 1], c=y)
    plot.title(name)
    plot.show()
</code></pre>

<p><a href=""http://i.stack.imgur.com/REd4p.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/REd4p.png"" alt=""normal SVM""></a> <a href=""http://i.stack.imgur.com/euDKR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/euDKR.png"" alt=""balanced SVM""></a></p>

<p>The balanced SVM is clearly superior when considering each model individually, at least as measured by the F1 score, which punishes false positives and false negatives equally.</p>

<p>But One-vs-Rest uses maximum score, which in this case is given by the distance to the decision hyperplane. It is not yet clear why this is worse in some cases, but clearly it is possible for one model to beat another for individual classes, but to lose for One-vs-Rest: how well the relative distances are modeled is what counts in One-vs-Rest.</p>
"
"['python', 'pandas']",How to binary encode multi-valued categorical variable from Pandas dataframe?,"<p>If <code>[0, 1, 2]</code> are numerical labels and is not the index, then <code>pandas.DataFrame.pivot_table</code> works:</p>

<pre>
In []:
data = pd.DataFrame.from_records(
    [[0, 'A'], [0, 'B'], [1, 'B'], [1, 'C'], [1, 'D'], [2, 'B'], [2, 'D']],
    columns=['number_label', 'category'])
data.pivot_table(index=['number_label'], columns=['category'], aggfunc=[len], fill_value=0)
</pre>

<pre>
Out[]:
              len
category      A      B      C      D
number_label                       
0             1      1      0      0
1             0      1      1      1
2             0      1      0      1
</pre>

<p>This blog post was helpful: <a href=""http://pbpython.com/pandas-pivot-table-explained.html"" rel=""nofollow"">http://pbpython.com/pandas-pivot-table-explained.html</a></p>

<hr>

<p>If <code>[0, 1, 2]</code> is the index, then <code>collections.Counter</code> is useful:</p>

<pre>
In []:
data2 = pd.DataFrame.from_dict(
    {'categories': {0: ['A', 'B'], 1: ['B', 'C', 'D'], 2:['B', 'D']}})
data3 = data2['categories'].apply(collections.Counter)
pd.DataFrame.from_records(data3).fillna(value=0)
</pre>

<pre>
Out[]:
       A      B      C      D
0      1      1      0      0
1      0      1      1      1
2      0      1      0      1
</pre>
"
"['machine-learning', 'algorithms', 'recommendation', 'recommender-system']",Match users based on the content of their articles,"<p>One way could be to apply word-embeddings for semantic similarity checking. word2vec model generates feature vectors which could capture semantic similarity. For example, the closest vector to <code>car</code> will be <code>honda, ferrari, vehicle, bike</code>. Train a model using large amount of data from wikipedia dumps or the one released by google. It has fine quality vectors. <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow"">Gensim</a> has a nice implementation of word2vec</p>

<p>For each blog articles, pre-process the data by removing the stop-words and stemming them. From the resulting words, collect the more frequent words. Do this for all the articles and check for the similarity among the frequent words in other articles as well. So that one article with frequent words <code>car, race, tournament, ferrari, F1</code> will have more closer vectors in article with frequent words <code>bike, honda, racer</code>.</p>

<p>Or other way is to look for similar vectors in tags itself. It is good to play with it for sometime, so that you get to know which features work better for the dataset you have.</p>
"
"['r', 'data-cleaning', 'data']",Data frame mutation in R,"<p>The fastest way would be,</p>

<pre><code>require(data.table)
data &lt;- data.table(data)
# Remove the percentage from your file and convert the field to numeric.
data[, Profit := as.numeric(gsub(""%"", """", Profit))]

data
##           Symbol      Date     Time Profit
##  1: BANKNIFTY  4/1/2010  9:55:00  -1.18
##  2: BANKNIFTY  4/1/2010 12:30:00  -2.84
##  3: BANKNIFTY  4/1/2010 12:45:00   7.17
##  4: BANKNIFTY  5/1/2010 11:40:00  -7.11
##  5:      ZEEL 26/6/2012 13:50:00  24.75
##  6:      ZEEL 27/6/2012 15:15:00  -1.90
##  7:      ZEEL 28/6/2012  9:45:00  37.58
##  8:      ZEEL 28/6/2012 14:55:00  23.95
##  9:      ZEEL 29/6/2012 14:20:00  -4.65
## 10:      ZEEL 29/6/2012 14:30:00  -6.01
## 11:      ZEEL 29/6/2012 14:55:00 -12.23
## 12:      ZEEL 29/6/2012 15:15:00  35.13

# Melt the data so that we can easily dcast afterwards.
molten_data &lt;- melt(data[, list(Symbol, Date, Profit)]

# Create a summary by date and Symbol.
dcast(molten_data, id = c(""Symbol"", ""Date"")), Date ~ variable + Symbol, fun = sum)

 ##         Date Profit_BANKNIFTY Profit_ZEEL
 ## 1: 26/6/2012             0.00       24.75
 ## 2: 27/6/2012             0.00       -1.90
 ## 3: 28/6/2012             0.00       61.53
 ## 4: 29/6/2012             0.00       12.24
 ## 5:  4/1/2010             3.15        0.00
 ## 6:  5/1/2010            -7.11        0.00
</code></pre>
"
"['machine-learning', 'data-mining', 'graphs']",Estimating destination according to previous data,"<p>This is a spatio-temporal clustering problem that is likely best solved with a Markov model. You could reasonable group this into machine learning or data mining. Develop your model using machine learning and then (the data mining part) leverage those pattern recognition techniques (that have been developed in machine learning). I think there are at least one or two threads on this over at Cross-Validated that go into more detail.  </p>

<p>Here are a couple of papers to look at if you are just getting started.</p>

<p><a href=""http://link.springer.com/article/10.1007/s00779-003-0240-0"" rel=""nofollow"">Using GPS to learn significant locations and predict movement across multiple users</a></p>

<p><a href=""http://lbsn2012.cmuchimps.org/papers/Paper16_Mathew.pdf"" rel=""nofollow"">Predicting Future Locations with Hidden Markov Models</a></p>
"
"['classification', 'neuralnetwork', 'supervised-learning']",Which value of output should be taken in multiple sessions of training Neural Network,"<p>This is normal behaviour of most classifiers. You are not guaranteed 100% accuracy in machine learning, and a direct consequence is that classifiers make mistakes. Different classifiers, even if trained on the same data, can make different mistakes. Neural networks with different starting weights will often converge to slightly different results each time.</p>

<p>Also, perhaps in your problem the classification is an artificial construct over some spectrum (e.g. ""car"" vs ""van"" or ""safe"" vs ""dangerous"") in which case the mistake in one case is entirely reasonable and expected?</p>

<p>You should use the value from the classifier that you trust the most. To establish which one that is, use cross-validation on a hold-out set (where you know the true labels), and use the classifier with the best accuracy, or other metric, such as logloss or area under ROC. Which metric you should prefer depends on the nature of your problem, and the consequences of making a mistake.</p>

<p>Alternatively, you could look at averaging the class probabilities to determine the best prediction - perhaps one classifier is really confident in the class assignment, and the other is not, so an average will go with the first classifier. Some kind of model aggregation will often boost accuracy, and is common in e.g. Kaggle competitions when you want the highest possible score and don't mind the extra effort and cost. However, if you want to use aggregation to solve your problem, again you should test your assumptions using validation and a suitable metric so you know whether or not it is really an improvement. </p>
"
"['r', 'classification', 'feature-selection', 'categorical-data']",Identifying top predictors from a mix of categorical and ordinal data,"<p>Ricky,</p>

<p>Loose thoughts: </p>

<ul>
<li>Depending on the algorithm you intend to use, centering might not be a good idea (e.g. if you go for SVM, centering will destroy sparsity)</li>
<li>I would suggest not to handle ordered / unordered separately, as <strong>you are likely to miss interactions that way</strong>. If the categorical ones don't have too many possible values, randomForest in R can handle factors.</li>
<li>if that is an issue (as you seem to hint), I think you have two possibilities: binary indicators or response rates</li>
<li>if it's feasible in terms of computational cost, i would convert all factors to binaries (use sparse matrices if necessary) and then try a greedy feature selection. caret, if memory serves, has rfe or somesuch.</li>
<li>if that's too much trouble, try calculating response rates / average values per factor level (I don't see any info whether your problem is classification or regression): you split your set into folds, and then for each fold fit a mixed effects model (e.g. via lme4) on the remainder, using the factor of interest as the main variable. It's a bit of a pain to setup all the cv correctly, but it's the only way to avoid leaking information.</li>
</ul>

<p>Hope this helps,
K</p>
"
['statistics'],Methods for standardizing / normalizing different rank scales,"<p>For item-ratings type of data with the restriction that an item's rating should be between 1 and 10 after transformation, I would suggest using a simple re-scaling, such that the item's transformed rating $x_t$ is given by:</p>

<p>$$x_t = 9\left(\frac{x_i - x_{min}}{x_{max} - x_{min}}\right) + 1$$</p>

<p>where $x_{min}$ and $x_{max}$ are the minimum and maximum possible rating in the specific scale for the item, and $x_i$ is the item rating. </p>

<p>In the case of the above scaling, the transformation applied is independent of the data. However, in the normalization, the transformation applied is dependent on the data (through mean and standard deviation), and might change as more data becomes available.</p>

<p>Section 4.3 on page 30 of <a href=""http://www.dai-labor.de/fileadmin/files/publications/DiplomaThesisStephanSpiegel.pdf"" rel=""nofollow"">this document</a> shows other ways of normalizing in which your restriction (transforming to the same absolute scale) might not be preserved. </p>
"
"['machine-learning', 'predictive-modeling', 'kaggle']",Hashing Trick - what actually happens,"<p>The a second bullet is the value in feature hashing.  Hashing and one hot encoding to sparse data saves space.  Depending on the hash algo you can have varying degrees of collisions which acts as a kind of dimensionality reduction.</p>

<p>Also, in the specific case of Kaggle feature hashing and one hot encoding help with feature expansion/engineering by taking all possible tuples (usually just second order but sometimes third) of features that are then hashed with collisions that explicitly create interactions that are often predictive whereas the individual features are not.</p>

<p>In most cases this technique combined with feature selection and elastic net regularization in LR acts very similar to a one hidden layer NN so it performs quite well in competitions.</p>
"
['algorithms'],Are there any unsupervised learning algorithms for time sequenced data?,"<p>What you have is a sequence of events according to time so do not hesitate to call it Time Series!</p>

<p>Clustering in time series has 2 different meanings:</p>

<ol>
<li><strong>Segmentation of time series</strong> i.e. you want to segment an individual time series into different time intervals according to internal similarities.</li>
<li><strong>Time series clustering</strong> i.e. you have several time series and you want to find different clusters according to similarities between them.</li>
</ol>

<p>I assume you mean the second one and here is my suggestion:</p>

<p>You have many vehicles and many observations per vehicle i.e you have many vehicles. So you have several matrices (each vehicle is a matrix) and each matrix contains <strong>N</strong> rows (Nr of observations) and <strong>T</strong> columns (time points). One suggestion could be applying PCA to each matrix to reduce the dimenssionality and observing data in PC space and see if there is meaningful relations between <em>different observations within a matrix (vehicle)</em>. Then you can put each observation for all vehicles on each other and make a matrix and apply PCA to that to see relations of a single observation between different vehicles. </p>

<p>If you do not have negative values <strong>Matrix Factorization</strong> is strongly recommended for dimension reduction of matrix form data.</p>

<p>Another suggestion could be putin all matrices on top of each other and build a <strong>N</strong>x<strong>M</strong>x<strong>T</strong> tensor where <em>N</em> is the number of vehicles, <em>M</em> is the number of observations and <em>T</em> is the time sequence and apply <strong>Tensor Decomposition</strong> to see relations globally.</p>

<p>A very nice approach to Time Series Clustering is shown in <a href=""https://www.researchgate.net/profile/Christian_Bauckhage/publication/262898400_Kernel_Archetypal_Analysis_for_Clustering_Web_Search_Frequency_Time_Series/links/5416e0770cf2bb7347db7a99.pdf"">this</a> paper where the implementation is quiet straight forward.</p>

<p>I hope it helped!</p>

<p>Good Luck :)</p>

<hr>

<h2>EDIT</h2>

<p>As you mentioned you mean Time Series Segmentation I add this to the answer.</p>

<p>Time series segmentation is the only clustering problem that has a ground truth for evaluation. Indeed you consider the generating distribution behind the time series and analyze it I strongly recommend <a href=""http://machinelearning.wustl.edu/mlpapers/papers/icml2014c1_khaleghi14"">this</a>, <a href=""https://tel.archives-ouvertes.fr/tel-00920184/document"">this</a>, <a href=""http://128.84.21.199/pdf/1203.1515.pdf"">this</a>, <a href=""http://arxiv.org/pdf/1203.1515.pdf"">this</a>, <a href=""http://papers.nips.cc/paper/4623-locating-changes-in-highly-dependent-data-with-unknown-number-of-change-points"">this</a> and <a href=""http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2012_KhaleghiRMP12.pdf"">this</a> where your problem is comprehensively studied. Specially the last one and the PhD thesis.</p>

<p>Good Luck!</p>
"
['interpolation'],Geometric weighting of temperature,"<p>It sounds like you're describing optimal interpolation (AKA Gauss-Markov analysis, objective analysis, probably others too). <a href=""http://modb.oce.ulg.ac.be/wiki/upload/diva_intro.pdf"" rel=""nofollow"">This</a> is a solid intro, and <a href=""http://www.atmos.berkeley.edu/~inez/MSRI-NCAR_CarbonDA/lectures/Kalnay2_CH5OptimalInterpolation.pdf"" rel=""nofollow"">this</a> is a powerpoint on the subject. It's a hard process to summarize quickly, but roughly speaking, you're on the right track.</p>

<p>Optimal interpolation is fairly common in meteorology and other environmental sciences, but I'm not convinced it's really the best tool for the job. Any statisticians want to take a crack at it?</p>
"
"['machine-learning', 'r']",rpart and rpart2,"<p>Both <code>rpart</code> and <code>rpart2</code> implement a CART and wrap the <code>rpart</code> function from the <strong>rpart</strong> library.  The difference is the constraints on the model each enforces.  <code>rpart</code> uses the complexity parameter, <code>cp</code>, while <code>rpart2</code> uses the max tree depth, <code>maxdepth</code>.</p>

<p>See: <code>train_model_list</code> section of <a href=""http://cran.r-project.org/web/packages/caret/caret.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/caret/caret.pdf</a>  and <code>rpart.control</code> section of <a href=""http://cran.r-project.org/web/packages/rpart/rpart.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/rpart/rpart.pdf</a>.</p>
"
"['machine-learning', 'classification']",kNN - what happens if more than K observation have the same distance to the centroid of the cluster,"<p>You are mixing up kNN classification and k-means.</p>

<p>There is nothing wrong with having more than k observations near a center in k-means. In fact, this it the usual case; you shouldn't choose k too large. If you have 1 million points, a k of 100 may be okay. <em>K-means does not guarantee clusters of a particular size</em>. Worst case, clusters in k-means can have only one element (outliers) or even disappear.</p>

<p>What you probably meant to write, but got mixed up, is <strong>what to do if a point is at the same distance to two centers</strong>.</p>

<p>From a statistical point of view, it doesn't matter. Both have the same squared error.</p>

<p>From an implementation point of view, choose any <em>deterministic</em> rule, so that your algorithm converges and doesn't go into an infinite loop of reassignment.</p>

<p><strong>Update:</strong> with respect to kNN classification:</p>

<p>There are many ways to resolve this, that will surprisingly often work just as good as the other, without a clear advantage of one over the other:</p>

<ol>
<li>randomly choose a winner from the tied objects</li>
<li>take all into account with equal weighting</li>
<li>if you have m objects at the same distance where you expected only r, then put a weight of r/k on each of them.</li>
</ol>

<p>E.g. k=5.</p>

<pre><code>distance   label   weight
    0        A       1
    1        B       1
    1        A       1
    2        A      2/3
    2        B      2/3
    2        B      2/3
</code></pre>

<p>yields A=2.66, B=2.33</p>

<p>The reason that randomly choosing works just as good as the others is that usually, the majority decision in kNN will not be changed by contributions with a weight of less than 1; in particular when k is larger than say 10.</p>
"
['bayesian-networks'],Is the direction of edges in a Bayes Network irrelevant?,"<p>TL;DR: sometimes you can make an equivalent Bayesian network by reversing arrows, and sometimes you can't.</p>

<p>Simply reversing the direction of the arrows yields another directed graph, but that graph is not necessarily the graph of an equivalent Bayesian network, because the dependence relations represented by the reversed-arrow graph can be different from those represented by the original graph. If the reversed-arrow graph represents different dependence relations than the original, in some cases it's possible to create an equivalent Bayesian network by adding some more arrows to capture dependence relations which are missing in the reversed-arrow graph. But in some cases there is not an exactly equivalent Bayesian network. If you have to add some arrows in order to capture dependencies, you might end up with a graph which represents fewer independence relations and therefore fewer opportunities for simplifying the computations of posterior probabilities.</p>

<p>For example, <code>a -&gt; b -&gt; c</code> represents the same dependencies and independencies as <code>a &lt;- b &lt;- c</code>, and the same as <code>a &lt;- b -&gt; c</code>, but not the same as <code>a -&gt; b &lt;- c</code>. This last graph says that <code>a</code> and <code>c</code> are independent if <code>b</code> is not observed, but <code>a &lt;- b -&gt; c</code> says <code>a</code> and <code>c</code> are dependent in that case. We can add an edge directly from <code>a</code> to <code>c</code> to capture that, but then <code>a</code> and <code>c</code> being independent when <code>b</code> is observed is not represented. That means that there is at least one factorization which we cannot exploit when computing posterior probabilities.</p>

<p>All this stuff about dependence/independence, arrows and their reversals, etc., is covered in standard texts on Bayesian networks. I can dig out some references if you want. </p>

<p>Bayesian networks don't express causality. Judea Pearl, who did a lot of work on Bayesian networks, has also worked on what he calls causal networks (essentially Bayesian networks annotated with causal relations).</p>
"
"['r', 'hadoop']",java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream,"<p>This issue is caused because the following environment variables are not set correctly
PKG_CONFIG_PATH and LD_LIBRARY_PATH
for my configuration, i should execute this:</p>

<p>export PKG_CONFIG_PATH = /usr/local/lib <br>
export LD_LIBRARY_PATH = /usr/local/lib</p>

<p>also set this commands on R console: </p>

<p>Sys.setenv(HADOOP_HOME=""/usr/local/hadoop/"")<br>
Sys.setenv(HADOOP_BIN=""/usr/local/hadoop/bin"")<br>
Sys.setenv(HADOOP_CONF_DIR=""/usr/local/hadoop/conf"")<br></p>
"
"['machine-learning', 'python', 'neuralnetwork']",Pylearn2 vs TensorFlow,"<p>You might want to take into consideration that <a href=""https://github.com/lisa-lab/pylearn2/pull/1565"">Pylearn2 has no more developer</a>, and now <a href=""https://github.com/lisa-lab/pylearn2/commit/48380daba195db8cf2a53ec86139a1ae7af89d7b#diff-88b99bb28683bd5b7e3a204826ead112"">points to other Theano-based libraries</a>:</p>

<blockquote>
  <p>There are other machine learning frameworks built on top of Theano that could interest you, such as: Blocks, Keras and Lasagne.</p>
</blockquote>

<p>As Dawny33 says, TensorFlow is just getting started, but it is interesting to note that the number of questions on TensorFlow (244) on Stack Overflow already surpasses Torch (166) and will probably catch up with Theano (672) in  2016.</p>
"
"['predictive-modeling', 'data-cleaning']",Web Framework Built for Recommendations,"<p>I haven't seen anything like that and very much doubt that such frameworks exist, at least, as complete frameworks. The reason for this is IMHO the fact that data <em>transformation</em> and <em>cleaning</em> is very <strong>domain-</strong> and <strong>project-specific</strong>. Having said that, there are multiple tools that can help with these activities in terms of <em>partial automation</em> and <em>integration</em> with and between existing statistical and Web frameworks.</p>

<p>For example, for <strong>Python</strong>, the use of <em>data manipulation</em> library <code>pandas</code> as well as <em>machine learning</em> library <code>scikit-learn</code> can be easily integrated with Web frameworks (especially Python-based, but not necessarily), as these libraries are also Python-based. These and other Python data science tools that might be of interest can be found here: <a href=""http://pydata.org/downloads"" rel=""nofollow"">http://pydata.org/downloads</a>. Specifically, for cleaning and pre-processing tasks, which you asked about, <code>pandas</code> seem to be the first tool to explore. Again, for Python, the following discussion on StackOverflow on methods and approaches might be helpful: <a href=""http://stackoverflow.com/q/14262433/2872891"">http://stackoverflow.com/q/14262433/2872891</a>.</p>

<p>Consider an example of <strong>another platform</strong>. The use of <code>pandas</code> for data transformation and cleaning is rather <em>low-level</em>. The platform that I like very much and currently use as the platform of choice for data science tasks is <code>R</code>. Rich <em>ecosystem of R packages</em> especially shines in the area of data transformation and cleaning. This is because, in addition to very flexible low-level methods of performing these tasks, there are some R packages, which take a <em>higher-level approach</em> to the problem, which may potentially improve developer's productivity and decrease the amount of defects. In particular, I'm talking about two packages, which I find very promising: <code>editrules</code> and <code>deducorrect</code>. You can find more detailed information about these and other R packages for data transformation and cleaning in my another answer here on <em>Data Science StackExchange</em> (paper that I reference in the last link there could be especially useful, as it presents an <em>approach</em> to data transformation and cleaning that is <em>generic</em> enough, so that could be used as a <strong>framework</strong> for this on any decent platform): <a href=""http://datascience.stackexchange.com/a/722/2452"">http://datascience.stackexchange.com/a/722/2452</a>.</p>

<p><strong>UPDATE</strong>: On the topic of <em>recommender systems</em> and their integration with data wrangling tools and Web frameworks, you may find my other answer here on DS SE useful: <a href=""http://datascience.stackexchange.com/a/836/2452"">http://datascience.stackexchange.com/a/836/2452</a>.</p>
"
"['r', 'dataset']",problem loading data into R,"<p>I am able to load the data set like so:</p>

<pre><code> dev &lt;- read.table(""iran_it_status_1394_detail_data_jadi_net.tsv"",
                   header=TRUE, sep=""\t"", blank.lines.skip = TRUE,
                   na.strings="""",
                   stringsAsFactors=FALSE, skipNul = TRUE, fill=T, quote="""")
</code></pre>

<p>Note the removal of the encoding (so that the function ""finds"" lines in the file), the fill attribute (to allow for a ragged table with empty cells), and the elimination of quotes (apparently there is a misquoted line somewhere near line 585).  </p>

<p>This results in a table full of encoded characters - you'll need to know more about the source data to figure out how to work with it, but if you open the file up in a raw text editor (e.g.: Sublime text) you might get some clues:</p>

<p><a href=""http://i.stack.imgur.com/xLG1L.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xLG1L.png"" alt=""Character encoding issues?""></a></p>
"
"['bigdata', 'software-recommendation']",Dealing with big data,"<blockquote>
  <p>What criteria should I mainly take into account before selecting a
  tool for analysing big data?</p>
</blockquote>

<p>There are a lot of criterion which are to be taken into account when the tool selection is concerned. The can be:</p>

<ul>
<li>Structure of the data. (The data model Ex: Hierarchical, tabular, etc)</li>
<li>Type of data and what is the problem statement.   (time series, or classification, etc)</li>
<li>Speed</li>
<li>Security</li>
</ul>

<blockquote>
  <p>Aim : To develop a software that analyses the data coming from the
  wind mills and generate reports. The software should be able to
  predict when a wind mill can fail based on the analysis.</p>
</blockquote>

<p>Almost all the existing analytics tools like Python, Julia, R, etc can do this.</p>

<blockquote>
  <p>And is it also possible that I find an algorithm for predictive
  analysis without knowing what would be the results of the tool after
  analysis.</p>
</blockquote>

<p><strong>Yes.</strong>  The predictive algorithm or technique can be inferred by looking at the data and the contents of the data. <strong>It is not dependant on the tool.</strong></p>

<p>Some points which I would like to include which I believe would be useful to you:</p>

<ul>
<li>Select the database depending on your data, and it's type.  According to your data, a <a href=""https://en.wikipedia.org/wiki/NoSQL"" rel=""nofollow"">NoSQL database</a> would be more relevant and suitable.</li>
<li>Select the algorithms and techniques only after you have a clear knowledge about the problem statement and takeaways and also after clearly looking at the data for an <a href=""https://en.wikipedia.org/wiki/Exploratory_data_analysis"" rel=""nofollow"">exploratory analysis</a>.</li>
<li>If you want more flexibility, then use a tool/programming language like Python, R and Julia. Else, you can use a tool like Knime, Orange (it has a <a href=""https://pypi.python.org/pypi/Orange/2.7"" rel=""nofollow"">Python library</a> too.), RapidMiner, etc.</li>
</ul>
"
"['python', 'data-cleaning', 'weighted-data', 'data-wrangling']",Sort by average votes/ratings,"<p>Yes, this is a well-studied problem: rank aggregation. <a href=""http://www.evanmiller.org/how-not-to-sort-by-average-rating.html"" rel=""nofollow"">Here</a> is a solution with code.</p>

<p>The problem is that the quantity you are trying to estimate, the ""score"" of the item, is subject to noise. The fewer votes you have the greater the noise. Therefore you want to consider the variance of your estimates when ranking them.</p>
"
"['deep-learning', 'education', 'bayesian']",Is deep learning a must in a Data Science MSc programme?,"<p>No, it's not problematic. Most data scientists do not need or use deep learning. Deep learning is very popular right now, but that does not mean it's widely used. Deep learning can lead to substantial overfitting on small to medium datasets (I'm arbitrarily going to say that means less than 2 GB), which are the sizes that most people have. Deep learning is primarily used for object recognition in images, or text/speech models. If you're not doing either of these two things, you probably don't need to use DL. </p>
"
"['machine-learning', 'algorithms', 'clustering', 'k-means', 'hierarchical-data-format']",Efficient dynamic clustering,"<p>I think hierarchical clustering would be more time efficient in your case (with a single dimension).
Depending on your task, you may implement something like this:</p>

<p>Having N datapoints d<sub>i</sub> with their 1-dimension value x<sub>i</sub>:</p>

<ol>
<li>Sort datapoints based on their x<sub>i</sub> value. </li>
<li>Calculate distances between adjacent datapoints (N-1 distances). Each distance must be assigned a pair of original datapoints (d<sub>i</sub>, d<sub>j</sub>).</li>
<li>Sort distances in descending order to generate list of datapoint pairs (d<sub>i</sub>, d<sub>j</sub>), starting from the closest one.</li>
<li>Iteratively unite datapoints (d<sub>i</sub>, d<sub>j</sub>) into clusters, starting from beginning of the list (the closest pair). (Depending on current state of d<sub>i</sub> and d<sub>j</sub>, uniting them means: (a) creating new cluster for two unclustered datapoints, (b) adding a datapoint to existing cluster and (c) uniting two clusters.)</li>
<li>Stop uniting, if the distance is over some threshold.</li>
<li>Create singleton clusters for datapoints which did not get into clusters.</li>
</ol>

<p>This algorithm implements <a href=""http://en.wikipedia.org/wiki/Single-linkage_clustering"" rel=""nofollow"">single linkage</a> clustering. It can be tuned easily to implement average linkage. <a href=""http://en.wikipedia.org/wiki/Complete_linkage_clustering"" rel=""nofollow"">Complete linkage</a> will be less efficient, but maybe easier ones will give good results depending on your data and task.</p>

<p>I believe for 200K datapoints it must take under second, if you use proper data structures for above operations.</p>
"
"['data-mining', 'clustering', 'dbscan']",Clustering pair-wise distance dataset,"<p>In the scikit-learn implementation of Spectral clustering and DBSCAN you do not need to precompute the distances, you should input the sample coordinates for all <code>id_1</code> ... <code>id_n</code>.  Here is a simplification of the <a href=""http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html"" rel=""nofollow"">documented example comparison of clustering algorithms</a>:</p>

<pre><code>import numpy as np
from sklearn import cluster
from sklearn.preprocessing import StandardScaler

## Prepare the data
X = np.random.rand(1500, 2)
# When reading from a file of the form: `id_n coord_x coord_y`
# you will need this call instead:
# X = np.loadtxt('coords.csv', usecols=(1, 2))
X = StandardScaler().fit_transform(X)

## Instantiate the algorithms
spectral = cluster.SpectralClustering(n_clusters=2,
                                      eigen_solver='arpack',
                                      affinity=""nearest_neighbors"")
dbscan = cluster.DBSCAN(eps=.2)

## Use the algorithms
spectral_labels = spectral.fit_predict(X)
dbscan_labels = dbscan.fit_predict(X)
</code></pre>
"
"['machine-learning', 'predictive-modeling']","Which, if any, machine learning algorithms are accepted as being a good tradeoff between explainability and prediction?","<blockquote>
  <p>Is there are any literature enumerating the characteristics of algorithms which allow them to be explainable?</p>
</blockquote>

<p>The only literature I am aware of is the recent <a href=""https://arxiv.org/pdf/1602.04938v1.pdf"" rel=""nofollow"">paper</a> by Ribero, Singh and Guestrin. They first define explainability of a single prediction:</p>

<blockquote>
  <p>By “explaining a prediction”, we mean presenting
  textual or visual artifacts that provide qualitative understanding
  of the relationship between the instance’s
  components (e.g. words in text, patches in an image)
  and the model’s prediction.</p>
</blockquote>

<p>The authors further elaborate on what this means for more concrete examples, and then use this notion to define the explainability of a model. Their objective is to try and so-to-speak add  explainability artificially to otherwise intransparent models, rather than comparing the explainability of existing methods. The paper may be helpful anyway, as tries to introduce a more precise terminology around the notion of ""explainability"".</p>

<blockquote>
  <p>Are there machine learning models commonly accepted as representing a good tradeoff between the two?</p>
</blockquote>

<p>I agree with @Winter that elastic-net for (not only logistic) regression may be seen as an example for a good compromise between prediction accuracy and explainability. </p>

<p>For a different kind of application domain (time series), another class of methods also provides a good compromise: Bayesian Structural Time Series Modelling. It inherits explainability from classical structural time series modelling, and some flexibility from the Bayesian approach. Similar to logistic regression, the explainability is helped by regression equations used for the modelling. See <a href=""http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-with-bsts.pdf"" rel=""nofollow"">this paper</a> for a nice application in marketing and further references.</p>

<p>Related to the Bayesian context just mentioned, you may also want to look at probabilistic graphical models. Their explainability doesn't rely on regression equations, but on graphical ways of modelling; see ""Probabilistic Graphical Models: Principles and Techniques"" by Koller and Friedman for a great overview.</p>

<p>I'm not sure whether we can refer to the Bayesian methods above as a ""generally accepted good trade-off"" though. They may not be sufficiently well-known for that, especially compared to the elastic net example.</p>
"
"['machine-learning', 'dataset', 'data', 'learning']",Verification of trained system,"<p>Did you also validate the data set with a separate test set so you can see how it performs with known data?</p>

<p>This answer has a good run down on how to do that:
<a href=""http://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set"">http://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set</a></p>

<p>It also is a best practice to ensure that you haven't overfit the data. The easiest way to do that is check the accuracy against the data that you used to train the data.  If its not substantially better than the test data (this data should never be used to train a model) then you have not over fit.</p>

<p>After that, if you eventually find out the correct classification in your system, I would compare the accuracy there to the accuracy you experienced in the training and test environments to determine if its performing as intended. </p>
"
"['predictive-modeling', 'data', 'c']","Why aren't languages like C, C++ used for data analytics instead of R, Python?","<p>Yes, you're correct -- it's that C and C++ are harder to use and are more burdened with boilerplate code that obfuscates your model building logic. When you build models, you have to iterate rapidly and frequently, often throwing away a lot of your code. Having to write boilerplate code each time substantially slows you down over the long run.  Using R's caret package or Python's scikit-learn library, I can train a model in just 5-10 lines of code. </p>

<p>Ecosystem also plays a big role. For example, Ruby is easy to use, but the community has never really seen a need for machine learning libraries to the extent that Python's community has. R is more widely used than Python (for stats and machine learning only) because of the strength of its ecosystem and its long history catering to that need. </p>

<p>It's worth pointing out that most of these R and Python libraries are written in low-level languages like C or Fortran for their speed. For example, I believe Google's TensorFlow is built with C, but to make things easier for end users, its API is in Python. </p>
"
"['r', 'scraping']","""Results do not have equal lengths"" using ldply in R package plyr","<p>Here's a slightly modified version using some newer ""tidyverse"" packages:</p>

<pre><code>library(rvest) 
library(purrr) # flatten/map/safely
library(dplyr) # progress bar

# just in case there isn't a valid page
safe_read &lt;- safely(read_html)

fetch_current_players &lt;- function(letter){

  URL &lt;- sprintf(""http://www.baseball-reference.com/players/%s/"", letter)
  pg &lt;- safe_read(URL)

  if (is.null(pg$result)) return(NULL)

  player_data &lt;- html_nodes(pg$result, ""b a"")

  html_text(player_data)

}

pb &lt;- progress_estimated(length(letters))
player_list &lt;- flatten_chr(map(letters, function(x) {
  pb$tick()$print()
  fetch_current_players(x)
}))
</code></pre>
"
"['nlp', 'nltk']",Similarity between two words,"<p>The closest would be like Jan has mentioned inhis answer, the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow"">Levenstein's distance</a> (also popularly called the edit distance).</p>

<blockquote>
  <p>In information theory and computer science, the Levenshtein distance
  is a string metric for measuring the difference between two sequences.
  Informally, the Levenshtein distance between two words is the minimum
  number of single-character edits (i.e. insertions, deletions or
  substitutions) required to change one word into the other.</p>
</blockquote>

<p>It is a very commonly used metric for identifying similar words. Nltk already has an implementation for the edit distance metric, which can be invoked in the following way:</p>

<pre><code>import nltk
nltk.edit_distance(""humpty"", ""dumpty"")
</code></pre>

<p>The above code would return <code>1</code>, as only one letter is different between the two words.</p>
"
"['dataset', 'pandas']",MovieLens data set,"<p>When you enter ratings on movie lens, you get pages with 10 movies or so. You set all the ratings, then submit by clicking ""next page"" or something. 
So I guess all the ratings for the same page are received at the same time, when you submit the page.</p>
"
"['nlp', 'text-mining', 'feature-extraction']",Clustering strings inside strings?,"<p>Interesting question! I have not encountered it before so here is a solution I just made up, inspired by the approach taken by the word2vec paper:</p>

<ol>
<li><p>Define the pair-wise similarity based on the longest common substring (LCS), or the LCS normalized by the products of the string lengths. Cache this in a matrix for any pair of strings considered since it is expensive to calculate. Also consider approximations.</p></li>
<li><p>Find a Euclidean (hyperspherical, perhaps?) embedding that minimizes the error (Euclidean distance if using the ball, and the dot product if using the sphere). Assume random initialization, and use a gradient-based optimization method by taking the Jacobian of the error.</p></li>
<li><p>Now you have a Hilbert space embedding, so cluster using your algorithm of choice!</p></li>
</ol>

<p><em>Response to deleted comment asking how to cluster multiple substrings</em>: The bulk of the complexity lies in the first stage; the calculation of the LCS, so it depends on efficiently you do that. I've had luck with genetic algorithms. Anyway, what you'd do in this case is define a similarity <em>vector</em> rather than a scalar, whose elements are the k-longest pair-wise LCS; see <a href=""http://cstheory.stackexchange.com/questions/8361/algorithm-find-the-first-k-longest-substrings-between-two-similar-strings"">this</a> discussion for algorithms. Then I would define the error by the sum of the errors corresponding to each substring.</p>

<p>Something I did not address is how to choose the dimensionality of the embedding. The word2vec paper might provide some heuristics; see <a href=""https://groups.google.com/forum/#!topic/word2vec-toolkit/HRvNPIqe6mM"" rel=""nofollow"">this</a> discussion. I recall they used pretty big spaces, on the order of a 1000 dimensions, but they were optimizing something more complicated, so I suggest you start at R^2 and work your way up. Of course, you will want to use a higher dimensionality for the multiple LCS case.</p>
"
"['machine-learning', 'data-mining', 'classification', 'naive-bayes-classifier']",How does the naive Bayes classifier handle missing data in training?,"<p>In general, you have a choice when handling missing values hen training a naive Bayes classifier. You can either choose to either</p>

<ol>
<li>Omit records with any missing values,</li>
<li>Omit only the missing attributes.</li>
</ol>

<p>I'll use the example linked to above to demonstrate these two approaches.  Suppose we add one more training record to that example. </p>

<pre><code>Outlook  Temperature  Humidity   Windy   Play
-------  -----------  --------   -----   ----
rainy    cool        normal    TRUE    no
rainy    mild        high      TRUE    no
sunny    hot         high      FALSE   no
sunny    hot         high      TRUE    no
sunny    mild        high      FALSE   no
overcast cool        normal    TRUE    yes
overcast hot         high      FALSE   yes
overcast hot         normal    FALSE   yes
overcast mild        high      TRUE    yes
rainy    cool        normal    FALSE   yes
rainy    mild        high      FALSE   yes
rainy    mild        normal    FALSE   yes
sunny    cool        normal    FALSE   yes
sunny    mild        normal    TRUE    yes
NA       hot         normal    FALSE   yes
</code></pre>

<ol>
<li><p>If we decide to omit the last record due to the missing <code>outlook</code> value, we would have the exact same trained model as discussed in the link. </p></li>
<li><p>We could also choose to use all of the information available from this record.  We could choose to simply omit the attribute <code>outlook</code> from this record. This would yield the following updated table. </p></li>
</ol>

<pre>
           Outlook            Temperature           Humidity   
====================   =================   =================  
          Yes    No            Yes   No            Yes    No 
Sunny       2     3     Hot     3     2    High      3     4
Overcast    4     0     Mild    4     2    Normal    7     1 
Rainy       3     2     Cool    3     1
          -----------         ---------            ---------- 
Sunny     2/9   3/5     Hot   3/10   2/5    High    3/10   4/5 
Overcast  4/9   0/5     Mild  4/10   2/5    Normal  7/10   1/5
Rainy     3/9   2/5     Cool  3/10   1/5


            Windy        Play
=================    ========
      Yes     No     Yes   No
False 7      2       10     5
True  3      3
      ----------   ----------
False  7/10    2/5   10/15  5/15
True   3/10    3/5
</pre>

<p>Notice there are 15 observations for each attribute <strong>except</strong> <code>Outlook</code>, which has only 14. This is since that value was unavailable for the last record. All further development would continue as discussed in the linked article.  </p>

<p>For example in the R package <code>e1071</code> naiveBayes implementation has the option <code>na.action</code> which can be set to na.omit or na.pass.</p>
"
"['dataset', 'visualization']",Interactive Graphing while logging data,"<p>For this answer, I have assumed that you prefer <strong>open source solutions</strong> to <em>big data visualization</em>. This assumption is based on budgetary details from your question. However, there is one <em>exclusion</em> to this - below I will add a reference to one commercial product, which I believe might be beneficial in your case (provided that you could afford that). I also assume that <em>browser-based solutions are acceptable</em> (I would even prefer them, unless you have specific contradictory requirements).</p>

<p>Naturally, the first candidate as a solution to your problem I would consider <strong>D3.js JavaScript library</strong>: <a href=""http://d3js.org"" rel=""nofollow"">http://d3js.org</a>. However, despite <em>flexibility</em> and other <em>benefits</em>, I think that this solution is <em>too low-level</em>.</p>

<p>Therefore, I would recommend you to take a look at the following <strong>open source projects for big data visualization</strong>, which are <em>powerful</em> and <em>flexible</em> enough, but operate at a <em>higher level of abstraction</em> (some of them are based on D3.js foundation and sometimes are referred to as D3.js <a href=""http://schoolofdata.org/2013/08/12/climbing-the-d3-js-visualisation-stack"" rel=""nofollow"">visualization stack</a>).</p>

<ul>
<li><strong>Bokeh</strong> - Python-based interactive visualization library, which supports big data and streaming data: <a href=""http://bokeh.pydata.org"" rel=""nofollow"">http://bokeh.pydata.org</a></li>
<li><strong>Flot</strong> - JavaScript-based interactive visualization library, focused on jQuery: <a href=""http://www.flotcharts.org"" rel=""nofollow"">http://www.flotcharts.org</a></li>
<li><strong>NodeBox</strong> - unique rapid data visualization system (not browser-based, but multi-language and multi-platform), based on generative design and visual functional programming: <a href=""https://www.nodebox.net"" rel=""nofollow"">https://www.nodebox.net</a></li>
<li><strong>Processing</strong> - complete software development system with its own programming language, libraries, plug-ins, etc., oriented to visual content: <a href=""https://www.processing.org"" rel=""nofollow"">https://www.processing.org</a> (allows executing Processing programs in a browser via <a href=""http://processingjs.org"" rel=""nofollow"">http://processingjs.org</a>)</li>
<li><strong>Crossfilter</strong> - JavaScript-based interactive visualization library for big data by Square (very fast visualization of large multivariate data sets): <a href=""http://square.github.io/crossfilter"" rel=""nofollow"">http://square.github.io/crossfilter</a></li>
<li><strong>bigvis</strong> - an R package for big data exploratory analysis (not a visualization library per se, but could be useful to process large data sets /aggregating, smoothing/ prior to visualization, using various R graphics options): <a href=""https://github.com/hadley/bigvis"" rel=""nofollow"">https://github.com/hadley/bigvis</a></li>
<li><strong>prefuse</strong> - Java-based interactive visualization library: <a href=""http://prefuse.org"" rel=""nofollow"">http://prefuse.org</a></li>
<li><strong>Lumify</strong> - big data integration, analysis and visualization platform (interesting feature: supports Semantic Web): <a href=""http://lumify.io"" rel=""nofollow"">http://lumify.io</a></li>
</ul>

<p>Separately, I'd like to mention two open source <em>big data analysis and visualization projects</em>, focused on <strong>graph/network data</strong> (with some support for <em>streaming data</em> of that type): <a href=""http://www.cytoscape.org"" rel=""nofollow"">Cytoscape</a> and <a href=""https://gephi.github.io"" rel=""nofollow"">Gephi</a>. If you are interested in some other, <em>more specific</em> (<em>maps</em> support, etc.) or <em>commercial</em> (basic free tiers), projects and products, please see this <strong>awesome compilation</strong>, which I thoroughly <em>curated</em> to come up with the main list above and <em>analyzed</em>: <a href=""http://blog.profitbricks.com/39-data-visualization-tools-for-big-data"" rel=""nofollow"">http://blog.profitbricks.com/39-data-visualization-tools-for-big-data</a>.</p>

<p>Finally, as I promised in the beginning, <strong>Zoomdata</strong> - a commercial product, which I thought you might want to take a look at: <a href=""http://www.zoomdata.com"" rel=""nofollow"">http://www.zoomdata.com</a>. The reason I made an exclusion for it from my open source software compilation is due to its <strong>built-in support for big data platforms</strong>. In particular, Zoomdata provides <em>data connectors</em> for Cloudera Impala, Amazon Redshift, MongoDB, Spark and Hadoop, plus search engines, major database engines and streaming data.</p>

<p><strong>Disclaimer:</strong> I have no affiliation with Zoomdata whatsoever - I was just impressed by their <em>range of connectivity options</em> (which might <strong>cost</strong> you dearly, but that's another <em>aspect</em> of this topic's analysis).</p>
"
"['feature-selection', 'regression']",Does high error rate in regression imply the data set is unpredictable?,"<p>It's a little hasty to make too many conclusions about your data based on what you presented here. At the end of the day, all the information you have right now is that ""GBT did not work well for this prediction problem and this metric"", summed up by a single RMSE comparison. This isn't very much information - it could be that this is a bad dataset for GBT and some 
other model would work, it could be that the label can't be predicted from these features with any model, or there could be some error in model setup/validation. </p>

<p>I'd recommend checking the following hypotheses: </p>

<p>1) Maybe, with your dataset size and the features you have, GBT isn't a very high-performance model. Try something completely different - maybe just a simple linear regression! Or a random forest. Or GBDT with very different parameter settings. Or something else. This will help you diagnose whether it's an issue with choice of models or with something else; if a few very different approaches give you roughly similar results, you'll know that it's not the model choice that is causing these results, and if one of those models behaves differently, then that gives you additional information to help diagnose the issue. </p>

<p>2) Maybe there's some issue with model setup and validation? I would recommend doing some exploration to get some intuition as to whether the RMSE you're getting is reasonable or whether you should expect better. Your post contained very little detail about what the data actually represents, what you know about the features and labels, etc. Perhaps you know those things but didn't include them here, but if not, you should go back and try to get additional understanding of the data before continuing. Look at some random data points, plot the columns against the target, look at the histograms of your features and labels, that sort of thing. There's no substitute for looking at the data. </p>

<p>3) Maybe there just aren't enough data points to justify complex models. When you have low numbers of data points (&lt; 100), a simpler parametric model built with domain expertise and knowledge of what the features are may very well outperform a nonparametric model.  </p>
"
"['machine-learning', 'classification']",What is the difference between feature generation and feature extraction?,"<p><strong>Feature Generation</strong> -- This is the process of taking raw, unstructured data and defining features (i.e. variables) for potential use in your statistical analysis. For instance, in the case of text mining you may begin with a raw log of thousands of text messages (e.g. SMS, email, social network messages, etc) and generate features by removing low-value words (i.e. stopwords), using certain size blocks of words (i.e. n-grams) or applying other rules.</p>

<p><strong>Feature Extraction</strong> -- After generating features, it is often necessary to test transformations of the original features and select a subset of this pool of potential original and derived features for use in your model (i.e. feature extraction and selection). Testing derived values is a common step because the data may contain important information which has a non-linear pattern or relationship with your outcome, thus the importance of the data element may only be apparent in its transformed state (e.g. higher order derivatives). Using too many features can result in multiply colinearity or otherwise confound statistical models, whereas extracting the minimum number of features to suit the purpose of your analysis follows the principal of parsimony.</p>

<p><em>Enhancing your feature space</em> in this way is often a necessary step in classification of images or other data objects because the raw feature space is typically filled with an overwhelming amount of unstructured and irrelevant data that comprises what's often referred to as ""noise"" in the paradigm of a ""signal"" and ""noise"" (which is to say that some data has predictive value and other data does not). By enhancing the feature space you can better identify the important data which has predictive or other value in your analysis (i.e. the ""signal"") while removing confounding information (i.e. ""noise"").</p>
"
"['python', 'neuralnetwork', 'deep-learning', 'keras']",Reshaping of data for deep learning using Keras,"<p><code>mnist.load_data()</code> supplies the MNIST digits with structure <code>(nb_samples, 28, 28)</code> i.e. with 2 dimensions per example representing a greyscale image 28x28.</p>

<p>The Convolution2D layers in Keras however, are designed to work with 3 dimensions per example. They have 4-dimensional inputs and outputs. This covers colour images <code>(nb_samples, nb_channels, width, height)</code>, but more importantly, it covers deeper layers of the network, where each example has become a set of feature maps i.e. <code>(nb_samples, nb_features, width, height)</code>.</p>

<p>The greyscale image for MNIST digits input would either need a different CNN layer design (or a param to the layer constructor to accept a different shape), or the design could simply use a standard CNN and you must explicitly express the examples as 1-channel images. The Keras team chose the latter approach, which needs the re-shape.</p>
"
"['image-recognition', 'learning']",Interpret User Interfaces with Machine Learning,"<p>I would try experimenting with recurrent neural networks: <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>. Recurrent neural networks can output sequences of variable length given inputs of variable length. In your case a recurrent neural network might output a sequence like the following when given a user interface: click a button, select a field, type some text, hit enter. For another interface, the network might output only: click one button, click another button, and that's it. This would be useful for you because the sequence and length of actions from interface to interface might change a lot. </p>

<p>You could also experiment with reinforcement learning and build an algorithm that has an objective (reach some final page in as few actions as possible). The algorithm would start by doing random things (like clicking the same button a bunch of times), and then gradually learn over time to take appropriate actions. If you go that route you could use deep learning and Monte Carlo Tree Search (MCTS) like what Alpha Go did. </p>

<p>In either case you're going to need a framework that can train an algorithm quickly because you're likely to have to go through a lot of iterations. TensorFlow (<a href=""https://www.tensorflow.org/"" rel=""nofollow"">https://www.tensorflow.org/</a>) is one option (I've started using it recently, and I like it a lot because of its easy of use). TensorFlow is capable of building both recurrent neural nets and deep neural nets. </p>
"
['methods'],Data Science Methodologies,"<p>Okay, I eventually found what I was looking for in the Data Mining Community. There seem to be two candidates, CRISP-DM which comes from SPSS originally but is ""Cross-Industry"", and SEMMA which comes from SAS. They are both pretty much what I was looking for.</p>

<p>CRISP-DM <a href=""http://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining"" rel=""nofollow"">http://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining</a></p>

<p>And </p>

<p>SEMMA <a href=""http://en.wikipedia.org/wiki/SEMMA"" rel=""nofollow"">http://en.wikipedia.org/wiki/SEMMA</a></p>
"
['apache-spark'],Local Development for Apache Spark,"<p>Spark is intended to be pointed at large distributed data sets, so as you suggest, the most typical use cases will involve connecting to some sort of Cloud system like AWS. </p>

<p>In fact, if the data set you aim to analyze can fit on your local system, you'll usually find that you can analyze it just as simply using pure python.  If you're trying to leverage a series of local VMs, you're going to run out of memory pretty quickly and jobs will either fail or grind to a halt.</p>

<p>With that said, a local instance of spark is very useful for the purpose of development. </p>

<p>One way that I've found that works is if I have a directory in HDFS with many files, I'll pull over a single file, develop locally, then port my spark script to my cloud system for execution.  If you're using AWS, this is really helpful for avoiding big fees while you're developing.</p>
"
['nosql'],How to learn noSQL databases and how to know when SQL or noSQL is better,"<p>Please have a look at my answer here: </p>

<p><strong><a href=""http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767"">Motivations for using relational database / ORM or document database / ODM</a></strong></p>

<p>Short version:</p>

<ul>
<li><p>Use NoSQL when data size and number of transactions per second forces it, which typically happens above a few tens of TB and millions of transactions per second (db in memory, running on cluster), <em>or</em> at hundreds of TB and thousands of transactions per second (traditional db on disk, transactions per second is <em>highly</em> dependent on the usage pattern). Traditional SQL scales up to that point just fine.</p></li>
<li><p>NoSQL is well suited for some problems (data has a natural sharding, schema is flexible, eventual consistency is ok).  You can use for those even if scaling doesn't force you to.</p></li>
<li><p>Developer familiarity with tools and ops ease of deployment are <em>major</em> factors, don't overlook them.  A solution may be technically better but you may have a hard time using it, make sure you need it and make sure you budget for the learning curve.</p></li>
</ul>

<p>As to how to learn it: fire up a MongoDB image on AWS, or DynamoDB, and have fun! </p>

<ul>
<li><p><a href=""http://docs.mongodb.org/ecosystem/platforms/amazon-ec2/"" rel=""nofollow"">MongoDB on AWS tutorial</a></p></li>
<li><p><a href=""http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStartedDynamoDB.html"" rel=""nofollow"">DynamoDB tutorial</a></p></li>
</ul>
"
"['nlp', 'topic-model', 'lda']",Latent Dirichlet Allocation vs Hierarchical Dirichlet Process,"<p>HDP is an extension of LDA, designed to address the case where the number of mixture components (the number of ""topics"" in document-modeling terms) is not known a priori.  So that's the reason why there's a difference.</p>

<p>Using LDA for document modeling, one treats each ""topic"" as a distribution of words in some known vocabulary.  For each document a mixture of topics is drawn from a Dirichlet distribution, and then each word in the document is an independent draw from that mixture (that is, selecting a topic and then using it to generate a word).</p>

<p>For HDP (applied to document modeling), one also uses a Dirichlet process to capture the uncertainty in the number of topics.  So a common base distribution is selected which represents the countably-infinite set of possible topics for the corpus, and then the finite distribution of topics for each document is sampled from this base distribution.</p>

<p>As far as pros and cons, HDP has the advantage that the maximum number of topics can be unbounded and learned from the data rather than specified in advance.  I suppose though it is more complicated to implement, and unnecessary in the case where a bounded number of topics is acceptable.</p>
"
"['nlp', 'language-model']",What is generative and discriminative model? How are they used in Natural Language Processing?,"<p>Let's say you are predicting the topic of a document given its words.</p>

<p>A generative model describes how likely each topic is, and how likely words are given the topic. This is how it says documents are actually ""generated"" by the world -- a topic arises according to some distribution, words arise because of the topic, you have a document. Classifying documents of words W into topic T is a matter of maximizing the joint likelihood: P(T,W) = P(W|T)P(T)</p>

<p>A discriminative model operates by only describing how likely a topic is given the words. It says nothing about how likely the words or topic are by themselves. The task is to model P(T|W) directly and find the T that maximizes this. These approaches do not care about P(T) or P(W) directly.</p>
"
"['machine-learning', 'dataset', 'recommendation']",How do you calculate how dense or sparse a dataset is?,"<p>It's actually defined on the first page:</p>

<blockquote>
  <p>... sparsity level (ratio of observed to total ratings) ...</p>
</blockquote>

<p>In other words, the fraction of the user/item rating matrix that is not empty. Remember that the problem is that most user-item pairs have no rating, and we wish to estimate them.</p>

<p><strong>Example</strong>:</p>

<p>Let there be three users and four products. The number of <em>possible</em> ratings is $3\times4 = 12$. If every user rates only one product each (regardless of which product), the density is 3/12 = 25%.</p>
"
"['predictive-modeling', 'regression']",Regression: how to interpret different linear relations?,"<p>What you are looking for is the Analysis of Covariance (<a href=""https://en.wikipedia.org/wiki/Analysis_of_covariance"" rel=""nofollow"">ANCOVA</a>) analysis, which is used to compare two or more regression lines by testing the effect of a categorical factor on a dependent variable (y-var) while controlling for the effect of a continuous co-variable (x-var).</p>

<p><a href=""http://r-eco-evo.blogspot.in/2011/08/comparing-two-regression-slopes-by.html"" rel=""nofollow"">Here</a> is an example for carrying out the ANCOVA analysis using R.</p>
"
"['feature-selection', 'sklearn', 'pca', 'randomized-algorithms']",Interpreting the results of randomized PCA in scikit-learn,"<p>Yes, through the <code>components_</code> property:</p>

<pre><code>import numpy, seaborn, pandas, sklearn.decomposition
data = numpy.random.randn(1000, 3) @ numpy.random.randn(3,3)
seaborn.pairplot(pandas.DataFrame(data, columns=['x', 'y', 'z']));
</code></pre>

<p><a href=""http://i.stack.imgur.com/9bp8r.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9bp8r.png"" alt=""Faceted scatter plot""></a></p>

<pre><code>sklearn.decomposition.RandomizedPCA().fit(data).components_

&gt; array([[ 0.43929754,  0.81097276,  0.38644644],
       [-0.54977152,  0.58291122, -0.59830243],
       [ 0.71047094, -0.05037554, -0.70192119]])

sklearn.decomposition.RandomizedPCA(2).fit(data).components_

&gt; array([[ 0.43929754,  0.81097276,  0.38644644],
       [-0.54977152,  0.58291122, -0.59830243]])
</code></pre>

<p>We see that the truncated decomposition is simply the truncation of the full decomposition. Each row contains the coefficients of the corresponding principal component.</p>
"
"['python', 'pandas']",How to count the number of missing values in each row in Pandas dataframe?,"<p>You can apply a count over the rows like this:</p>

<pre><code>test_df.apply(lambda x: x.count(), axis=1)
</code></pre>

<p>test_df:</p>

<pre><code>    A   B   C
0:  1   1   3
1:  2   nan nan
2:  nan nan nan
</code></pre>

<p>output:</p>

<pre><code>0:  3
1:  1
2:  0
</code></pre>

<p>You can add the result as a column like this:</p>

<pre><code>test_df['full_count'] = test_df.apply(lambda x: x.count(), axis=1)
</code></pre>

<p>Result:</p>

<pre><code>    A   B   C   full_count
0:  1   1   3   3
1:  2   nan nan 1
2:  nan nan nan 0
</code></pre>
"
"['regression', 'correlation']",Non-linear regression line fit,"<p>I tried to estimate a few of your data values from the scatter plot you provided.</p>

<p>Then I performed a power model regression and came up with</p>

<p>$y = (5.777 \cdot 10^{-16}) \cdot 1.404^{x}.$</p>

<p><a href=""http://i.stack.imgur.com/ni7RV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ni7RV.png"" alt=""Graph of model""></a></p>

<p>The estimated values I used are below.</p>

<p>$(70, 0.01), (75, 0.012), (80, 0.015), (90, 0.025), (95, 0.075), (98, 0.15), (99, 0.20), (99.5, 0.25), (99.9, 0.32)$</p>

<p>Of course, your actual model will differ because you have the actual data set. I just eyeballed a few points so I could test the power fit.</p>
"
"['machine-learning', 'markov-process']",Markov Chains: How much steps to conclude a Transition Matrix,"<p>I expect you have, or can make, a matrix of transition counts. Consider the data in each row to be draws from a multinomial distribution. Then you should be able to use <a href=""http://stats.stackexchange.com/questions/19120/sample-size-for-a-variable-number-of-answers"">sample size calculations for the multinomial</a> to get off the ground. </p>

<p>It is also possible that your data is not well described by a simple Markov chain. There are some available techniques for this, e.g. <a href=""https://cran.r-project.org/web/packages/msm/vignettes/msm-manual.pdf"" rel=""nofollow"">multistate modelling</a>, but which may or may not fit your particular problem. </p>
"
['machine-learning'],"Are there any machine learning algorithms that focus on comparing items, rather than classification or regression?","<p>There are ranking algorithms based on machine learning that are aimed to build ranking models. Training data for these models is given in the form of partial ordering between each pair of elements in a sample. A brief description, together with a list of useful references, is given in the corresponding <a href=""https://en.wikipedia.org/wiki/Learning_to_rank"">Wikipedia page</a>.</p>
"
"['r', 'clustering', 'k-means']",How to calculate most frequent value combinations,"<p>If I understand your question you want to know which combination is most frequent or how frequent a combination is relative to others. This is a static method that will determine the unique combinations in total (i.e., combinations of all five columns). </p>

<p>The <code>plyr</code> package has a nifty utility for grouping unique combinations of columns in a <code>data.frame</code>. We can specify the names of the columns we want to group by, and then specify a function to perform for each of those combinations. In this case, we specify the columns associated with your golf shot qualities and use the function <code>nrow</code> which will count the number of rows in every subset of the large data.frame for which the columns are the identical.</p>

<pre><code># You need this library for the ddply() function
require(plyr)

# These are the columns that determine a unique situation (change this if you need)
qualities &lt;- c(""shotType"",""clubType"",""desiredShape"",""lineDirection"",""shotQuality"")

# The call to ddply() actually gives us what we want, which is the number 
# of times that combination is present in the dataset
countedCombos &lt;- ddply(golf,qualities,nrow)

# To be nice, let's give that newly added column a meaningful name
names(countedCombos) &lt;- c(qualities,""count"")

# Finally, you probably want to order it (decreasing, in this case)
countedCombos &lt;- countedCombos[with(countedCombos, order(-count)),]
</code></pre>

<p>Now check out your product. The final column has the count associated with each unique combination of columns you provided to <code>ddply</code>:</p>

<pre><code>head(countedCombos)
   shotType clubType desiredShape lineDirection shotQuality count
16     putt   putter     straight      straight        good    30
10 approach    wedge     straight      straight        good     6
9  approach    wedge     straight      straight         bad     5
19      tee   driver         draw      straight        good     5
22      tee   driver     straight      straight        good     4
2  approach     iron         draw      straight        good     3
</code></pre>

<p>To see the results for a particular cross-section (say, for example, the driver <code>clubType</code>):</p>

<pre><code>countedCombos[which(countedCombos$clubType==""driver""),]
   shotType clubType desiredShape lineDirection shotQuality count
19      tee   driver         draw      straight        good     5
22      tee   driver     straight      straight        good     4
21      tee   driver     straight          left        good     2
17      tee   driver         draw         right        good     1
18      tee   driver         draw      straight         bad     1
20      tee   driver         fade         right         bad     1
</code></pre>

<p>As a bonus, you can dig into these results with <code>ddply</code> again. For example, if you wanted to look at the ratio of ""good"" to ""bad"" shotQuality based on <code>shotType</code> and <code>clubType</code>:</p>

<pre><code>shotPerformance &lt;- ddply(countedCombos,c(""shotType"",""clubType""),
    function(x){
        total&lt;- length(x$shotQuality)
            good &lt;- length(which(x$shotQuality==""good""))
        bad &lt;- length(which(x$shotQuality==""bad""))
        c(total,good,bad,good/(good+bad))
    }
 )
names(shotPerformance)&lt;-c(""count"",""shotType"",""clubType"",""good"",""bad"",""goodPct"")
</code></pre>

<p>This gives you a new breakdown of some math performed on the counts of a character field (<code>shotQuality</code>) and shows you how you can build custom functions for <code>ddply</code>. Of course, you can still order these whichever way you want, too.</p>

<pre><code>head(shotPerformance)
  shotType clubType total good bad   goodPct
1 approach   hybrid    1  1   0 1.0000000
2 approach     iron    6  4   2 0.6666667
3 approach    wedge    3  1   2 0.3333333
4 approach     wood    3  1   2 0.3333333
5     chip    wedge    1  1   0 1.0000000
6     putt   putter    2  1   1 0.5000000
</code></pre>
"
"['r', 'python']",Using R and Python together,"<p>Several clarifications: </p>

<ul>
<li><p>you can program with object-oriented (OOP) concepts in R, even though OOP in R has slightly different syntax from other languages. Methods do not bind to objects. In R, different method versions will be involved based on the input argument classes (and types). (Ref: <a href=""http://adv-r.had.co.nz/OO-essentials.html"" rel=""nofollow"">Advanced R</a>)</p></li>
<li><p>you can also replace nans with mean / any stat. / value in R using a mask if you store the data in a dataframe (See <a href=""http://stackoverflow.com/questions/25835643/replacing-missing-values-in-r-with-column-mean"">SO post</a>)</p></li>
<li><p>There is no problem using them interchangeably. I use R from Python using the package RPy2. I assume it is equally easy to do it the other way round. </p></li>
</ul>

<p>At the end of the day, any language is only as good as how much the users know about it. Use one that you are more familiar with and try to learn it properly using the vast online resources online.</p>
"
"['python', 'pandas', 'scraping']",How can I read in a .csv file with special characters in it in pandas?,"<p>There is <code>df.drop</code> command that can be used as follows to remove certain rows (in this case, 15 &amp; 16):</p>

<p><code>df.drop(df.index[[15,16]])</code></p>

<p>If the rows you don't need are regular (e.g. you never need row 15) then this is a quick and dirty solution.</p>

<p>If you only want to drop arbitrary rows containing some value, this should do the trick:</p>

<p><code>df = df.drop([df.column_name == ©1990-2016 AAR])</code></p>
"
"['classification', 'predictive-modeling', 'xgboost']",Classification using xgboost - predictions,"<p>The gradient boost algorithm create a set of decision tree.</p>

<p>The prediction process used <a href=""https://gist.github.com/shanebutler/5456942"" rel=""nofollow"">here</a> use these steps:</p>

<ul>
<li>for each tree, create a temporary ""predicted variable"", applying the tree to the new data set.</li>
<li>use a formula to aggregate all these tree. Depending on the model:

<ul>
<li>bernoulli: 1/(1 + exp(-(intercept + SUM(temporary pred))))</li>
<li>poisson, gamma: exp(intercept + SUM(temporary pred))</li>
<li>adaboost: 1 /(1 + exp(-2*(intercept + SUM(temporary pred))))</li>
</ul></li>
</ul>

<p>The temporary ""predicted variable"" is a probability, having no sense by its own.</p>

<p>The more tree you have, the more smooth is your prediction.( as for each tree, only a finite set of value is spread across your observations)</p>

<p>The R process is probably optimised, but it is enough to understand the concept.</p>

<p>In the h2o implementation of the gradient boost, the output is a flag 0/1.
I think the <a href=""https://en.wikipedia.org/wiki/F1_score"" rel=""nofollow"">F1 score</a> is used by default to convert probability into flag. I'll do some search/test to confirm that.</p>

<p>In that same implementation, one of the default output for a binary outcome is a confusion matrix, which is a great way to assess your model ( and open a whole new bunch of interrogations).</p>

<p>The intercept is ""the initial predicted value to which trees make adjustments"". Basically,just an initial adjustment.</p>

<p>In addition: <a href=""http://h2o-release.s3.amazonaws.com/h2o/rel-tibshirani/8/docs-website/h2o-docs/booklets/GBM_Vignette.pdf"" rel=""nofollow"">h2o.gbm documentation</a></p>
"
"['nlp', 'text-mining']",What is the difference between NLP and text mining?,"<p>I agree with Sean's answer.
<a href=""https://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow"">NLP</a> and <a href=""https://en.wikipedia.org/wiki/Text_mining"" rel=""nofollow"">text mining</a> are usually used for different goals.
Also, there is indeed an overlap and both definitions are vogue.</p>

<p>Other than the difference in goal, there is a difference in methods.
Text mining techniques are usually shallow and do not consider the text structure.Usually, text mining will use bag of words, n-grams and possibly stemming over that.</p>

<p>In NLP methods usually involve the test structure. You can find there sentence splitting, part of speech tagging and parse tree construction.  </p>

<p>Typical text mining method will consider the following sentences to indicate happiness while typical NLP methods detects that they are not</p>

<ol>
<li>I am not happy</li>
<li>I will be happy when it will rain</li>
<li>If it will rain, I'll be happy.</li>
<li>She asked whether I am happy</li>
<li>Are you happy?</li>
</ol>
"
"['machine-learning', 'classification', 'beginner', 'decision-trees']",How would I map categories between two similar but different sets of categories,"<p>This sounds like a pretty standard supervised learning problem. In this case, your records would be businesses on site X and their actual category on site Z. Your predictors would be tags/categories for a particular business on site X, and your target variable, y (i.e., what you're trying to predict), would be the category on the other website. As far as the code goes, you have a variety of options depending on your preferred language. You could use the caret package in R, the scikit-learn library in Python, or the Weka library (maybe even Spark's ML lib because of its simplicity) in Java/Scala. </p>

<p>Side note, in your question I think you meant to say ""logistic regression"" instead of ""logical regression"". You don't need to use logistic regression (although it wouldn't hurt). You could also try algorithms like Random Forests or Naive Bayes.</p>

<p>Also worth noting: your target variable will have many classes (ie every possible category for the site you're trying to predict), so don't get alarmed if it seems like there are a lot of classes. That's normal for a problem like the one you've described.   </p>
"
['lda'],"In Latent Dirichlet Allocation (LDA), is it reasonable to reconstruct the original bag-of-words using the document and word representations?","<p>It is possible to produce a corpus from the learned LDA parameters ($\theta$ and $\phi$) according to the generative model of LDA but it is not realistic to expect that you would recreate the original documents (in bag-of-words form). To be more specific, it is <em>possible</em> - but highly improbable - that you would generate the bag-of-words documents corresponding to the input corpus.</p>
"
"['apache-spark', 'scala']",value saveAsTextFile is not a member of org.apache.spark.sql.DataFrame,"<p>This is not standard part of the API of DataFrames. You can either map it to a RDD, join the row entries to a string and save that or the more flexible way is to use the DataBricks spark-csv package that can be found <a href=""https://github.com/databricks/spark-csv"" rel=""nofollow"">here</a>.</p>

<p>If it's just one column you can map it to a RDD and just call <code>.saveAsTextFile(filename)</code></p>
"
"['text-mining', 'social-network-analysis', 'feature-engineering']",How can i add weights in a bag of words model in text analysis?,"<p>One possible solution is to introduce prior counts for words (higher counts for words that are more important) that could be added to the term-document matrix.</p>

<p>An alternative solution is to compute tf-idf features (weights that modify word counts based on frequency) and apply additional weighting to tf-idf with higher weights corresponding to important words.</p>
"
"['logistic-regression', 'probability']",Equation for likelihood in logistic regression,"<p>By the definition of the logistic regression model $\mathrm P(y = 1 | x,w) = \sigma\left(\left&lt;w, x\right&gt;\right)$ thus $\mathrm P(y = 0 | x,w) = 1- \sigma\left(\left&lt;w, x\right&gt;\right)$</p>

<p>You should be able to verify (by setting y=0/y=1) that this is equivalent to</p>

<p>$\mathrm P(y | x,w) = \sigma\left(\left&lt;w, x\right&gt;\right)^y \left[ 1- \sigma \left( \left&lt;w, x \right&gt; \right) \right]^{1-y}$</p>

<p>I think you understand the rest?</p>
"
"['machine-learning', 'r', 'classification', 'performance']",ROC curves/AUC values as a performance metric,"<p>The results you posted are correct, I did a quick check with <code>library(pROC)</code> and got the same thing.  The important feature for AUC/ROC is that the cutpoint of calling a sample ""1"" or ""0"" is not set at 0.5.  For each of your predictors, the range of the numbers do not matter at all.  What does matter is how often higher numbers are associated with true positive labels.  Example:</p>

<pre><code>Method 1:[1, 3, 5, 6, 9, 14] (predictor)
Method 1:[0, 0, 1, 0 ,1 , 1] (true label)
AUC: 0.8889

Method 2:[[0.01, 0.02, 200 , 250, 300, 1000000] (predictor)
Method 2:[0, 0, 1, 0 ,1 , 1] (true label)
AUC: 0.8889
</code></pre>

<p>The AUC measure is just a measure of how often true ""1"" samples have a higher number than true ""0"" samples.  The predictor does not have to even be bounded by [0,1], it can be any range.</p>

<p>Your plot would be more informative if it was sorted.  Consider the below sorted plot (crude, sorry) for each of the three methods, with the ""1"" labels highlighted in blue.  badaI clearly has the closest grouping of ""1""s towards the higher values.</p>

<p><a href=""http://i.stack.imgur.com/sHxra.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sHxra.png"" alt=""Sorted Values by Method""></a></p>
"
"['data-formats', 'hierarchical-data-format']",Hierarchical Data Format. What are the advantages compared to alternative formats?,"<p>Perhaps a good way to paraphrase the question is, what are the advantages compared to alternative formats?  </p>

<p>The main alternatives are, I think: a database, text files, or another packed/binary format.</p>

<p>The database options to consider are probably a columnar store or NoSQL, or for small self-contained datasets SQLite.  The main advantage of the database is the ability to work with data much larger than memory, to have random or indexed access, and to add/append/modify data quickly.  The main *dis*advantage is that it is much slower than HDF, for problems in which the entire dataset needs to be read in and processed.  Another disadvantage is that, with the exception of embedded-style databases like SQLite, a database is a system (requiring admnistration, setup, maintenance, etc) rather than a simple self-contained data store.  </p>

<p>The text file format options are XML/JSON/CSV.  They are cross-platform/language/toolkit, and are a good archival format due to the ability to be self-describing (or obvious :).  If uncompressed, they are huge (10x-100x HDF), but if compressed, they can be fairly space-efficient (compressed XML is about the same as HDF).  The main disadvantage here is again speed: parsing text is much, much slower than HDF.</p>

<p>The other binary formats (npy/npz numpy files, blz blaze files, protocol buffers, Avro, ...) have very similar properties to HDF, except they are less widely supported (may be limited to just one platform: numpy) and may have specific other limitations.  They typically do not offer a compelling advantage.</p>

<p>HDF is a good complement to databases, it may make sense to run a query to produce a roughly memory-sized dataset and then cache it in HDF if the same data would be used more than once.  If you have a dataset which is fixed, and usually processed as a whole, storing it as a collection of appropriately sized HDF files is not a bad option.  If you have a dataset which is updated often, staging some of it as HDF files periodically might still be helpful.</p>

<p>To summarize, HDF is a good format for data which is read (or written) typically as a whole; it is the lingua franca or common/preferred interchange format for many applications due to wide support and compatibility, decent as an archival format, and very fast.</p>

<p>P.S. To give this some practical context, my most recent experience comparing HDF to alternatives, a certain small (much less than memory-sized) dataset took 2 seconds to read as HDF (and most of this is probably overhead from Pandas); ~1 minute to read from JSON; and 1 <em>hour</em> to write to database.  Certainly the database write could be sped up, but you'd better have a good DBA!  This is how it works out of the box.</p>
"
['machine-learning'],What are some easy to learn machine-learning applications?,"<p>I would recommend to start with some MOOC on machine learning. For example Andrew Ng's <a href=""https://www.coursera.org/course/ml"">course</a> at coursera.</p>

<p>You should also take a look at <a href=""http://orange.biolab.si/"">Orange</a> application. It has a graphical interface and probably it is easier to understand some ML techniques using it. </p>
"
"['machine-learning', 'sports']",Can machine learning algorithms predict sports scores or plays?,"<p>There are a lot of good questions about Football (and sports, in general) that would be awesome to throw to an algorithm and see what comes out. The tricky part is to know <em>what</em> to throw to the algorithm.</p>

<p>A team with a good RB could just pass on 3rd-and-short just because the opponents would probably expect run, for instance. So, in order to actually produce some worthy results, I'd break the problem in smaller pieces and analyse them statistically while throwing them to the machines!</p>

<p>There are a few (good) websites that try to do the same, you should check'em out and use whatever they found to help you out:</p>

<ul>
<li><a href=""http://www.footballoutsiders.com/"">Football Outsiders</a></li>
<li><a href=""http://www.advancedfootballanalytics.com/"">Advanced Football Analytics</a></li>
</ul>

<p>And if you truly want to explore Sports Data Analysis, you should definitely check the <a href=""http://www.sloansportsconference.com/"">Sloan Sports Conference</a> videos. There's a lot of them spread on Youtube.</p>
"
"['machine-learning', 'classification', 'regression', 'scoring']",Comparing accuracy of models in ordinal regression / classification,"<p>Your classes express a certain order. You can classify apples to, say, ""green"", ""red"" or ""yellow"", and then every disagreement with a reference set is equal. After all, colours express no order. So as you already suggested, I would certainly use regression. Assume that the classes could be distributed as something like this:</p>

<ol>
<li>Very bad = 0 - 0.25</li>
<li>Bad = 0.25 - 0.50</li>
<li>Good = 0.50 - 0.75</li>
<li>Very good = 0.75 - 1.00</li>
</ol>

<p>Now, the mismatch of <em>Very good</em> vs. <em>Bad</em> is at least 0.25, where is must be at least 0.50 with <em>Very good</em> vs. <em>Very bad</em>, which gives a better and more honest impression of the performance of your model.</p>
"
"['random-forest', 'ensemble-modeling']",Assumptions/Limitations of Random Forest Models,"<p>Reproducing the accepted answer from CrossValidated here, as it is the most complete and the best answer to this question.</p>

<p>In order to understand this, remember the ""ingredients"" of random forest classifier (there are some modifications, but this is the general pipeline):</p>

<ol>
<li>At each step of building individual tree we find the <strong>best split</strong> of data</li>
<li>While building a tree we use not the whole dataset, but <strong>bootstrap sample</strong></li>
<li>We aggregate the individual tree outputs by <strong>averaging</strong> (actually 2 and 3 means together more general <a href=""http://en.wikipedia.org/wiki/Bootstrap_aggregating"" rel=""nofollow"">bagging procedure</a>).</li>
</ol>

<p>Assume first point. It is not always possible to find the best split. For example in the following dataset each split will give exactly one misclassified object.
<img src=""http://i.stack.imgur.com/aG3Ps.png"" alt=""Example of the dataset with no best split""></p>

<p>And I think that exactly this point can be confusing: indeed, the behaviour of the individual split is somehow similar to the behaviour of Naive Bayes classifier: if the variables are dependent - there is no better split for Decision Trees and Naive Bayes classifier also fails (just to remind: independent variables is the main assumption that we make in Naive Bayes classifier; all other assumptions come from the probabilistic model that we choose).</p>

<p>But here comes the great advantage of decision trees: we take <strong>any</strong> split and <strong>continue</strong> splitting further. And for the following splits we will find a perfect separation (in red).
<img src=""http://i.stack.imgur.com/ptLS4.png"" alt=""Example of the decision boundary""></p>

<p>And as we have no probabilistic model, but just binary split, we don't need to make any assumption at all.</p>

<p>That was about Decision Tree, but it also applies for Random Forest. The difference is that for Random Forest we use Bootstrap Aggregation. It has no model underneath, and the only assumption that it relies is that <strong>sampling is representative</strong>. But this is usually a common assumption. For example, if one class consist of two components and in our dataset one component is represented by 100 samples, and another component is represented by 1 sample - probably most individual decision trees will see only the first component and Random Forest will misclassify the second one.
<img src=""http://i.stack.imgur.com/Ndk6t.png"" alt=""Example of weakly represented second component""></p>
"
"['machine-learning', 'predictive-modeling']",Create a prediction formula from data input,"<p>I'm not sure if I understood your question! Probably better to plot a scheme at least. But according to what I guess from your question:</p>

<p><strong>Q2-</strong> You probably need a simple <a href=""http://en.wikipedia.org/wiki/Multilayer_perceptron"" rel=""nofollow"">MLP</a> (Multilayer Perceptron)! it's a traditional architecture for Neural Networks where you have $n$ input neurons (here 20-25), one or more hidden layers with several neurons and 3 neurons as output layer. If you use a <a href=""http://en.wikipedia.org/wiki/Sigmoid_function"" rel=""nofollow"">sigmoid</a> activation function ranged from 0 to 1, the output for each class will be $P(Y=1|X=x)$.</p>

<p><strong>Q1-</strong> So your question probably is: how many training data you need for learning a model? and to the best of my knowledge the answer is <em>as many as possible</em>!</p>

<p>and about the last question, I really could not figure out what you mean. You apparently have a very specific task so I suggest to share more insight for sake of clarification.</p>

<p>I hope I could help a little!</p>
"
"['classification', 'python', 'neuralnetwork', 'clustering']",What is the best Keras model for multi-label classification?,"<p>Your choices of <code>activation='softmax'</code> in the last layer and compile choice of <code>loss='categorical_crossentropy'</code> are good for a model to predict multiple mutually-exclusive classes.</p>

<p>Regarding more general choices, there is rarely a ""right"" way to construct the architecture. Instead that should be something you test with different meta-params (such as layer sizes, number of layers, amount of drop-out), and should be results-driven (including any limits you might have on resource use for training time/memory use etc). </p>

<p>Use a cross-validation set to help choose a suitable architecture. Once done, to get a more accurate measure of your model's general performance, you should use a separate test set. Data held out from your training set separate to the CV set should be used for this. A reasonable split might be 60/20/20 train/cv/test, depending on how much data you have, and how much you need to report an accurate final figure.</p>

<p>For Question #2, you can either just have two outputs with a softmax final similar to now, or you can have <em>final</em> layer with one output, <code>activation='sigmoid'</code> and <code>loss='binary_crossentropy'</code>.</p>

<p>Purely from a gut feel from what might work with this data, I would suggest trying with <code>'tanh'</code> or <code>'sigmoid'</code> activations in the hidden layer, instead of <code>'relu'</code>, and I would also suggest increasing the number of hidden neurons (e.g. 100) and reducing the amount of dropout (e.g. 0.2). Caveat: Gut feeling on neural network architecture is not scientific. Try it, and test it.</p>
"
"['python', 'logistic-regression', 'theano']",Theano logistic regression example,"<blockquote>
  <p>I do not understand the next line, the cost. Shouldn't the cost be equal to the xent, i.e. the loss? What is the cost function representing here? </p>
</blockquote>

<p>The cost is the error (xent.mean()) + some regularization (0.01 * (w ** 2).sum())</p>

<blockquote>
  <p>Why is the bias initially set to 0 and not a random number like the weights?</p>
</blockquote>

<p>It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights. </p>

<p>More details <a href=""http://cs231n.github.io/neural-networks-2/#init"">here</a>.</p>
"
"['classification', 'neuralnetwork', 'autoencoder']",do autoencoders work well for non images?,"<p>Yes, but no-one can tell if they will work well for your problem, so just try it and see. Don't give up if it does not work at first, because training neural networks requires some practice; there are lots of parameters, and not every configuration will work well. Even the optimization algorithm is a hyperparameter. </p>
"
"['r', 'distance']",How do I find the minimum distance between zip codes in R?,"<p>I think I have read your question correctly it looks like you need a nearest neighbor implementation. If you are unfamiliar with the concept you can find the wiki article here <a href=""https://en.wikipedia.org/wiki/Nearest_neighbor_search"" rel=""nofollow"">https://en.wikipedia.org/wiki/Nearest_neighbor_search</a>. </p>

<p>I went ahead a wrote an example implementation you can use as a guide. Please note that this is a brute force method and not useful for big data sets. Once you have a grasp of the material I suggest checking out some libraries like RANN that have ""real"" implementations. </p>

<p>Read in some random test data and clean
   For this test let us assume that we want to find the 
   closest AMERICAN city for each location</p>



<pre><code>coord_data = read.csv(""~/Downloads/SalesJan2009.csv"", stringsAsFactors = F)
coord_data$id = c(1:nrow(coord_data))
coord_data$is_usa = ifelse(coord_data$Country == ""United States"", 1, 0)
coord_data = coord_data[ , c(""id"", ""Latitude"", ""Longitude"", ""is_usa"")]
names(coord_data) = tolower(names(coord_data))
</code></pre>

<p>Define your distance function.
         Here we have geo-coordinates over long distance so Euclidean will not do.
         I am using the Law of Cosines to calculate great circle distance
        but Haversine and Vincenty should be considered given your needs.
        To learn more start here: <a href=""https://en.wikipedia.org/wiki/Great-circle_distance"" rel=""nofollow"">https://en.wikipedia.org/wiki/Great-circle_distance</a>.</p>

<pre><code>greatCircleDistance = function(latAlpha, longAlpha, latBeta, longBeta, radius = 6371) {
  ## Function taken directly from Wikipedia
  ## Earth radius in km is default (6371)
  ## Long/Lats are in degrees so need helper function to convert to radians
  degreeToRadian = function(degree) (degree * pi / 180)
  deltaLong = degreeToRadian(longBeta) - degreeToRadian(longAlpha)
  sinLat = sin(degreeToRadian(latAlpha)) * sin(degreeToRadian(latBeta))
  cosLat = cos(degreeToRadian(latAlpha)) * cos(degreeToRadian(latBeta))
  ## acos is finicky with precision so we will assume if NA is thrown
  ## the argument was very close to 1 and therefore will return 0
  ## acos(1) == 0
  acosRaw = suppressWarnings(acos(sinLat + cosLat * cos(deltaLong)))
  acosSafe = ifelse(is.na(acosRaw), 0, acosRaw)
  acosSafe * radius
}
</code></pre>

<p>Distance between Basildon, UK and Parkville, US</p>

<pre><code>greatCircleDistance(coord_data$latitude[1],
coord_data$longitude[1],
coord_data$latitude[2],
coord_data$longitude[2])

 Returns [1] [1] 6929.351 km.
</code></pre>

<p>It matches Google's calc so we are good to go!</p>

<p>Brute Force Example:
     As you noticed in your Excel sheet this will blow up quickly as data set gets larger. There are much more efficient ways of implementing the search. One idea is to start with the the geo-data structure itself and write an R-Tree, but I'll leave that for you.</p>

<pre><code> bruteForceNearestNeighbor = function(geoData) {
      makeCoordinate = function(idx) {
        c(""id"" = idx, ""latitude"" = geoData$latitude[idx], ""longitude"" = geoData$longitude[idx])
      }
      singleCoordMinDistance = function(coordinate, locations) {
        locationsUS = locations[locations$is_us == 1 &amp; locations$id != coordinate[""id""], ]
        distances = mapply(greatCircleDistance,
              latAlpha = coordinate[""latitude""],
              longAlpha = coordinate[""longitude""],
              latBeta = locationsUS$latitude,
              longBeta = locationsUS$longitude)
        closestIndex = which(distances == min(distances))
        locations[closestIndex, ""id""]
      }
      nearestNeighbors = vector(""numeric"", nrow(geoData))
      for ( i in 1:nrow(geoData) ) {
        coord = makeCoordinate(i)
        nearestNeighbors[i] = singleCoordMinDistance(coord, geoData)
      }
      nearestNeighbors
    }

    coord_data$nearest_neighbor = bruteForceNearestNeighbor(coord_data)
</code></pre>
"
['visualization'],Modelling population changes with years on a network graph,"<p>The <a href=""https://en.wikipedia.org/wiki/Sankey_diagram"" rel=""nofollow"">Sankey diagram</a> would be nicely suited for your problem statement.</p>

<p>The flow width can be the number of refugees migrated, and the end nodes can be the destinations (from and to) of the refugees, and the flow width would be the magnitude of refugee migration.</p>

<p>If you want to model the graph on a geographical map, it can look something like this:</p>

<p><a href=""http://i.stack.imgur.com/VA78L.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VA78L.png"" alt=""enter image description here""></a></p>

<p>I am not sure whether it can be done in NodeXL or not.  But, a google search has returned me this <a href=""http://djjr-courses.wikidot.com/ppol225:sankey-diagram"" rel=""nofollow"">link</a>.</p>
"
"['predictive-modeling', 'correlation']",Correlation and Naive Bayes,"<p>As you probably know, naive here implies that the ""fields"" are independent. So your question boils down to does correlation imply dependence. Yes, it does. See here.</p>

<p><a href=""http://stats.stackexchange.com/questions/113417/does-non-zero-correlation-imply-dependence"">http://stats.stackexchange.com/questions/113417/does-non-zero-correlation-imply-dependence</a></p>

<p>So, if your features show correlation then this will have an adverse effect on the naive assumption. Despite this fact, Naive Bayes has been shown to be robust against this assumption. If your model still suffers from this, however, you could consider transforming the space to be independent with methods such as PCA.  </p>
"
"['apache-spark', 'map-reduce']",Spark vs Map reduce,"<p>No, this is not in general true. For a map-only job, or map and reduce, MapReduce is a bit faster. It's more optimized for this pattern and a lot easier to scale / tune. However, the problem is that few problems are just one or two operations. Once you have a chain of them to execute, considering the entire DAG and execution plan is a win even if memory is not used for persistence.</p>

<p>Developer productivity also related to efficiency. If I don't have to spend my time writing lower-level code then I can spend more time designing a more sophisticated distributed application that could be faster.</p>

<p>You can see this in Crunch, for example, which gives you a high-level language similar to Spark Java on MapReduce, and I'd say you can write more efficient Crunch jobs in a fixed time than M/R.</p>
"
"['r', 'correlation', 'confusion-matrix']",result of correlation function in r,"<p><strong>No</strong>, it is not a confusion matrix. </p>

<p>Please have a look at the <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.html"" rel=""nofollow"">docs</a></p>

<p><a href=""http://www.inside-r.org/node/86995"" rel=""nofollow"">This</a> is how the confusion matrix is created in R.</p>
"
"['machine-learning', 'neuralnetwork', 'visualization', 'deep-learning']",Visualizing deep neural network training,"<p>The closes thing I know is <a href=""http://cs.stanford.edu/people/karpathy/convnetjs/"">ConvNetJS</a>:</p>

<blockquote>
  <p>ConvNetJS is a Javascript library for training Deep Learning models (mainly Neural Networks) entirely in your browser. Open a tab and you're training. No software requirements, no compilers, no installations, no GPUs, no sweat.</p>
</blockquote>

<p>Demos on this site plot weighs and how do they change with time (bear in mind, its many parameters, as practical networks do have a lot of neurons). Moreover, if you are not satisfied with their plotting, there is access to networks parameters and you can plot as you wish (since it is JavaScript).</p>
"
"['classification', 'feature-selection', 'feature-extraction']",Document classification: tf-idf prior to or after feature filtering?,"<p>As you've described it, Step 4 is where you want to use TF-IDF. Essentially, TD-IDF will count each term in each document, and assign a score given the relative frequency across the collection of documents.</p>

<p>There's one big step missing from your process, however: annotating a training set. Before you train your classifier, you'll need to manually annotate a sample of your data with the labels you want to be able to apply automatically using the classifier.</p>

<p>To make all of this easier, you might want to consider using the <a href=""http://nlp.stanford.edu/software/classifier.shtml"">Stanford Classifier</a>. It will perform the feature extraction and build the classifier model (supporting several different machine learning algorithms), but you'll still need to annotate the training data by hand.</p>
"
"['machine-learning', 'classification']",does pruning a decision tree always make it more general?,"<p>If you filter something out by choosing one branch over another branch in the tree, the observations you did not choose are forever lost.</p>

<p>But to directly answer your question - no, it does not always make it more general. If you construct a tree where all the decision are exactly the same, then pruning does not make it more general.</p>
"
"['dataset', 'time-series', 'infographics']",Time series change rate calculations for displaying trend line chart,"<p>While it is not very clear to me what specific relationships between your data you want to display, I can give some general advice. I think that this time series visualization calls for so-called <a href=""http://geog.uoregon.edu/datagraphics/color_scales.htm"" rel=""nofollow"">diverging color scheme</a> (as opposed to <em>sequential</em>, <em>categorical</em> and other ones). <a href=""http://colorbrewer2.org"" rel=""nofollow"">ColorBrewer</a> is nice online tool for selecting an appropriate <em>color scheme</em> and other parameters (note that <em>ColorBrewer</em>'s color schemes have built-in support in major data analysis and visualization software, such as R, Python, d3.js, plot.ly and others).</p>

<p>If you would use <code>R</code> environment, it would be quite easy to produce the time series <em>trend line chart</em> that you want by using <code>ggplot2</code> package and its <code>scale_color_gradient2()</code> function:</p>

<p><code>... + scale_color_gradient2(midpoint=midValue, low=""blue"", mid=""white"", high=""orange"" )</code></p>

<p>A quick glance at your profile suggested me that you would prefer a JavaScript solution. In that case, I would advise to use <code>d3.js</code> library or one of multiple other <em>JavaScript visualization</em> libraries or packages. For more details, check <a href=""http://datascience.stackexchange.com/a/3723/2452"">this relevant answer</a> of mine.</p>
"
"['machine-learning', 'regression', 'linear-regression']",How to move forward on Regression problem,"<p>You have little features (as in none hehe). Your data reminds me a lot of stock market predictions, where you know nothing about what you're studying except for price. Furthermore, price behaves chaotically, because if it was easily predictable, then everybody would be predicting it, thus removing its predictability.</p>

<p>Therefore, if you are serious about studying that data, you may want to look up literature on stock market prediction.</p>

<p>I suggested in a comment that one approach could be to see this as a classification problem rather than a regression problem. This is in fact what many people do in the stock market. Instead of predicting actual price, people study whether the price will go up or down (and therefore whether they should buy or sell).</p>

<p>Here is an example of what I mean, implemented in sklearn:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.cross_validation import cross_val_score
from sklearn.linear_model import LogisticRegression

# random signal
n = 100
t = np.arange(n)
x = np.random.rand(n)
plt.plot(t, x)
plt.show()
</code></pre>

<p><a href=""http://i.stack.imgur.com/waavw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/waavw.png"" alt=""random signal""></a></p>

<pre><code># model ups and downs: convert to 0s (down) and 1s (up)
t = t[1:]
y = ((np.sign(x[1:] - x[:-1])+1)/2).astype(int)
plt.plot(t, y)
plt.show()
</code></pre>

<p><a href=""http://i.stack.imgur.com/Gu42l.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Gu42l.png"" alt=""ups and downs""></a></p>

<pre><code># one look-ahead model using the four previous observations
X = np.c_[y[:-4], y[1:-3], y[2:-2], y[3:-1]]
y = y[4:]
scores = cross_val_score(LogisticRegression(), X, y, cv=10, n_jobs=-1)
print('Acc: %.2f (%.2f)' % (np.mean(scores), np.std(scores)))
</code></pre>

<p>My results:</p>

<pre><code>Acc: 0.75 (0.17)
</code></pre>

<p>One advantage of this approach is that it gives you probabilities of going up or down. Remember, a logistic regression is a probabilistic model $P(y|x)$, and here we are taking $P(y|x)&lt;0.5$ to mean down and $P(y|x)&gt;0.5$ to mean up. But if the probability is close to 0.5, you could have your model say ""I don't know"". This way you only take action when the model reports down or up with high confidence (close to 0 (down) or 1 (up)). A colleague of mine wrote <a href=""https://sigarra.up.pt/flup/pt/pub_geral.show_file?pi_gdoc_id=91387"" rel=""nofollow"">his thesis</a> on this kind of thing. Are you trying to model sports in order to place bets? If so, this is a pretty simple but effective model of choosing when it is worthy to place bets.</p>

<p>But if the idea is to learn about data mining, then I would try another dataset.</p>

<ul>
<li>sklearn comes with a bunch of <a href=""http://scikit-learn.org/stable/datasets/"" rel=""nofollow"">nice datasets</a></li>
<li>kaggle has a lot of 101 training competitions <a href=""https://www.kaggle.com/competitions"" rel=""nofollow"">rich in tutorials</a></li>
</ul>
"
"['image-classification', 'computer-vision']",Is there an open source implementation for bag-of-visual words?,"<p>There is one implementation of BoVW in openCV. You can find the documentation here :</p>

<p><a href=""http://docs.opencv.org/2.4/modules/features2d/doc/object_categorization.html"" rel=""nofollow"">http://docs.opencv.org/2.4/modules/features2d/doc/object_categorization.html</a></p>
"
"['machine-learning', 'bigdata', 'data-mining', 'statistics', 'predictive-modeling']",Looking for a strong Phd Topic in Predictive Analytics in the context of Big Data,"<p>As a fellow CS Ph.D. defending my dissertation in a Big Data-esque topic this year (I started in 2012), the best piece of material I can give you is in a link:
<a href=""http://www.rpajournal.com/dev/wp-content/uploads/2014/10/A3.pdf"" rel=""nofollow"">http://www.rpajournal.com/dev/wp-content/uploads/2014/10/A3.pdf</a></p>

<p>This is an article written by two Ph.D.s from MIT who have talked about Big Data and MOOCs. Probably, you will find this a good starting point. BTW, along this note, if you really want to come up with a valid topic (that a committee and your adviser will let you propose, research and defend) you need to read LOTS and LOTS of papers. The majority of Ph.D. students make the fatal error of thinking that some 'idea' they have is new, when it's not and has already been done. You'll have to do something truly original to earn your Ph.D. Rather than actually focus on forming an idea right now, you should do a good literature survey and the ideas will 'suggest themselves'. Good luck! It's an exciting time for you.</p>
"
['beginner'],Measuring Value in Data Science?,"<p>I read your question as:</p>

<blockquote>
  <p>tl;dr. How do you know that a data scientist adds value to a company?</p>
</blockquote>

<p>The job of a data scientist is to pick out the best data set, scrub it, fit a statistical model. And most importantly, a data scientist needs to know the problem statement and why is the analysis being done.</p>

<p>For example, consider a DS team in an e-commerce company. Their primary (let's assume) is to build a highly effective recommender system, which needs to adapt itself to a lot of variables, like the user behaviour, the session-based behaviour, user segments, etc.</p>

<p>So, if the recommender system is still not being able to get the company a considerable CTR or upsells/cross-sells, then maybe the team is not adding proper value to the company.</p>

<p>So, a data scientist needs to have a keen knowledge of the business value which an experiment is expected to add to the company. The decisions made from the experiments should ultimately add value to a company (monetary or otherwise). If not, then just like any other employee, it means that the person have failed in the job.</p>
"
"['machine-learning', 'deep-learning', 'training']",Deep learning - rule generation,"<p>One way of stating what you are looking for is to find a simple mathematical model to explain your data.</p>

<p>One thing about neural networks is that (once they have more than 2 layers, and enough neurons total) they can in theory emulate any function, no matter how complex. This is useful for machine learning as often the function we want to predict is complex and cannot be expressed simply with a few operators. However, it is kind of the opposite of what you want - the neural network behaves like a ""black box"" and you don't get a simple function out, even if there is one driving the data.</p>

<p>You can try to fit a model (any model) to your data using very simple forms of regression, such as <a href=""https://en.wikipedia.org/wiki/Linear_regression"" rel=""nofollow"">linear regression</a>. So if you are reasonably sure that your system is a cubic equation $y= ax^3 + bx^2 +cx +d$ then you could create a table like this:</p>

<pre><code>  bias   |   x  |  x*x  |  x*x*x  |     y
     1       0       0         0        0
     1       2       4         8        4
     1       3       9        27        9
     1       .         .        .       .
     1     100   1000000    10000   10000
</code></pre>

<p>and then use a <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"" rel=""nofollow"">linear regression optimiser</a> (sci-kit learn's SGD optimiser linked). With the above data this should quickly tell you $b=1, a,c,d=0$. But what it won't tell you is whether your model is the best possible or somehow ""correct"". You can scan for more possible formulae by creating more columns - any function of any combination of inputs (if there is more than one) that could be feasible. </p>

<p><em>However</em>, the more columns you add in this way, the more likely it is you will find an incorrect <em>overfit</em> solution that matches all your data using a clever combination of parameters, but which is not a good general predictor. To address this, you will need to add regularisation - a simple L1 or L2 regularisation of the parameters will do (in the link I gave to scikit-learn, the <code>penalty</code> argument can control this), which will penalise large parameters and help you home in on a simple formula if there is one.</p>
"
['data-mining'],Method for finding top-k cosine similarity based closest item on large dataset,"<p><a href=""https://en.wikipedia.org/wiki/Locality-sensitive_hashing"" rel=""nofollow"">Locality sensitive hashing</a> is a great tool for this problem.</p>

<p>Pick <em>n</em> random 400-dimensional vectors. (Be careful or not all directions will be chosen with equal probability; pick each dimension as a standard Gaussian.) Each really defines a hyperplane through the origin cutting your space in half. The sign of the dot product of any of these vectors with some new vector tells you which side of the hyperplane it's on. So computing <em>n</em> dot products gives <em>n</em> 0/1 bits, which make an n-bit hash. </p>

<p>Any new vector hashing to the same value must be in the same small sliver of space from the origin. And those are exactly the vectors that have a high cosine similarity with each other since their mutual angles are very small. Likewise anything hashing to almost the same value -- differing in a few bits -- is likely to be nearby. So you can restrict your search for most-similar vectors to things within one or more buckets of hashed candidate vectors. </p>

<p>It doesn't help directly with memory since you might need any particular bucket to satisfy a request. You also lose some accuracy since there is not a guarantee the most similar vectors lie in the buckets you examine (though it's likely, the more you examine). It lets you trade speed for accuracy mostly. However you may find you can get away with some caching scheme, where some buckets are rarely if ever accessed and so don't stay in memory.</p>

<p>You can see an implementation of this in Oryx, which I think is pretty straightforward: <a href=""https://github.com/OryxProject/oryx/blob/master/app/oryx-app-serving/src/main/java/com/cloudera/oryx/app/serving/als/model/LocalitySensitiveHash.java"" rel=""nofollow"">https://github.com/OryxProject/oryx/blob/master/app/oryx-app-serving/src/main/java/com/cloudera/oryx/app/serving/als/model/LocalitySensitiveHash.java</a></p>

<p>Most of the complexity comes because it lets you specify a target percentage of vectors to evaluate, and works out the optimal hash size based on that and your machine's number of cores.</p>
"
"['python', 'scikit', 'random-forest', 'churn']",Understanding churn prediction model,"<p>A couple good options would be to look at a <a href=""http://freakonometrics.hypotheses.org/19835"" rel=""nofollow"">Feature/Variable importance plot</a> for your RF model.  Alternatively, depending on the model you could try extracting a couple individual trees from the model and examining them.  However, these methods  wouldn't be definitive; i.e. determining what variables are strong predictors for churn does not mean that they have a causal impact on churn, and an individual tree may be biased and not representative of the aggregation output presented by the RF model. To determine causation, you could use these methods as a starting point to design a test.  </p>
"
"['python', 'clustering', 'k-means']",Determinate K in K-Means Clustering,"<p>Wang, Kaijun, Baijie Wang, and Liuqing Peng. ""CVAP: Validation for cluster analyses."" Data Science Journal 0 (2009): 0904220071.:</p>

<blockquote>
  <p>To measure the quality of clustering results, there are two kinds of
  validity indices: external indices and internal indices. </p>
  
  <p>An external
  index is a measure of agreement between two partitions where the first
  partition is the a priori known clustering structure, and the second
  results from the clustering procedure (Dudoit et al., 2002). </p>
  
  <p>Internal
  indices are used to measure the goodness of a clustering structure
  without external information (Tseng et al., 2005).</p>
  
  <p>For external indices, we evaluate the results of a clustering algorithm based on a known cluster structure of a data set (or cluster labels).</p>
  
  <p>For internal indices, we evaluate the results using quantities and features inherent in the data set. The optimal number of clusters is usually determined based on an internal validity index.</p>
</blockquote>

<p>(Dudoit et al., 2002): Dudoit, S. &amp; Fridlyand, J. (2002) A prediction-based resampling method for estimating the number of clusters in a dataset. Genome Biology, 3(7): 0036.1-21.</p>

<p>(Tseng et al., 2005): Thalamuthu, A, Mukhopadhyay, I, Zheng, X, &amp; Tseng, G. C. (2006) Evaluation and comparison of gene clustering methods in microarray analysis. Bioinformatics, 22(19):2405-12.</p>

<hr>

<p>In your case, you need some internal indices since you have no a priori clustering structure. There exist tens of internal indices, like:</p>

<ul>
<li>Silhouette index (implementation in <a href=""http://www.mathworks.com/help/stats/silhouette.html"">MATLAB</a>)</li>
<li>Davies-Bouldin</li>
<li>Calinski-Harabasz</li>
<li>Dunn index (implementation in <a href=""http://www.mathworks.com/matlabcentral/fileexchange/27859-dunns-index"">MATLAB</a>)</li>
<li>R-squared index</li>
<li>Hubert-Levin (C-index)</li>
<li>Krzanowski-Lai index</li>
<li>Hartigan index</li>
<li>Root-mean-square standard deviation (RMSSTD) index</li>
<li>Semi-partial R-squared (SPR) index</li>
<li>Distance between two clusters (CD) index</li>
<li>weighted inter-intra index</li>
<li>Homogeneity index</li>
<li>Separation index</li>
</ul>

<p>Each of them have pros and cons, but at least they'll give you a more formal basis for your comparison. The MATLAB toolbox <a href=""http://www.mathworks.com/matlabcentral/fileexchange/14620-cvap-cluster-validity-analysis-platform-cluster-analysis-and-validation-tool"">CVAP</a> might be handy as it contains many internal validity indices.</p>

<hr>

<p>More information:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set"">Determining the number of clusters in a data set</a></li>
<li><a href=""http://stats.stackexchange.com/q/76093/12359"">How to determine which method is the most reasonable clustering results?</a></li>
<li><a href=""https://www.quora.com/How-can-I-detect-if-my-dataset-is-clustered-or-unclustered-i-e-forming-one-single-cluster?share=1"">How can I detect if my dataset is clustered or unclustered (i.e. forming one single cluster)?</a></li>
<li><a href=""https://www.quora.com/How-can-we-choose-a-good-K-for-K-means-clustering?srid=3p9p"">How can we choose a ""good"" K for K-means clustering?</a></li>
<li><a href=""http://stackoverflow.com/q/15376075/395857"">Cluster analysis in R: determine the optimal number of clusters</a></li>
</ul>
"
"['time-series', 'feature-selection', 'predictive-modeling', 'optimization']",Time series: variations as a feature,"<p>Using derivatives as features is <strong>almost the same</strong> as using past values, as both reconstruct phase or state space for dynamic system behind the time series. but they differ in some points, like noise amplification and how they carry information.
(see <strong>Ref</strong>: State space reconstruction in the presence of noise; Martin Casdagli; Physica D - 1991 - section 2)</p>

<p>Notice all information is embedded in time series, but using derivatives is going to reinterpret this information, which may be useful or useless.</p>

<p>In your case, if you use all parameters and terms, i believe there is no use in it. but in case of using some algorithms like orthogonal forward regression (OFR) it may be beneficial. (see <strong>Ref</strong>: Orthogonal least squares methods and their application to non-linear
system identification; S. CHEN, S. A. BILLINGS; INT. J. CONTROL, 1989)</p>
"
['time-series'],"How to merge monthly, daily and weekly data?","<blockquote>
  <p><strong>when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?</strong></p>
</blockquote>

<p>For your timeseries analysis you should do both: get to the highest granularity possible with the daily dataset, and also repeat the analysis with the monthly dataset. With the monthly dataset you have 120 data points, which is sufficient to get a timeseries model even with seasonality in your data.</p>

<blockquote>
  <p><strong>For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?</strong></p>
</blockquote>

<p>To obtain say weekly or monthly data from daily data, you can use smoothing functions. For financial data, you can use moving average or exponential smoothing, but if those do not work for your data, then you can use the spline smoothing function ""smooth.spline"" in R: <a href=""https://stat.ethz.ch/R-manual/R-patched/library/stats/html/smooth.spline.html"" rel=""nofollow"">https://stat.ethz.ch/R-manual/R-patched/library/stats/html/smooth.spline.html</a></p>

<p>The model returned will have less noise than the original daily dataset, and you can get values for the desired time points. Finally, these data points can be used in your timeseries analysis. </p>

<blockquote>
  <p><strong>For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?</strong></p>
</blockquote>

<p>To obtain daily data when you have monthly or weekly data, you can use interpolation. First, you should find an equation to describe the data. In order to do this you should plot the data (e.g. price over time). When factors are known to you, this equation should be influenced by those factors. When factors are unknown, you can use a best fit equation. The simplest would be a linear function or piecewise linear function, but for financial data this won't work well. In that case, you should consider piecewise cubic spline interpolation. This link goes into more detail on possible interpolation functions: <a href=""http://people.math.gatech.edu/~meyer/MA6635/chap2.pdf"" rel=""nofollow"">http://people.math.gatech.edu/~meyer/MA6635/chap2.pdf</a>. </p>

<p>In R, there is a method for doing interpolation of timeseries data. Here you would create a vector with say weekly values and NAs in the gaps for the daily values, and then use the ""interpNA"" function to get the interpolated values for the NAs. However, this function uses the ""approx"" function to get the interpolated values, which applies either a linear or constant interpolation. To perform cubic spline interpolation in R, you should use the ""splinefun"" function instead.</p>

<p>Something to be aware of is that timeseries models typically do some sort of averaging to forecast future values whether you are looking at exponential smoothing or Auto-Regressive Integrated Moving Average (ARIMA) methods amongst others. So a timeseries model to forecast daily values may not be the best choice, but the weekly or monthly models may be better. </p>
"
"['classification', 'performance']",Measuring performance of different classifiers without class type in data,"<p>Normal classification will not work in this case, classifiers learn functions that are able to seperate the training examples. In your case you only have one class which will not work for all the classical examples. There is a machine learning task called One-class classification, see this <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow"">Wikipedia page</a>. In your case PU Learning (Positive and Unlabeled) seems most appropriate. Evaluating the performance is extremely difficult without having some negative examples as well however. See <a href=""http://stats.stackexchange.com/questions/223956/how-are-performance-measures-affected-in-pu-learning"">this question and answer</a> on the Stats stackexchange.</p>
"
"['machine-learning', 'svm', 'regularization']",Why does an L2 penalty prefer smaller and more diffuse weight vectors?,"<p>You answered this in your question. ""Prefer"" means ""produces a smaller penalty"", and you've identified that the penalty in the first case is smaller. Why would this be a good thing? It amounts to preferring an explanation based a bit on many features, rather than one based entirely on one features. That's often a good bet to avoid overfitting.</p>

<p>These two weight vectors do not produce the same dot product with other vectors in general. If the vector X contained all identical values, they would.</p>

<p>If the 4 input features were regularly identical or nearly so, then it means they're redundant, and you may prefer to use just 1 of the features (your second case), instead of a bit of each. In this case, an L1 penalty would at least be indifferent to the two, not penalize the second one.</p>
"
"['nosql', 'performance']",What is the Best NoSQL backend for a mobile game,"<p>Some factors you might consider:</p>

<p>Developer familiarity: go with whatever you or your developers are familiar with.  Mongo, Couch, Riak, DynamoDB etc all have their strengths but all should do ok here, so rather than going for an unfamiliar solution that might be slightly better go for familiar and save a bunch of development time.</p>

<p>Ease of cloud deployment:  for example, if you are using Amazon AWS, then DynamoDB is likely an excellent choice.  Sure, you could use Mongo on AWS, but why bother?  Other cloud providers have their own preferred db, for example if you are using Google AppEngine, it makes sense to use BigTable or Cloud Datastore. </p>

<p>Your use case seems both well suited to NoSQL and not very challenging since your data has a natural partition by user.  I think you'd be technically ok with anything, which is why I'm mainly covering other factors.</p>
"
"['machine-learning', 'classification', 'svm', 'supervised-learning']",How do we know Kernels are successful in making data linearly Separable?,"<p>You cannot guarantee this. Some data <em>is</em> not separable by any kernel because of duplicates.</p>

<p>By trying too hard, you will <strong>cause overfitting</strong>. Essentially, you force the implicit mapping to be so complex it contains a copy of your training data (which is exactly what happens if you choose a too small bandwidth with RBF).</p>

<p>If you want a good generalization performance, you will have to tolerate some errors, and use e.g. soft-margin and such techniques.</p>

<p>Perfect separation is not something to aim for. Such a guarantee is just a guarantee of being able to overfit! Use cross-validation to reduce the risk of overfitting and find the right balance between being optimal on training data and actual performance.</p>
"
"['dataset', 'neuralnetwork']",How many observations in a neural networks dataset?,"<p>A neural network is nothing but a set of equations. And the basic rule of any set of equations is that you must have as many data points as the number of parameters.</p>

<p>The parameters of any neural network are its weights and biases.</p>

<p>So that means that as the neural network gets deeper and wider, the number of parameters increase a lot, and so must the data points.</p>

<p>This being said, the more proper and detailed way to know whether the model is <a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow"">overfitting</a> is to check if the validation error is close to the training error. If yes, then the model is working fine. If no, then the model is most likely overfitting and that means that you need to reduce the size of your model or introduce regularization techniques.</p>
"
"['python', 'clustering', 'scikit', 'clusters', 'dbscan']",How to plot/visualize clusters in scikit-learn (sklearn)?,"<p>When I run the code you posted, I get three points on my plot:</p>

<p><a href=""http://i.stack.imgur.com/KpbTX.png""><img src=""http://i.stack.imgur.com/KpbTX.png"" alt=""clusters""></a></p>

<p>The ""point"" at (0, 4) corresponds to <code>X[1]</code> and the ""point"" at (0, 5) is actually three points, corresponding to <code>X[0]</code>, <code>X[2]</code>, and <code>X[3]</code>. The point at (5, 5) is the last point in your <code>X</code> array. The data at (0, 4) and (0, 5) belong to one cluster, and the point at (5, 5) is considered noise (plotted in black).</p>

<p>The issue here seems to be that you're trying to run the <code>DBSCAN</code> algorithm on a dataset containing 5 points, with at least 4 points required per cluster (the second argument to the <code>DBSCAN</code> constructor). In the <code>sklearn</code> example, the clustering algorithm is run on a dataset containing 750 points with three distinct centers. Try creating a larger <code>X</code> dataset and running this code again.</p>

<p>You might also want to remove the <code>plt.ylim([0,10])</code> and <code>plt.xlim([0,10])</code> lines from the code; they're making it a bit difficult to see the points on the edge of the plot! If you omit the <code>ylim</code> and <code>xlim</code> then <code>matplotlib</code> will automatically determine the plot limits.</p>
"
['sas'],Why do we need to use sysfunc when we call a SAS function inside a SAS macro,"<p>Without the sysfunc(), the expression will not be evaluated. You will not be assigning the value of the expression trim(&amp;num) to the macrovariable, but rather the whole expression.</p>

<p>If you want to store the result of an expression, you need to execute that function with sysfunc()</p>

<p><a href=""http://support.sas.com/documentation/cdl/en/mcrolref/61885/HTML/default/viewer.htm#z3514sysfunc.htm"" rel=""nofollow"">http://support.sas.com/documentation/cdl/en/mcrolref/61885/HTML/default/viewer.htm#z3514sysfunc.htm</a></p>
"
"['machine-learning', 'predictive-modeling']",Predicting app usage on mobile phone,"<p>So you have collected data that shows which app is being used at any time, binned into hours in the day. And you have several apps. You mention other dimensions, like user state when using the app (walking, not-walking), (active, not-active - it appears to me that you are not collecting much usage 2-6. is that because the usage is from a self-directed ping from an app while the user is truly away?), location (is this going to be all possible values, or are you going to use something like the fact that this location has been seen often before?). Another interesting relationship could be pairing apps, i.e., mining for a relationship between App A being used after using App B or before App B. </p>

<p>Regardless, then you will definitely have many different dimensions upon which to measure the usage characteristics for any particular usage measurement, and so you are definitely going to have a multiple dimensional problem. You might try to visualize this as an N-space problem, with an axis of measurement for each of your characteristics. Each of your previous measurements represents vectors and you are producing a new vector with your next measurement.</p>

<p>From this, you want to predict future behavior based on measuring the input characteristics from your usage space. You could go for something that classifies as nearest neighbor, and you probably want to do this for your first stab at the problem. You might end up wanting to make the predictive model more sophisticated by adding probabilities to the classifier and acting on that. This means getting estimates of class membership probability rather than just simple classifications. But I would build the whole thing incrementally. Start simple and add complexity as you require it. The increased complexity will also have effects on performance, so why not baseline with something.</p>

<p>For the aging of data, are you wanting to reduce the predictive power of characteristics that are too long in the tooth? If so, be explicit with yourself about what that means, quantitively. Do I trust the usage data from last month less than yesterday's data? Perhaps so, but then why? is my usage different because I am different or because last month was special compared to yesterday, or vice-versa? Again, you might benefit from ignoring this at first, but then trying to <strong>search</strong> for ""seasonal"" or periodicity characteristics from the data. Once you determine if/how it changes, you can weight that contribution compared to your immediate usage in different ways. Perhaps you want to amplify the contribution of a similar period (same time of day &amp;&amp; same location &amp;&amp; same previous app usage). Perhaps you want to provide an exponential dampening on historical data because the usage is always adapting and changing, and recent usage seems to be a <strong>much</strong> better predictor than 3xcurrent. </p>

<p>For all of this, the proper data science perspective is to let the data lead you. </p>
"
"['python', 'bigdata', 'hadoop']",what should learn for become data scientist?,"<p><strong>From personal experience</strong> (so take into consider that I might not be representative although I'm probably not that far away too) the people that approached me with a job offer for </p>

<blockquote>
  <p>Data Scientist</p>
</blockquote>

<p>did so because:</p>

<p><strong>1)</strong> Considerable knowledge in one or more programming language typically used for data analysis. In my case <a href=""https://www.python.org/"" rel=""nofollow"">Python</a>.</p>

<p><strong>2)</strong> Knowledge in applied mathematics (usually they don't even care about the base field). You just have to know how to interpret data and take valid conclusions from it (as a starting point at least).</p>

<p><strong>3)</strong> Past experience with libraries such as <a href=""http://www.numpy.org/"" rel=""nofollow"">numpy</a>, <a href=""https://www.scipy.org/"" rel=""nofollow"">scipy</a>, <a href=""http://scikit-learn.org/stable/"" rel=""nofollow"">scikit-learn</a> (very relevant), <a href=""http://scikit-image.org/"" rel=""nofollow"">scikit-image</a> (if you are going to do image analysis also), <a href=""http://pandas.pydata.org/"" rel=""nofollow"">pandas</a>.</p>

<p><strong>4)</strong> Past experience with data visualization libraries such as <a href=""http://matplotlib.org/"" rel=""nofollow"">matplotlib</a>, <a href=""https://stanford.edu/~mwaskom/software/seaborn/"" rel=""nofollow"">seaborn</a>, <a href=""http://code.enthought.com/projects/chaco/"" rel=""nofollow"">Chaco</a>, <a href=""http://ggplot.yhathq.com/"" rel=""nofollow"">ggplot</a>, <a href=""http://www.pyqtgraph.org/"" rel=""nofollow"">pyQtGraph</a>, <a href=""http://bokeh.pydata.org/en/latest/"" rel=""nofollow"">Bokeh</a>, etc.</p>

<p><strong>5)</strong> Knowledge about <a href=""http://scikit-learn.org/stable/supervised_learning.html#supervised-learning"" rel=""nofollow"">regression techniques</a>, <a href=""http://scikit-learn.org/stable/modules/clustering.html#clustering"" rel=""nofollow"">clustering</a>, and <a href=""http://scikit-learn.org/stable/supervised_learning.html#supervised-learning"" rel=""nofollow"">classification</a>.</p>

<p><strong>6)</strong> Valid extras depending on the field are typical applied mathematics in <a href=""http://hpgl.github.io/hpgl/"" rel=""nofollow"">space estimation</a>, <a href=""http://scikit-image.org/docs/dev/auto_examples/"" rel=""nofollow"">image analysis and processing</a> and <a href=""http://opencv.org/"" rel=""nofollow"">computer vision</a>, <a href=""http://docs.enthought.com/mayavi/mayavi/"" rel=""nofollow"">3D visualization</a> .</p>

<p><strong>7)</strong> If you already have experience in building <a href=""https://sourceforge.net/projects/geoms2/"" rel=""nofollow"">scientific software solutions</a> using those programming languages, it might be a great advantage.</p>

<ul>
<li>With point <strong>7)</strong> in mind you might consider looking at <a href=""https://www.riverbankcomputing.com/software/pyqt/download5"" rel=""nofollow"">PyQt5</a> and <a href=""http://www.wxpython.org/"" rel=""nofollow"">wxPython</a>.</li>
</ul>

<p><strong>8)</strong> Ideally you are also able to present your results to an assistance that is not necessarily made of scientists only (I advise lots of illustrations..., actually, now that I think about it, lots of illustrations even if it's only scientists). So this takes some skill into building appropriate diagrams and figures (see vector graphics software such as <a href=""https://inkscape.org/en/"" rel=""nofollow"">Inkscape</a>, together with plotting libraries it can make wonders).</p>

<p><strong>9)</strong> Last but not least quite a bit of flexibility (this is common for scientific and development staff). Sometimes you need to change your technology and this takes some learning.</p>

<p>Notice that my experience does not say much in terms of web development <em>per se</em>. Mine is a scientific background with very little of web development so people that approach me, do so with this in mind. Other fields might request for different skills (and by the way, you don't need to be a web developer to deal with web data).</p>
"
"['machine-learning', 'apache-spark', 'ranking']",Algorithm Suggestion For a Specific Problem,"<p>You can use a clustering algorithm such as k-means to divide the generators into groups.  You never know what kind of groups you'll get until you try it.  Try and assess the character of each group of generators as you increase the number of clusters.  At some point you should find a meaningful division of generators.  The inputs to your k-means algorithm will be the criteria you mentioned in your post: the number of times it was activated, the number of activation problems, and so forth.  When you are finished, the group a generator belongs to is its ranking.  This method will not generate a ranking of 1-1000 if you have 1000 generators.  Rather it will give you, for example with k=3: a group of 243 outstanding generators, 320 average generators, and 446 terrible generators.   </p>
"
"['r', 'predictive-modeling', 'regression']",Kernel regression results in diverse outputs,"<p>You have a very small number of observations (32?) and a non-trivial number of predictors. </p>

<p>It is known that the cross-validation function possesses multiple local minima/maxima. If you increase the number of multistarts to, say, nmulti=100 (add this option to your call to npregbw()), you ought to see that the same results occur on each invocation of the optimization process. </p>

<p>Note you can do this all in the call to npreg() and skip the bandwidth call for convenience (npreg() will call npregbw() automatically but accept the arguments intended for npregbw()). </p>

<p>Also, you will get the same results if you restart R each time you run the routine (seeds are set automatically to ensure this).</p>

<pre><code>model &lt;- npreg(power ~ temperature  
                     + prevday1 
                     + prevday2
                     + prev_instant1 
                     + prev_instant2 
                     + prev_2_hour,
                regtype=""ll"",
                bwmethod=""cv.aic"", 
                nmulti=100,
                data=new_tr_dat)
</code></pre>

<p>To see whether things are stable with respect to the number of multistarts, look at the value of the cross-validation function and summary provided. Also, you can look at partial regression plots along with resampled variability bounds via plot().</p>

<pre><code>summary(model$bws)
plot(model,common.scale=FALSE,plot.errors.method=""bootstrap"")
</code></pre>

<p>Also, I note that your predictor `temperature' is discrete, so you might consider using</p>

<pre><code>+ ordered(temperature)
</code></pre>

<p>(i.e. use a discrete support kernel). Doing so reveals that there is little signal in this model, but the same holds for a simple parametric model (adjusted r-squared is negative).</p>

<pre><code>model.lm &lt;- lm(model$bws$formula)
summary(model.lm)
</code></pre>

<p>Hope this helps!</p>
"
"['feature-extraction', 'preprocessing', 'featurization']",How to define a distance measure between two IP addresses?,"<p>If I understood them correctly, both Jeremy and Edmund's (first) solutions are the same, namely, plain euclidean distance in a 4-dimensional space of IP addresses.BTW, I think a very fast alternative to euclidean distance would be to calculate a hamming distance bit-wise.</p>

<p>Edmund's first update would be better than his second. The reason is simple to state: his 2nd update tries to define a distance measure by considering a <strong>non-linear</strong> function of the coordinates of a 4D vector. That however will most likely destroy the key properties that it needs to satisfy in order to be a metric, namely </p>

<ol>
<li>Injectivity: $d(IP_1,IP_2)=0 \iff IP_1=IP_2$, </li>
<li>Symmetry: $d(IP_1,IP_2)=d(IP_2,IP_1)$, and </li>
<li>Triangular inequality: $d(IP_1,IP_2)\leq d(IP_1,IP_3)+d(IP_3,IP_2)\,\forall IP_3$. </li>
</ol>

<p>The latter is key for later interpreting small distances as close points in IP space. One would need a linear (in the coordinates) distance function. However, simple euclidean distance is not enough as you saw. </p>

<p>Physics (well, differential geometry actually) could lead to a nice solution to this problem: define a metric tensor $g$. In plain english, give weights to each <strong>pair of coordinates</strong>, take each pair difference, square it and multiply it by its weight, and then add those products. Take the square root of that sum and define it as your distance.</p>

<p>For the sake of simplicity, one could start trying with a diagonal metric tensor.</p>

<blockquote>
  <p>Example: Say you take $g=\begin{pmatrix}1000 &amp;0 &amp;0 &amp;0 \\0 &amp;100&amp;0&amp;0\\0&amp;0&amp;10&amp;0\\0&amp;0&amp;0&amp;1\end{pmatrix}$ $IP_1=(x_1,x_2,x_3,x_4)$ and
  $IP_2=(y_1,y_2,y_3,y_4)$. Then the square of the distance is given by
  $$d(IP_1,IP_2)^2=1000*(x_1-y_1)^2+100*(x_2-y_2)^2+\\ \,+10*(x_3-y_3)^2+1*(x_4-y_4)^2$$
  For $IP_1=192.168.1.1,\,IP_2=192.168.1.2$ the distance is clearly 1.
  However, for $192.168.1.1$ and $191.168.1.1$ the distance is
  $\sqrt{1000}\approx 32$</p>
</blockquote>

<p>Eventually you could play around with different weights and set a kind of normalization where you could fix the value of the maximal distance $d(0.0.0.0,FF.FF.FF.FF)$. </p>

<p>Furthermore, this set up allows for more complex descriptions of your data where the relevant distance would contain ""cross-products"" of coordinates like say $g_{13}*(x_1-y_1)*(x_3-y_3)$.</p>

<p>EDIT: While this would be a better ""weighting"" method than using those other I addressed, I realize now it is actualy meaningless: As Anony-Mousse and Phillip mention, IP are indeed 32 dimensional. This means in particular that giving the same weight to all bits in say the 2nd group is in general not sound: One bit could be part of the netmask while the other not. See Anony-Mousse answer for additional objections. </p>
"
"['machine-learning', 'classification', 'random-forest', 'decision-trees', 'preprocessing']",Should we convert independent continous variables (features) to categorical variable before using decision tree like classifier?,"<p>There is no need to split continuous variables because the tree already does that automatically. The only way you can test for overfitting is by either using a holdout set or by doing cross validation. If you are overfitting, changing a continuous variable to a categorical variable likely won't make a difference. If you get the sense that you're overfitting, you should reduce the depth of your tree. </p>
"
"['dataset', 'neuralnetwork', 'sampling']",On fitting a Poisson distribution to make sense of data,"<p>Unlike other graphs, the degree distribution is a function of N. Specifically, the number of nodes with k=2 is constant (4), the  number of nodes with k=3 grows linearly with the length of one side $4l-4$, while the number of nodes with k=4 grows exponentially with the length of a side, $l^2-4l-4$. In the large $l$ (or $N$) limit, the degree distribution is 4. </p>

<p>You can get the exact distribution by normalizing this piecewise defined function (divide by $l^2=N$). </p>
"
"['neuralnetwork', 'deep-learning']",Why does sigmoid/tanh activation function is still used for deep NN when we have ReLU?,"<ul>
<li><p>In certain network structures having symmetric activation layers has advantages (certain autoencoders for example)</p></li>
<li><p>In certain scenarios having an activation with mean 0 is important (so tanh makes sense).</p></li>
<li><p>Sigmoid activation in the output layer is still important for classification</p></li>
</ul>

<p>In 95% of the cases ReLU is much better though.</p>
"
"['machine-learning', 'classification']",Using Machine Learning to Predict Musical Scales,"<p>From a very high level -- You can convert the song to a spectrogram, there are a large number of implementations to do this. From there you can analyze the sound waves. In the case of the key, for instance, the note A is equal to 440 hz. Look into FFT as well.  Hope this helps get you started. I know spotify trains neural networks on spectrograms of songs to find similar songs based on ""sound"".</p>
"
"['classification', 'dataset', 'neuralnetwork', 'svm']",Reason behind choosing Neural Network for classification,"<p>SVM is Parametric. Parametric models are something with fixed finite number of parameters independent of dataset size. Anything which is not parametric model is non-parametric model. ANN is non parametric. Also ANN has 'deep architectures"" which can represent ""intelligent"" behaviour/functions etc more efficiently than ""shallow architectures"" like SVMs. ANN may have any number of outputs, while support vector machines have only one. </p>
"
"['feature-extraction', 'feature-scaling', 'featurization']",What is a good way to transform Cyclic Ordinal attributes?,"<p>The most logical way to transform hour is into two variables that swing back and forth out of sink. Imagine the position of the end of the hour hand of a 24-hour clock. The <code>x</code> position swings back and forth out of sink with the <code>y</code> position. For a 24-hour clock you can accomplish this with <code>x=sin(2pi*hour/24)</code>,<code>y=cos(2pi*hour/24)</code>.  </p>

<p>You need both variables or the proper movement through time is lost.  This is due to the fact that the derivative of either sin or cos changes in time where as the <code>(x,y)</code> position varies smoothly as it travels around the unit circle.</p>

<p>Finally, consider whether it is worthwhile to add a third feature to trace linear time, which can be constructed my hours (or minutes or seconds) from the start of the first record or a Unix time stamp or something similar.  These three features then provide proxies for both the cyclic and linear progression of time e.g. you can pull out cyclic phenomenon like sleep cycles in people's movement and also linear growth like population vs. time.</p>

<p>Hope this helps!</p>

<p><strong>Adding some relevant example code that I generated for another answer:</strong></p>

<p><strong>Example of if being accomplished:</strong></p>

<pre><code># Enable inline plotting
%matplotlib inline

#Import everything I need...

import numpy as np
import matplotlib as mp

import matplotlib.pyplot as plt
import pandas as pd

# Grab some random times from here: https://www.random.org/clock-times/
# put them into a csv.
from pandas import DataFrame, read_csv
df = read_csv('/Users/angus/Machine_Learning/ipython_notebooks/times.csv',delimiter=':')
df['hourfloat']=df.hour+df.minute/60.0
df['x']=np.sin(2.*np.pi*df.hourfloat/24.)
df['y']=np.cos(2.*np.pi*df.hourfloat/24.)

df
</code></pre>

<p><a href=""http://i.stack.imgur.com/1UTYj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1UTYj.png"" alt=""enter image description here""></a></p>

<pre><code>def kmeansshow(k,X):

    from sklearn import cluster
    from matplotlib import pyplot
    import numpy as np

    kmeans = cluster.KMeans(n_clusters=k)
    kmeans.fit(X)

    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_
    #print centroids

    for i in range(k):
        # select only data observations with cluster label == i
        ds = X[np.where(labels==i)]
        # plot the data observations
        pyplot.plot(ds[:,0],ds[:,1],'o')
        # plot the centroids
        lines = pyplot.plot(centroids[i,0],centroids[i,1],'kx')
        # make the centroid x's bigger
        pyplot.setp(lines,ms=15.0)
        pyplot.setp(lines,mew=2.0)
    pyplot.show()
    return centroids
</code></pre>

<p>Now lets try it out:</p>

<pre><code>kmeansshow(6,df[['x', 'y']].values)
</code></pre>

<p><a href=""http://i.stack.imgur.com/MkVNg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MkVNg.png"" alt=""enter image description here""></a></p>

<p>You can just barely see that there are some after midnight times included with the before midnight green cluster.  Now lets reduce the number of clusters and show that before and after midnight can be connected in a single cluster in more detail:</p>

<pre><code>kmeansshow(3,df[['x', 'y']].values)
</code></pre>

<p><a href=""http://i.stack.imgur.com/bWg2B.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bWg2B.png"" alt=""enter image description here""></a></p>

<p>See how the blue cluster contains times that are from before and after midnight that are clustered together in the same cluster...</p>

<p>QED!</p>
"
['beginner'],Tips for a new data scientist,"<p>I think there are a lot of important soft skills to consider in the Data Science domain. Here are some of them.</p>

<p>1) Know for a fact what the goal is, spending a lot of time on data wrangling, models, visualization and reports when it was not all for the specific goal in mind is a waste. Communicating with less technical people is a skill in itself.</p>

<p>2) Iterate repeatedly with the product owner. Keep making sure you are on the right path.</p>

<p>3) If the data doesn't tell the story they thought/want tell them it is not the case, be clear in why this is happening, what biases might be playing a role etcetera. Do not apply all kinds of filters or keep changing parameters to get to the desired results.</p>

<p>With regards to the second question:</p>

<p>The objective has to be either gotten from the product owner explicitly or derived from a less mathematical objective. An example could be where you need to predict train arrivals based on some features. They want the model to predict as many times as possible within a 10 minute error range. This is relatively explicit. </p>

<p>Sometimes it is less clear than that, they might say we need it as accurate as possible. Then you will have to decide what to optimize, in some cases this will just be minimizing the MSE but in other cases other things might make more sense for your case. Usually this will be clear from the implicit objective and something that you will get better at with more experience. Both implicit and explicit objectives derive from clear communication with the product owner.</p>
"
"['machine-learning', 'r', 'open-source']",Exporting R model to OpenCV's Machine Learning Library,"<p>Instead of <em>exporting</em> your models, consider creating an R-based <strong>interoperable environment</strong> for your <em>modeling</em> needs. Such environment would consists of <em>R environment</em> proper as well as <em>integration layers</em> for your third-party libraries. In particular, for the <em>OpenCV</em> project, consider either using <code>r-opencv</code> open source project (<a href=""https://code.google.com/p/r-opencv"" rel=""nofollow"">https://code.google.com/p/r-opencv</a>), or integration via <em>OpenCV</em> C++ APIs and <em>R</em> <code>Rcpp</code> package (<a href=""http://dirk.eddelbuettel.com/code/rcpp.html"" rel=""nofollow"">http://dirk.eddelbuettel.com/code/rcpp.html</a>). Finally, if you want to add <em>PMML</em> support to the mix and create a <strong>deployable-to-cloud solution</strong>, take a look at the following excellent blog post with relevant examples: <a href=""http://things-about-r.tumblr.com/post/37861967022/predictive-modeling-using-r-and-the"" rel=""nofollow"">http://things-about-r.tumblr.com/post/37861967022/predictive-modeling-using-r-and-the</a>.</p>
"
"['machine-learning', 'data-cleaning']",What are the best practices to anonymize user names in data?,"<p>I suspected you were using the names as identifiers. You shouldn't; they're not unique and they raise this privacy issue. Use instead their student numbers, which you can verify from their IDs, stored in hashed form. Use the student's last name as a salt, for good measure (form the string to be hashed by concatenating the ID number and the last name).</p>
"
"['neuralnetwork', 'normalization']",When to use (He or Glorot) normal initialization over uniform init? And what are its effects with Batch Normalization?,"<p>The normal vs uniform init seem to be rather unclear in fact.</p>

<p>If we refer solely on the <a href=""http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf"" rel=""nofollow"">Glorot</a>'s and <a href=""http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf"" rel=""nofollow"">He</a>'s initializations papers, they both use a similar theoritical analysis: they find a good variance for the distribution from which the initial parameters are drawn. This variance is adapted to the activation function used and is derived without explicitly considering the type of the distribution. As such, their theorical conclusions hold for any type of distribution of the determined variance. In fact, in the Glorot paper, a uniform distribution is used whereas in the He paper it is a gaussian one that is chosen. The only ""explaination"" given for this choice in the He paper is:</p>

<blockquote>
  <p>Recent deep CNNs are mostly initialized by random weights drawn from Gaussian distributions</p>
</blockquote>

<p>with a reference to <a href=""http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"" rel=""nofollow"">AlexNet paper</a>. It was indeed released a little later than Glorot's initialization but however there is no justificaion in it of the use of a normal distribution.</p>

<p>In fact, in a <a href=""https://github.com/fchollet/keras/issues/52"" rel=""nofollow"">discussion on Keras issues tracker</a>, they also seem to be a little confused and basically it could only be a matter of preference... (i.e. hypotetically Bengio would prefer uniform distribution whereas Hinton would prefer normal ones...) One the discussion, there is a small benchmark comparing Glorot initialization using a uniform and a gaussian distribution. In the end, it seems that the uniform wins but it is not really clear.</p>

<p>In the original <a href=""https://arxiv.org/pdf/1512.03385v1.pdf"" rel=""nofollow"">ResNet paper</a>, it only says they used a gaussian He init for all the layers, I was not able to find where it is written that they used a uniform He init for the first layer. (maybe you could share a reference to this?)</p>

<p>As for the use of gaussian init with Batch Normalization, well, with BN the optimization process is less sensitive to initialization thus it is just a convention I would say.</p>
"
"['python', 'pandas']",Replacing column values in Pandas,"<p>As <a href=""http://datascience.stackexchange.com/users/381/emre"">Emre</a> already said you may use the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow"">groupby</a> function. After that you should apply <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""nofollow"">reset_index</a> to move the MultiIndex to the columns:</p>

<pre><code>import pandas as pd

df = pd.DataFrame( [ ['Hospital 1', 'District 19', 5],
                     ['Hospital 1', 'District 19', 10],
                     ['Hospital 1', 'District 19', 6],
                     ['Hospital 2', 'District 10', 50],
                     ['Hospital 2', 'District 10', 51]], columns = ['Hospital_ID', 'District_ID', 'Employee'] )

df = df.groupby( ['Hospital_ID', 'District_ID'] ).mean()
</code></pre>

<p>which gives you</p>

<pre><code>  Hospital_ID  District_ID  Employee
0  Hospital 1  District 19       7.0
1  Hospital 2  District 10      50.5
</code></pre>
"
"['search', 'ranking', 'xgboost', 'gbm']",How fit pairwise ranking models in xgBoost?,"<p>According to the <a href=""http://xgboost.readthedocs.org/en/latest/input_format.html"" rel=""nofollow"">XGBoost documentation</a>, XGboost expects:</p>

<ul>
<li>the examples of a same group to be consecutive examples,</li>
<li>a list with the size of each group (which you can set with <code>set_group</code> method of <code>DMatrix</code> in Python).</li>
</ul>
"
"['machine-learning', 'gradient-descent']",Invariance Property of Vowpal Wabbit Updates - Explaination,"<p>Often different data samples have different weighting ( eg the costs of misclassification error  for one group of data  is higher than for other classes).
Most error metrics are of the form $\sum_i e_i$ where e_i is the loss ( eg squared error) on data point $i$. Therefore weightings of the form $\sum_i w_i e_i$ are equivalent to duplicating the data w_i times (eg for w_i integer). </p>

<p>One simple case is if you have repeated data - rather than keeping all the duplicated data points, you just ""weight"" your one repeated sample by the number of instances.</p>

<p>Now whilst this is easy to do in a batch setting, it is hard in vowpal wabbits online big data setting: given that you have a large data set, you do not just want to represent the data n times to deal with the weighting ( because it increases your computational load). Similarly, just multiplying the gradient vector by the weighting - which is correct in batch gradient descent - will cause big problems for stochastic/online gradient descent: essentially you shoot off in one direction ( think of large integer weights) then you shoot off in the other - causing significant instability. SGD essentially relies on all the errors to be of roughly the same order ( so that the learning rate can be set appropriately). So what they propose is to ensure that the update for training sample x_i with weight n is equivalent to presenting training sample x_i n times consecutively.</p>

<p>The idea being that presenting it consecutively reduces the problem because the error gradient (for that single example $x_i$) reduces for each consecutive presentation and update (as you get closer &amp; closer to the minimum for that specific example). In other words the consecutive updates provides a kind of feedback control. </p>

<p>To me it sounds like you would still have instabilities (you get to zero error on x_i, then you get to zero error on x_i+1,...). the learning rate will need to be adjusted to take into account the size of the weights.</p>
"
"['classification', 'performance', 'dimensionality-reduction', 'online-learning']",Online/incremental unsupervised dimensionality reduction for use with classification for event prediction,"<p>My take:</p>

<ul>
<li>I agree with the issues raised in 1., so not much to add here - retraining and storage is indeed inefficient</li>
<li>Vowpal Wabbit <a href=""http://hunch.net/~vw/"" rel=""nofollow"">http://hunch.net/~vw/</a> would be my first choice</li>
<li>stability of output between increments is really more of a data than algorithm feature - if you have plenty of variation on input, you won't have that much stability on output (at least not by default)</li>
<li>hashing can take care of the variation - you can control by a combination of three parameters: the size of the hashing table and the l1/l2 regularization</li>
<li>same for the new features / users (I <em>think</em> - most of the applications i used it had a ercord representing a user clicking or not, so new users / ads were sort of treated ""the same"")</li>
<li>normally I use VW from the command line, but an example approach (not too elegant) for controlling from Python is given here: <a href=""http://fastml.com/how-to-run-external-programs-from-python-and-capture-their-output/"" rel=""nofollow"">http://fastml.com/how-to-run-external-programs-from-python-and-capture-their-output/</a></li>
<li>if you prefer sth purely Python, then a version (without decomposition) of an online learner in the Criteo spirit can be found here: <a href=""https://www.kaggle.com/c/tradeshift-text-classification/forums/t/10537/beat-the-benchmark-with-less-than-400mb-of-memory"" rel=""nofollow"">https://www.kaggle.com/c/tradeshift-text-classification/forums/t/10537/beat-the-benchmark-with-less-than-400mb-of-memory</a></li>
<li>I am not sure how to handle the concept drift - haven't paid that much attention to it so far beyond rolling statistics: for the relevant variables of interest, keep track of mean / count over recent N periods. It is a crude approach, but does seem to get the job done it terms of capturing lack of ""stationarity"" </li>
<li>helpful trick 1: single pass over data before first run to create per feature dictionary and flag certain values as rare (lump them into a single value)</li>
<li>helpful trick 2: ensembling predictions from more than one model (varying interaction order, learning rate)</li>
</ul>
"
"['machine-learning', 'reinforcement-learning']",Does reinforcement learning require the help of other learning algorithms?,"<p>You do not need additional learning algorithms to perform reinforcement learning in simple systems where you can explore all states. For those, simple iterative <a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""nofollow"">Q-learning</a> can do very well. If all your state, action pairs can be enumerated into a table that fits into memory, then you can use a Q-learning technique which is reinforcement learning without any deeper help of neural networks or other models.</p>

<p>The simplest form of <em>model-free</em> Q-learning just stores and updates a table of <code>&lt;state, action&gt; =&gt; &lt;estimated reward&gt;</code> pairs. There is no deeper statistical model inside that. Q-learning relies on estimates of reward from this table in order to take an action and then updates it with a more refined estimate after each action.</p>

<p>For large, complex problems, which may even have infinite possible states, this is not feasible, and you need good generalised estimates based on some function of the state. In those cases, you can use a neural network to create a function approximator, that can estimate the rewards from similar states to those already seen. The neural network replaces the function of the simple table in Q-Learning. However, the neural network (or other model) does not perform the whole process, you still need an ""outer"" RL method that explores states and actions in order to provide data for the NN to learn.</p>
"
"['machine-learning', 'bigdata', 'libsvm']",Use liblinear on big data for semantic analysis,"<p>Note that there is an early version of LIBLINEAR ported to <a href=""http://spark.apache.org"">Apache Spark</a>. See <a href=""http://apache-spark-user-list.1001560.n3.nabble.com/Spark-LIBLINEAR-td5546.html"">mailing list comments</a> for some early details, and the <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/"">project site</a>.</p>
"
"['classification', 'categorical-data', 'feature-extraction', 'feature-construction']",How to deal with categorical feature of very high cardinality?,"<p>One-hot-encoded ZIP codes shouldn't present a problem with modern tools, where features can be <em>much</em> wider (millions, billions even), but if you really want you could aggregate area codes into regions, such as states. Of course, you should not use strings, but bit vectors. Two other dimensionality reduction options are <a href=""https://github.com/esafak/mca"" rel=""nofollow"">MCA</a> (PCA for categorical variables) and <a href=""https://en.wikipedia.org/wiki/Random_projection"" rel=""nofollow"">random projection</a>.</p>
"
"['classification', 'binary', 'svm', 'random-forest', 'logistic-regression']",Choose binary classification algorithm,"<p>It's hard to say without knowing a little more about your dataset, and how separable your dataset is based on your feature vector, but I would probably suggest using extreme random forest over standard random forests because of your relatively small sample set.</p>

<p>Extreme random forests are pretty similar to standard random forests with the one exception that instead of optimizing splits on trees, extreme random forest makes splits at random. Initially this would seem like a negative, but it generally means that you have significantly better generalization and speed, though the AUC on your training set is likely to be a little worse.</p>

<p>Logistic regression is also a pretty solid bet for these kinds of tasks, though with your relatively low dimensionality and small sample size I would be worried about overfitting. You might want to check out using K-Nearest Neighbors since it often performs very will with low dimensionalities, but it doesn't usually handle categorical variables very well.</p>

<p>If I had to pick one without knowing more about the problem I would certainly place my bets on extreme random forest, as it's very likely to give you good generalization on this kind of dataset, and it also handles a mix of numerical and categorical data better than most other methods.</p>
"
"['bigdata', 'efficiency']",Filtering spam from retrieved data,"<p>Spam filtering, especially in email, has been revolutionized by neural networks, here are a couple papers that provide good reading on the subject:</p>

<p>On Neural Networks And The Future Of Spam 
A. C. Cosoi, M. S. Vlad, V. Sgarciu 
<a href=""http://ceai.srait.ro/index.php/ceai/article/viewFile/18/8"" rel=""nofollow"">http://ceai.srait.ro/index.php/ceai/article/viewFile/18/8</a></p>

<p>Intelligent Word-Based Spam Filter Detection Using 
Multi-Neural Networks 
Ann Nosseir, Khaled Nagati and Islam Taj-Eddin
<a href=""http://www.ijcsi.org/papers/IJCSI-10-2-1-17-21.pdf"" rel=""nofollow"">http://www.ijcsi.org/papers/IJCSI-10-2-1-17-21.pdf</a></p>

<p>Spam Detection using Adaptive Neural Networks: Adaptive Resonance Theory 
David Ndumiyana, Richard Gotora, and Tarisai Mupamombe
<a href=""http://onlineresearchjournals.org/JPESR/pdf/2013/apr/Ndumiyana%20et%20al.pdf"" rel=""nofollow"">http://onlineresearchjournals.org/JPESR/pdf/2013/apr/Ndumiyana%20et%20al.pdf</a></p>

<p>EDIT:
The basic intuition behind using a neural network to help with spam filtering is by providing a weight to terms based on how often they are associated with spam.</p>

<p>Neural networks can be trained most quickly in a supervised -- you explicitly provide the classification of the sentence in the training set -- environment.  Without going into the nitty gritty the basic idea can be illustrated with these sentences:</p>

<p>Text = ""How is the loss of the Viagra patent going to affect Pfizer"", Spam = false
Text = ""Cheap Viagra Buy Now"", Spam = true
Text = ""Online pharmacy Viagra Cialis Lipitor"", Spam = true</p>

<p>For a two stage neural network, the first stage will calculate the likelihood of spam based off of if the word exists in the sentence.  So from our example:</p>

<p>viagra => 66%
buy => 100%
Pfizer => 0%
etc..</p>

<p>Then for the second stage the results in the first stage are used as variables in the second stage:</p>

<p>viagra &amp; buy => 100%
Pfizer &amp; viagra=> 0%</p>

<p>This basic idea is run for many of the permutations of the all the words in your training data.  The end results once trained is basically just an equation that based of the context of the words in the sentence can assign a probability of being spam.  Set spamminess threshold, and filter out any data higher then said threshold.</p>
"
['predictive-modeling'],What techniques are used to understand call patterns?,"<p>When you said ""a broader topic"", did you mean what algorithms to use to examine the event log with the goal of reducing future calls from the same customer on the same topic? In other words, a customer may call for help for a different topic. If a customer calls in for the same topic repetitively, something can be improved.</p>

<p>You may get ideas from a Coursera class <a href=""https://www.coursera.org/course/procmin"" rel=""nofollow"">Processing Mining</a> since the issue you're solving is similar to the example of a spaghetti process in lecture 6.7:</p>

<p>""Spaghetti process describing the diagnosis and treatment of 2765 patients in a Dutch hospital. The process model was constructed based on an event log containing 114,592 events. There are 619 different activities (taking event types into account) executed by 266 different individuals (doctors, nurses, etc.).""</p>

<p>By the way, you can click the drop down menu under ""Sessions"", choose ""April 1, 2015 to May 19, 2015"" then you can register and view the lectures. On the right of each lecture, there are some icons. The first icon is to download slides for the lecture. You may find reading the slides is faster than listening to the lecture.</p>

<p>Suppose a customer called for installation of software release 1.5 then called a day later for running the new features of software release 1.5. Are these two issues logged as two of the 200 call categories? If so, we can use a time period (say one week) to judge whether it is about the same topic. Within a short time period, a customer is likely to work on the same topic, especially with the same key words such as ""software release 1.5"". We can coach the call center employees on solving possible follow up questions by saying something like ""now that we finished installation, let me show you a couple of new features. It'll save you time."" This will reduce the number of calls on the same topic from the same customer.</p>
"
"['machine-learning', 'neuralnetwork', 'deep-learning', 'theano', 'image-recognition']",Character recognition neural net topology/design,"<p>Your design makes some sense, but there is no need to limit connections even if you expect to represent probabilities of upper/lower case separately, because they will interact usefully. E.g if the character could most likely be one of <code>o, O, Q, G</code> then this might be useful information to choose the correct one. </p>

<p>If you went ahead, you would need to train this network without the final layer (so that it learns the representations you expect, not some other group of 52 features), then add the final layer later, with no need for special connection rules, just use existing ones. Initially you would training the new layer separately from the full output of the 52-class net i.e. probability values, not selected class. Then you would combine with the existing net and fine-tune the result by running a few more epochs with a low learning rate on the final model.</p>

<p>That all seems quite complex, and IMO unlikely to gain you much accuracy (although I am guessing, it could be great - so if you have time to explore ideas, you could still try). Personally I would not take your hidden layer idea further. The full 52-class version with simple logic to combine results is I think simpler. This is also not necessary, the neural net can learn to have two different-looking images be in the same class quite easily, provided you supply examples of them in training. However, it may give you useful insights into categorisation failures in training or testing.</p>

<p>It is not clear from the question, but if you are not already using convolutional neural network for lower layers, then you should do so. This will make the largest impact on your results by far.</p>
"
['bigdata'],SAP HANA vs Exasol,"<p>There's not an enormous difference between what you can do with the two databases, it's more a question of the focus and the way the functionality is implemented and that's where it becomes difficult to explain without using words like ""better"" and ""faster"" (and for sure words like ""cheaper"") </p>

<p>EXASOL was designed for speed and ease of use with Analytical processing and is designed to run on clusters of commodity hardware. SAP is a more complex, aims to do more than ""just"" Analytical processing and runs only on a range of ""approved"" hardware.</p>

<p>What type of differences did you have in mind ?</p>
"
"['beginner', 'tools', 'career', 'reference-request', 'books']",Is there any book for modern optimization in Python?,"<p>You should be able to translate code written in one language -- even pseudo-code -- to another, so I see no reason to avoid books for R. If you want one specifically for python, there's <em>Machine Learning in Action</em> by Peter Harrington.</p>

<p>One of scikit-learn's core committers is a releasing a book in October: <a href=""http://shop.oreilly.com/product/0636920030515.do"" rel=""nofollow"">Introduction to Machine Learning with Python: A Guide for Data Scientists</a>. </p>
"
['neuralnetwork'],Input for individual perceptron in input layer in MLP,"<p>With MLPs you can follow a set of simple rules:</p>

<ol>
<li>The number of neurons in your input layer equals the number of features / data instances you have.</li>
<li>A single neuron takes only one input (one feature)</li>
<li>The output from one neuron can go to multiple neurons in the next layer which may or may not be fully connected.</li>
</ol>

<p>May the force be with you ;)</p>
"
"['statistics', 'cross-validation', 'ab-test']",What is the minimum size of the test set?,"<p>This is a great question since it is illuminating to examination the best practices of both about traditional statistics and machine learning as they are brought together.</p>

<p><strong>Two Separate Rules/Best Practices -</strong></p>

<p>First, the two items that you mentioned should be examined separately and not conflated i.e. they should be carefully combined as you suggest in your question.</p>

<p><strong>Significance</strong>: You have estimated that you want greater than 1000 cases to have statistical significance and would further benefit from greater than 3000 test cases.</p>

<p><strong>Cross validation</strong>: Cross validation is performed by splitting the data (often 80% train-20% test) so that tests for bias (~underfitting) and variance (~overfitting) can be assessed.</p>

<p><strong>Combining significance with cross validation</strong>: Now we know that we want to have significance in our tests so we want greater than 3000 records.  We also want to perform cross validation, so in order for both the testing and training data to return significant results we want both to have a minimum of 3000 records.  The best scenario for this would be to have 15,000 total records. This way, the data can be split 80/20 and the testing set is still significant.</p>

<p>Lets assume you only have 4000 records in your data set. In this case, I would opt to make my training data significant while allowing the testing set to drop to lower significance.</p>

<p><strong>More rigor:</strong> Everything above has been quite hand wavey and lacks statistical rigor. For this, I refer you to a couple of papers referenced in <a href=""http://stats.stackexchange.com/questions/45851/how-to-statistically-compare-the-performance-of-machine-learning-classifiers"">another Stack Exchange question</a> -  </p>

<p><a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">Dietterich, ""Approximate Statistical Tests for Comparing
Supervised Classication Learning Algorithms""</a></p>

<p><a href=""http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=21D16E681FF16B0BA91D741A63806A31?doi=10.1.1.29.5194&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">Salzberg, Data Mining and Knowledge Discovery, 1, 317–327 (1997), ""On Comparing Classifiers: Pitfalls to Avoid and a
Recommended Approach""</a>.</p>

<p>Hope this helps!</p>
"
['terminology'],What is the term for data that is too sparse to represent the underlying data model?,"<p>Small sample size is probably the concept you're looking for. A common failure in statistics is trying to draw conclusions from data that isn't ""big"" enough to accurately represent the underlying distribution. </p>

<p>It's worth noting that ""small data"" increases your chances of overfitting - which is another way of saying that your model is weak to outliers and noise. It's also worth noting that bootstrapping, simulation, and duplication might help (but not always).</p>

<p>There may be some other niche term for this, but if you relate it to basic stats there's a high probability everyone in the room will understand what you're talking about.</p>
"
"['machine-learning', 'statistics', 'feature-selection']",How to specify important attributes?,"<p>A possible solution is to calculate the <a href=""http://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain"">information gain</a> associated to each attribute:</p>

<p><img src=""http://i.stack.imgur.com/sUheW.png"" alt=""Information Gain""></p>

<p>Initially you have the whole dataset, and compute the information gain of each item. The item with the best information gain is the one you should use to partition the dataset (considering the item's values). Then, perform the same computations for each item (but the ones selected), and always choose the one which best <em>describes/differentiates</em> the entries from your dataset.</p>

<p>There are implementations available for such computations. <a href=""http://en.wikipedia.org/wiki/Decision_tree_learning"">Decision trees</a> usually base their feature selection on the features with best information gain. You may use the resulting tree structure to find these <em>important</em> items.</p>
"
"['machine-learning', 'logistic-regression']",What is the right algorithm to detect segmentations of a line chart?,"<p>One way to do what you're talking about is called ""change point analysis."" There is an R package for this called <a href=""http://www.jstatsoft.org/v58/i03/paper"">changepoint</a> that you might want to check out. </p>

<p>In Python, you could try <a href=""https://pypi.python.org/pypi/changefinder/0.03"">changefinder</a>.</p>
"
"['neuralnetwork', 'deep-learning', 'image-classification', 'keras']",Image Captioning in Keras,"<p>This example trains an image and a partial caption to predict the next word in the caption. </p>

<pre><code>Input: [, ""&lt;BEGIN&gt; The cat sat on the""]
Output: ""mat""
</code></pre>

<p>Notice the model doesn't predict the entire output of the caption only the next word. To construct a new caption, you would have to predict multiple times for each word.</p>

<pre><code>Input: [, ""&lt;BEGIN&gt;""] # predict ""The""
Input: [, ""&lt;BEGIN&gt; The""] # predict ""cat""
Input: [, ""&lt;BEGIN&gt; The cat""] # predict ""sat""
...
</code></pre>

<p>To predict the entire sequence, I believe you need to use <code>TimeDistributedDense</code> for the output layer.</p>

<pre><code>Input: [, ""&lt;BEGIN&gt; The cat sat on the mat""]
Output: ""The cat sat on the mat &lt;END&gt;""
</code></pre>

<p>See this issue: <a href=""https://github.com/fchollet/keras/issues/1029"" rel=""nofollow"">https://github.com/fchollet/keras/issues/1029</a></p>
"
"['r', 'sql']",Left Join with b.key being NULL in R,"<p>If I understand correctly:</p>

<pre><code>Table1 &lt;- data.frame(key = seq(1,100),a.data = rnorm(100))
Table2 &lt;- data.frame(key = c(seq(1,30),rep(NA,30)), b.data = seq(1,60))


##Assuming this is what you want
library(sqldf)
sql.ans &lt;- sqldf(""select a.*, b.key from Table1 a LEFT OUTER JOIN Table2 b on a.key = b.key where b.key is null"")

## dplyr version
library(dplyr)

dplyr.ans &lt;- Table1 %&gt;% filter(!key %in% Table2$key)

## Regular R version
R.ans &lt;- Table1[which(!Table1$key %in% Table2$key),]
</code></pre>

<p>EDIT after dummy data and expected output</p>

<pre><code>dplyr.ans2 &lt;- left_join(df1,df2, by = c(""Key1"" = ""Key2""))
 Key1 BV1  BV2
 1   A1 100  150
 2   A2 200  250
 3   A3 300  350
 4   A4 400 &lt;NA&gt;
 5   A5 500 &lt;NA&gt;
</code></pre>
"
"['clustering', 'image-classification']",Cluster Similar Images into Folders,"<p>I would impressed if there is already a dedicated 9gag clustering :P</p>

<p>However, you can read this blog post about <a href=""http://www.janeriksolem.net/2009/04/hierarchical-clustering-in-python.html"" rel=""nofollow"">Hierarchical clustering in Python</a> for images, which is close to what you want. The problem is that the author uses the average color of the image as a feature and it could proved crude and not inefficient. You may find something more interesting to use. But in the end, you need to experiment a lot with your own dataset.</p>
"
"['machine-learning', 'nlp', 'search']",What are some standard ways of computing the distance between individual search queries?,"<p>From my experience only some classes of queries can be classified on lexical features (due to ambiguity of natural language). Instead you can try to use boolean search results (sites or segments of sites, not documents, without ranking) as features for classification (instead on words). This approach works well in classes where there is a big lexical ambiguity in a query but exists a lot of good sites relevant to the query (e.g. movies, music, commercial queries and so on).</p>

<p>Also, for offline classification you can do LSI on query-site matrix. See ""Introduction to Information Retrieval"" book for details.</p>
"
"['machine-learning', 'r', 'random-forest']",How to combine two different random forest models into one in R?,"<p>This question has already been answered <a href=""http://stackoverflow.com/questions/19170130/combining-random-forests-built-with-different-training-sets-in-r"">here</a>. It addresses the problem in hand. </p>
"
"['neuralnetwork', 'anomaly-detection', 'autoencoder', 'outlier']",Difference: Replicator Neural Network vs. Autoencoder,"<p>Both types of networks try to reconstruct the input after feeding it through some kind of compression / decompression mechanism. For outlier detection the reconstruction error between input and output is measured - outliers are expected to have a higher reconstruction error.</p>

<p>The main difference seems to be the way how the input is compressed:</p>

<p><strong>Plain autoencoders</strong> squeeze the input through a hidden layer that has fewer neurons than the input/output layers.. that way the network has to learn a compressed representation of the data.</p>

<p><strong>Replicator neural networks</strong> squeeze the data through a hidden layer that uses a staircase-like activation function. The staircase-like activation function makes the network compress the data by assigning it to a certain number of clusters (depending on the number of neurons and number of steps).</p>

<p><a href=""http://i.stack.imgur.com/R6vvi.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/R6vvi.png"" alt=""staircase-like activation function""></a></p>

<p>From <a href=""http://www.inf.u-szeged.hu/~ggabor/publlist/2004-isnn.pdf"" rel=""nofollow"">Replicator Neural Networks for Outlier Modeling in
Segmental Speech Recognition</a>:</p>

<blockquote>
  <p>RNNs were originally introduced in the field of data compression [5].
  Hawkins et al. proposed it for outlier modeling [4]. In both papers a
  5-layer structure is recommended, with a linear output layer and <strong>a
  special staircase-like activation function in the middle layer</strong> (see
  Fig. 2). <strong>The role of this activation function is to quantize the
  vector of middle hidden layer outputs into grid points and so arrange
  the data points into a number of clusters.</strong></p>
</blockquote>
"
"['predictive-modeling', 'regression']",Best regression model to use for sales prediction,"<p>This is a pretty classic <strong>ARIMA</strong> dataset. <strong>ARIMA</strong> is implemented in the StatsModels package for Python, the documentation for which is available <a href=""http://statsmodels.sourceforge.net/stable/index.html"" rel=""nofollow"">here</a>.</p>

<p>An <strong>ARIMA</strong> model with seasonal adjustment may be the simplest reasonably successful forecast for a complex time series such as sales forecasting. It may (probably will) be that you need to combine the method with an additional model layer to detect additional fluctuation beyond the auto-regressive function of your sales trend.</p>

<p>Unfortunately, simple linear regression models tend to fare quite poorly on time series data.</p>
"
"['clustering', 'feature-selection', 'correlation']",Feauture selection for clustering regarding zero-correlated feature,"<p>Lack of correlation with other features is not a reason to omit a feature. On the contrary, it is usually a reason to keep the feature because it may provide unique information. Typically, highly correlated features provide redundant information and feature reduction techniques (e.g., <a href=""https://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow"">Principal Components Analysis</a>) are used to remove the redundancy.</p>

<p>While it is <em>possible</em> that the uncorrelated feature is noise, you should not make that assumption. It could be that the uncorrelated feature is the <em>only</em> one containing information and the other 4 features are all correlated noise.</p>
"
"['machine-learning', 'accuracy']",Advantages of AUC vs standard accuracy,"<p>Really great question, and one that I find that most people don't really understand on an intuitive level. <code>AUC</code> is in fact often predicted over accuracy for binary classification for a number of different reasons. First though, let's talk about exactly what <code>AUC</code> is. Honestly, for being one of the most widely used efficacy metrics, it's surprisingly obtuse to figure out exactly how <code>AUC</code> works.</p>

<p><code>AUC</code> stands for <code>Area Under the Curve</code>, which curve you ask? Well that would be the <code>ROC</code> curve. <code>ROC</code> stands for <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"">Receiver Operating Characteristic</a>, which is actually slightly non-intuitive. The implicit goal of <code>AUC</code> is to deal with situations where you have a very skewed sample distribution, and don't want to overfit to a single class.</p>

<p>A great example is in spam detection. Generally spam data sets are STRONGLY biased towards ham, or not-spam. If your data set is 90% ham, you can get a pretty damn good accuracy by just saying that every single email is ham, which is obviously something that indicates a non-ideal classifier. Let's start with a couple of metrics that are a little more useful for us, specifically the true positive rate (<code>TPR</code>) and the false positive rate (<code>FPR</code>):</p>

<p><img src=""http://i.stack.imgur.com/hNxTl.png"" alt=""ROC axes""></p>

<p>Now in this graph, <code>TPR</code> is specifically the ratio of true positive to all positives, and <code>FPR</code> is the ratio of false positives to all negatives. (Keep in mind, this is only for binary classification.) On a graph like this, it should be pretty straightforward to figure out that a prediction of all 0's or all 1's will result in the points of <code>(0,0)</code> and <code>(1,1)</code> respectively. If you draw a line through these lines you get something like this:</p>

<p><img src=""http://i.stack.imgur.com/B1WT1.png"" alt=""Kind of like a triangle""></p>

<p>Which looks basically like a diagonal line (it is), and by some easy geometry, you can see that the <code>AUC</code> of such a model would be <code>0.5</code> (height and base are both 1). Similarly, if you predict a random assortment of 0's and 1's, let's say 90% 1's, you could get the point <code>(0.9, 0.9)</code>, which again falls along that diagonal line.</p>

<p>Now comes the interesting part. What if we weren't only predicting 0's and 1's? What if instead we wanted to say that, theoretically we were going to set a cutoff, above which every result was a 1, and below which every result were a 0. This would mean that at the extremes you get the original situation where you have all 0's and all 1's (at a cutoff of 0 and 1 respectively), but also a series of intermediate states that fall within the <code>1x1</code> graph that contains your <code>ROC</code>. In practice you get something like this:
<img src=""http://i.stack.imgur.com/13McM.png"" alt=""Courtesy of Wikipedia""></p>

<p>So basically, what you're actually getting when you do an <code>AUC</code> over accuracy is something that will strongly discourage people going for models that are representative, but not discriminative, as this will only actually select for models that achieve false positive and true positive rates that are significantly above random chance, which is not guaranteed for accuracy.</p>
"
"['tools', 'education', 'definitions']",What's the difference between data products and intelligent systems?,"<p>This is a very vague question. However, I will try to make sense of it. Considering rules of <em>logic</em> as well as your statement that both <em>entities</em> are ""software systems that are based on data and algorithms"", it appears that <strong>data products</strong> are <em>intelligent systems</em> and <strong>intelligent systems</strong> are, to some degree, <em>data products</em>. Therefore, it can be argued that the difference between the terms ""data products"" and ""intelligent systems"" is purely in the <strong>focus</strong> (<em>source</em> of information or <em>purpose</em> of system dimensions) of each type of systems (<em>data</em> <strong>vs.</strong> <em>intelligence/algorithms</em>).</p>
"
"['time-series', 'markov-process', 'reference-request']",Markov switching models,"<p>Firstly, for understanding the Markov switching models, a nice knowledge of Markov models and the way they work. Most importantly, an idea of time series models and how they work, is very important.</p>

<p>I found <a href=""http://homepage.ntu.edu.tw/~ckuan/pdf/Lec-Markov_note_spring%202010.pdf"" rel=""nofollow"">this tutorial</a> good enough for getting up to speed with the concept.</p>

<p><a href=""http://econweb.ucsd.edu/~jhamilto/palgrav1.pdf"" rel=""nofollow"">This</a> is another tutorial on a similar application of the switching model, which is the regime switching model.</p>

<p>The <a href=""http://statsmodels.sourceforge.net/"" rel=""nofollow"">statsmodels</a> library has a nice support for building the Morkov switching models.</p>

<p><a href=""http://pages.uoregon.edu/cfulton/posts/mar_hamilton.html"" rel=""nofollow"">Here</a> is one simple and quick Python tutorial which uses the statsmodels library.</p>
"
"['r', 'beginner']",Do you have to normalize data when building decision trees using R?,"<p>Most common types of decision trees you encounter are not affected by any monotonic transformation. So, as long as you preserve orde, the decision trees are the same (obviously by the same tree here I understand the same decision structure, not the same values for each test in each node of the tree). </p>

<p>The reason why it happens is because how usual impurity functions works. In order to find the best split it searches on each dimension (attribute) a split point which is basically an if clause which groups target values corresponding to instances which has test value less than split value, and on the right the values greater than equal. This happens for numerical attributes (which I think is your case because I do not know how to normalize a nominal attribute). Now you might note that the criteria is less than or greater than. Which means that the real information from the attributes in order to find the split (and the whole tree) is only the order of the values. Which means that as long as you transform your attributes in such a way that the original ordering is reserved, you will get the same tree. </p>

<p>Not all models are insensitive to such kind of transformation. For example linear regression models give the same results if you multiply an attribute with something different than zero. You will get different regression coefficients, but the predicted value will be the same. This is not the case when you take a log of that transformation. So for linear regression, for example, normalizing is useless since it will provide the same result. </p>

<p>However this is not the case with a penalized linear regression, like ridge regression. In penalized linear regressions a constraint is applied to coefficients. The idea is that the constraint is applied to the sum  of a function of coefficients. Now if you inflate an attribute, the coefficient will be deflated, which means that in the end the penalization for that coefficient it will be artificially modified. In such kind of situation, you normalize attributes in order that each coefficient to be constraint 'fairly'.</p>

<p>Hope it helps</p>
"
"['r', 'neuralnetwork', 'genetic-algorithms']",Using the GA R package to optimize the weights of a MLP neural network,"<p>I finally got the results I was looking for.</p>

<p>Apparently a single hidden layer neural network cannot learn the cubic function. Instead of just evolving the weights, I included the topology of the neural network into the chromosome. My neural network can have maximum 5 hidden layers with maximum 5 neurons in each layer and the chromosome contains information for the structure of the network and the selection of the weights. </p>

<p>When I ran the GA with population of 200 and for 100 iterations I got a full topology of:</p>

<p>1(input) - 4-2-3-2-2 - 1(output), a mean squared error of 0.0051 and the following plot<a href=""http://i.stack.imgur.com/87WsZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/87WsZ.png"" alt=""enter image description here""></a></p>

<p>I would be glad if anyone has any comments or questions.</p>
"
['cosine-distance'],Is Vector in Cosine Similarity the same as vector in Physics?,"<p>As you ask specifically for the Cosine Similarity technique, it has magnitude and direction, and similar to a vector which is used in Physics, as Cosine Similarity deals with vectors in an inner product space. </p>

<p><a href=""http://i.stack.imgur.com/RxFDJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RxFDJ.png"" alt=""enter image description here""></a></p>

<p>So, the magnitude of vectors is exactly the same as the formula in Physics (summating over the squares of the vector elements.)</p>
"
['python'],Question on decision tree in the book Programming Collective Intelligence,"<p>There are four features:</p>

<ul>
<li>referer,</li>
<li>location,</li>
<li>FAQ,</li>
<li>pages.</li>
</ul>

<p>In your case, you're trying to classify an instance where <code>FAQ</code> and <code>pages</code> are unknown: <code>mdclassify(['google','France',None,None], tree)</code>.</p>

<p>Since the first known attribute is <code>google</code>, in your decision tree you're only interested in the edge that comes out of <code>google</code> node on the right-hand side.</p>

<p>There are five instances: three labeled <code>Premium</code>, one labeled <code>Basic</code> and one labeled <code>None</code>.</p>

<p>Instances with labels <code>Basic</code> and <code>None</code> split on the <code>FAQ</code> attribute. There's two of them, so the weight for both of them is <code>0.5</code>.</p>

<p>Now, we split on the <code>pages</code> attribute. There are 3 instances with <code>pages</code> value larger than 20, and two with <code>pages</code> value no larger than 20.</p>

<p>Here's the trick: we already know that the weights for two of these were altered from <code>1</code> to <code>0.5</code> each. So, now we have three instances weighted <code>1</code> each, and 2 instances weighted <code>0.5</code> each. So the total value is <code>4</code>.</p>

<p>Now, we can count the weights for <code>pages</code> attribute:</p>

<ul>
<li>pages_larger_than_20 = 3/4</li>
<li>pages_not_larger_than_20 = 1/4 # the 1 is: 0.5 + 0.5</li>
</ul>

<p>All weights are ascribed. Now we can multiply the weights by the ""frequencies"" of instances (remembering that for <code>Basic</code> and <code>None</code> the ""frequency"" is now <code>0.5</code>):</p>

<ul>
<li><code>Premium</code>: <code>3 * 3/4 = 2.25</code> # because there are three <code>Premium</code> instances, each weighting <code>0.75</code>;</li>
<li><code>Basic</code>: <code>0.5 * 1/4 = 0.125</code> # because <code>Basic</code> is now <code>0.5</code>, and the split on
<code>pages_not_larger_than_20</code> is <code>1/4</code></li>
<li><code>None</code>: <code>0.5 * 1/4 = 0.125</code> # analogously</li>
</ul>

<p>That's at least where the numbers come from. I share your doubts about the maximum value of this metric, and whether it should sum to 1, but now that you know where these numbers come from you can think how to normalize them.</p>
"
"['classification', 'python', 'svm']",How to classify whether text answer is relevant to an initial text question,"<p>Both message and answer are your input, so your feature vector should contain information about both.</p>

<p>Here's a simple structure of a possible solution using scikit-learn:</p>

<pre><code>import numpy as np
from sklearn.svm import SVC
from sklearn.feature_extraction import DictVectorizer

dataset = ((""Hey @foobar, have you been hacked?"",
            ""@barfoo it seems so, thx for suggesting"",
            True), # True for relevant, False for not relevant
           (""Hey @foobar, have you been hacked?"",
            ""Lose 20 pounds quickly! http://blabla.com"",
            False))

def extractMessageFeatures(message):
    # here comes your real feature extraction algorithm
    return { 'message_predicted_spam': False,
             'message_contains_valid_username': True }

def extractAnswerFeatures(answer):
    # here comes your real feature extraction algorithm
    return { 'answer_predicted_spam': False,
             'answer_contains_valid_username': True }

def extractFeatures(data):
    features = []
    for instance in data:
        instanceFeatures = extractMessageFeatures(data[0])
        instanceFeatures.update(extractAnswerFeatures(data[1]))
        features.append(instanceFeatures)
    return features

def trainClassifier(data):
    features = extractFeatures(data)
    vec = DictVectorizer()
    featureVector = vec.fit_transform(features)

    print vec.get_feature_names()
    print featureVector.toarray()

    svc = SVC()
    svc.fit(featureVector, np.array([i[2] for i in data]))
    return svc

clf = trainClassifier(dataset)

# now, you can clf.predict(...)
</code></pre>

<p>Now, the hardest part is to decide which features to extract from both messages and answers. It's up to you.</p>

<p>One of the simplest solutions would be to use n-gram features.</p>

<p>Other approach would be to use some spam detection to decide whether answer is spam or not and treat this information as a feature.</p>

<p>You can also use Twitter-specific information (for example, whether users are mentioning each other in their tweets, using the same hashtags etc.).</p>

<p>You can combine these features in whatever fashion you like, of course.</p>

<p>Except of creating feature extraction functionality you need a labeled dataset of messages, answers and relevant/non-relevant labels.</p>

<p>But once you have both (feature extraction functionality and a proper dataset), you're good to go with a task which clearly matches standard machine learning approaches.</p>
"
"['classification', 'python']",Training Dataset required for Classifier,"<p><a href=""http://www.kdnuggets.com/datasets/index.html"" rel=""nofollow"">http://www.kdnuggets.com/datasets/index.html</a></p>

<p>This should get you maximum datasets for your classification exercise.</p>
"
"['python', 'optimization', 'genetic-algorithms']",Simple example of genetic alg minimization,"<p><a href=""http://lethain.com/genetic-algorithms-cool-name-damn-simple/"" rel=""nofollow"">Here is a trivial example</a>, which captures the essence of genetic algorithms more meaningfully than the polynomial you provided.  The polynomial you provided is solvable via <a href=""https://en.wikipedia.org/wiki/Stochastic_gradient_descent"" rel=""nofollow""><code>stochastic gradient descent</code></a>, which is a simpler minimimization technique.  For this reason, I am instead suggesting this excellent article and example by Will Larson.</p>

<p><a href=""http://lethain.com/genetic-algorithms-cool-name-damn-simple/"" rel=""nofollow"">Quoted from the original article</a>:</p>

<blockquote>
  <p><strong>Defining a Problem to Optimize</strong> Now we're going to put together a simple example of using a genetic algorithm in Python. We're going to
  optimize a very simple problem: trying to create a list of N numbers
  that equal X when summed together.</p>
  
  <p>If we set N = 5 and X = 200, then these would all be appropriate
  solutions. </p>

<pre><code>lst = [40,40,40,40,40]
lst = [50,50,50,25,25]
lst = [200,0,0,0,0]
</code></pre>
</blockquote>

<p>Take a look at the <a href=""http://lethain.com/genetic-algorithms-cool-name-damn-simple/"" rel=""nofollow"">entire article</a>, but here is the <strong>complete code</strong>:</p>

<blockquote>
<pre><code># Example usage
from genetic import *
target = 371
p_count = 100
i_length = 6
i_min = 0
i_max = 100
p = population(p_count, i_length, i_min, i_max)
fitness_history = [grade(p, target),]
for i in xrange(100):
    p = evolve(p, target)
    fitness_history.append(grade(p, target))

for datum in fitness_history:
   print datum
""""""
from random import randint, random
from operator import add

def individual(length, min, max):
    'Create a member of the population.'
    return [ randint(min,max) for x in xrange(length) ]

def population(count, length, min, max):
    """"""
    Create a number of individuals (i.e. a population).

    count: the number of individuals in the population
    length: the number of values per individual
    min: the minimum possible value in an individual's list of values
    max: the maximum possible value in an individual's list of values

    """"""
    return [ individual(length, min, max) for x in xrange(count) ]

def fitness(individual, target):
    """"""
    Determine the fitness of an individual. Higher is better.

    individual: the individual to evaluate
    target: the target number individuals are aiming for
    """"""
    sum = reduce(add, individual, 0)
    return abs(target-sum)

def grade(pop, target):
    'Find average fitness for a population.'
    summed = reduce(add, (fitness(x, target) for x in pop))
    return summed / (len(pop) * 1.0)

def evolve(pop, target, retain=0.2, random_select=0.05, mutate=0.01):
    graded = [ (fitness(x, target), x) for x in pop]
    graded = [ x[1] for x in sorted(graded)]
    retain_length = int(len(graded)*retain)
    parents = graded[:retain_length]
    # randomly add other individuals to
    # promote genetic diversity
    for individual in graded[retain_length:]:
        if random_select &gt; random():
            parents.append(individual)
    # mutate some individuals
    for individual in parents:
        if mutate &gt; random():
            pos_to_mutate = randint(0, len(individual)-1)
            # this mutation is not ideal, because it
            # restricts the range of possible values,
            # but the function is unaware of the min/max
            # values used to create the individuals,
            individual[pos_to_mutate] = randint(
                min(individual), max(individual))
    # crossover parents to create children
    parents_length = len(parents)
    desired_length = len(pop) - parents_length
    children = []
    while len(children) &lt; desired_length:
        male = randint(0, parents_length-1)
        female = randint(0, parents_length-1)
        if male != female:
            male = parents[male]
            female = parents[female]
            half = len(male) / 2
            child = male[:half] + female[half:]
            children.append(child)
    parents.extend(children)
    return parents
</code></pre>
</blockquote>

<p>I think it could be quite pedagogically useful to also solve your original problem using this algorithm and then also construct a solution using <code>stochastic grid search</code> or <code>stochastic gradient descent</code> and you will gain a deep understanding of the juxtaposition of those three algorithms.</p>

<p>Hope this helps!</p>
"
"['logistic-regression', 'theano']",Multi-class logistic regression,"<p>I found my own answer. I had defined </p>

<pre><code>w = theano.shared(rng.randn(feats), name=""w"")
</code></pre>

<p>and that was wrong.
The correct definition is:</p>

<pre><code>w = theano.shared(rng.randn(feats, num_classes), name=""w"")
</code></pre>

<p>since the weights link 'feats'-number of input neuron to 'num_classes'-number of output neurons.</p>
"
"['data-mining', 'classification', 'clustering']",Mine webshop history for clusters,"<p><a href=""https://en.wikipedia.org/wiki/Association_rule_learning"" rel=""nofollow"">Frequent Item-Set Mining</a> is what you are looking for. You can see the tree structure of your frequent itemsets and the association rules afterwards.</p>

<p>For your data I'd suggest to look at the whole for a while to get a sense on what you have in hand. Playing with concepts like Probability Distributions, <a href=""https://en.wikipedia.org/wiki/Entropy_%28information_theory%29"" rel=""nofollow"">Entropy</a>, etc would be really helpful in case you can reduce the size of your features. </p>

<p><a href=""https://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow"">PCA</a> also gives you the opportunity of projecting your data into a low-dimenstional space and you can see also plots showing first several PCs in 2-D or 3-D and get an impression about your data.</p>

<p>Before all above I strongly suggest to see if you have <a href=""https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf"" rel=""nofollow"">Missing Values</a> and if yes try to cope with them.</p>
"
['r'],How to replace levels with certain value?,"<p>I could be able to achieve this with the following code:</p>

<pre><code>testing$var1 &lt;- as.character(testing$var1)
a &lt;- data.frame(a)
testing$var1 [testing$var1 %in% a[1,] &lt;- ""Others""
testing$var1 &lt;- as.factor(testing$var1)
</code></pre>

<p>In case if there is any other better/effective solution/function to achieve this, please let me know. </p>

<p>Thanks all.</p>
"
['data-mining'],Identifying repeating sequences of data in byte array,"<p>I believe the problem that you are referring to, is that of ""Motif Discovery in Time Series Data"". An appreciable amount of research literature already exists in this domain, so you can look through those. If the data that you handle is not very large, you can find some relatively easy to implement algorithms. </p>

<p>If the data is large , you can look at more recent publications in this domain. As a starting point I would recommend taking a look at how Motif Discovery is done in SAX. SAX takes continuous signals as inputs and discretizes them. These discrete levels are then stored as alphabets. This resulting data looks very much like yours in my opinion. Take a look at what they do in ""Mining Motifs in Massive Time Series Databases"".</p>
"
"['machine-learning', 'data-mining', 'r', 'predictive-modeling', 'data']",Advise on making predictions given collection of dimensions and corresponding probabilities,"<h2>Aggregate Data</h2>

<p>Since you're only given aggregate data, and not individual examples, machine learning techniques like decision trees won't really help you much. Those algorithms gain a lot of traction by looking at correlations within a single example. For instance, the increase in risk from being both obese and over 40 might be much higher than the sum of the individual risks of being obese or over 40 (i.e. the effect is greater than the sum of its parts). Aggregate data loses this information.</p>

<h2>The Bayesian Approach</h2>

<p>On the bright side, though, using aggregate data like this is fairly straightforward, but requires some probability theory. If $D$ is whether the person has diabetes and $F_1,\ldots,F_n$ are the factors from that link you provided, and if I'm doing my math correctly, we can use the formula:
$$ \text{Prob}(D\ |\ F_1,\ldots,F_n) \propto \frac{\prod_{k=1}^n \text{Prob}(D\ |\ F_k)}{\text{Prob}(D)^{n-1}} $$
(The proof for this is an extension of the one found <a href=""http://stats.stackexchange.com/a/112361"">here</a>). This assumes that the factors $F_1,\ldots,F_n$ are conditionally independent given $D$, though that's usually reasonable. To calculate the probabilities, compute the outputs for $D=\text{Diabetes}$ and $\neg D=\text{No diabetes}$ and divide them both by their sum so that they add to 1.</p>

<h2>Example</h2>

<p>Suppose we had a married, 48-year-old male. Looking at the 2010-2012 data, 0.73% of all people get diabetes ($\text{Prob}(D) = 0.73\%$), 0.77% of married people get diabetes ($\text{Prob}(D\ |\ F_1)$$= 0.77\%$), 1.02% of people age 45-54 get diabetes ($\text{Prob}(D\ |\ F_2) = 1.02\%$), and 0.70% of males get diabetes ($\text{Prob}(D\ |\ F_3) = 0.70\%$). This gives us the unnormalized probabilities:
$$ \begin{align*} P(D\ |\ F_1,F_2,F_3) &amp;= \frac{(0.77\%)(1.02\%)(0.70\%)}{(0.73\%)^2} &amp;= 0.0103 \\
P(\neg D\ |\ F_1,F_2,F_3) &amp;= \frac{(99.23\%)(98.98\%)(99.30\%)}{(99.27\%)^2} &amp;= 0.9897 \end{align*}$$
After normalizing these to add to one (which they already do in this case), we get a 1.03% chance of this person getting diabetes, and a 98.97% chance for them not getting diabetes.</p>
"
"['r', 'clustering']",Perform clustering on individual data in categories,"<p>As always clustering is about what is the meaning of distance, since that encodes at least some part of your question. So you question is which channels are similar, where similarity is defined on users. Usually you do not assume some nesting on your instances, but what you have is basically a nesting of users in channels. So you have to incorporate this kind of nesting into the distance / similarity function. </p>

<p>I would start with some observation on similarity function. We denote similarity between instance $i$ and $j$ with $d(i,j)$. Usually this function obeys the following condition $d(i,j)\ge0$ for any $i$ and $j$. Note that we usually have equality only on identical data points. The main consequence of this observation is that we can have basically an identity at user level. What you want is an identity at channel level.</p>

<p>One way would be to define the similarity function like:</p>

<p>$$d_c(i,j) = I_{c_i\ne c_j}d(i,j)$$ </p>

<p>where $I_{c_i\ne c_j}$ is $1$ when channel of instance $i$ is different that channel of instance $j$. The main effect is that now all the points from the same channel are considered identical since the distance between them is zero. When the identity function is $1$ the distance is given by your real business distance which should be constructed by you and answer your question. </p>

<p>If you use such kind of function in a hierarchical clustering basically it will start to find first some clusters which are identically with your grouping on channels and later on it will join clusters which are similar. This kind of approach will work even with a kmeans algorithm and perhaps with most of the clustering approaches. </p>

<p>A slight different approach would be to define your $I$ function to return $1-\lambda$ when cluster is different and $\lambda$ when channels are equal, and have $\lambda$ a positive value close to $0$. This will not guarantee that all the clients will go into the same cluster, but it gives you the benefit that you have a slider which you can use to fine tune the compromise between <em>all user of the same channel goes into the same cluster</em> and <em>a distance measure which is more robust to outliers</em>.</p>

<p>A totally different approach from an implementation point of view would be to define a more complex function directly on channel samples. This would be similar with how hierarchical clustering works since in order to joins to clusters it should have a distance function which would measure inter clusters similarity. See more on <a href=""https://en.wikipedia.org/wiki/Hierarchical_clustering#Linkage_criteria"" rel=""nofollow"">linkage-criteria</a>. For example average linkage clustering. Note that I said that the approach is different only algorithmic. I would bet that the results would be similar with the first approach. </p>

<p>A totally different approach would be to use a more robust criteria. It is known that sample average is not robust since one point can blow away the estimation. Median instead is much stable. You can use a median or a trimmed mean to have a more robust aggregation values. This would have the advantage that the clustering would be much faster since you would work with channels instead of clients and the running time for computing the clustering would be reduced. </p>

<p>And finally, another approach which comes to my mind would be to go further with comparing channels, but this time using a distance which would be based on statistical tests. I will give you a scenario to clarify. Suppose that your users have an attribute named <em>Weight</em>, which as expected would be a continuous variable. How could you define a distance between the <em>Weight</em> of users of one channel and <em>Weight</em> of users of other channel? If you can assume a distribution, like a Gaussian on that weight you can build a two sample t-test on the two samples which are the two clusters. If you can't assume a distribution you can employ a homogeneity / independent test like <a href=""https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test"" rel=""nofollow"">two sample KS test</a>. KS test is valuable since does not assume any distribution and is sensitive to both changes is shape and location. If you have nominal attributes you can employ a <a href=""https://en.wikipedia.org/wiki/Pearson&#39;s_chi-squared_test#Test_of_independence"" rel=""nofollow"">chi-square independence test</a>. Be careful with what you can use from that tests. In order to have equal contribution for each attribute used in distance function you have to you p-values. Also note that if the test is significant it will have a small p-value, since the null hypotheses for both tests is the independence assumption, which can be translated as same marginal for both samples. So, smaller p-values means bigger distance. You can use $1-\text{p_value}$ or even you can try $\frac{1}{\text{p_value}}$.</p>
"
"['r', 'data-wrangling']",Sum up counts in a data.frame grouped by multiple variables,"<p>You can do this using the <code>xtabs</code> function! Here's how I did it using your example data:</p>

<pre><code># Create example data...
name &lt;- c(""Maria"", ""Thomas"", ""Maria"", ""Maria"", ""Thomas"", ""Maria"")
sex &lt;- c(""f"", ""m"", ""m"", ""f"", ""m"", ""m"")
count &lt;- c(97, 12, 5, 97, 8, 4)
data &lt;- data.frame(""name""=name, ""sex""=sex, ""count""=count)

# Create table...
xtabs(formula=count~name + sex, data=data)
</code></pre>

<p>which gives the following output:</p>

<pre><code>        sex
name       f   m
Maria    194   9
Thomas     0  20
</code></pre>
"
"['neuralnetwork', 'backpropagation']",Why do we calculate partial derivative of Error w.r.t output of Neural Network during backpropagation?,"<p>The partial derivative is used precisely because it separates concerns about how the value is calculated (from all the other parameters and outputs in the network) from how the value affects the output. This is purely by definition so you can do the calculation.</p>

<p>The ""normal"" derivative of the error function can be expressed as the sum of any ""complete"" set of independent partial derivatives. There are a few such sets possible - in a simple feed-forward network one for each layer plus one for all the weights combined. </p>

<p>So in essence you <em>are</em> calculating the ""normal"" derivative w.r.t. the network weights, but due to the nature of the problem this is done by calculating the partial derivatives in multiple steps.</p>

<hr>

<p>Caveat: I am probably not using those maths terms 100% accurately. For instance, I am ignoring the role of the training data and treating it as a constant.</p>
"
"['bigdata', 'hadoop']",Cascaded Error in Apache Storm,"<p>Twitter uses Storm for real-time processing of data.  Problems can happen with real-time data.  Systems might go down.  Data might be inadvertently processed twice.  Network connections can be lost.  A lot can happen in a real-time system.  </p>

<p>They use hadoop to reliably process historical data.  I don't know specifics, but for instance, getting solid information from aggregated logs is probably more reliable than attaching to the stream.</p>

<p>If they simply relied on Storm for everything - Storm would have problems due to the nature of providing real-time information at scale.  If they relied on hadoop for everything, there's a good deal of latency involved.  Combining the two with Summingbird is the next logical step.</p>
"
"['bigdata', 'efficiency', 'hadoop', 'distributed']",Tradeoffs between Storm and Hadoop (MapReduce),"<p><strong>MapReduce</strong>: A fault tolerant distributed computational framework. MapReduce allows you to operate over huge amounts of data- with a lot of work put in to prevent failure due to hardware. MapReduce is a poor choice for computing results on the fly because it is slow. (A typical MapReduce job takes on the order of minutes or hours, not microseconds)</p>

<p>A MapReduce job takes a file (or some data store) as an input and writes a file of results. If you want these results available to an application, it is your responsibility to put this data in a place that is accessible. This is likely slow, and there will be a lag between the values you can display, and the values that represent your system in its current state.</p>

<p>An important distinction to make when considering using MapReduce in building realtime systems is that of training your model, and applying your model. If you think your model parameters do not change quickly, you can fit them with MapReduce, and then have a mechanism for accessing these pre-fit parameters when you want to apply your model.</p>

<p><strong>Storm</strong>: A real-time, streaming computational system. Storm is online framework, meaning, in this sense, a service that interacts with a running application. In contrast to MapReduce, it receives small pieces of data (not a whole file) as they are processed in your application. You define a DAG of operations to perform on the data. A common and simple use case for Storm is tracking counters, and using that information to populate a real-time dashboard.</p>

<p>Storm doesn't have anything (necessarily) to do with persisting your data. Here, streaming is another way to say keeping the information you care about and throwing the rest away. In reality, you probably have a persistence layer in your application that has already recorded the data, and so this a good and justified separation of concerns.</p>

<p><strong>If you want to know more...</strong>
If you would like to learn more about real-time systems that that fit parameters with MR and apply the models a different way <a href=""http://www.slideshare.net/cloudera/hbasecon-2013-24063525"">here are slides for a talk I gave on building real-time recommendation engines on HBase.</a> </p>

<p>An excellent paper that marries real-time counting and persistence in an interesting way is <a href=""http://dl.acm.org/citation.cfm?id=1242610"">Google News Personalization: Scalable Online Collaborative Filtering</a> </p>

<p>Another interesting marriage of MR and Storm is <a href=""https://github.com/twitter/summingbird/wiki"">SummingBird</a>. Summingbird allows you to define data analysis operations that can be applied via Storm or MR.</p>
"
['r'],How to run R scripts without closing X11,"<p><a href=""http://stackoverflow.com/questions/24220676/r-script-using-x11-window-only-opens-for-a-second"">This</a> looks like a similar kind of a problem.</p>

<p>Solutions: (taken from above source)</p>

<ol>
<li><p>Just sleep via  Sys.sleep(10) which would wait ten seconds.</p></li>
<li><p>Wait for user input via readLines(stdin()) or something like that [untested]</p></li>
<li><p>Use the tcltk package which comes with R and is available on all platforms to pop up a window the user has to click to make the click disappear. That solution has been posted a few times over the years on r-help.</p></li>
</ol>

<p>2nd option is better to use for the user.</p>

<p>P.S. Since I did not come up with the answer myself, I tried to put it in comment but my reputation is too low for that.</p>
"
"['machine-learning', 'classification', 'deep-learning']",Deep Learning vs gradient boosting: When to use what?,"<p>Why restrict yourself to those two approaches? Because they're cool? I would always start with a simple linear classifier \ regressor. So in this case a Linear SVM or Logistic Regression, preferably with an algorithm implementation that can take advantage of sparsity due to the size of the data. It will take a long time to run a DL algorithm on that dataset, and I would only normally try deep learning on specialist problems where there's some hierarchical structure in the data, such as images or text. It's overkill for a lot of simpler learning problems, and takes a lot of time and expertise to learn and also DL algorithms are very slow to train. Additionally, just because you have 50M rows, doesn't mean you need to use the entire dataset to get good results. Depending on the data, you may get good results with a sample of a few 100,000 rows or a few million. I would start simple, with a small sample and a linear classifier, and get more complicated from there if the results are not satisfactory. At least that way you'll get a baseline. We've often found simple linear models to out perform more sophisticated models on most tasks, so you want to always start there.</p>
"
"['recommendation', 'data-cleaning']",Data scheduling for recommender,"<p>Another good normalization is <a href=""http://en.wikipedia.org/wiki/Standard_score"" rel=""nofollow"">zScore</a> normalization. Was already implemented in python in the module <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.mstats.zscore.html"" rel=""nofollow"">scipy.stats</a></p>
"
['machine-learning'],How to determine whether a bad performance is caused by data quality?,"<p>First, it sounds like your choice of model selection is a problem here.  Your outputs are binary-valued, not continuous. Specifically you may have a classification problem on your hands rather than a traditional regression problem.  My first recommendation would be to try a simple classification approach such as logistic regression or linear discriminant analysis.</p>

<p>Regarding your suspicions of <em>bad data</em>, what would bad data look like in this situation? Do you have reason to suspect that your $X$ values are noisy or that your $y$ values are mislabeled? It is also possible that there is not a strong relationship between any of your features and your targets.  Since your targets are binary, you should look at histograms of each of your features to get a rough sense of the class conditional distributions, i.e. $p(X_1|y=1)$ vs $p(X_1|y=0)$. In general though, you will need to be more specific about what ""bad data"" means to you.</p>
"
"['machine-learning', 'neuralnetwork', 'terminology']",Where does the name 'LSTM' come from?,"<p>In <a href=""http://goo.gl/JSaqb4"" rel=""nofollow"">Sepp Hochreiter's original paper on the LSTM </a>  where he introduces the algorithm and method to the scientific community, he explains that the long term memory refers to the learned weights and the short term memory refers to the gated cell state values that change with each step through time <em>t</em>. </p>

<p><strong>edit:</strong> quote from paper <em>""Recurrent networks can in principle use their feedback connections to store representations of recent input events in the form of activations (""short-term memory"", as opposed to ""long-term memory embodied by slowly changing weights)</em>""</p>
"
"['machine-learning', 'classification', 'svm']",Are there other large margin classifiers than SVMs?,"<p>Yes, one famous example are boosting techniques like <a href=""https://en.wikipedia.org/wiki/AdaBoost"" rel=""nofollow"">Adaboost</a>. It uses small classifiers to create a big one.</p>

<p>Here you can find more info about <a href=""https://en.wikipedia.org/wiki/Margin_classifier"" rel=""nofollow"">margin classifiers</a>.</p>
"
"['r', 'clustering']",How can we evaluate DBSCAN parameters?,"<p><a href=""http://www.dbs.informatik.uni-muenchen.de/Publikationen/Papers/OPTICS.pdf"" rel=""nofollow"">OPTICS</a> gets rid of $\varepsilon$, you might want to have a look at it. Especially the <strong>reachability plot</strong> is a way to visualize what good choices of $\varepsilon$ in DBSCAN might be.</p>

<p>Wikipedia (<a href=""https://en.wikipedia.org/wiki/OPTICS_algorithm"" rel=""nofollow"">article</a>) illustrates it pretty well. The image on the top left shows the data points, the image on the bottom left is the reachability plot:</p>

<p><a href=""http://i.stack.imgur.com/TamFM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TamFM.png"" alt=""enter image description here""></a></p>

<p>The $y$-axis are different values for $\varepsilon$, the valleys are the clusters. Each ""bar"" is for a single point, where the height of the bar is the minimal distance to the already printed points.</p>
"
"['machine-learning', 'python', 'data-cleaning', 'pandas', 'keras']",String Values in a dataframe in Pandas,"<p>To convert from string to float in pandas(assuming you want to convert Employees and you loaded the dataframe with df), you can use </p>

<pre><code>df['Employees'].apply(lambda x:float(x))
</code></pre>

<p>You have not given your enough information about your input and expected output. So let us assume that hospital name or anything for that matter which is the input for your model is <code>nan</code>, you would like to remove it from the dataset because extracting features from '<code>nan</code>' wouldn't make sense. Apart from that if they are just other peripheral features, then it might be alright. In that case, if you wish to convert them into blank then use <code>df.replace(np.nan,' ', regex=True)</code>. Else if you wish to remove that frame, you can check for nan using <a href=""http://pandas.pydata.org/pandas-docs/stable/missing_data.html"" rel=""nofollow"">this</a>.</p>
"
['tsne'],Is t-SNE just for vizualization?,"<p>It's a dimensionality reduction algorithm. Inference is the problem of determining the parameters, or labels, that best fit the model for a given input once the model parameters have been learned, or estimated.</p>
"
['machine-learning'],To learn machine learning which one is good?,"<p>I can think of the following pros and cons for each. As for learning to code your own machine learning algorithms such as logistic regression:</p>

<ul>
<li>You will definitely learn more about specific algorithms, their details, role of different parameters and etc. That is also a good practice of coding itself. Then you can validate your implementation by benchmarking it against other packages and implementations.</li>
<li>You will have more freedom in controlling different aspects of your method. You can add functions and modules as you wish and do not necessary have to deal with predefined variables, methods and etc.</li>
</ul>

<p>On the other hand, implementing algorithms when it is not necessary and you can just use existing packages is like reinventing the wheel. It normally takes a lot of time and you have to verify your results for each one of them. Packages like <code>sklearn</code> are popular because of the following:</p>

<ul>
<li>A group of people are working on those, constantly making them up to date, testing the  methods in different situations for different needs. That makes packages like <code>sklearn</code> very dependable and usually scalable.</li>
<li>If you have a question about them, there are tons of resources out there; documentation, forums, source code, communities like StackOverflow where thousands of people are eager to help you literally for any error you face while running your code. </li>
<li>Another important feature is automated hyperparameters tuning. Most of machine learning algorithms have a series of hyperparameters that need to be optimized in order to achieve the best performance. Packages like <code>sklearn</code> efficiently search for the optimal tuning parameters (or ""hyperparameters"") for your machine learning model in order to maximize its performance.</li>
<li>Still if you are interested in implementing machine learning algorithms and like the coding, you can always contribute to the existing packages. They usually have Github repositories where you can raise an issue, ask for a new feature or provide help improving them.</li>
</ul>

<p>All in all, if you have enough time and you are keen to learn low level details about those models, go ahead and give implementation them a shot. That is certainly fun. However, if you need to get to the results as soon as possible and looking for a reliable package where a huge group of people both in industry and academia are already using, <code>sklearn</code>, <code>pandas</code> and others are you options.</p>

<p>Hope this is helpful and good luck.</p>
"
"['machine-learning', 'classification', 'statistics', 'svm', 'naive-bayes-classifier']",Soccer Field Segmentation,"<p>I would not suggest GMM at this point as the distribution of points in the space is not well-shaped enough. Even if you want to use it it's better to look at your data in PC space (i.e. using PCA). My suggestion would be:</p>

<p>1) Think of your features. What are they? Are you going to use these gr-chromacity as features? If yes you should know that kernel methods work better on this as the features are highly nonlinear. The image show that you need a feature mapping anyway.</p>

<p>2) It seems you have already thought of kernel methods as you put SVM as a tag. you can use it for classification. Might work better than GMM here. Also think of probabilistic graphical models as they have been used intensively for image segmentation and your images are structured enough (a football field has its fixed position in the image anyway).</p>

<p>3) If you have raw labeled dataset, I'd recommend to think of smarter features for segmentation. in gr-chromacity you already loose some information about colors which is the most important thing for you here. I would recommend taking the position of pixels into account as well. Then a PCA on the new data may reveal some more linearly separated classes.</p>
"
"['predictive-modeling', 'decision-trees']",How to interpret a decision tree correctly?,"<p>Let me evaluate each of your observations one by one, so that it would be more clear:</p>

<blockquote>
  <p>The dependent variable of this decision tree is Credit Rating which
  has two classes, Bad or Good. The root of this tree contains all 2464
  observations in this dataset.</p>
</blockquote>

<p>If <code>Good, Bad</code> is what you mean by credit rating, then <strong>Yes</strong>. And you are right with the conclusion that all the 2464 observations are contained in the root of the tree.</p>

<blockquote>
  <p>The most influential attribute to determine how to classify a good or
  bad credit rating is the Income Level attribute.</p>
</blockquote>

<p><strong>Debatable</strong> Depends on how you consider something to be <em>influential</em>. Some might argue that the number of cards might be the most influential, and some might agree with your point. So, you are both right and wrong here.</p>

<blockquote>
  <p>The majority of the people (454 out of 553) in our sample that had a
  less than low income also had a bad credit rating. If I was to launch
  a premium credit card without a limit I should ignore these people.</p>
</blockquote>

<p><strong>Yes</strong>, but it would also be better if you consider the probability of getting a bad credit from these people. But, even that would turn out to be NO for this class, which makes your observation correct again.</p>

<blockquote>
  <p>If I were to use this decision tree for predictions to classify new
  observations, are the largest number of class in a leaf used as the
  prediction? E.g. Observation x has medium income, 7 credit cards and
  34 years old. Would the predicted classification for credit rating =
  ""Good""</p>
</blockquote>

<p><strong>Depends on the probability</strong>. So, <a href=""https://social.msdn.microsoft.com/Forums/sqlserver/en-US/97c9ce39-024f-450f-8b21-a2d2961d8be7/decision-trees-how-is-prediction-probability-calculated?forum=sqldatamining"" rel=""nofollow"">calculate the probability</a> from the leaves and then make a decision depending on that. Or much simpler, use a library like the Sklearn's decision tree classifier to do that for you.</p>

<blockquote>
  <p>Another new observation could be Observation Y, which has less than
  low income so their credit rating = ""Bad""</p>
</blockquote>

<p>Again, same as the explanation above.</p>

<blockquote>
  <p>Is this the correct way to interpret a decision tree or have I got
  this completely wrong?</p>
</blockquote>

<p><strong>Yes</strong>, this is a correct way of interpreting decision trees. You might be tempted to sway when it comes to selection of <em>influential</em> variables, but that is dependant on a lot of factors, including the problem statement, construction of the tree, analyst's judgement, etc.</p>
"
"['classification', 'svm']",Combine multiple classifiers to build a multi-modal classifier,"<p>Basically, you can do one of two things: </p>

<ol>
<li><strong>Combine features</strong> from both classifiers. I.e., instead of <code>SVM-text</code> and <code>SVM-image</code> you may train single <code>SVM</code> that uses both - textual and visual features. </li>
<li>Use <a href=""http://en.wikipedia.org/wiki/Ensemble_learning"" rel=""nofollow""><strong>ensemble learning</strong></a>. If you already have probabilities from separate classifiers, you can simply use them as weights and compute weighted average. For more sophisticated cases there are Bayesian combiners (each classifier has its prior), boosting algorithms (e.g. see <a href=""http://en.wikipedia.org/wiki/AdaBoost"" rel=""nofollow"">AdaBoost</a>) and others. </li>
</ol>

<p>Note, that ensembles where initially created for combining different learners, not different sets if features. In this later case ensembles have advantage mostly in cases when different kinds of features just can't be combined in a single vector efficiently. But in general, combing features is simpler and more straightforward. </p>
"
"['classification', 'text-mining', 'random-forest']",Difference between tf-idf and tf with Random Forests,"<p>Decision trees (and hence Random Forests) are insensitive to monotone transformations of input features. </p>

<p>Since multiplying by the same factor is a monotone transformation, I'd assume that for Random Forests there indeed is no difference. </p>

<p>However, you eventually may consider using other classifiers that do not have this property, so it may still make sense to use the entire TF * IDF. </p>
"
['optimization'],Optimizing parameters for a closed (black-box) system,"<p>There are lots of function optimising routines that could be applied, based on the description so far. Random search, grid search, hill-climbing, gradient descent, genetic algorithms, simulated annealing, particle swarm optimisation are all possible contenders that I have heard of, and I am probably missing a few.</p>

<p>The trouble is, starting with next to zero knowledge of the black box, it is almost impossible to guess a good candidate from these search options. All of them have strengths and weaknesses. To start with, you seem to have no indication of <em>scale</em> - should you be trying input parameters in any particular ranges? So you might want to try very crude searches through a range of magnitudes (positive and negative values) to find the area worth searching. Such a grid search is expensive - if you have $k$ dimensions and want to search $n$ different magnitudes, then you need to call your black box $n^k$ times. </p>

<p>This can be done in parallel though, and given you are confident that the function is roughly unimodal, you can start with a relatively low number of n (maybe check -10, -1, 0, +1, +10 for 15625 calls to your function taking roughly 8 hours 40 mins using 5 boxes). You may need to repeat with other params once you know whether you have found a bounding box for the mode or need to try yet more values, so this process could take a while longer - potentially days if the optimal value for param 6 is more like 20,000. You could also refine more closely, once you have a potential mode you might want to define another grid of values to search based around. This basic grid search might be my first point of attack on a black box system where I had no clue about parameter meaning, but some confidence that the black box output had a rough unimodal form. </p>

<p>Given the speed of response you should be storing all input and output values in a database for faster lookup and better model building later. No point repeating a call taking 10 seconds when a cache could look it up in 1 millisecond.</p>

<p>Once you have some range of values you think that a mode might be in, then it is time to pick a suitable optimiser.</p>

<p>Given the information so far, I would be tempted to run either more grid search (with a separate linear scaling between values of each param) and/or a random search, constrained roughly to the boxes defined by the set of $2^6$ corner points <em>around</em> the best result found in initial order-of-magnitude search.</p>

<p>At that point you could also consider graphing the data, to see if there is any intuition about which other algorithms could perform well. </p>

<p>With the possibility of parallel calls, then gradient descent might be a reasonable guess, because you can get approximate gradients by adding a small offset to each param and dividing difference that causes in the output by it. In addition, gradient descent (or simple hill climbing) has some chance of optimising with less calls to evaluate the function than approaches that rely on many iterations (simulated annealing) or lots of work in parallel (particle swarm or genetic algorithms). Gradient descent optimisers as used in neural networks, with additions like Nesterov momentum or RMSProp, can cope with changes in function output ""feature scale"" such as different sizes and heights of peaks, ridges, saddle points.</p>

<p>However, gradient descent and hill climbing algorithms are not robust against all function shapes. A graph or several of what your explorations are seeing may help you to decide on a different approach. So keep all the data and graph it in case you can get clues.</p>

<p>Finally, don't rule out random brute-force search, and being able to just accept ""best so far"" under time constraints. With low knowledge of the internals of the black box, it is a reasonable strategy.</p>
"
"['r', 'algorithms']",Simple implementation of Apriori algorithm in R,"<p>Have you checked the following reference out?
Link: <a href=""http://www.borgelt.net/docs/apriori.pdf"" rel=""nofollow"">http://www.borgelt.net/docs/apriori.pdf</a></p>

<p>The above link has the explanation along with the code. </p>
"
"['machine-learning', 'r', 'information-retrieval', 'named-entity-recognition']",Extracting list of locations from text using R,"<p>This might be better for <a href=""http://opendata.stackexchange.com"">opendata</a>, but nonetheless, you have a few options. One would be to go to <a href=""http://www.geohive.com/cntry/"" rel=""nofollow"">geohive</a> which has other pages, including <a href=""http://www.geohive.com/earth/population_now.aspx"" rel=""nofollow"">this one</a>. There is also the UN categorization, available on <a href=""https://en.wikipedia.org/wiki/List_of_sovereign_states"" rel=""nofollow"">wikipedia</a> which uses membership within the United Nations system divides the 206 listed states into three categories: 193 member states,[1] two observer states, and 11 other states. The sovereignty dispute column indicates states whose sovereignty is undisputed (190 states) and states whose sovereignty is disputed (16 states).</p>

<p>You can <code>read.table</code> or <code>rvest</code> those sources and grab them at runtime.</p>
"
['r'],3d contour plot in R,"<p>This plot not use frequencies but kernel density:</p>

<pre><code>freqz &lt;- with(data.frame(x,y), MASS::kde2d(x, y, n = 50))
with(freqz, plot_ly(x = x, y = y, z = z, type = ""surface"")) 
</code></pre>

<p><a href=""http://i.stack.imgur.com/067d6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/067d6.png"" alt=""enter image description here""></a></p>
"
"['parameter-estimation', 'gaussian']",EM parameter estimation for conditional Gaussians,"<p>When $X_1$ is unobserved, at iteration $k=1$ of EM, the posterior mean value (when $X_2=3$) is $5.18$ by using an inference algorithm, i.e. Junction tree/kalman filter. 
Then the sufficient statistics for $X_1$ is: </p>

<p>$s_1=\Sigma_{i=1}^nx_{i1}=9+4+5.18$ and $s_{11}=\Sigma_{i=1}^nx_{i1}^2=9^2+4^2+(5.18^2+\sigma_{11.2})$</p>

<p>where $\sigma_{11.2}$ is the posterior conditional variance of $X_1$ given $X_2$, when $X_1$ is unobserved. </p>

<p>$s_2=\Sigma_{i=1}^nx_{i2}$ and $s_{22}=\Sigma_{i=1}^nx_{i2}^2$. When $X_2$ is unobserved a term $\sigma_{22.1}$ is also added to $x_{i2}^2$. </p>

<p>So parameters $\mu_{X_1}$ and $\sigma_{X_1}$ are obtained. Similarly, posterior mean of unobserved $X_2$ is also obtained by using i.e. Junction tree inference.</p>

<p>The sufficient statistics for $s_{12}=\Sigma_{i=1}^nx_{i1}x_{i2}$, and $s_{21}=\Sigma_{i=1}^nx_{i2}x_{i1}$.</p>

<p>Mean vector is then: $\mu^{(k)}=(s_1/n, s_2/n)$.</p>

<p>Covariance matrix is: $\Sigma^{(k)}=$$\left( \begin{matrix}
   \Sigma _{11} &amp; \Sigma _{12}  \\
   \Sigma _{21} &amp; \Sigma _{22}  \\
\end{matrix} \right)$$=$$\left( \begin{matrix}
   s_{11}/n-(s_1/n)^2 &amp; s_{21}/n-(s_2/n)(s_1/n)  \\
   s_{12}/n-(s_1/n)(s_2/n) &amp; s_{22}/n-(s_2/n)^2  \\
\end{matrix} \right)$$
 $</p>

<p>Other parameters in the question are obtained. Conditional mean and conditional variance of $X_2|X_1$ is calculated by the formula in the reference of the question.</p>
"
"['statistics', 'distributed']",How to decided which test of normality to use,"<p>Mathematical detail of the two different tests can be found in two papers:</p>

<p>[1]. <a href=""http://sci2s.ugr.es/keel/pdf/algorithm/articulo/shapiro1965.pdf"" rel=""nofollow"">http://sci2s.ugr.es/keel/pdf/algorithm/articulo/shapiro1965.pdf</a></p>

<p>[2]. <a href=""http://www.jstor.org/stable/2334522?seq=1#page_scan_tab_contents"" rel=""nofollow"">http://www.jstor.org/stable/2334522?seq=1#page_scan_tab_contents</a></p>

<p>To summarize, both methods are based on hypothesis testing. But they are using different test statistics. </p>

<p>Shapiro-Wilk test has many good analytical properties and was designed to apply to data with sample size less than $50$. However, when the sample size becomes greater, Shapiro-Walk test might be unreliable. As I quote from the second paper:</p>

<blockquote>
  <p>Shapiro and Wilk did not extend their test beyond samples of size 50. A number of reasons indicate that it is best not to make such an extension. </p>
</blockquote>

<p>Essentially, that is why people invented the second normality test. As I quote from the second paper:</p>

<blockquote>
  <p>We present a new test of normality applicable for samples of size 50 or larger which possesses the desirable omnibus property</p>
</blockquote>
"
"['clustering', 'bigdata', 'dataset', 'statistics']","How do I determine the best statistical way for data transformation for standardization (like log, sq root) to remove bias between different datasets?","<p>Its unclear what the OP is asking (so this response is somewhat general), but the table below illustrates common contexts and the transformations that are typical:</p>

<p><strong>sales, revenue, income, price</strong> --> <code>log(x)</code></p>

<p><strong>distance</strong> --> <code>1/x</code>, <code>1/x^2</code>, <code>log(x)</code></p>

<p><strong>market share, preference share</strong> --> <code>(e^x)/(1+e^x)</code></p>

<p><strong>right-tailed dist</strong> --> <code>sqrt(x)</code>, <code>log(x)</code> caution log(x&lt;=0)</p>

<p><strong>left-tailed dist</strong> --> <code>x^2</code></p>

<p>You can also use John Tukey's <strong>three-point method</strong> as discussed in this <a href=""http://stats.stackexchange.com/questions/35711/box-cox-like-transformation-for-independent-variables/35717#35717"">post</a>.  When specific transformations don't work, use Box-Cox transformation. Use package <strong>car</strong> to <code>lambda &lt;- coef(powerTransform())</code> to compute lambda and then call <code>bcPower()</code> to transform. Consider Box-Cox transformations on all variables with skewed distributions before computing correlations or creating scatterplots.</p>
"
"['nosql', 'efficiency', 'indexing', 'data-indexing-techniques', '.net']",What is the most efficient data indexing technique,"<p>I think you messed up some things in your question. Lucene (I know nothing about Lucene,NET, but I suppose is the same) is a library used to analyze, split in tokens, and store documents in order to be able to query and retrieve them later. Lucene has a pretty old but effective model, it uses inverted trees to find and retrieve documents. Without further details, all documents are split in tokens (terms), and for each term is maintained a data structure, which stores all the documents which contains the given term. As a data structure could be used a BTree, a hash table and in the latest major revisions you can even plug in your own data structures.</p>

<p>A BTree (see <a href=""http://en.wikipedia.org/wiki/B-tree"">Wikipedia page</a> for further details), is a kind of a tree data structure, which is appropriate for working with big chunks of data and is often used for storing tree-like ordered structures on disk. For in-memory other trees performs better.</p>

<p>Murmur hash (see <a href=""http://en.wikipedia.org/wiki/MurmurHash"">Wikipedia page</a> for further details), is a family of hash functions used in hash table. The implementation of the hash table is not important, it could be a standard chained implementation or more advanced open hash addressing scheme. The idea is that the hash tables allows one to get fast a key, from an unordered set of keys, and can answer to tasks like: is this key part of this set of keys? which is the value associated with this key? </p>

<p>Now back to your main problem. You have one library (Lucene) and to data structures, both data structures are used in Lucene. Now you see that it is not possible to answer your question in these terms since they are not comparable.</p>

<p>However, regarding you footprint and performance part of the question. First of all you have to know which kind of operations you need to implement. </p>

<p><em>Do you need only get value for key, or do you need to find all elements in a range? In other words do you need order or not?</em> If you do, than a tree can help. If you do not, than a hash table, which is faster could be used instead. </p>

<p><em>Do you have a lot of data which does not fit the memory?</em> If yes than a disk-based solution would help (like BTree). If your data fit the memory, than use the fastest in-memory solution and use disk only as a storage (with a different structure, much simpler).</p>
"
"['python', 'time-series', 'forecast']",Analyze performance Poisson regression model on a time series(count forecasting),"<p>I'm not sure what you mean by ""performance"", but if what you mean is fit the answer is clear. You need to be using the log-likelihood to differentiate between different models. Basically, when you are fitting the model you are trying to maximize the log-likelihood. Thus the log-likelihood is giving you some sense of how well the parameters of your model are doing at fitting the data.</p>

<p>In your case, you want to get log-likelihood to be as close to zero as possible. Now this is kind of terrible advice, because if you were clever enough to come up with a feature for every observation, you could get a perfect fit. That's bad because your model would be completely useless. There are functions that take your log-likelihood as an input and transform it to penalize you for adding more variables, etc. We won't worry about those right now. Just keep in mind not to mindlessly lower the log-likelihood.</p>

<p>Once you have a model that you can live with you should run some sort of cross-validation, and/or hold out set. Then you can use any number of metrics to validate the predictive performance of your model. I think that is the more important of the two issues. You could calculate the mean square error on your hold out set. $$MSE=\sum_{i=1}^n(\hat y_i - y_i)^2$$
This would give you a really basic metric to assess how well your model is predictive of the output.</p>
"
"['data-mining', 'dataset', 'data-cleaning', 'parameter']",What are the best ways to tune multiple parameters?,"<p>Generally people perform a <a href=""http://scikit-learn.org/stable/modules/grid_search.html"">grid search</a>, which in its simplest ""exhaustive"" form is similar to Method 1.  However there are also more 'intelligent' ways to choose what to explore, which optimize in parameter space in a fashion similar to how each individual model is optimized. It can be tricky to do greedy optimization in this space, as it is often strongly non-convex.</p>

<p><a href=""https://en.wikipedia.org/wiki/Hyperparameter_optimization"">This page</a> describes the basics of optimizing model parameters.</p>
"
"['python', 'clustering', 'scikit']",Can you use clustering to pick out signals in noisy data?,"<p>Have you looked at <a href=""https://en.wikipedia.org/wiki/DBSCAN"" rel=""nofollow"">DBSCAN</a>? It is a density-based spatial clustering of data with noise that can define non-linear clusters (unlike k-means).</p>

<p>It doesn't require knowing the number of clusters. However, it does require two parameters (minimum cluster size and neighborhood size) that measure density. But you may be able to estimate them in your particular domain.</p>
"
"['python', 'predictive-modeling', 'time-series']",Please list some well tested api's for arima model,"<p><strong>Statsmodels:</strong></p>

<p><a href=""http://statsmodels.sourceforge.net/"" rel=""nofollow"">Statsmodels</a> is your best bet for a python library that includes ARIMA.  I have used it fairly extensively and am quite happy with it. But, its certainly not as well tested as R based ARIMA models. </p>

<p><strong>R:</strong></p>

<p>If you want something ""well-tested"" then your best bet is likely to use <a href=""http://rpy.sourceforge.net/"" rel=""nofollow"">Rpy2</a> to call an <a href=""https://cran.r-project.org/"" rel=""nofollow"">R</a> based <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html"" rel=""nofollow"">ARIMA library</a> from python. Rpy2 can be a bit tricky based on version reconciliation between python, R and Rpy2.</p>

<p>Here's a <a href=""https://sites.google.com/site/aslugsguidetopython/data-analysis/pandas/calling-r-from-python"" rel=""nofollow"">turorial on calling R from python using Rpy2</a></p>

<p>Hope this helps!</p>
"
"['similarity', 'information-retrieval']",Finding the top K most similar sets,"<p>Do you have any information about your data set? Is it sparse, will most similarities be zero? Is the total dictionary small? You could consider a inverted index. For example</p>

<pre><code>word  query_id
W1    [1, 3, 6]
W2    [2, 5]
W3    [1, 3, 4]
W4    [2, 3, 4]
W5    [2, 3, 6]

query_id  query
1         W1 W3
2         W2 W4 W4
3         W1 W3 W4 W5
4         W3 W4
5         W2
6         W1 W5
</code></pre>

<p>Here W_i is a word, e.g. birthday and query_id is the id of the query in the database. e.g. {how, are, you} might have id 22. Now you get a query {W1 W3 W5}. Aggregate counts on the inverted index. W1 was seen in queries 1, 3, and 6. W3 in 1, 3, and 4, etc.</p>

<pre><code>query_id  count
1         2
2         1
3         3
4         1
6         2
</code></pre>

<p>The count will the number of words in common with the incoming query, this is the numerator of the jaccard similarity. So, to find the top k you can start with the queries with the highest count. query_id 3 has the highest count and its similarity is 3/4.</p>

<p>If you have a massive database there are techniques like locality sensitive hashing which will basically reduce the search space into a small bucket. The incoming query gets hashed and lands in a bucket. You can then do a linear search with all the queries in this bucket to find the nearest k.</p>
"
"['bigdata', 'hadoop']",Pig script code error?,"<p>'group' in strictly lower case in the FOREACH is the thing you are looping/grouping over.</p>

<p><a href=""http://squarecog.wordpress.com/2010/05/11/group-operator-in-apache-pig/"" rel=""nofollow"">http://squarecog.wordpress.com/2010/05/11/group-operator-in-apache-pig/</a> says:</p>

<blockquote>
  <p>When you group a relation, the result is a new relation with two
  columns: “group” and the name of the original relation.</p>
</blockquote>

<p>Column names are case sensitive, so you have to use lower-case 'group' in your FOREACH.</p>

<p>'GROUP' in upper case is the grouping operator. You can't mix them. So don't do that.</p>
"
['search'],How can we effectively measure the impact of our data decisions,"<p>In A/B testing, bias is handled very well by ensuring visitors are randomly assigned to either version A or version B of the site.  This creates independent samples drawn from the same population.  Because the groups are independent and, on average, only differ in the version of the site seen, the test measures the effect of the design decision.</p>

<p><em>Slight aside</em>: Now you might argue that the A group or B group may differ in some demographic.  That commonly happens by random chance.  To a certain degree this can be taken care of by covariate adjusted randomization.  It can also be taken care of by adding covariates to the model that tests the effect of the design decision.  It should be noted that there is still some discussion about the proper way to do this within the statistics community.  Essentially A/B testing is an application of a <a href=""http://en.wikipedia.org/wiki/Randomized_controlled_trial"" rel=""nofollow"">Randomized Control Trial</a> to website design.  Some people disagree with adding covariates to the test.  Others, such as Frank Harrel (see <a href=""http://rads.stackoverflow.com/amzn/click/0387952322"" rel=""nofollow"">Regression Modeling Strategies</a>) argue for the use of covariates in such models.</p>

<p>I would offer the following suggestions:</p>

<ul>
<li>Design the study in advance so as to take care of as much sources of bias and variation as possible.   </li>
<li>Let the data speak for itself.  As you get more data (like about searches for David Beckham), let it dominate your assumptions about how the data should be (as how the posterior dominates the prior in Bayesian analysis when the sample size becomes large).    </li>
<li>Make sure your data matches the assumptions of the model.</li>
</ul>
"
['social-network-analysis'],How should ethics be applied in data science,"<p>I think ethics in Data Science is important. There is a fundamental difference in using user data to better their experience and show relevant ads and using user data to trick people into clicking on ads for the sake of monetary profit. Personally I like ads that give me relevant information like deals on things I would buy anyway. However, showing me weight loss ads because I got dumped is creepy and unethical. As my friend Peter always says, ""don't be creepy with data"". </p>
"
"['machine-learning', 'deep-learning', 'cross-validation', 'preprocessing', 'competitions']",How to approach the numer.ai competition with anonymous scaled numerical predictors?,"<p>I've looked at the approach and I'd select K by trying a range, i.e. 5k, 10k, 15k etc and then exploring the range in which the best result falls, say the best is 15k then I might do 13, 14, 15, 16, 17 and so on.</p>

<p>So far I've not found any pre-processing to be effective.</p>

<p>Answering the comment:</p>

<p>I've tried using LogisticRegression, SVM, Neural Networks, RandomForests, Multinomial NB, Extra Trees. All except Neural Networks using the implementations in sklearn. PyBrain for the NN.</p>
"
"['dataset', 'categorical-data', 'scoring', 'normalization', 'weighted-data']",Creating validation data for model comparison,"<p>Predicting and scoring are two different tasks.</p>

<p>And according to your answers and comments you are not solving prediction problem. You just want to set to each student a number in range [1,100] according to some rule. This is <strong>ranking</strong> (or scoring, whatever).</p>

<p>Therefore, the terms #prediction_model, #accuracy, #validation, #training_set are <strong>out of this scope</strong>. You don't need to validate anything. You are not making predictions.</p>

<p>What you want is to map ranks to students.</p>

<p>But a problem is that you have mostely categorical data (school name, location etc) that cannot be 'ranked'. Some of them are useless at all: how does the student name refer to his school progress? :)</p>

<p>If you change it somehow to numerical (e.g. 'Skill_1_level', 'Skill_2_level', 'remoteness_of_location', 'school rank' etc) than you can do some ranking:</p>

<ol>
<li>Normalize data: each of your factors</li>
</ol>

<p><a href=""http://i.stack.imgur.com/cdJkC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cdJkC.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li>Multiple by 100, as you want [0,100] range instead of [0,1]</li>
<li>Set up weights based on your experience according to factor's importance. So that the sum of weights is 1.</li>
<li>And finally build a rank (score):</li>
</ol>

<p><strong>Rank = 0.1 * skill_1_level + 0.2 * skill_2_level + 0.05 * remoteness_of_location + 0.5 * school_rank + ...</strong></p>
"
"['supervised-learning', 'unsupervised-learning']",supervised learning and labels,"<p>The main difference between supervised and unsupervised learning is the following:</p>

<p>In supervised learning you have a set of labelled data, meaning that you have the values of the inputs and the outputs. What you try to achieve with machine learning is to find the <strong>true</strong> relationship between them, what we usually call <strong>the model</strong> in math. There are many different algorithms in machine learning that allow you to obtain a model of the data. The objective that you seek, and how you can use machine learning, is to predict the output given a new input, once you know the model.</p>

<p>In unsupervised learning you don't have the data labelled. You can say that you have the inputs but not the outputs. And the objective is to find some kind of pattern in your data. You can find groups or <strong>clusters</strong> that you think that belong to the same group or output. Here you also have to obtain a model. And again, the objective you seek is to be able to predict the output given a new input. </p>

<p>Finally, going back to your question, if you don't have labels you can not use supervised learning, you have to use unsupervised learning.</p>
"
"['r', 'software-development']",Software Testing for Data Science in R,"<p>Packages for unit testing and assertive testing that are actively maintained:
Packages for unit testing</p>

<ol>
<li>testthat: more information on how to use you can find <a href=""http://r-pkgs.had.co.nz/tests.html"" rel=""nofollow"">here</a> or on <a href=""https://github.com/hadley/testthat"" rel=""nofollow"">github</a></li>
<li>Runit: <a href=""https://cran.r-project.org/web/packages/RUnit/index.html"" rel=""nofollow"">Cran page</a></li>
</ol>

<p>Packages for assertions:</p>

<ol>
<li><p>assertthat: info on <a href=""https://github.com/hadley/assertthat"" rel=""nofollow"">github</a></p></li>
<li><p>assertive: Assertive has a lot of subpackages available in case you do not need all of them. check on cran</p></li>
<li><p>assertr: info on <a href=""https://github.com/tonyfischetti/assertr"" rel=""nofollow"">github</a></p></li>
<li><p>ensurer: info on <a href=""https://github.com/smbache/ensurer"" rel=""nofollow"">github</a></p></li>
<li><p>tester: info on <a href=""https://github.com/gastonstat/tester"" rel=""nofollow"">github</a></p></li>
</ol>

<p>It is a matter of preference what you want to use for assertions. 
Read <a href=""http://bioconductor.org/developers/unitTesting-guidelines/"" rel=""nofollow"">this bioconductor</a> page for more info on the difference between RUnit and testthat.</p>
"
"['r', 'classification']",what is nn.index mean in KNN output,"<p>I guess you are using the <code>fnn</code> package.</p>

<p><code>attr</code> is a list of attributes which can be used for both <code>nn.index</code> and <code>nn.dist</code>. In this case, you are using <code>index</code>.</p>

<p>So, index returns an n x k matrix for the nearest neighbor indice.</p>

<p>And the definition of the nearest neighbor index is:</p>

<blockquote>
  <p>The nearest neighbor index is expressed as the ratio of the observed
  distance divided by the expected distance.</p>
</blockquote>

<p><a href=""http://edndoc.esri.com/arcobjects/9.2/net/shared/geoprocessing/spatial_statistics_tools/average_nearest_neighbor_spatial_statistics_.htm"" rel=""nofollow"">Definition reference</a></p>
"
"['reinforcement-learning', 'supervised-learning']",Supervised learning vs reinforcement learning for a simple self driving rc car,"<p>I'd suggest you to try a hybrid approach:</p>

<ul>
<li>First, <a href=""https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network"">train your car in supervised fashion by demonstration</a>. Just
control it and use your commands as labels. This will let you get all
the pros of SL.</li>
<li>Then, fine tune your neural net using reinforcement learning. You
don't need extra sensors for that: the rewards may be obtained from
distance sensors (larger distances = better) and from the speed
itself. This will give you the pros of RL and train your NN to the correct goal of driving fast while avoiding obstacles instead of the goal of imitating you.</li>
<li>Combining both approaches will get you the pros of both SL and RL
while avoiding their cons. RL won't start from random behavior, just
small gradual deviations from what you tought the NN. A similar approach was applied successfuly by Google DeepMind with <a href=""http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html"">AlphaGo</a>.</li>
<li>You can always include explicit rules on top of this. Implement them with high priority and call the NN only when there is no explicit rule for current situation. This is reminiscent of the <a href=""https://en.wikipedia.org/wiki/Subsumption_architecture"">Subsumption Architecture</a>.</li>
</ul>
"
"['feature-scaling', 'preprocessing']",Does it makes sense to apply feature scaling on timestamp,"<p>For time series analysis.  <strong>Yes</strong></p>

<p>But, turning data into a computable object for using in the ML computation?  <strong>No</strong></p>

<p>Using the data as a feature.  Then, <strong>Yes</strong></p>

<p>I would give you a general time series example:</p>

<p>Consider the number of days in months. The irregularity causes friction while analyzing the model.</p>

<p>Consider this:
<a href=""http://i.stack.imgur.com/oMTB6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oMTB6.png"" alt=""enter image description here""></a></p>

<p>So, such type of transformations would be helpful which analyzing time series', which can help reduce friction in the models sensibly.
<a href=""http://stats.stackexchange.com/questions/165166/number-of-days-in-a-monthly-forecast/165169#165169"">Link of the explanation</a></p>
"
"['r', 'python', 'data-mining']",Data Mining Gear/Goods Websites for Specific Prices,"<p>I would look into the Python package BeautifulSoup. It parses HTML documents into a tree structure and allows for all kind of filtering and manipulation of the tree. I've taken a look at the Gear Patrol website and I would advice these steps:</p>

<p>Scrape the front page looking for articles. By taking a look using a web inspector like the one in Chrome (Ctrl+Shift+I) you can look at the HTML structure. Looking there we can see that the links to the articles are in:</p>

<pre><code>body &gt; div:content &gt; div:home-this-week &gt; div:posts hfeed &gt; article &gt; div:body-wrap
</code></pre>

<p>Now you look for all article tags and create a list of the corresponding URLs. You can use the link of the pagination to retrieve more article links or use their URL structure to make an educated guess where more articles are.</p>

<p>Now that we have a list of URLs, iterate over them and parse these pages in BeautifulSoup to retrieve the necessary information in a similar fashion. I advice taking breaks of a few seconds in between requests for new pages to not get banned from their server and not overly disrupt it. There are a lot of tutorials available how to do webscraping using BeautifulSoup and this site looks like an easy victim.</p>
"
"['bigdata', 'beginner', 'social-network-analysis']",Reference about social network data-mining,"<p>My favorite place to find information about social network analysis is from SNAP, the Stanford Network Analysis Project. Led by Jure Leskovec, this team of students and professors has built software tools, gathered data sets, and published papers on social network analysis.</p>

<p><a href=""http://snap.stanford.edu/"" rel=""nofollow"">http://snap.stanford.edu/</a></p>

<p>The collection of research papers there is outstanding. </p>

<p>They also have a Python tool you could try. <a href=""http://snap.stanford.edu/snappy/index.html"" rel=""nofollow"">http://snap.stanford.edu/snappy/index.html</a></p>

<p>The focus is on graph analysis, because social networks fit this model well. If you are new to graph analysis, I suggest you take a undergraduate level discrete mathematics course, or check out my favorite book on the topic ""Graph Theory with Algorithms and its Applications"" by Santanu Ray.</p>

<p>For a hands-on approach to social network analysis, check out ""Mining the Social Web"" by Matthew A Russell. It has examples which cover how to collect and analyze data from the major social networks like Twitter, Facebook, and LinkedIn.</p>

<p>It was Jure Leskovec who initially excited me about this field. He has many great talks on YouTube, for example: <a href=""https://www.youtube.com/watch?v=LmQ_3nijMCs"" rel=""nofollow"">https://www.youtube.com/watch?v=LmQ_3nijMCs</a></p>
"
"['apache-spark', 'pyspark']","Spark, optimally splitting a single RDD into two","<p>First of all let me tell you that I'm not a Spark expert; I've been using it quite a lot in the last few months, and I believe I now understand it, but I may be wrong.</p>

<p>So, answering your questions:</p>

<p>a.) they are equivalent, but not in the way you're seeing it; Spark will not optimize the graph if you are wondering, but the <code>customMapper</code> will still be executed twice in both cases; this is due to the fact that for spark, <code>rdd1</code> and <code>rdd2</code> are two completely different RDDs, and it will build the transformation graph bottom-up starting from the leafs; so Option 1 will translate to:</p>

<pre><code>rdd0 = rddIn.map(lambda x: customMapper(x)).filter(lambda x: x[0]==0).cache()
rdd1 = rddIn.map(lambda x: customMapper(x)).filter(lambda x: x[0]==1).cache()
</code></pre>

<p>As you said, <code>customMapper</code> is executed twice (moreover, also <code>rddIn</code> will be read twice, which means that if it comes from a database, it may be even slower).</p>

<p>b.) there is a way, you just have to move <code>cache()</code> in the right place:</p>

<pre><code>mappedRdd = rddIn.map(lambda x: customMapper(x)).cache()
rdd0 = mappedRdd.filter(lambda x: x[0]==0)
rdd1 = mappedRdd.filter(lambda x: x[0]==1)
</code></pre>

<p>By doing this, we are telling spark that it can store the partial results of <code>mappedRdd</code>; it will then use these partial results both for <code>rdd1</code> and <code>rdd2</code>. From the spark point of view this is equivalent to:</p>

<pre><code>mappedRdd = rddIn.map(lambda x: customMapper(x)).saveAsObjectFile('..')
# forget about everything
rdd0 = sc.objectFile('..').filter(lambda x: x[0]==0)
rdd1 = sc.objectFile('..').filter(lambda x: x[0]==1)
</code></pre>
"
['predictive-modeling'],Analyzing survey data for predictions,"<p>Can you give a bit more information on the size of the data you're training on (and if it's really 6 parameters you're basing the predictions off of)?  If it's really 6 questions with binary answers (1, 0 as you suggest), then there are 2^6 (i.e. 64) unique answer combinations, and to determine a probability for them you'll want a multiple entries per combination.  Standard error scales like 1/sqrt(n) so for 10% accuracy you'll need roughly 6,400 inputs which given your description, sounds like more data than you may have.  You may want to invest time into automating data collection.</p>

<p>If on the other hand, you have a reasonably large data set and are hoping for some alternative models, both boosted decision trees and random forest models sound like good candidates for this problem.  </p>
"
"['bigdata', 'scalability', 'efficiency', 'performance']",How big is big data?,"<p>To me (coming from a relational database background), ""Big Data"" is not primarily about the data size (which is the bulk of what the other answers are so far).</p>

<p>""Big Data"" and ""Bad Data"" are closely related. Relational Databases require 'pristine data'. If the data is in the database, it is accurate, clean, and 100% reliable. Relational Databases require ""Great Data""  and a huge amount of time, money, and accountability is put on to making sure the data is well prepared before loading it in to the database. If the data is in the database, it is 'gospel', and it defines the system understanding of reality.</p>

<p>""Big Data"" tackles this problem from the other direction. The data is poorly defined, much of it may be inaccurate, and much of it may in fact be missing. The structure and layout of the data is linear as opposed to relational.</p>

<p>Big Data has to have enough volume so that the amount of bad data, or missing data becomes statistically insignificant. When the errors in your data are common enough to cancel each other out, and when the missing data is proportionally small enough to be negligible. When your data access requirements and algorithms are functional even with incomplete and inaccurate data, then you have ""Big Data"".</p>

<p>""Big Data"" is not really about the volume, it is about the characteristics of the data.</p>
"
['optimization'],fit model with sd square,"<p>I would suggest parametrizing with a logarithm of volatility so you don't have to care about positivity, run the estimation and then invert back to original scale. Alternatively, you can consider constrained optimization routine. Without knowing more about the problem (at least the language you're using), that's about it.</p>
"
"['machine-learning', 'neuralnetwork']",Autoencoders for feature creation,"<p>When you want to use Auto-Encoders (AEs) for dimensionality reduction, you usally add a bottleneck layer. This means, for example, you have 1234-dimensional data. You feed this into your AE, and - as it is an AE - you have an output of dimension 1234. However, you might have many layers in that network and one of them has significantly less dimensions. Lets say you have the topology <code>1234:1024:784:1024:1234</code>. You train it like this, but you only use the weights from the <code>1234:1024:784</code> part.</p>

<p>When you get new input, you just feed it into this network. You can see it as a kind of preprocessing. For the later stages, this is a black box.</p>

<p>This is manly useful when you have a lot of unlabeled data. It is called <em>Semi Supervised Learning</em> (SSL).</p>
"
"['machine-learning', 'clustering', 'dataset', 'visualization', 'data-cleaning']",What would be the best way to structure and mine this set of data?,"<blockquote>
  <p>How best could I organize this data, in what type of database?</p>
</blockquote>

<p>A simple relational database should do, but you could also use a ""fancy"" graph database if you want. One table for the users, and one for the ""interactions"". Each interaction would have foreign key columns for the two participants, labeled winner and loser, and the number of the episode the interaction it occurred.</p>

<blockquote>
  <p>Also any ideas on the best way to visually represent this data?</p>
</blockquote>

<p>A graphical representation for <a href=""https://en.wikipedia.org/wiki/Social_network_analysis"" rel=""nofollow"">social network analysis</a> suggests itself. Here are <a href=""https://www.researchgate.net/publication/225765648_Social_network_analysis_in_a_movie_using_character-net"" rel=""nofollow"">some</a> <a href=""http://bi.snu.ac.kr/Publications/Conferences/International/ASONAM2015_CJNan.pdf"" rel=""nofollow"">papers</a> and a <a href=""https://www.reddit.com/r/sna/"" rel=""nofollow"">subreddit</a> for inspiration. In your case, there is a concept of competition with clear winners/losers, so you could make your graph directed. Have the characters be the nodes, and add <em>directed</em> edges from the winning party to the losing party for each interaction. Collapse repeated interactions, etc. This approach would let you quickly identify overall winners and losers, as well as simply who interacts with whom. </p>
"
"['nlp', 'named-entity-recognition']",NLP : Is Gazetteer a cheat,"<p>Gazetteer or any other option of intentionally fixed size feature seems a very popular approach in <em>academic</em> papers, when you have a problem of finite size, for example NER in a fixed corpora, or POS tagging or anything else. I would not consider it cheating unless the only feature you will be using is Gazetteer matching.</p>

<p>However, when you train any kind of NLP model, which does rely on dictionary while training, you may get real world performance way lower than your initial testing would report, unless you can include all objects of interest into the gazetteer (and why then you need that model?) because  your trained model will rely on the feature at some point and, in a case when other features will be too weak or not descriptive, new objects of interest would not be recognized.</p>

<p>If you do use a Gazetteer in your models, you should make sure, that that feature has a counter feature to let model balance itself, so that simple dictionary match won't be the only feature of positive class (and more importantly, gazetteer should match not only positive examples, but also negative ones).</p>

<p>For example, assume you do have a full set of infinite variations of all person names, which makes general person NER irrelevant, but now you try to decide whether the object mentioned in text is capable of singing. You will rely on features of inclusion into your Person gazetteer, which will give you a lot of false positives; then, you will add a verb-centric feature of ""<strong>Is Subject of verb sing</strong>"", and that would probably give you false positives from all kind of objects like birds, your tummy when you're hungry and a drunk fellow who <em>thinks</em> he can sing (but let's be honest, he can not) -- but that verb-centric feature will balance with your person gazetteer to assign positive class of 'Singer' to persons and not animals or any other objects. Though, it doesn't solve the case of drunk performer.</p>
"
"['parallel', 'clusters', 'aws']",Instances vs. cores when using EC2,"<p>When using IPython, you very nearly don't have to worry about it (at the expense of some loss of efficiency/greater communication overhead).  The parallel IPython plugin in StarCluster will by default start one engine per physical core on each node (I believe this is configurable but not sure where).  You just run whatever you want across all engines by using the DirectView api (map_sync, apply_sync, ...) or the %px magic commands.   If you are already using IPython in parallel on one machine, using it on a cluster is no different.</p>

<p>Addressing some of your specific questions:</p>

<p>""how to reconcile distributing work across cores on an instance vs. instances on a cluster"" - You get one engine per core (at least); work is automatically distributed across all cores and across all instances.</p>

<p>""Is it even practical to parallelize across instances as well as across cores on each instance?"" - Yes :)  If the code you are running is embarrassingly parallel (exact same algo on multiple data sets) then you can mostly ignore where a particular engine is running.  If the core requires a lot of communication between engines, then of course you need to structure it so that engines primarily communicate with other engines on the same physical machine; but that kind of problem is not ideally suited for IPython, I think.</p>

<p>""If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance?"" - Use the largest c3 instances for compute-bound, and the smallest for memory-bandwidth-bound problems (or small enough that the problem almost stops being memory-bandwidth-bound); for message-passing-bound problems, also use the largest instances but try to partition the problem so that each partition runs on one physical machine and most message passing is within the same partition.  Problems which run significantly slower on N quadruple c3 than on 2N double c3 are rare (an artificial example may be running multiple simple filters on a large number of images, where you go through all images for each filter rather than all filters for the same image).  Using largest instances is a good rule of thumb.</p>
"
['recommendation'],How should one deal with implicit data in recommendation,"<p>Your system isn't just trained on items that are recommended right? if so you have a big feedback loop here. You want to learn from all clicks/views, I hope.</p>

<p>You suggest that not-looking at an item is a negative signal. I strongly suggest you do not treat it that way. Not interacting with something is almost always best treated as no information. If you have an explicit signal that indicates a dislike, like a down vote (or, maybe watched 10 seconds of a video and stopped), maybe that's valid.</p>

<p>I would not construe this input as rating-like data. (Although in your case, you may get away with it.) Instead think of them as weights, which is exactly the treatment in the Hu Koren Volinsky paper on ALS that @Trey mentions in a comment. This lets you record relative strength of positive/negative interactions.</p>

<p>Finally I would note that this paper, while is very likely to be what you're looking for, does not provide for negative weights. It is simple to extend in this way. If you get that far I can point you to the easy extension, which exists already in two implementations that I know of, in <a href=""https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala"" rel=""nofollow"">Spark</a> and <a href=""https://github.com/cloudera/oryx/blob/master/als-common/src/main/java/com/cloudera/oryx/als/common/factorizer/als/AlternatingLeastSquares.java"" rel=""nofollow"">Oryx</a>.</p>
"
['clustering'],Can some one explain how PCA is relevant in extracting parameters of Gaussian Mixture Models,"<p>I believe the claim that you are referring to is that the <em>maximum-likelihood estimate</em> of the component means in a GMM must lie in the span of the eigenvectors of the second moment matrix.  This follows from two steps:</p>

<ol>
<li>Each component mean in the maximum-likelihood estimate is a linear combination of the data points.  (You can show this by setting the gradient of the log-likelihood function to zero.)</li>
<li>Any linear combination of the data points must lie in the span of the eigenvectors of the second moment matrix.  (You can show this by first showing that any individual data point must lie in the span, and therefore any linear combination must also be in the span.)</li>
</ol>
"
"['text-mining', 'clustering']",Using Clustering in text processing,"<p>I don't know if you ever read SenseCluster by Ted Pedersen : <a href=""http://senseclusters.sourceforge.net/"" rel=""nofollow"">http://senseclusters.sourceforge.net/</a>. Very good paper for sense clustering. </p>

<p>Also, when you analyze words, think that ""computer"", ""computers"", ""computering"", ... represent one concept, so only one feature. Very important for a correct analysis.</p>

<p>To speak about the clustering algorithm, you could use a <a href=""http://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow"">hierarchical clustering</a>. At each step of the algo, you merge the 2 most similar texts according to their features (using a measure of dissimilarity, euclidean distance for example). With that measure of dissimilarity, you are able to find the best number of clusters and so, the best clustering for your texts and articles.</p>

<p>Good luck :)</p>
"
['dataset'],DBPedia as Table not having all the properties,"<p>The question was already answered on the DBpedia-discussion mailing list, by Daniel:</p>

<blockquote>
  <p>Hi Abhay,</p>
  
  <p>the DBpediaAsTables dataset only contains the properties in the
  dbpedia-owl namespace (mapping-based infobox data) and not those from
  the dbpprop (raw infobox properties) namespace (regarding the
  differences see [1]).</p>
  
  <p>However, as you are only interested in the data about specific
  entities, take a look at the CSV link at the bottom of the entity's
  description page, e.g., for your example this link is [2].</p>
  
  <p>Cheers, 
  Daniel</p>
  
  <p>[1] wiki.dbpedia.org/Datasets#h434-10</p>
  
  <p>[2] dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&amp;query=DESCRIBE+%3Chttp://dbpedia.org/resource/Sachin_Tendulkar%3E&amp;format=text%2Fcsv</p>
</blockquote>

<p>On the <a href=""http://wiki.dbpedia.org/DBpediaAsTables"" rel=""nofollow"">DBpediaAsTables web page</a>, you can find out which datasets were used to generate the tables: instance_types_en, labels, short_abstracts_en, mappingbased_properties_en, geo_coordinates_en. 
Also, I want to clarify that DBpediaAsTables contains all instances from DBpedia 2014, and with ""we provide some of the core DBpedia data"" we want to say that not all datasets are included in the tables (but only the 5 I stated before) </p>

<p>If you want to generate your own tables that will contain custom properties, please refer to the section <a href=""http://wiki.dbpedia.org/DBpediaAsTables#h347-4"" rel=""nofollow"">Generate your own Custom Tables</a>.</p>

<p>Cheers,</p>

<p>Petar</p>
"
"['r', 'decision-trees']",Visualizing N-way frequency table as a Decision Tree in R,"<p>You could try</p>

<pre><code>library(data.tree)
test$pathString &lt;- with(test, paste(""lm"", Type, Treatment, round(res, 2), sep=""/""))
(tree &lt;- as.Node(test))
#             levelName
# 1  lm                
# 2   ¦--Quebec        
# 3   ¦   ¦--nonchilled
# 4   ¦   ¦   °--36.97 
# 5   ¦   °--chilled   
# 6   ¦       °--30.11 
# 7   °--Mississippi   
# 8       ¦--nonchilled
# 9       ¦   °--24.31 
# 10      °--chilled   
# 11          °--17.45 
plot(tree)
</code></pre>

<p>The plot uses the <code>DiagrammeR</code> package. </p>

<p><a href=""http://i.stack.imgur.com/oTjry.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oTjry.gif"" alt=""enter image description here""></a></p>
"
"['data-mining', 'data']",Algorithm or formula to measure happiness?,"<p>You should be looking towards Natural Language Processing, specifically at <a href=""http://nlp.stanford.edu/sentiment/"" rel=""nofollow"">Sentiment Analysis</a>.  </p>

<p>The link I provided is a good starting point for learning about sentiment analysis.  If this is what you are looking for, it is available as part of Stanford's <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">Core NLP</a>.  </p>
"
"['neuralnetwork', 'deep-learning', 'computer-vision']",Using Neural Networks to extract multiple parameters from images,"<p>A CNN could be a good choice for this task if you expect variation in the original image scale, rotation lighting etc, and also have a <em>lot</em> of training data.</p>

<p>The usual CNN architecture is to have convolutional layers close to the input, and fully-connected layers in the output. Those fully-connected layers can have the output arranged for different classification or regression tasks as you see fit. Predicting the values of parameters describing the image is a regression task.</p>

<p>If you want accurate measures of size, you may need to avoid using max pooling layers. Unfortunately, not using pooling will make your network larger and harder to train - you <em>might</em> get away with strided convolution instead if that is a problem for you.</p>

<p>If your input images are very simple and clear (because they are always computer generated), then other approaches may be more reliable. You may be able to reverse-engineer image production and derive simple rules such as identifying lines, corners, circles and other easy-to-filter image components, and make direct measurements. There may also be a middle ground in complexity where extracting this data as features and using it to train a simple NN (or other ML model) will have good performance.</p>
"
"['scikit', 'ab-test']",How would I chi-squared test these simple results from A/B experiment?,"<p>If what you're trying to answer is if the action taken by a user (watch, interact, nothing) is influenced by the group they are in (A or B) you can use the chi2 test of independence.</p>

<p>For that you can use the scipy.stats.chi2_contingency function:</p>

<pre><code>a = [327445, 271602, 744702]
b = [376455, 140737, 818204]

chi2, pvalue, _, _ = scipy.stats.chi2_contingency([a, b])
</code></pre>

<p>In this case it returns a chi2 test statistic of 48376.48 and a p-value of 0.0, so the null hypothesis (""action is independent of group"") is rejected.</p>

<p>You can also use the scipy.stats.chisquare function to arrive at the same results, but other than with the chi2_contingency function you'll have to calculate the ""expected frequencies"" yourself. The data you have recorded are your observed frequencies:</p>

<pre><code>obs = np.array([a, b]).astype(float)
</code></pre>

<p>(Note that I converted the numbers to float, as the chisquare function will run into some weird integer overflows otherwise...!?)</p>

<p>The expected frequencies are calculated like that:</p>

<pre><code>exp = np.outer(obs.sum(axis=1), obs.sum(axis=0)) / obs.sum()
</code></pre>

<p>Finally, calling</p>

<pre><code>chi2, pvalue = scipy.stats.chisquare(obs.ravel(), exp.ravel(), ddof=sum(obs.shape)-2)
</code></pre>

<p>returns the same chi2 test statistic and p-value as before.</p>
"
"['classification', 'svm', 'supervised-learning', 'matlab', 'accuracy']",Different accuracy for different rng values,"<p>The training errors in this dataset has a huge difference (99% vs 57%). So, maybe the one with the <code>rng(1)</code> split has overfitted your dataset.</p>

<blockquote>
  <p>So there is a huge change in accuracy as visible. What does this mean?
  Am I training it wrong?</p>
</blockquote>

<p>The huge change might be due to overfitting. (Also, judge the model through validation curves, and then fit a model which balances the bias-variance plot.)</p>
"
['text-mining'],SUMMARIST: Automated Text Summarization,"<p>It dates back to 1998, so most likely has been abandoned, or ""acquired"" by microsoft as the creator currently works there and has done since publishing that research.</p>

<p>see <a href=""http://research.microsoft.com/en-us/people/cyl/ists97.pdf"" rel=""nofollow"">http://research.microsoft.com/en-us/people/cyl/ists97.pdf</a></p>

<p>and <a href=""http://research.microsoft.com/en-us/people/cyl"" rel=""nofollow"">http://research.microsoft.com/en-us/people/cyl</a> for the author. Maybe you could try to contact him.</p>
"
"['r', 'accuracy', 'cross-validation', 'sampling', 'beginner']",Avoid iterations while calculating average model accuracy,"<p>Yes, you can do all this using the Caret (<a href=""http://caret.r-forge.r-project.org/training.html"" rel=""nofollow"">http://caret.r-forge.r-project.org/training.html</a>) package in R. For example,</p>

<pre><code>fitControl &lt;- trainControl(## 10-fold CV
                           method = ""repeatedcv"",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)

gbmFit1 &lt;- train(Class ~ ., data = training,
                 method = ""gbm"",
                 trControl = fitControl,
                ## This last option is actually one
                ## for gbm() that passes through
                verbose = FALSE)
gbmFit1
</code></pre>

<p>which will give the output</p>

<pre><code>Stochastic Gradient Boosting 

157 samples
 60 predictors
  2 classes: 'M', 'R' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 

Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... 

Resampling results across tuning parameters:

  interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD
  1                  50       0.8       0.5    0.1          0.2     
  1                  100      0.8       0.6    0.1          0.2     
  1                  200      0.8       0.6    0.09         0.2     
  2                  50       0.8       0.6    0.1          0.2     
  2                  100      0.8       0.6    0.09         0.2     
  2                  200      0.8       0.6    0.1          0.2     
  3                  50       0.8       0.6    0.09         0.2     
  3                  100      0.8       0.6    0.09         0.2     
  3                  200      0.8       0.6    0.08         0.2     

Tuning parameter 'shrinkage' was held constant at a value of 0.1
Accuracy was used to select the optimal model using  the largest value.
The final values used for the model were n.trees = 150, interaction.depth = 3     
and shrinkage = 0.1.
</code></pre>

<p>Caret offers many other options as well so should be able to suit your needs. </p>
"
"['machine-learning', 'classification', 'text-mining']",Can I classify set of documents using classifying method using limited number of concepts ?,"<p>Both methods work. However, if you retain all words in documents you would essentially be working with high dimensional vectors (each term representing one dimension). Consequently, a classifier, e.g. SVM, would take more time to converge.</p>

<p>It is thus a standard practice to reduce the term-space dimensionality by pre-processing steps such as stop-word removal, stemming, Principal Component Analysis (PCA) etc.</p>

<p>One approach could be to analyze the document corpora by a topic modelling technique such as LDA and then retaining only those words which are representative of the topics, i.e. those which have high membership values in a single topic class.</p>

<p>Another approach (inspired by information retrieval) could be to retain the top K tf-idf terms from each document.</p>
"
"['pandas', 'cross-validation', 'sklearn']",how to impute missing values on numpy array created by train_test_split from pandas.DataFrame?,"<p>Can you just cast your nparray from train_test_split back into a pandas dataFrame so you can carry out your same strategy. This is very common to what I do when dealing with pandas and scikit. For example,</p>

<pre><code> a = train_test_split
 new_df = pd.DataFrame(a)
</code></pre>
"
['regression'],How to see which transformation is the best,"<p>If your dependent variable is discrete, you should be using <code>glm</code> with a <code>poisson</code> model. You can use <code>lm</code> but you're obviously violating assumptions.</p>

<p>See example: <a href=""http://www.ats.ucla.edu/stat/r/dae/poissonreg.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/poissonreg.htm</a></p>

<p>And I concur with the post by TBSRounder.</p>
"
"['python', 'categorical-data', 'sklearn', 'feature-engineering']",Handling categorical features in Factorization Machines algorithm - Feature Hashing vs. One-Hot encoding,"<p>I decided to expand a bit on my comment and make a full answer. </p>

<p>So the reason why somebody may say that performing the hashing trick can destroy interactions is because it may map different features to the same bucket. But usually it's not a big deal. </p>

<p>Note that <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"" rel=""nofollow"">DictVectorizer</a> doesn't perform the hashing trick:</p>

<blockquote>
  <p>When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-valued feature is constructed for each of the possible string values that the feature can take on. For instance, a feature “f” that can take on the values “ham” and “spam” will become two features in the output, one signifying “f=ham”, the other “f=spam”.</p>
</blockquote>

<p>To do it, you need to use a different vectorizer: <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html"" rel=""nofollow"">HashingVectorizer</a></p>
"
"['machine-learning', 'dataset', 'data-cleaning', 'feature-extraction']",Handling a feature with multiple categorical values for the same instance value,"<p>One possible approach is to perform an encoding, where each level of the feature2 corresponds to a new feature (column).
This way you may describe the 1:N relation between the feature 1 and 2</p>

<p>Here a small example in R</p>

<pre><code>&gt; table1  &lt;- data.frame(id = c(1,2),  feature1 = c(""xyz"",""abc""), predict = c(T,T))
&gt; table2  &lt;- data.frame(id = c(1,1,1,2), feature2 = c(""class1"", ""class2"", ""class3"", ""class2""))
&gt; 
&gt; ## encoding
&gt; table(table2)
   feature2
id  class1 class2 class3
  1      1      1      1
  2      0      1      0
</code></pre>

<p>The new object contains the (now unique) <code>id</code> and setting of the feature2.
You need only to merge (join) the result to the <code>table1</code> (basically same task a DB join - which variance: inner, outer or full depends on your requirements).</p>
"
"['r', 'visualization']","Data visualization for pattern analysis (language-independent, but R preferred)","<p>I would use a visual analysis. Since you know there is a repetition every 256 bytes, create an image 256 pixels wide by however many deep, and encode the data using brightness. In (i)python it would look like this:</p>

<pre><code>import os, numpy, matplotlib.pyplot as plt

%matplotlib inline

def read_in_chunks(infile, chunk_size=256):
    while True:
        chunk = infile.read(chunk_size)
        if chunk:
            yield chunk
        else:
            # The chunk was empty, which means we're at the end
            # of the file
            return

fname = 'enter something here'
srcfile = open(fname, 'rb')
height = 1 + os.path.getsize(fname)/256
data = numpy.zeros((height, 256), dtype=numpy.uint8)    

for i, line in enumerate(read_in_chunks(srcfile)):
    vals = list(map(int, line))
    data[i,:len(vals)] = vals

plt.imshow(data, aspect=1e-2);
</code></pre>

<p>This is what a PDF looks like:</p>

<p><img src=""http://i.stack.imgur.com/bicgF.png"" alt=""A PDF file visualized""></p>

<p>A 256 byte periodic pattern would have manifested itself as vertical lines. Except for the header and tail it looks pretty noisy.</p>
"
['regression'],Predicting Age of Birth,"<p>It seems like you have a classical bayesian problem. You have some sort of prior distribution, a distribution over years of birth, your prior distribution is bimodal with peaks at the two years,  you can probably use a convolution of two normal distributions to model this variable. Then have it spit out a posterior distribution after you feed in some data.</p>

<p>The real problem that I have with this analysis is it seems your features aren't particularly good. It is true these vars might have information about birth year, for example for the 20th century the average age of first marriage has steadily been increasing. But I suspect that the signal is going to be fairly weak. Essentially, if I tell you that I got married at age 24, had my first child at 26, and that my older brother is 3 years older than me and my younger sister is 2 years younger than me, can you tell me in what year was I born, 1956 or 1989?</p>

<p>I suspect that without additional data this information that I provided would be completely useless, mostly because it is a very noisy signal. That information could apply equally to someone born in 1956 or 1989. It isn't very helpful.</p>

<p>Essentially, what I am saying is that when you update your prior, it isn't going to change very much. (Your posterior distribution would look very similar to the prior distribution.) Instead of doing some mustache twirling over what is the right algorithm to crack this problem, I think a much more fruitful exercise would be to think up some better features.</p>
"
"['machine-learning', 'time-series']",Why should I care about seasonal data when I forecast?,"<p>""Because its there"".</p>

<p>The data has a seasonal pattern. So you model it. The data has a trend. So you model it. Maybe the data is correlated with the number of sunspots. So you model that. Eventually you hope to get nothing left to model than uncorrelated random noise.</p>

<p>But I think you've screwed up your STL computation here. Your residuals are clearly not serially uncorrelated. I rather suspect you've not told the function that your ""seasonality"" is a 24-hour cycle rather than an annual one. But hey you haven't given us any code or data so we don't really have a clue what you've done, do we? What do you think ""seasonality"" even means here? Do you have any idea?</p>

<p>Your data seems the have three peaks every 24 hours. Really? Is this 'gas'='gasoline'='petrol' or gas in some heating/electric generating system? Either way if you know a priori there's an 8 hour cycle, or an 8 hour cycle on top of a 24 hour cycle on top of what looks like a very high frequency one or two hour cycle you <strong>put that in your model</strong>.</p>

<p>Actually you don't even say what your x-axis is so maybe its days and then I'd fit a daily cycle, a weekly cycle, and then an annual cycle. But given how it all changes at time=85 or so I'd not expect a model to do well on both sides of that.</p>

<p>With statistics (which is what this is, sorry to disappoint you but you're not a data scientist yet) you don't just robotically go ""And.. Now.. I.. Fit.. An... S TL model...."". You look at your data, try and get some understanding, then propose a model, fit it, test it, and use the parameters it make inferences about the data. Fitting cyclic seasonal patterns is part of that.</p>
"
['bigdata'],What are the applications of Solr in Big Data?,"<p>Solr is a highly scalable, fault tolerant search server.</p>

<p>So, you can store files in the form of JSON, XML, CSV or binary over HTTP.
And you can query it via <strong>GET</strong> and receive JSON, XML, CSV and binary results. It also has a distributed architecture for the purpose, which is the primary and most important concept in Big Data and it's handling.</p>

<p>So, as it is highly scalable and fault tolerant for high traffic and huge documents, it is a choice for a reliable search engine for huge documents, or in short <strong>Big Data</strong>.</p>
"
"['data-mining', 'dataset']",NASDAQ Trade Data,"<p>You can pull stock data very easyly in python and R (probably other languages as well) with the following packages:</p>

<p>In python with: <a href=""https://pypi.python.org/pypi/ystockquote"" rel=""nofollow"">https://pypi.python.org/pypi/ystockquote</a></p>

<p>This is also a really nice tutorial in iPython which shows you how to pull the stock data and play with it: <a href=""http://nbviewer.ipython.org/github/twiecki/financial-analysis-python-tutorial/blob/master/1.%20Pandas%20Basics.ipynb"" rel=""nofollow"">http://nbviewer.ipython.org/github/twiecki/financial-analysis-python-tutorial/blob/master/1.%20Pandas%20Basics.ipynb</a></p>

<p>In R with: <a href=""http://www.quantmod.com/"" rel=""nofollow"">http://www.quantmod.com/</a></p>

<p>HTH. </p>
"
['hadoop'],Hadoop Resource Manager Won't Start,"<p>So I was never able to find the error by looking through my logs. I ended up reinstalling it with CDH5 (which was MUCH easier than installing ""poor"" Hadoop)
Now everything runs fine! </p>

<p>I'm still having trouble getting things to save to the hdfs, but thats a question for another day... </p>
"
"['algorithms', 'graphs', 'named-entity-recognition']",Algorithm to construct similarity structure from hash lookup table,"<p>The algorithm you are asking about is very straightforward. </p>

<p>What you do, you are looking for <a href=""https://en.wikipedia.org/wiki/Connected_component_%28graph_theory%29"" rel=""nofollow"">connected components</a> in a certain graph where edges are determined by matching hash values. You can implement this with a modification of the <a href=""https://en.wikipedia.org/wiki/Disjoint-set_data_structure"" rel=""nofollow"">disjoint-set data structure</a>. </p>

<p>Your particular variation would be that apart from keeping track of vertices in each component, you have to keep track of m (number of rows) sets of hash values that have been found for each component.</p>
"
"['machine-learning', 'classification', 'dataset', 'unbalanced-classes']",Quick guide into training highly imbalanced data sets,"<ul>
<li>Max Kuhn covers this well in Ch16 of <em>Applied Predictive Modeling</em>.     </li>
<li>As mentioned in the linked thread, imbalanced data is essentially a cost sensitive training problem. Thus any cost sensitive approach is applicable to imbalanced data.</li>
<li>There are a large number of such approaches. Not all implemented in R: C50, weighted SVMs are options. Jous-boost. Rusboost I think is only available as Matlab code. </li>
<li>I don't use Weka, but believe it has a large number of cost sensitive classifiers. </li>
<li><em>Handling imbalanced datasets: A review</em>: Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas'</li>
<li><em>On the Class Imbalance Problem</em>: Xinjian Guo, Yilong Yin, Cailing Dong, Gongping Yang, Guangtong Zhou</li>
</ul>
"
"['nlp', 'language-model']",Improve CoreNLP POS tagger and NER tagger?,"<p>Your best best is to train your own models on the kind of data you're going to be working with.</p>
"
"['regression', 'linear-regression']","What does ""linear in parameters"" mean?","<p>Consider an equation of the form</p>

<p>$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon$ </p>

<p>where $x$'s are the variables and $\beta$'s are the parameters. Here, y is a linear function of $\beta$'s (linear in parameters) and also a linear function of $x$'s (linear in variables). If you change the equation to</p>

<p>$y = \beta_0 + \beta_1x_1 + \beta_2x_1^2 + \epsilon$</p>

<p>Then, it is no longer linear in variables (because of the squared term) but it is still linear in parameters. And for (multiple) linear regression, that's all that matters because in the end, you are trying to find a set of $\beta$'s that minimizes a loss function. For that, you need to solve a system of <a href=""https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)#Derivation_of_the_normal_equations"" rel=""nofollow""><em>linear</em> equations</a>. Given its nice properties, it has a closed form solution that makes our lives easier. Things get harder when you deal with nonlinear equations.</p>

<p>Assume you are not dealing with a regression model but instead you have a mathematical programming problem: You are trying to minimize an objective function of the form $c^Tx$ subject to a set of constraints: $Ax \geq b$ and $x\geq0$. This is a <a href=""https://en.wikipedia.org/wiki/Linear_programming"" rel=""nofollow"">linear programming</a> problem in the sense that it is linear in variables. Unlike the regression model, you are trying to find a set of $x$'s (variables) that satisfies the constraints and minimizes the objective function. This will also require you to solve systems of linear equations but here it will be linear in variables. Your parameters won't have any effect on that system of linear equations.</p>
"
"['r', 'unbalanced-classes', 'gbm']",Kappa near to 60% in unbalanced (1:10) data set,"<p>The Kappa is Cohen's Kappa score for inter-rater agreement. It's a commonly-used metric for evaluating the performance of machine learning algorithms and human annotaters, particularly when dealing with text/linguistics. </p>

<p>What it does is compare the level of agreement between the output of the (human or algorithmic) annotater and the ground truth labels, to the level of agreement that would occur through random chance. There's a very good overview of how to calculate Kappa and use it to evaluate a classifier in this stats.stackexchange.com answer <a href=""http://stats.stackexchange.com/questions/82162/kappa-statistic-in-plain-english"">here</a>, and a more in-depth explanation of Kappa and how to interpret it in <a href=""http://www1.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf"" rel=""nofollow"">this paper</a>, entitled ""Understanding Interobserver Agreement: The Kappa Statistic"" by Viera &amp; Garrett (2005).</p>

<p>The benefit of using Kappa, particularly in an unbalanced data set like yours, is that with a 90-10% imbalance between the classes, you can achieve 90% accuracy by simply labeling all of the data points with the label of the more commonly occurring class. The Kappa statistic is describing how well the classifier performs above that baseline level of performance.</p>

<p>Kappa ranges from -1 to 1, with 0 indicating no agreement between the raters, 1 indicating a perfect agreement, and negative numbers indicating systematic disagreement. While interpretation is somewhat arbitrary (and very task-dependent), Landis &amp; Koch (1977) defined the following interpretation system which can work as a general rule of thumb:</p>

<pre><code>Kappa Agreement
&lt; 0 Less than chance agreement
0.01–0.20 Slight agreement
0.21– 0.40 Fair agreement
0.41–0.60 Moderate agreement
0.61–0.80 Substantial agreement
0.81–0.99 Almost perfect agreement
</code></pre>

<p>Which would indicate that your algorithm is performing moderately well. Accuracy SD and Kappa SD are the respective Standard Deviations of the Accuracy and Kappa scores. I hope this is helpful!</p>
"
"['machine-learning', 'python', 'dataset', 'scikit']",Splitting Data in scikit-learn,"<p><code>train_test_split</code> is just a utility function around <code>ShuffleSplit</code>, which on its turn just randomly assigns each sample to either <code>train</code> or <code>test</code>, taking the desired probability into account.
You can do that however you'd like, and there's no real reason to use that specific function.</p>

<p>Its not too hard to come up with some code that does that for three values or N values, if you rather avoid calling <code>train_test_split</code> twice.</p>
"
"['logistic-regression', 'linear-regression']",Why for logistic regression the error is given by [y ln(sigma(x)) + (1 − y) ln(1 − sigma(x)],"<p>This is the log-likelihood: </p>

<p>$\log P(x; w) \equiv \log \prod_i P(x_i | w) = \sum_i \log P(x_i | w)$, where $P(x_i | w) \equiv \left\{ \begin{array}{rl}\sigma(x_i), &amp; y_i =1 \\ 1 - \sigma(x_i), &amp;y_i = 0\end{array} \right.$</p>

<p>Why the log-likelihood? When you have a probabilistic model, such as logistic regression, it's one way (the <a href=""https://en.wikipedia.org/wiki/Maximum_likelihood"" rel=""nofollow"">MLE</a>) of finding the parameters that fit best. Recall that in logistic regression we are, contrary to the name, trying to <em>classify</em> rather than regress, and the MSE is a regression loss; it seeks to minimize the distance from a point, while <a href=""https://en.wikipedia.org/wiki/Loss_functions_for_classification"" rel=""nofollow"">we wish to penalize being in the wrong <em>subspace</em></a> (the parts that don't correspond to the correct class). If you squint a bit, you can see that the negative log-likelihood minimizes the <a href=""https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression"" rel=""nofollow"">cross entropy</a>.</p>
"
"['machine-learning', 'data-mining', 'classification', 'clustering']",I am trying to classify/cluster users profile but don't know how with my attributes,"<p>Right now, I only have time for a very brief answer, but I'll try to expand on it later on.</p>

<p>What you want to do is a <strong>clustering</strong>, since you want to discover some labels for your data.  (As opposed to a classification, where you would have labels for at least some of the data and you would like to label the rest).</p>

<p>In order to perform a clustering on your users, you need to have them as some kind of points in an abstract space.  Then you will measure distances between points, and say that points that are ""near"" are ""similar"", and label them according to their place in that space.</p>

<p>You need to transform your data into something that looks like a user profile, i.e.: a user ID, followed by a vector of numbers that represent the features of this user.  In your case, each feature could be a ""category of website"" or a ""category of product"", and the number could be the amount of dollars spent in that feature.  Or a feature could be a combination of web and product, of course.</p>

<p>As an example, let us imagine the user profile with just three features:</p>

<ul>
<li>dollars spent in ""techy"" webs, </li>
<li>dollars spent on ""fashion"" products, </li>
<li>and dollars spent on ""aggressive"" video games on ""family-oriented"" webs (who knows).</li>
</ul>

<p>In order to build those profiles, you need to map the ""categories"" and ""keywords"" that you have, which are too plentiful, into the features you think are relevant.  Look into <a href=""http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html"" rel=""nofollow"">topic modeling</a> or <a href=""http://en.wikipedia.org/wiki/Semantic_similarity"" rel=""nofollow"">semantic similarity</a> to do so.  Once that map is built, it will state that all dollars spent on webs with keywords ""gadget"", ""electronics"", ""programming"", and X others, should all be aggregated into our first feature; and so on.</p>

<p>Do not be afraid of ""imposing"" the features!  You will need to refine them and maybe completely change them once you have clustered the users.</p>

<p>Once you have user profiles, proceed to cluster them using <a href=""http://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow"">k-means</a> or whatever else you think is interesting.  Whatever technique you use, you will be interested in getting the ""representative"" point for each cluster.  This is usually the geometric ""center"" of the points in that cluster.</p>

<p>Plot those ""representative"" points, and also plot how they compare to other clusters.  Using a <a href=""http://en.wikipedia.org/wiki/Radar_chart"" rel=""nofollow"">radar chart</a> is very useful here.  Wherever there is a salient feature (something in the representative that is very marked, and is also very prominent in its comparison to other clusters) is a good candidate to help you label the cluster with some catchy phrase (""nerds"", ""fashionistas"", ""aggressive moms"" ...).</p>

<p>Remember that a clustering problem is an open problem, so there is no ""right"" solution!  And I think my answer is quite long already; check also about normalization of the profiles and filtering outliers.</p>
"
"['classification', 'dataset', 'sampling']",When should we consider a dataset as imbalanced?,"<p>I think subsampling (downsampling) is a popular method to control class imbalance at the base level, meaning it fixes the root of the problem.  So for all of your examples, randomly selecting 1,000 of the majority of the class each time would work.  You could even play around with making 10 models (10 folds of 1,000 majority vs the 1,000 minority) so you will use your whole data set.  You can use this method, but again you're kind of throwing away 9,000 samples unless you try some ensemble methods.  Easy fix, but tough to get an optimal model based on your data.</p>

<p>The degree to which you need to control for the class imbalance is based largely on your goal.  If you care about pure classification, then imbalance would affect the 50% probability cut off for most techniques, so I would consider downsampling.  If you only care about the order of the classifications (want positives generally more higher than negatives) and use a measure such as AUC, the class imbalance will only bias your probabilities, but the relative order should be decently stable for most techniques.</p>

<p>Logistic regression is nice for class imbalance because as long as you have  >500 of the minority class, the estimates of the parameters will be accurate enough and the only impact will be on the intercept, which can be corrected for if that is something you might want.  Logistic regression models the probabilities rather than just classes, so you can do more manual adjustments to suit your needs.</p>

<p>A lot of classification techniques also have a class weight argument that will help you focus on the minority class more.  It will penalize a miss classification of a true minority class, so your overall accucracy will suffer a little bit but you will start seeing more minority classes that are correctly classified.</p>
"
['correlation'],Correlating company entities between different data sources,"<p>First of all I would clean up INC, LLC, BV etcetera from both the sources. After this there are a few options. Since Levenshtein is a metric you can use metric trees to search your space more efficiently (about O(n*log(m))). This will still be very slow so there are approximations available, for example the cosine similarity on bi-grams of the names. You can do this using matrix multiplication which is both very efficient and easily distributable. Instead of taking the highest similarity you could take the top-n and do further analysis on these, for example the real Levenshtein distance. The fact that you have additional information could be useful, you could add this to your similarity function in some way but this will be guess work. Most of these ideas I got from a PyData meetup that was recorded, a speaker from ING (a big bank) discusses the exact problem you have albeit on a bigger set with less additional information:</p>

<p><a href=""https://www.youtube.com/watch?v=4ohTsblxOJs"" rel=""nofollow"">https://www.youtube.com/watch?v=4ohTsblxOJs</a></p>
"
"['python', 'visualization']",Colouring points based on cluster on matplotlib,"<p>The <a href=""http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html"" rel=""nofollow"">sklearn documentation</a> shows you how:</p>

<pre><code>colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])
colors = np.hstack([colors] * 20)
...
if hasattr(algorithm, 'cluster_centers_'):
        centers = algorithm.cluster_centers_
        center_colors = colors[:len(centers)]
        plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)
</code></pre>

<p><a href=""http://i.stack.imgur.com/quQKq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/quQKq.png"" alt=""enter image description here""></a></p>
"
"['machine-learning', 'deep-learning']",Training Deep Nets on an Ordinary Laptop,"<p><strong>Yes, a laptop will work just fine for getting acquainted with some deep learning projects:</strong></p>

<p>You can pick a smallish deep learning problem and gain some tractable insight using a laptop so give it a try.</p>

<p>The <a href=""http://deeplearning.net/software/theano/"" rel=""nofollow"">Theano</a> project has a <a href=""http://deeplearning.net/tutorial/lenet.html"" rel=""nofollow"">set of tutorials</a> on digit recognition that I've played with and moded on a laptop.</p>

<p><a href=""https://www.tensorflow.org/"" rel=""nofollow"">Tensorflow</a> also has a <a href=""https://www.tensorflow.org/versions/r0.8/tutorials/index.html"" rel=""nofollow"">set of tutorials</a>.</p>

<p>I let some of the longer runs go overnight, but nothing was intractable.</p>

<p>You might also consider availing yourself of <a href=""https://aws.amazon.com/"" rel=""nofollow"">AWS</a> or one of the other cloud services.  For 20-30 dollars you can perform some of the bigger calculations in the cloud on some sort of <a href=""https://aws.amazon.com/ec2/"" rel=""nofollow"">elastic computing node</a>. The secondary advantage is that you can also list AWS or other cloud services as skill on your resume also :-)</p>

<p>Hope this helps!</p>
"
"['neuralnetwork', 'multiclass-classification', 'binary']",Binary Neural Network Classification or Multiclass Neural Network Classification?,"<p>I think you are making things more confusing then they are. </p>

<p><strong>Binary</strong></p>

<p>In this case you have two possible outputs: </p>

<ul>
<li><p>Obama = 1. </p></li>
<li><p>Not-Obama (who in this case can only be Romney) = 0. </p></li>
</ul>

<p><strong>Multi-Class</strong></p>

<p>In this case you have <em>k</em> possible outputs, for example when <em>k = 4</em>: </p>

<ul>
<li><p><em>k = 0</em>: Obama</p></li>
<li><p><em>k = 1</em>: Romney</p></li>
<li><p><em>k = 2</em>: Clinton</p></li>
<li><p><em>k = 3</em>: Bush</p></li>
</ul>

<p>There are approaches to tackle multi-class classification as binary classification which are called <em>One-vs-rest classification</em> and <em>One-vs-one classification</em>. </p>
"
"['machine-learning', 'neuralnetwork', 'tensorflow']",How should a neural network for unbound function approximation be structured?,"<p>This does not answere your question directly, but might include some helpful pointers:</p>

<p>In the recent <a href=""http://arxiv.org/abs/1512.03385"" rel=""nofollow"" title=""Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun Submitted on 10 Dec 2015"">Deep Residual Learning for Image Recognition</a> paper it is written:</p>

<blockquote>
  <p>If one hypothesizes that multiple nonlinear layers can asymptotically
  approximate complicated functions[...]</p>
  
  <p>This hypothesis, however, is still an open question. See [<a href=""http://arxiv.org/abs/1402.1869"" rel=""nofollow"" title=""G. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In NIPS, 2014."">28</a>].</p>
</blockquote>
"
"['machine-learning', 'optimization']",How to speed up optimization using Differential Evolution?,"<p>Do you run your analysis algos in batch or live? Which programing language under which environment do you use?</p>

<p>At a first naive look, i would recommend to parallelize your code as each indicator calculation seems to be independent of the other's.</p>
"
['predictive-modeling'],Predicted features combined with original ones,"<p>This procedure exists and is called <em>stacked generalization</em> or simply <em>stacking</em>. See stacking section from <a href=""https://en.wikipedia.org/wiki/Ensemble_learning"" rel=""nofollow"">wikipedia page</a>. Strating from there you can red more by following references from the page. The first paper on subject was published by Wolpert in 1992. </p>

<p>[Later edit]</p>

<p>Do not combine the results with original features, but keep only the predictions combined only with the target. </p>
"
"['machine-learning', 'bigdata', 'dataset']",Data Science Project Ideas,"<p>I would try to analyze and solve one or more of the problems published on <strong>Kaggle Competitions</strong> (<a href=""https://www.kaggle.com/competitions"">https://www.kaggle.com/competitions</a>). Note that the competitions are grouped by their expected <em>complexity</em>, from <code>101</code> (bottom of the list) to <code>Research</code> and <code>Featured</code> (top of the list). A color-coded vertical band is a <em>visual guideline</em> for grouping. You can <strong>assess time</strong> you could spend on a project by <strong>adjusting</strong> the expected <em>length</em> of corresponding competition, based on your <em>skills</em> and <em>experience</em>.</p>

<p>A number of <strong>data science project ideas</strong> can be found by browsing the following <code>Coursolve</code> webpage: <a href=""https://www.coursolve.org/browse-needs?query=Data%20Science"">https://www.coursolve.org/browse-needs?query=Data%20Science</a>.</p>

<p>If you have skills and desire to work on a <strong>real data science project</strong>, focused on <strong>social impacts</strong>, visit <code>DataKind</code> projects page: <a href=""http://www.datakind.org/projects"">http://www.datakind.org/projects</a>. More projects with social impacts focus can be found at <code>Data Science for Social Good</code> fellowship webpage: <a href=""http://dssg.io/projects"">http://dssg.io/projects</a>.</p>

<p><strong>Science Project Ideas</strong> page at <code>My NASA Data</code> site looks like another place to visit for inspiration: <a href=""http://mynasadata.larc.nasa.gov/804-2"">http://mynasadata.larc.nasa.gov/804-2</a>.</p>

<p>If you would like to use <strong>open data</strong>, this long list of applications on <code>Data.gov</code> can provide you with some interesting <em>data science</em> project ideas: <a href=""http://www.data.gov/applications"">http://www.data.gov/applications</a>.</p>
"
"['machine-learning', 'dataset', 'sampling']","With unbalanced class, do I have to use under sampling on my validation/testing datasets?","<p>Great question...  Here are some specific answers to your numbered questions:</p>

<p><code>1)</code> You should cross validate on B not B`.  Otherwise, you won't know how well your class balancing is working.  It couldn't hurt to cross validate on both B <strong>and</strong> B` and will be useful based on the answer to 4 below.</p>

<p><code>2)</code> You should test on both C and C` based on 4 below.</p>

<p><code>3)</code> I would stick with F1 and it could be useful to use ROC-AUC and this provides a good sanity check.  Both tend to be useful with unbalanced classes.</p>

<p><code>4)</code> This gets really tricky.  The problem with this is that the best method requires that you reinterpret what the learning curves should look like or use both the re-sampled and original data sets.  </p>

<p>The classic interpretation of learning curves is: </p>

<ul>
<li><strong>Overfit</strong> - Lines don't quite come together; </li>
<li><strong>Underfit</strong> - Lines come together but at too low an F1 score;</li>
<li><strong>Just Right</strong> - Lines come together with a reasonable F1 score.  </li>
</ul>

<p>Now, if you are training on A` and testing on C, the lines will never completely come together.  If you are training on A` and testing on C` the results won't be meaningful in the context of the original problem.  So what do you do?  </p>

<p>The answer is to train on A` and test on B`, but also test on B.  Get the F1 score for B` where you want it to be, then check the F1 score for B.  Then do your testing and generate learning curves for C.  The curves won't ever come together, but you will have a sense of the acceptable bias... its the difference between F1(B) and F1(B`).</p>

<p>Now, the new interpretation of your learning curves is: </p>

<ul>
<li><strong>Overfit</strong> - Lines don't come together and are farther apart than F1(B`)-F1(B);</li>
<li><strong>Underfit</strong> - Lines don't come together but the difference is less than F1(B`)-F1(B) and the F1(C) score is under F1(B);</li>
<li><strong>Just right</strong> - Lines don't come together but the difference is less than F1(B`)-F1(B) with an F1(C) score similar to F1(B).</li>
</ul>

<p><strong>General</strong>: I strenuously suggest that for unbalanced classes you first try adjusting your class weights in your learning algorithm instead of over/under-sampling as it avoids all of the rigor moral that we've outlined above.  Its very easy in libraries like scikit-learn and pretty easy to hand code in anything that uses a sigmoid function or a majority vote.</p>

<p>Hope this helps!</p>
"
"['machine-learning', 'r', 'cross-validation']",How to test/validate unlabeled data in association rules in R?,"<p>You may want to consider using your own <code>APparameter</code> object to put ""significance"" constraints on the rules learned by Apriori. See page 13 of the <a href=""http://cran.r-project.org/web/packages/arules/arules.pdf"" rel=""nofollow"">arules documentation</a>. This could reduce the number of uninteresting rules returned in your run.</p>

<p>In lieu of gold standard data for your domain, consider bootstrap resampling as a form of validation, as described <a href=""http://eprints.pascal-network.org/archive/00003198/01/lal.pdf"" rel=""nofollow"">in this article</a>.</p>
"
"['machine-learning', 'statistics', 'glm']",Is GLM a statistical or machine learning model?,"<p>A GLM is absolutely a statistical model, but statistical models and machine learning techniques are not mutually exclusive. In general, statistics is more concerned with inferring parameters, whereas in machine learning, prediction is the ultimate goal.</p>
"
"['machine-learning', 'classification']",Deriving Confidences from Distribution of Class Probabilities for a Prediction,"<p>As @David says, in your initial example, your confidence about C1 is the same in both cases. In your second example, you most certainly are less confident about the most-probable class in the second case, since the most-probable class is far less probable!</p>

<p>You may have to unpack what you're getting at when you say 'confidence' then, since here you're not using it as a term of art but an English word.</p>

<p>I suspect you may be looking for the idea of <a href=""https://en.wikipedia.org/wiki/Entropy_(information_theory)"" rel=""nofollow"">entropy</a>, or uncertainty present in the distribution of all class probabilities. In your first example, it is indeed lower in the second case than the first. I don't think what you're getting at is just a function of the most-probable class, that is.</p>
"
"['logistic-regression', 'linear-regression']",When to use Linear Discriminant Analysis or Logistic Regression,"<p>This question was asked and answered at the Cross-Validate SE. The answer is a few years old but it is still useful. If you want you can <a href=""http://stats.stackexchange.com/questions/95247/logistic-regression-vs-lda-as-two-class-classifiers"">click here</a> to find their answer.</p>
"
"['machine-learning', 'neuralnetwork']","What is a ""residual mapping""?","<p>It's $F(x)$; the difference between the mapping $H(x)$ and its input $x$. It's a <a href=""https://en.wikipedia.org/wiki/Residual_%28numerical_analysis%29"" rel=""nofollow"">common term in mathematics</a> (<a href=""https://de.wikipedia.org/wiki/Residuum_%28Numerische_Mathematik%29"" rel=""nofollow"">DE</a>).</p>
"
"['python', 'distributed', 'sampling']",Sampling from a multivariate von Mises-Fisher distribution in Python,"<p>Thanks to your help. I finally got my code working, plus some bibliography.</p>

<p>I put my hands on <em>Directional Statistics</em> (Mardia and Jupp, 1999) and on the Ulrich-Wood's algorithm for sampling. I post here what I understood from it, i.e. my code (in Python), with a 'movMF' flavour.</p>

<p>The rejection sampling scheme:</p>

<pre><code>def rW(n,kappa,m):
dim = m-1
b = dim / (np.sqrt(4*kappa*kappa + dim*dim) + 2*kappa)
x = (1-b) / (1+b)
c = kappa*x + dim*np.log(1-x*x)

y = []
for i in range(0,n):
    done = False
    while not done:
        z = sc.stats.beta.rvs(dim/2,dim/2)
        w = (1 - (1+b)*z) / (1 - (1-b)*z)
        u = sc.stats.uniform.rvs()
        if kappa*w + dim*np.log(1-x*w) - c &gt;= np.log(u):
            done = True
    y.append(w)
return y
</code></pre>

<p>Then, the desired sampling is $v \sqrt{1-w^2} + w  \mu$, where $w$ is the result from the rejection sampling scheme, and $v$ is uniformly sampled over the hypersphere.</p>

<pre><code>def rvMF(n,theta):
dim = len(theta)
kappa = np.linalg.norm(theta)
mu = theta / kappa

result = []
for sample in range(0,n):
    w = rW(kappa,dim)
    v = np.random.randn(dim)
    v = v / np.linalg.norm(v)

    result.append(np.sqrt(1-w**2)*v + w*mu)

return result
</code></pre>

<p>And, for effectively sampling with this code, here is an example:</p>

<pre><code>import numpy as np
import scipy as sc
import scipy.stats

n = 10
kappa = 100000
direction = np.array([1,-1,1])
direction = direction / np.linalg.norm(direction)

res_sampling = rvMF(n, kappa * direction)
</code></pre>
"
['preprocessing'],What does normalizing and mean centering data do?,"<p>Welcome to DataScience.SE! If you don't center before you normalize, you either don't take advantage of the full [0,1] range if your input is non-negative. The combination of centering and normalization is called <em>standardization</em>.</p>

<p>Sometimes one normalizes by the standard variation, and other times by just the range (max-min). The latter is called <em>feature scaling</em>. The effect is much the same. Normalizing by the range is easier computationally. Normalizing by the standard deviation fixes the sample variance, which is nice from a statistical perspective. When using the standard deviation, the subtraction is usually against the sample mean rather than the minimum.</p>

<p>There are several reasons for performing standardization. Sometimes we are interested in relative rather than absolute values. Standardization achieves invariance to these irrelevant differences. By explicitly preprocessing the data to reflect this disinterest, we relieve the model from having to learn it, allowing us to use a simpler one. Another reason is computational; it reduces the <em>condition number</em> -- you can think of this as the skewness or niceness of the loss surface -- making optimization easier and faster.</p>
"
"['dataset', 'open-source', 'freebase']",Data available from industry,"<p>Maybe there's another way to go. The idea would be to generate the dataset you will be processing with your algorithms. You define the models of behaviour of events (those you're looking for and those into which they are hiden). Then generate the data, then analyse.</p>

<p>This approach has the benefit to let you control exactly what is inside the processed data. And make sure your algorithm identifies exactly what it is supposed to identify, no more, no less.</p>

<p>With GEDIS Studio we model events behaviours with activity profiles and the generator produces those events. We have implemented generators for telecom CDR, credit card usages, smart metering, etc.</p>

<p>They are freely available online from the evaluation account on <a href=""http://www.data-generator.com"" rel=""nofollow"">http://www.data-generator.com</a></p>

<p>Check the detailed CDR use case at <a href=""http://www.gedis-studio.com/online-call-detail-records-cdr-generator.html"" rel=""nofollow"">http://www.gedis-studio.com/online-call-detail-records-cdr-generator.html</a></p>

<p>Regards </p>
"
"['machine-learning', 'regression']",Proper way of fighting negative outputs of a regression algorithms where output must be positive all the way,"<p>The problem is your model choice, as you seem to recognize. In the case of linear regression, there is no restriction on your outputs.  Often this is fine when predictions need to be non-negative so long as they are far enough away from zero.  However, since many of your training examples are zero-valued, this isn't the case.</p>

<p>If your data is non-negative and discrete (as in the case with number of cars on the road), you could model using a generalized linear model (GLM) with a log link function.  This is known as Poisson regression and is helpful for modeling discrete non-negative counts such as the problem you described. The Poisson distribution is parameterized by a single value $\lambda$, which describes both the expected value and the variance of the distribution.</p>

<p>This results in an approach similar to the one described by Emre in that you are attempting to fit a linear model to the log of your observations.</p>
"
"['machine-learning', 'data-mining', 'statistics', 'search']",When is there enough data for generalization?,"<p>It is my understanding that <em>random sampling</em> is a <strong>mandatory condition</strong> for making any <em>generalization</em> statements. IMHO, other parameters, such as sample size, just affect probability level (confidence) of generalization. Furthermore, clarifying the @ffriend's comment, I believe that you have to <strong>calculate</strong> needed <em>sample size</em>, based on desired values of <em>confidence interval</em>, <em>effect size</em>, <em>statistical power</em> and <em>number of predictors</em> (this is based on Cohen's work - see References section at the following link). For multiple regression, you can use the following calculator: <a href=""http://www.danielsoper.com/statcalc3/calc.aspx?id=1"">http://www.danielsoper.com/statcalc3/calc.aspx?id=1</a>.</p>

<p>More information on <strong>how to select, calculate and interpret effect sizes</strong> can be found in the following nice and comprehensive paper, which is freely available: <a href=""http://jpepsy.oxfordjournals.org/content/34/9/917.full"">http://jpepsy.oxfordjournals.org/content/34/9/917.full</a>.</p>

<p>If you're using <code>R</code> (and even, if you don't), you may find the following Web page on <strong>confidence intervals and R</strong> interesting and useful: <a href=""http://osc.centerforopenscience.org/static/CIs_in_r.html"">http://osc.centerforopenscience.org/static/CIs_in_r.html</a>.</p>

<p>Finally, the following <strong>comprehensive guide</strong> to survey <strong>sampling</strong> can be helpful, even if you're not using survey research designs. In my opinion, it contains a wealth of useful information on <em>sampling methods</em>, <em>sampling size determination</em> (including calculator) and much more: <a href=""http://home.ubalt.edu/ntsbarsh/stat-data/Surveys.htm"">http://home.ubalt.edu/ntsbarsh/stat-data/Surveys.htm</a>.</p>
"
"['dataset', 'statistics', 'ab-test']","Analyzing A/B test results which are not normally distributed, using independent t-test","<p>The distribution of your data doesn't need to be normal, it's the <a href=""https://en.wikipedia.org/wiki/Sampling_distribution"">Sampling Distribution</a> that has to be nearly normal. If your sample size is big enough, then the sampling distribution of means from Landau Distribution should to be nearly normal, due to the <a href=""https://en.wikipedia.org/wiki/Central_limit_theorem"">Central Limit Theorem</a>. </p>

<p>So it means you should be able to safely use t-test with your data.</p>

<h3>Example</h3>

<p>Let's consider this example: suppose we have a population with <a href=""https://en.wikipedia.org/wiki/Log-normal_distribution"">Lognormal distribution</a> with mu=0 and sd=0.5 (it looks a bit similar to Landau)</p>

<p><img src=""http://i.stack.imgur.com/Hw5mM.png"" alt=""lognormal density""></p>

<p>So we sample 30 observations 5000 times from this distribution each time calculating the mean of the sample </p>

<p>And this is what we get </p>

<p><img src=""http://i.stack.imgur.com/wjueS.png"" alt=""sampling distribution""></p>

<p>Looks quite normal, doesn't it? If we increase the sample size, it's even more apparent</p>

<p><img src=""http://i.stack.imgur.com/M0FQS.png"" alt=""sampling distribution""></p>

<h3>R code</h3>

<pre><code>x = seq(0, 4, 0.05)
y = dlnorm(x, mean=0, sd=0.5)
plot(x, y, type='l', bty='n')


n = 30
m = 1000

set.seed(0)
samp = rep(NA, m)

for (i in 1:m) {
  samp[i] = mean(rlnorm(n, mean=0, sd=0.5))
}

hist(samp, col='orange', probability=T, breaks=25, main='sample size = 30')
x = seq(0.5, 1.5, 0.01)
lines(x, dnorm(x, mean=mean(samp), sd=sd(samp)))


n = 300
samp = rep(NA, m)

for (i in 1:m) {
  samp[i] = mean(rlnorm(n, mean=0, sd=0.5))
}

hist(samp, col='orange', probability=T, breaks=25, main='sample size = 300')
x = seq(1, 1.25, 0.005)
lines(x, dnorm(x, mean=mean(samp), sd=sd(samp)))
</code></pre>
"
"['neuralnetwork', 'random-forest']",Similar output from random forest and neural network,"<p>I think that your neural network is probably learning the same features as your Random Forest. I do not think it is obvious because it is hard to say what the neural network is learning, but in this case based on the your model it might be working similar.</p>
"
"['machine-learning', 'image-classification', 'training']",Build a tool for manually classifying training data images,"<p>I would recommend building your own database-backed Web app, since you have proprietary data and few (only two?) classes. I would create tables for the images, users, and labels.</p>

<p>user: (id, name)<br>
image: (id, url)<br>
label: (user.id, image.id, time, class)</p>

<p>The label class can be an enum. If you don't want to let users rate the same image multiple times you can drop the time column, and set the first two ID columns as the compound primary key.</p>

<p>If you've never dealt with Web applications and databases it will seem complicated, but it is easy once you get the hang of it. <a href=""https://blog.pythonanywhere.com/121/"" rel=""nofollow"">Here</a> is a tutorial. The benefit of this approach is persistence; you can turn off your computer and the start where you left off thanks to the database.</p>

<p>A simpler alternative is to collect all your data in one session using GUI components such as <a href=""https://github.com/ipython/ipywidgets"" rel=""nofollow"">ipywidgets for jupyter</a>, and writing the labels to a file. With this approach you do not get persistence.</p>
"
"['machine-learning', 'neuralnetwork', 'beginner']",RNN vs CNN at a high level,"<p>A CNN will learn to recognize patterns across space.  So, as you say, a CNN will learn to recognize components of an image (e.g., lines, curves, etc.) and then learn to combine these components to recognize larger structures (e.g., faces, objects, etc.). </p>

<p>You could say, in a very general way, that a RNN will similarly learn to recognize patterns across time.  So a RNN that is trained to translate text might learn that ""dog"" should be translated differently if preceded by the word ""hot"".</p>

<p>The mechanism by which the two kinds of NNs represent these patterns is different, however.  In the case of a CNN, you are looking for the <em>same</em> patterns on all the different subfields of the image.  In the case of a RNN you are (in the simplest case) feeding the hidden layers from the previous step as an additional input into the next step.  While the RNN builds up memory in this process, it is not looking for the same patterns over different slices of time in the same way that a CNN is looking for the same patterns over different regions of space.</p>

<p>I should also note that when I say ""time"" and ""space"" here, it shouldn't be taken too literally.  You could run a RNN on a single image for image captioning, for instance, and the meaning of ""time"" would simply be the order in which different parts of the image are processed.  So objects initially processed will inform the captioning of later objects processed.</p>
"
"['machine-learning', 'neuralnetwork', 'nlp', 'language-model']",Neural Networks for Predictive typing,"<p>A neural network is in principle a good choice when you have A LOT of similar data and classification tasks. Predicting the next character (or word... which is just multiple characters) is such a szenario. I don't think it really matters which kind of language you have, as long as you have enough training data of the same kind.</p>

<p>See <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">The Unreasonable Effectiveness of Recurrent Neural Networks</a> for a nice article where a recurrent neural network (RNN) was used as a character predictor to write complete texts. They also have code on <a href=""https://github.com/karpathy/char-rnn"" rel=""nofollow"">github.com/karpathy/char-rnn</a> ready to train / go. You can feed it with a start string and ask for the next characters / words.</p>
"
"['information-retrieval', 'lsi']","Tokenizing words of length 1, what would happen if I do topic modeling?","<p>The libraries usually exclude 1-length tokens and tokens with no alpha-numeric characters because typically they are noise and do not have any descriptive power. That is, these tokens are usually not helpful, say, in distinguishing between relevant vs not relevant documents. </p>

<p>However, if in your domain you feel like 1-length tokens can be helpful, feel free to use them as well. For example, if all the document that contain <code>1</code> belong to the same topic, it may be a good idea to preserve this token. <code>1</code> has descriptive power in this case: it can help distinguish between one particular topic and the rest.</p>

<p>Now, your next question is about LSI. For LSI there is no difference if a column in the document-term matrix corresponds to a 1-char token or to a 5-char token. So you can use LSI in your analysis. </p>
"
"['data-mining', 'sampling']","What is a ""good"" sample size","<p>This is a tough question to answer without more information. I'm going to assume that this is for model building, but without more detail it's hard to recommend something.</p>

<p>However, there are some things which should <strong>generally</strong> be known:</p>

<p><strong>Population size</strong></p>

<p>How large is the population? Does your 2TB of data comprise the total population, or is this a sample of a given timeframe? What frame of data are you looking at - is this 2 days worth of data that is only representative for a given subset of the population, or is this everything? You'll need to know this to know what conclusions you can draw from this dataset.</p>

<p><strong>Variance</strong></p>

<p>What's the variance of the sample? If it's categorical data, how many unique values are there? Having a metric around this will help determine the number of samples you'll need. If this is a low variance set, you may only need a few hundred/thousand observations.</p>

<p><strong>Stratification/grouping</strong></p>

<p>Is your data grouped in a meaningful way? If so, you'll need to factor this into your sample. Depending on what you're doing, you'll want a meaningful representation of the population. If the data is not grouped, but has distinct groups within it that you care about, you may need to stratify or pre-process your data.</p>

<p><strong>Model and goals</strong></p>

<p>All of this ends up coming down to what you're trying to do. If you're trying to classify or parse a set of unique entities, you may be better off streaming a large set of your data rather than trying to sample it. If you're trying to classify images or customers based on behavior, you may only need a small subset depending on how these groups differ.</p>
"
"['dataset', 'statistics', 'time-series', 'error-handling']",Uncertainty calculation through integration and correct analysis methodology,"<p>I found an answer to my question which in my opinion is correct (or at least has a point). Nevertheless it would be nice to have a validation from someone that might know more on the topic.</p>

<p>Since the average is calculated with a trapezoidal (which in reality is a summation) then the standard deviation can also be calculated by uncertainty propagation algebra for addition.</p>

<p>Based on <a href=""https://en.wikipedia.org/wiki/Trapezoidal_rule"" rel=""nofollow"">Wikipedia for trapezoidal rule</a>: 
$$ \int_c^d f(x) dx \approx (d-c) \left[ \frac{f(c)+f(d)}{2} \right] = \left( \frac{d-c}{2} \right) f(c) + \left( \frac{d-c}{2} \right) f(d)$$</p>

<p>which will follow the <a href=""https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulas"" rel=""nofollow"">2nd uncertainty propagation rule from Wikipedia</a>:
$$ f=aA+bB$$</p>

<p>with:</p>

<ul>
<li>$ a = b = \frac{d-c}{2}$</li>
<li>$ A = f(c) $</li>
<li>$ B = f(d) $</li>
</ul>

<p>In terms of Python3 code that translates to:</p>

<pre><code>import numpy as np

# FUNCTIONS
def uncAddition(A=1.,B=1.,sigmaA=0.,sigmaB=0.,a=1.,b=1.,sigmaAB=0.):
    f =  (a * A) + (b * B)
    sigmaf = np.sqrt( (a * sigmaA)**2 \
                    + (b * sigmaB)**2 \
                    + (2 * a * b * sigmaAB) )
    return f, sigmaf


# MAIN
# Data: is ab nx2 array that contains n number of f values 
#           and their absolute standard deviation
# x:    is the independent variable of the f values
# Obviously x and f should have the same number of rows

integralAve=0.
integralStD=0.
for i in range(1,len(data[:,0])):
    trapzPartAve,trapzPartStd = uncAddition(A=f[i-1,0],
                                            sigmaA=f[i-1,1],
                                            a=(x[i]-x[i-1])/2.0,
                                            B=f[i,0],
                                            sigmaB=f[i,1],
                                            b=(x[i]-x[i-1])/2.0) 
    integralAve += trapzPartAve
    integralStd += trapzPartStd

print(integralAve,integralStd)
</code></pre>
"
"['r', 'data-cleaning']",problem with regular expression,"<p>You could do</p>

<pre><code>reshape2::recast(
  data = setNames(strsplit(language, "", "", T), language), 
  formula = L1~value, 
  fun.aggregate = length
)
#                                                                L1 Assembly Go R Ruby Rust Java Javascript SQL C C++ Haskell Matlab PHP Swift Perl Bash Python
# 1                                                          C, C++        0  0 0    0    0    0          0   0 1   1       0      0   0     0    0    0      0
# 2                                                            Java        0  0 0    0    0    1          0   0 0   0       0      0   0     0    0    0      0
# 3                                                         Java, C        0  0 0    0    0    1          0   0 1   0       0      0   0     0    0    0      0
# 4                                     Java, Javascript, Ruby, SQL        0  0 0    1    0    1          1   1 0   0       0      0   0     0    0    0      0
# 5                                                      Java, Ruby        0  0 0    1    0    1          0   0 0   0       0      0   0     0    0    0      0
# 6                                          Javascript, Ruby, Bash        0  0 0    1    0    0          1   0 0   0       0      0   0     0    0    1      0
# 7                                             Perl, Javascript, R        0  0 1    0    0    0          1   0 0   0       0      0   0     0    1    0      0
# 8  PHP, Javascript, Ruby, Assembly, Swift, R, Matlab, Go, Haskell        1  1 1    1    0    0          1   0 0   0       1      1   1     1    0    0      0
# 9                                         Python, PHP, Javascript        0  0 0    0    0    0          1   0 0   0       0      0   1     0    0    0      1
# 10                                                              R        0  0 1    0    0    0          0   0 0   0       0      0   0     0    0    0      0
# 11                                    Ruby, Assembly, R, Go, Rust        1  1 1    1    1    0          0   0 0   0       0      0   0     0    0    0      0
</code></pre>
"
"['machine-learning', 'classification', 'bigdata', 'clustering', 'algorithms']",Optimizing Weka for large data sets,"<p>I would go for dimensionality reduction. You can start with SVD (should be available in Weka). If SVD is too slow / too memory consuming, then there are still some options:</p>

<ul>
<li><p>CUR-decomposition: a variant of singular-value decomposition that keeps the matrices of the decomposition sparse if the original matrix is sparse (see: this <a href=""http://infolab.stanford.edu/~ullman/mmds/ch11.pdf"" rel=""nofollow"">chapter</a> of Mining Massive Datasets book)</p></li>
<li><p>Random projections: projecting the data onto a random lower-dimensional subspace (see: the <a href=""http://people.inf.elte.hu/fekete/algoritmusok_msc/dimenzio_csokkentes/randon_projection_kdd.pdf"" rel=""nofollow"">Random projection in dimensionality reduction: Applications to image and text data</a> paper)</p></li>
<li><p>Coresets: Given a matrix A, a coreset C is defined as a weighted subset of rows of A such that the sum of squared distances from any given k-dimensional subspace to the rows of A is approximately the same as the sum of squared weighted distances to the rows in C <a href=""http://arxiv.org/pdf/1503.01663v1.pdf"" rel=""nofollow"">see the Dimensionality Reduction of Massive Sparse Datasets Using Coresets</a> paper)</p></li>
</ul>

<p>That's the tip of an iceberg. More approaches are there in the wild. The problem is that I doubt any of these solutions come with Weka (please, correct me if I am wrong on this). I would search for a usable Java implementation of any of these algorithms and try to port it to work with Weka's arff files.</p>
"
"['data-mining', 'clustering', 'octave', 'k-means', 'categorical-data']",K-Means clustering for mixed numeric and categorical data,"<p>The standard k-means algorithm isn't directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn't have a natural origin.  A Euclidean distance function on such a space isn't really meaningful.  As someone put it, ""The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs."" (from <a href=""http://www.daylight.com/meetings/mug04/Bradshaw/why_k-modes.html"">here</a>)</p>

<p>There's a variation of k-means known as k-modes, introduced in <a href=""http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf"">this paper</a> by Zhexue Huang, which is suitable for categorical data.   Note that the solutions you get are sensitive to initial conditions, as discussed <a href=""http://arxiv.org/ftp/cs/papers/0603/0603120.pdf"">here</a> (PDF), for instance.</p>

<p>Huang's paper (linked above) also has a section on ""k-protoypes"" which applies to data with a mix of categorical and numeric features.  It uses a distance measure which mixes the Hamming distance for categorical features and the Euclidean distance for numeric features.</p>

<p>A Google search for ""k-means mix of categorical data"" turns up quite a few more recent papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven't yet read them, so I can't comment on their merits.)  </p>

<hr>

<p>Actually, what you suggest (converting categorical attributes to binary values, and then doing k-means as if these were numeric values) is another approach that has been tried before (predating k-modes).  (See Ralambondrainy, H. 1995. A conceptual version of the k-means algorithm. Pattern Recognition Letters, 16:1147–1157.)  But I believe the k-modes approach is preferred for the reasons I indicated above.</p>
"
['data-mining'],what can be done with the following data?,"<p>I think you're looking at it the wrong way. First, you need to have a question and then you may find out whether the data could potentially hold the answer to that question. Right now, you're asking us to what questions this data may hold the answer. And that is a very difficult question!</p>

<p>But to provide you with at least some ideas:</p>

<ul>
<li><p>Find out what people deem important (the correlation between overall rating and the other ratings in (linear) regression analyses)</p></li>
<li><p>Find out the relation between certain characteristics of people (age/gender/nationality, if available) and their overall rating (or whatever rating available)</p></li>
</ul>

<p>Basically, the relation between certain variables and their outcomes. But make sure you have a clear question before you start looking for answers.</p>
"
"['dataset', 'aws']",Downloading a large dataset on the web directly into AWS S3,"<p>Since you obviously posses an AWS account I'd recommend the following:  </p>

<ul>
<li>Create an EC2 instance (any size)  </li>
<li>Use <a href=""http://www.gnu.org/software/wget/"" rel=""nofollow"">wget</a>(or curl) to fetch the file(s) to that EC2 instance. For example: <code>wget http://example.com/my_large_file.csv</code>.  </li>
<li>Install <a href=""http://s3tools.org/"" rel=""nofollow"">s3cmd</a>  </li>
<li>Use <code>s3cmd</code> to upload the file to S3. For example: <code>s3cmd cp my_large_file.csv s3://my.bucket/my_large_file.csv</code></li>
</ul>

<p>Since connections made between various AWS services leverage AWS's internal network, uploading from an EC2 instance to S3 is pretty fast. Much faster than uploading it from your own computer. This way allows you to avoid downloading the file to your computer and saving potentially significant time uploading it through the web interface.</p>
"
"['graphs', 'neo4j']",Is it ideally correct to benchmark neo4j as graph processing platform?,"<p><strong>No</strong></p>

<p>As Emre has rightly pointed out, the Chief Scientist of the company himself has written a blog post claiming the same.</p>

<p>However, Neo4j does a lot of computations which graph processing tools can do. In fact, it does graph traversals much faster than giraph, due to the Hadoop overhead and also as it stores the adjacent nodes in a doubly linked list.</p>

<p>So, it's not rare to confuse Neo4j with a graph processing platform (thus the claim in the paper) due to it's overlap with the features of a processing platform like Pregel and Giraph.</p>
"
['logistic-regression'],logistic regression - why exponent (log ratio) is linear,"<p>p is a probability so it is strictly between 0 and 1. So ln(p/(1-p)) is:</p>

<p>for p = 0: ln(0/1) = -Inf</p>

<p>for p = 1: ln(1/0) = +Inf</p>

<p>So now you've rescaled the probability to +/- Inf. In the GLM framework you can use practically any function that scales probability to +/-Inf (see any GLM textbook, or <a href=""http://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models"">http://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models</a>).</p>

<p>How did it come about? Well, mathematicians and statisticians realised they needed a function with the above properties, had a think, came up with a few, decided on the ones that had tractable asymptotic properties, explored how they worked with real data and decided the sensible ones were logit (as above), probit (see references) and a few others. Of course you should always test your model assumptions against your data, and choice of link is just another one of those assumptions, just like assuming linearity in a covariate.</p>
"
"['machine-learning', 'classification']",Cosine similarity or logistic regression for spam filtering,"<p>Use logistic regression which allows the weights to be learnt. By using cosine similarity you are forcing the weights to be the same for all features (assuming that you normalize the features first). This is putting unnecessary restrictions on the model.</p>
"
"['machine-learning', 'logistic-regression', 'predictive-modeling', 'regression']",Regression Model for explained model(Details inside),"<p>AFAIK if you want to predict the value of one variable, you need to have one or more variables as predictors; i.e.: you assume the behaviour of one variable can be explained by the behaviour of other variables.
In your case you have three independent variables whose value you want to predict, and since you don't mention any other variables, I assume that each variable depends on the others. In that case you could fit three models (for instance, regression models), each of which would predict the value of one variable, based on the others. As an example, to predict x:</p>

<pre><code>x_prediction=int+cy*y_test+cz*z_test
</code></pre>

<p>, where int is the intercept and cy, cz, the coefficients of the linear regression. 
Likewise, in order to predict y and z:</p>

<pre><code>y_prediction=int+cx*x_test+cx*z_test
z_prediction=int+cx*x_test+cy*y_test
</code></pre>
"
"['classification', 'scikit', 'feature-selection']",Features selection: should I mix features?,"<p>Let LASSO pick the best ones. If the features are highly correlated and you want them picked as a group, add some L2 regularization too. This is called Elastic Net regularization, and it is a generalization of L1 and L2 regularization. Other than that, do not feel obliged to artificially group features.</p>
"
"['clustering', 'time-series', 'similarity']",similarity measure for multivariate time series with heterogeous length and content,"<p>Try this recent paper: <a href=""http://jmlr.org/papers/v17/khaleghi16a.html"" rel=""nofollow"">Consistent Algorithms for Clustering Time Series</a>.</p>

<p>Your question is very much a current research topic.</p>

<p>Here's an older but excellent paper which talks about the fundamentals: <a href=""https://www.semanticscholar.org/paper/Generalized-Feature-Extraction-for-Structural-Olszewski-Maxion/7838bcd87bb6616e9fd3ffd92d4676a7082da34c/pdf"" rel=""nofollow"">Generalized Feature Extraction for Structural Pattern Recognition in Time-series Data</a>.</p>
"
"['data-mining', 'nlp', 'text-mining']",How can I infer psychological intentions from email corpus using text mining?,"<p>If you have a broad meaning of ""intentions"" in mind, you might be interested in research showing that a person's personality (in the sense of the ""big five"" psychological personality theory) can be inferred with remarkable accuracy from facebook likes. Original research showing this can be found <a href=""http://www.pnas.org/content/110/15/5802.full.pdf"" rel=""nofollow"">here</a> and <a href=""http://www.pnas.org/content/112/4/1036.full.pdf"" rel=""nofollow"">here</a>. Obviously, facebook likes are not the same as the textual information you work with, but it may be possible to infer certain likes and dislikes from the text.</p>
"
"['machine-learning', 'neuralnetwork', 'q-learning']",Understanding Reinforcement Learning with Neural Net (Q-learning),"<p>In Q-Learning, on every step you will use observations and rewards to update your Q-value function:</p>

<p>$$ 
Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) +  \alpha [R_{t+1}+ \gamma  \underset{a'}{\max} Q_t(s_{t+1},a') - Q_t(s_t, a_t)]
$$</p>

<p>You are correct in saying that the neural network is just a function approximation for the q-value function. </p>

<p>In general, the approximation part is just a standard supervised learning problem. Your network uses (s,a) as input and the output is the q-value. As q-values are adjusted, you need to train these new samples to the network. Still, you will find some issues as you as using correlated samples and SGD will suffer.</p>

<p>If you are looking at the DQN paper, things are slightly different. In that case, what they are doing is putting samples in a vector (experience replay). To teach the network, they sample tuples from the vector, bootstrap using this information to obtain a new q-value that is taught to the network. When I say teaching, I mean adjusting the network parameters using stochastic gradient descent or your favourite optimisation approach. By not teaching the samples in the order that are being collected by the policy the decorrelate them and that helps in the training.</p>

<p>Lastly, in order to make a decision on state $ s $, you choose the action that provides the highest q-value:</p>

<p>$$
a^*(s)= \underset{a}{argmax} \space Q(s,a)
$$</p>

<p>If your Q-value function has been learnt completely and the environment is stationary, it is fine to be greedy at this point. However, while learning, you are expected to explore. There are several approaches being $\varepsilon$-greedy one of the easiest and most common ways.</p>
"
['r'],"Error in R:: Error: Unexpected '}' in "" }""","<p>Error is due to missing <code>')'</code> in the second <code>if</code> statement in your code.</p>

<pre><code>if(is.na(training_main[j,i]) 
</code></pre>

<p>change to </p>

<pre><code>if(is.na(training_main[j,i]))
</code></pre>
"
"['svm', 'classification', 'binary']","Using SVM as a binary classifier, is the label for a data point chosen by consensus?","<p>The term <em>consensus</em>, as far as I'm concerned, is used rather for cases when you have more a than one source of metric/measure/choice from which to make a decision. And, in order to choose a possible result, you perform some <em>average evaluation/consensus</em> over the values available.</p>

<p>This is not the case for SVM. The algorithm is based on a <a href=""http://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png"">quadratic optimization</a>, that maximizes the distance from the closest documents of two different classes, using a hyperplane to make the split.</p>

<p><img src=""http://i.stack.imgur.com/CCO7Z.png"" alt=""Hyperplane separating two different classes""></p>

<p>So, the only <em>consensus</em> here is the resulting hyperplane, computed from the closest documents of each class. In other words, the classes are attributed to each point by calculating the distance from the point to the hyperplane derived. If the distance is positive, it belongs to a certain class, otherwise, it belongs to the other one.</p>
"
['tools'],Google prediction API: What training/prediction methods Google Prediction API employs?,"<p>If you take a look over the specifications of PMML which you can find <a href=""http://www.dmg.org/v3-0/GeneralStructure.html"" rel=""nofollow"">here</a> you can see on the left menu what options you have (like ModelTree, NaiveBayes, Neural Nets and so on).</p>
"
['clustering'],"How do i cluster binarized categorial data, without knowing the number of clusters?","<p>For categorical data, robust hierarchical clustering algorithm ( ROCK) will work better that employs links and not distances when merging clusters, which improves quality of clusters of categorical data. Boolean and categorical are two types of attributes that are most suited in this algorithm.</p>

<p>ROCK is a static model that combines nearest neighbor, relocation, and hierarchical agglomerative methods. In this algorithm, cluster similarity is based on the number of points from different clusters that have neighbors in common. </p>

<p>You can use CBA Package in R to perform the ROCK clustering.</p>

<p>Algorithm Steps:</p>

<p>Data----->Draw Random Sample----->Cluster with Links----->Label Data in DIsk</p>

<ol>
<li>A random sample is drawn from the database</li>
<li>A hierarchical clustering algorithm employing links is applied to the samples</li>
<li>This means: Iteratively merge clusters Ci, Cj that maximise the goodness function
merge(point1,point2) = total number of crosslinks /expected number of crosslinks 
Stop merging once there are no more links between clusters or the required number of clusters has been reached.</li>
<li>Clusters involving only the sampled points are used to assign the remaining data points on disk to the appropriate clusters  </li>
</ol>

<p>Hope it helps!!</p>

<p>For more details with examples, refer the following links:
<a href=""https://www.cis.upenn.edu/~sudipto/mypapers/categorical.pdf"" rel=""nofollow"">https://www.cis.upenn.edu/~sudipto/mypapers/categorical.pdf</a>
<a href=""https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/RockCluster"" rel=""nofollow"">https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/RockCluster</a></p>
"
"['neuralnetwork', 'rbm']",Hidden neuron representation of weights,"<p>It has learnt to recognize the digits, but it might have put too much weight on single pixels. Try to add different amounts of L2 regularization or dropout and compare the visualizations of the weights. Adding some kind of regularization should make the net rely less on single / independent pixels and more on the inherent structure of the digits, giving you smoother weights / visualization.</p>
"
['neuralnetwork'],Neural Network parse string data?,"<p>Using a neural network for prediction on natural language data can be a tricky task, but there are tried and true methods for making it possible.  </p>

<p>In the Natural Language Processing (NLP) field, text is often represented using the bag of words model.  In other words, you have a vector of length n, where n is the number of words in your vocabulary, and each word corresponds to an element in the vector.  In order to convert text to numeric data, you simply count the number of occurrences of each word and place that value at the index of the vector that corresponds to the word. <a href=""http://en.wikipedia.org/wiki/Bag-of-words_model."">Wikipedia does an excellent job of describing this conversion process.</a>  Because the length of the vector is fixed, its difficult to deal with new words that don't map to an index, but there are ways to help mitigate this problem (lookup <a href=""http://en.wikipedia.org/wiki/Feature_hashing"">feature hashing</a>).  </p>

<p>This method of representation has many disadvantages -- it does not preserve the relationship between adjacent words, and results in very sparse vectors.  Looking at <a href=""http://en.wikipedia.org/wiki/N-gram"">n-grams</a> helps to fix the problem of preserving word relationships, but for now let's focus on the second problem: sparsity.  </p>

<p>It's difficult to deal directly with these sparse vectors (many linear algebra libraries do a poor job of handling sparse inputs), so often the next step is dimensionality reduction. For that we can refer to the field of <a href=""http://en.wikipedia.org/wiki/Topic_model"">topic modeling</a>:  Techniques like <a href=""http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"">Latent Dirichlet Allocation</a> (LDA) and <a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"">Latent Semantic Analysis</a> (LSA) allow the compression of these sparse vectors into dense vectors by representing a document as a combination of topics.  You can fix the number of topics used, and in doing so fix the size of the output vector producted by LDA or LSA. This dimensionality reduction process drastically reduces the size of the input vector while attempting to lose a minimal amount of information.  </p>

<p>Finally, after all of these conversions, you can feed the outputs of the topic modeling process into the inputs of your neural network.     </p>
"
"['python', 'parsing']",How to build parse tree with BNF,"<p>I suggest you use <a href=""http://www.antlr.org/"" rel=""nofollow"">ANTLR</a>, which is a very powerful parser generator. It has a good GUI for entering your BNF. It has a <a href=""https://theantlrguy.atlassian.net/wiki/display/ANTLR4/Python+Target"" rel=""nofollow"">Python target</a> capability.</p>
"
"['machine-learning', 'r', 'beginner']",Predicting next medical condition from past conditions in claims data,"<p>I've never worked with medical data, but from general reasoning I'd say that relations between variables in healthcare are pretty complicated. Different models, such as random forests, regression, etc. could capture only part of relations and ignore others. In such circumstances it makes sense to use general <strong>statistical exploration</strong> and <strong>modelling</strong>. </p>

<p>For example, the very first thing I would do is finding out <strong>correlations</strong> between possible precursor conditions and diagnoses. E.g. in what percent of cases chronic kidney disease was preceded by long flu? If it is high, it <a href=""http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation"">doesn't always mean causality</a>, but gives pretty good food for thought and helps to better understand relations between different conditions. </p>

<p>Another important step is data visualisation. Does CKD happens in males more often than in females? What about their place of residence? What is distribution of CKD cases by age? It's hard to grasp large dataset as a set of numbers, plotting them out makes it much easier. </p>

<p>When you have an idea of what's going on, perform <a href=""http://en.wikipedia.org/wiki/Statistical_hypothesis_testing""><strong>hypothesis testing</strong></a> to check your assumption. If you reject null hypothesis (basic assumption) in favour of alternative one, congratulations, you've made ""something real"". </p>

<p>Finally, when you have a good understanding of your data, try to create complete <strong>model</strong>. It may be something general like <a href=""https://www.coursera.org/course/pgm"">PGM</a> (e.g. manually-crafted Bayesian network), or something more specific like linear regression or <a href=""http://en.wikipedia.org/wiki/Support_vector_machine"">SVM</a>, or anything. But in any way you will already know how this model corresponds to your data and how you can measure its efficiency. </p>

<hr>

<p>As a good starting resource for learning statistical approach I would recommend <a href=""https://www.udacity.com/course/st101"">Intro to Statistics</a> course by Sebastian Thrun. While it's pretty basic and doesn't include advanced topics, it describes most important concepts and gives systematic understanding of probability theory and statistics. </p>
"
"['feature-extraction', 'k-means']",Going from report to feature matrix,"<p>I don't understand what you mean by </p>

<blockquote>
  <p>So I have a report that lists the url and the number of visits a person did. <strong>So a combination of ip and url result in an amount of visits.</strong></p>
</blockquote>

<p>Assuming that you equate an IP with a user, and you wish to cluster users by their URL visitation frequencies, your matrix, <code>M</code>, would have </p>

<ul>
<li>One row per IP (user)</li>
<li>One column for each URL that you are tracking (your features)</li>
<li>and the entries in <code>M</code> would be ""visits"" of a given URL by a particular IP</li>
</ul>

<p>Given these assumptions, and your report, <code>M</code> would be:</p>

<pre><code>    abc.be  abc.be/a  abc.be/b
123   5        2         2
321   0        0         4
</code></pre>
"
"['machine-learning', 'neuralnetwork']",What is a Dichotomy?,"<p>In a machine learning context, a <a href=""https://en.wikipedia.org/wiki/Dichotomy"" rel=""nofollow"">dichotomy</a> is simply a split of a set into two mutually exclusive subsets whose union is the original set. The point being made in your quoted text is that for four points, a linear boundary can not form all possible dichotomies (i.e., it does not <a href=""https://en.wikipedia.org/wiki/Shattered_set"" rel=""nofollow"">shatter</a> the set). For example, if the four points are arranged on the corners of a square, a linear boundary can be used to create all possible dichotomies <em>except</em> it cannot produce a boundary that splits the two points lying along one diagonal from the other two points (and vice versa), as you indicated in your own answer.</p>
"
"['data-mining', 'bigdata', 'software-development']",Big data and data mining for CRM?,"<p>The two modules where you can really harness data mining and big data techniques are probably Leads and Opportunities. The reason is that, as you've written yourself, both contain 'potential' information that you can harness (through predictive algorithms) to get more customers. Taking Leads as an example, you can use a variety of machine learning algorithms to assign a probability to each account, based on that account's potential for becoming your customer in the near future. Since you already have an Accounts module which gives you information about your current customers, you can use this information to train your machine learning algorithms. This is all at a very high level but hopefully, you're getting the gist of what I'm saying.</p>
"
"['machine-learning', 'classification', 'beginner']",How to further improve the kaggle titanic submission accuracy?,"<p>big question.</p>

<p>Ok so here's a few things I'd look at if I was you..</p>

<ol>
<li>Have you tried any feature engineering ?(it sounds like you've just used the features in the training set but I can't be 100%)</li>
<li>Random Forests should do pretty well, but maybe try xgboost too? It's quite good at everything on Kaggle. SVM's could be worth a go also if you're thinking of stacking/ensembling.</li>
<li>Check out some of the tutorials around this competition. There's hundreds of them and most of them are great.</li>
</ol>

<p><strong>Links:</strong></p>

<p><a href=""http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/"" rel=""nofollow"">R #1 (my favourite)</a></p>

<p><a href=""http://www.numbrcrunch.com/blog/the-good-ol-titanic-kaggle-competition-pt-1"" rel=""nofollow"">R #2</a></p>

<p><a href=""http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/"" rel=""nofollow"">Python #1</a></p>

<p><a href=""https://github.com/agconti/kaggle-titanic"" rel=""nofollow"">Python #2</a></p>

<p>...Hopefully this helps</p>
"
"['machine-learning', 'hadoop', 'map-reduce']",Can all statistical algorithms be parallelized using a Map Reduce framework,"<p>Indeed there are:</p>

<ul>
<li>Gradient Boosting is by construction sequential, so parallelization is not really possible

<ul>
<li>Generalized Linear Models need all data at the same time, although technically you can parallelize some of the inner linear algebra nuts and bolts</li>
<li>Support Vector Machines</li>
</ul></li>
</ul>
"
"['hadoop', 'databases', 'map-reduce']",Can Hadoop be beneficial when data is in database tables and not in a file system,"<p>'SQL' on Hadoop is very much a thing, though I use quotes since it's probably more accurate to say it's SQL-like. Some options for bringing SQL-like capabilities to Hadoop include Hue, Hive/bee (Heading towards Stinger? So punny Apache), Impala, SparkSQL (probably not a great solution for a bank given the possibility of concurrency issues), among others (Seems like everyone has their own version of it these days)</p>

<p>To be honest though, if you're asking if it could be helpful, you probably don't need Hadoop (sorry in advance of that comes off harshly, it's not intended to). Lots an lots of places <em>think</em> they need Hadoop, but very few actually <em>need</em> Hadoop. There are of business that are down on the tech because they transitioned to it when the need didn't really exist. If you truly do need Hadoop or another distributed system it'd be almost impossible to determine which setup would be beneficial to your org without an intimate understanding of your data, and your specific business means.  </p>
"
"['machine-learning', 'reinforcement-learning']",Confusion in Policy Iteration and Value iteration in Reinforcement learning in Dynamic Programming,"<p>In value iteration, you implicitly solve for the state values under an ideal policy. There is no need to define an actual policy during the iterations, you can derive it at the end from the utilities that you calculate. You could if you wish, after any iteration, use the state values to determine what ""current"" policy is predicted.</p>

<p>In policy iteration, you define a starting policy and iterate towards the best one. So the policy is more explicitly tracked and re-calculated on each step.</p>
"
"['bigdata', 'apache-spark']",Apache Spark Question,"<p>It automatically determines the amount of mappers by the number of partitions your data is in. You can call getNumberPartitions on your data source (RDD/DataFrame) to see how much it is and use repartition for scaling this up or coalesce to scale this down (you can use repartition for this as well but this is slower). Repartitioning is expensive however and should be avoided when unneccesary.</p>
"
['r'],How to keep a subsetted value for calculating mean,"<p>To just get the mean for month 6:</p>

<pre><code>mean(df$temp[df$mon==""Jun""], na.rm=T)
</code></pre>

<p>You were nearly there but didn't assign your subset to a value, if you had:</p>

<pre><code>x = data[data$X..Month.. == 6,]
mean(x$X..Temp.., na.rm=TRUE)
</code></pre>

<p>that should work.</p>
"
"['machine-learning', 'markov-process', 'reinforcement-learning']",When to stop calculating values of each cell in the grid in Reinforcement Learning(dynamic programming) applied on gridworld,"<p>1- You should set a threshold (a hyper-param) that will allow you to quit the loop.</p>

<p><em>Let V the values for all state s and V' the new values after value iteration.</em></p>

<p>if $\sum_s|V(s) - V’(s)| \le threshold$, quit</p>

<p>2 - V is a function for every cell in the grid yes because you need to update every cell. </p>

<p>Hope it helps.</p>
"
"['data-mining', 'missing-data']",filling missing data with other than mean values,"<p>There are of course other choices to fill in for missing data. The median was already mentioned, and it may work better in certain cases.</p>

<p>There may even be much better alternatives, which may be very specific to your problem. To find out whether this is the case, you must find out more about the <em>nature</em> of your missing data. When you understand in detail why data is missing, the probability of coming up with a good solution will be much higher. </p>

<p>You might want to start your investigation of missing data by finding out whether you have <em>informative</em> or <em>non-informative</em> missings. The first category is produced by random data loss; in this case, the observations with missing values are no different from the ones with complete data. As for <em>informative</em> missing data, this one tells you something about your observation. A simple example is a customer record with a missing contract cancellation date meaning that this customer's contract has not been cancelled so far. You usually don't want to fill in informative missings with a mean or a median, but you may want to generate a separate feature from them. </p>

<p>You may also find out that there are several kinds of missing data, being produced by different mechanisms. In this case, you might want to produce default values in different ways.</p>
"
"['python', 'text-mining']",Optimizing co-occurrence matrix computation,"<p>From easiest to hardest:</p>

<ol>
<li>Try running it in <a href=""http://pypy.org"" rel=""nofollow"">pypy</a> or <a href=""http://numba.pydata.org/"" rel=""nofollow"">numba</a></li>
<li><a href=""https://github.com/search?q=co-occurrence&amp;type=Repositories"" rel=""nofollow"">Find a faster implementation</a>. Unfortunately I can not recommend one.</li>
<li>Parallelize the loop over the documents. Not so hard since your vocabulary is precomputed. (Even if it weren't you could get away with it using the <a href=""https://en.wikipedia.org/wiki/Feature_hashing#Feature_vectorization_using_the_hashing_trick"" rel=""nofollow"">hashing trick</a>.) Combine this with the first bullet.</li>
<li>Rewrite the <a href=""http://docs.scipy.org/doc/numpy/reference/arrays.nditer.html#putting-the-inner-loop-in-cython"" rel=""nofollow"">inner loop</a> in <a href=""http://docs.cython.org/src/userguide/numpy_tutorial.html"" rel=""nofollow"">Cython</a>.</li>
<li>Rewrite the whole thing in a faster language like C++ or Scala.</li>
</ol>
"
"['data-mining', 'predictive-modeling', 'bigdata', 'decision-trees', 'research']",Master thesis topics,"<p>This is the approach I took:</p>

<ol>
<li>Find journals related to your field of studies</li>
<li>Skim through the proceedings, see if there are titles that catch your interest</li>
<li>Read the papers (carefully or globally) that seemed interesting</li>
<li>Carefully consider the approaches and whatever future suggestions they present in their papers</li>
<li>Think critically: What would you change? What do you want to find out? Don't limit yourself to data but rather orient from the perspective of research. Solutions for data might only become apparent when you know exactly <strong>what</strong> you want to examine.</li>
</ol>

<p>I think this has advantages because these papers outline details regarding data as well -- perhaps you can use the same.</p>

<p>Present some papers and your idea to your prospective supervisor and he/she will make some suggestions. Researchers generally have a lot of knowledge about the possibilities and might even be curious about some things themselves.</p>

<p>Good luck! And enjoy.</p>
"
"['machine-learning', 'neuralnetwork', 'pybrain']",How to train neural networks with large sized data sets?,"<p>Here are some of the things that influence your training speed:</p>

<ul>
<li>Number of weights in your network</li>
<li>Speed of your CPU</li>
<li>Package you are using (mostly engine it is working on, in PyLearn this is Theano)</li>
<li>If all your data fits in memory or you are reading from disk in between batches</li>
</ul>

<p>With regards to network design the only thing you can really do is make the network more shallow to reduce the number of weights. To reduce the number of epochs there might be other options like adding residual connections but that will not decrease the training time of 1 epoch.</p>

<p>Without more information it is unclear where the bottleneck is, but 20 hours for one epoch seems a bit high. The easiest and biggest improvement you will be able to get is to use a good GPU, which should be possible using pylearn since it is built on top of Theano.</p>
"
"['apache-spark', 'scala', 'association-rules']",Apply GroupByKey to identify the Frequently Products Purchase Together,"<p>Since you are using Spark 1.6, I'd rather do these kind of transformations with DataFrame as it's much easier to manipulate.</p>

<p>You'll need to use SQLContext implicits for this :</p>

<pre><code>import sqlContext.implicits._ // not need in spark-shell
</code></pre>

<p>Now, let's create some dummy data just to follow the code snippet that you have provided :</p>

<pre><code>scala&gt; val data = sc.parallelize(Seq(""1,2,3,4"", ""2,3,4,5"", ""1,3,4,5"", ""1,6,6,7""))
// data: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[13] at parallelize at &lt;console&gt;:27
</code></pre>

<p>Back to your code, we define a case class <code>Transactions</code> and the CSV converter : </p>

<pre><code>scala&gt; case class Transactions(Transaction_ID: String, Customer_ID: String, Department: String, Product_ID: String)
// defined class Transactions

scala&gt; def csvToMyClass(line: String) = { val split = line.split(','); Transactions(split(0), split(1), split(2), split(3)) }
// csvToMyClass: (line: String)Transactions
</code></pre>

<p>We will use implicits now to convert our data into a DataFrame after converting it to Transactions :</p>

<pre><code>scala&gt; val df = data.map(csvToMyClass).toDF(""Transaction_ID"", ""Customer_ID"", ""Department"", ""Product_ID"")
// df: org.apache.spark.sql.DataFrame = [Transaction_ID: string, Customer_ID: string, Department: string, Product_ID: string]
</code></pre>

<p>Let's take a look at the DataFrame :</p>

<pre><code>scala&gt; df.show
// +--------------+-----------+----------+----------+
// |Transaction_ID|Customer_ID|Department|Product_ID|
// +--------------+-----------+----------+----------+
// |             1|          2|         3|         4|
// |             2|          3|         4|         5|
// |             1|          3|         4|         5|
// |             1|          6|         6|         7|
// +--------------+-----------+----------+----------+
</code></pre>

<p>Now all we have to do is a simple group by and perform a collect_list aggregation on the first dataframe : </p>

<pre><code>scala&gt; val df2 = df.groupBy(""Transaction_ID"").agg(collect_list($""Product_ID""))
// df2: org.apache.spark.sql.DataFrame = [Transaction_ID: string, collect_list(Product_ID): array&lt;string&gt;]
</code></pre>

<p>We can check the content of our new DataFrame <code>df2</code> :</p>

<pre><code>scala&gt; df.groupBy(""Transaction_ID"").agg(collect_list($""Product_ID"")).show
// +--------------+------------------------+
// |Transaction_ID|collect_list(Product_ID)|
// +--------------+------------------------+
// |             1|               [4, 5, 7]|
// |             2|                     [5]|
// +--------------+------------------------+
</code></pre>

<p>I hope that this answers your question. </p>

<p><strong>Note:</strong> If you wish to know what's the difference between RDD and DataFrames, I advice you to read Databrick's blog entry about it <a href=""https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html"" rel=""nofollow"">here</a>.</p>
"
['classification'],Decision tree or rule,"<p>JRip implements a propositional rule learner, “Repeated Incremental Pruning to Produce Error Reduction” <a href=""http://sci2s.ugr.es/keel/pdf/algorithm/congreso/slipper.pdf"" rel=""nofollow"">(RIPPER)</a>, as proposed by Cohen (1995) and OneR builds a simple <a href=""http://www.mlpack.org/papers/ds.pdf"" rel=""nofollow"">1-R classifier</a>, proposed by Holte (1993).</p>

<p>Its hard to say which algorithm works better. Best approach is to compare different classification algorithms performance in terms of precision, recall, accuracy, f1 score, AUC, specificity and sensitivity on your train/test data set and pick the one that gives best results or use ensemble of top performing algorithms to build your final model. </p>

<p>There is a good white paper doing similar exercise of comparing different classification algorithms including OneR and JRIP <a href=""https://www.researchgate.net/publication/273015702_Applying_Naive_bayes_BayesNet_PART_JRip_and_OneR_Algorithms_on_Hypothyroid_Database_for_Comparative_Analysis"" rel=""nofollow"">here</a>.</p>

<p>Hope this helps.</p>
"
"['classification', 'graphs']",What is a discrimination threshold of binary classifier?,"<p>Just to add a bit.</p>

<p>Like it was mentioned before, if you have a classifier (probabilistic) your output is a probability (a number between 0 and 1), ideally you want to say that everything larger than 0.5 is part of one class and anything less than 0.5 is the other class.</p>

<p>But if you are classifying cancer rates, you are deeply concerned with false negatives (telling some he does not have cancer, when he does) while a false positive (telling someone he does have cancer when he doesn't) is not as critical (IDK - being told you've cancer coudl be psychologically very costly). So you might artificially move that threshold from 0.5 to higher or lower values, to change the sensitivity of the model in general.</p>

<p>By doing this, you can generate the ROC plot for different thresholds.</p>
"
"['machine-learning', 'python', 'deep-learning', 'regression', 'keras']",Regression in Keras,"<p>The syntax is not exact, you should pass the features <code>X_test</code> and the true labels <code>Y_test</code> to <code>clt.score</code> (the method performs the prediction on itself, no need to do it explicitly).</p>

<pre><code>score = clf.score(X_test, Y_test)
</code></pre>

<p>You can also use other metrics available in the <code>metrics</code> module of sklearn. For example,</p>

<pre><code>from sklearn.metrics import mean_squared_error
score = mean_squared_error(Y_test, clf.predict(X_test))

from sklearn.metrics import mean_absolute_error
score = mean_absolute_error(Y_test, clf.predict(X_test))
</code></pre>

<p>Just some other remarks on your code that are not directly related to the question:</p>

<ul>
<li><p>you should not call <code>clf.fit</code> on the test data, you should instead fit on the training data and use the test set to compute the score to check the generalization of your model</p></li>
<li><p>you should fit <code>StandardScaler</code> only on the training data and use <code>X_test = scale.transform(X_test)</code> to apply the same transformation on the test set</p></li>
</ul>
"
"['machine-learning', 'python', 'pca']",PCA algorithm problems - Python,"<p><strong>Regarding First Question</strong>.</p>

<p>In the above formula, if I'm not wrong, x is a matrix of elements. So what the formula wants from you, is to sum all dot products of every line with it's transpose. This will give you scalar.</p>

<pre><code>x = np.array([1, 2, 3, 4])
res = x.dot(x.transpose())
# res = 30
</code></pre>

<p>So my sugestion would be to change that line of code to:</p>

<pre><code>for i in range(row):
    sigma += x[i].transpose().dot(x[i])
sigma = sigma/row
</code></pre>

<p><strong>Second Question</strong></p>

<p>Because you reduced the dimensionality, the x_new matrix will not be the same. </p>

<p><strong>Third Question</strong></p>

<p>When to use the PCA is a thing of domain problem. The point of dimensionality reduction is, to get new data set, which will not be as hard to process, but it will loose some information. So if you are your ""result""/""time to process"" is good, I don't think you should use it.</p>
"
"['classification', 'text-mining', 'topic-model']",what is difference between text classification and topic models?,"<p><strong>Text Classification</strong></p>

<p>I give you a bunch of documents, each of which has a label attached. I ask you to learn why you think the contents of the documents have been given these labels based on their words. Then I give you new documents and ask what you think the label for each one should be. The labels have meaning to me, not to you necessarily.</p>

<p><strong>Topic Modeling</strong></p>

<p>I give you a bunch of documents, without labels. I ask you to explain why the documents have the words they do by identifying some topics that each is ""about"". You tell me the topics, by telling me how much of each is in each document, and I decide what the topics ""mean"" if anything.</p>

<p>You'd have to clarify what you me by ""identify one topic"" or ""classify the text"".</p>
"
['classification'],Large Scale Personalization - Per User vs Global Models,"<p>The answer to this question is going to vary pretty wildly depending on the size and nature of your data. At a high level, you could think of it as a special case of multilevel models; you have the option of estimating a model with complete pooling (i.e., a universal model that doesn't distinguish between users), models with no pooling (a separate model for each user), and partially pooled models (a mixture of the two). You should really read Andrew Gelman on this topic if you're interested.</p>

<p>You can also think of this as a learning-to-rank problem that either tries to produce point-wise estimates using a single function or instead tries to optimize on some list-wise loss function (e.g., NDCG). </p>

<p>As with most machine learning problems, it all depends on what kind of data you have, the quality of it, the sparseness of it, and what kinds of features you are able to extract from it. If you have reason to believe that each and every user is going to be pretty unique in their behavior, you might want to build a per-user model, but that's going to be unwieldy fast -- and what do you do when you are faced with a new user?</p>
"
"['recommendation', 'similarity']","Create most ""average"" cosine similarity observation","<p>You are doing the correct thing. Technically, this averaging leads to computing the <a href=""http://en.wikipedia.org/wiki/Centroid"">centroid</a> in the Euclidean space of a set of N points. The centroid works pretty well with cosine similarities (cosine of the angles between normalized vectors), e.g. <a href=""http://en.wikipedia.org/wiki/Rocchio_algorithm"">the Rocchio algorithm</a>.</p>
"
"['python', 'gradient-descent', 'regression']",Stochastic gradient descent based on vector operations?,"<p>First of all, word ""sample"" is normally used to describe <a href=""http://en.wikipedia.org/wiki/Sample_%28statistics%29""><em>subset</em> of population</a>, so I will refer to the same thing as ""example"". </p>

<p>Your SGD implementation is slow because of this line: </p>

<pre><code>for each training example i:
</code></pre>

<p>Here you explicitly use exactly one example for each update of model parameters. By definition, vectorization is a technique for converting operations on one element into operations on a vector of such elements. Thus, no, you cannot process examples one by one and still use vectorization. </p>

<p>You can, however, approximate true SGD by using <strong>mini-batches</strong>. Mini-batch is a small subset of original dataset (say, 100 examples). You calculate error and parameter updates based on mini-batches, but you still iterate over many of them without global optimization, making the process stochastic. So, to make your implementation much faster it's enough to change previous line to: </p>

<pre><code>batches = split dataset into mini-batches
for batch in batches: 
</code></pre>

<p>and calculate error from batch, not from a single example. </p>

<p>Though pretty obvious, I should also mention vectorization on per-example level. That is, instead of something like this: </p>

<pre><code>theta = np.array([...])  # parameter vector
x = np.array([...])      # example
y = 0                    # predicted response
for i in range(len(example)):
    y += x[i] * theta[i]
error = (true_y - y) ** 2  # true_y - true value of response
</code></pre>

<p>you should definitely do something like this: </p>

<pre><code>error = (true_y - sum(np.dot(x, theta))) ** 2
</code></pre>

<p>which, again, easy to generalize for mini-batches:</p>

<pre><code>true_y = np.array([...])     # vector of response values
X = np.array([[...], [...]]) # mini-batch
errors = true_y - sum(np.dot(X, theta), 1)
error = sum(e ** 2 for e in errors)
</code></pre>
"
"['visualization', 'javascript', 'ipython']",How to plot large web-based heatmaps?,"<p><a href=""https://plot.ly/api/"" rel=""nofollow"">Plotly</a> and <a href=""http://lightning-viz.org/"" rel=""nofollow"">Lightning</a> are [supposedly] able to visualize extremely large data sets.</p>
"
"['convnet', 'keras']",border_mode for convolutional layers in keras,"<p>With border mode ""valid"" you get an output that is smaller than the input because the convolution is only computed where the input and the filter fully overlap.</p>

<p>With border mode ""same"" you get an output that is the ""same"" size as the input. That means that the filter has to go outside the bounds of the input by ""filter size / 2"" - the area outside of the input is normally padded with zeros.</p>

<p>Note that some libraries also support the border mode ""full"" where the filter goes even further outside the bounds of the input - up to ""filter size - 1"". This results in an output shape larger than the input.</p>

<p>There's a short explanation in.. numpy's convolve documentation:</p>

<p><a href=""http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.convolve.html"">http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.convolve.html</a></p>
"
"['data-mining', 'python']",Can someone explain the following error in my python code?,"<p>Seems that <code>remove border</code> is not defined. You have to define the function before used. </p>

<p>I do not know where the string error comes, is not clear to me. If you post the full traceback it will be clearer. </p>

<p>Finally your label is not show because you have to call the method <code>plt.legend()</code></p>
"
"['hadoop', 'map-reduce', 'aws']",Does Amazon RedShift replace Hadoop for ~1XTB data?,"<p><strong>tl;dr:</strong> They markedly differ in many aspects and I can't think Redshift will replace Hadoop.  </p>

<p>-Function<br>
You can't run anything other than SQL on Redshift. Perhaps most importantly, you can't run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it's more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.</p>

<p>-Performance Profile<br>
Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I'm using the term <code>indexing</code> very loose here). Therefore, it's great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.</p>

<p>-Cost Profile<br>
Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon's Elastic Map Reduce). For example, if you are doing OLAP, it's very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.  </p>

<p>Having said that, we've replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly for the ease of development. Redshift's Query Engine is based on PostgreSQL and is very mature, compared to Hive's. Its ACID characteristics make it easier to reason about it, and the quicker response time allows more testing to be done. It's a great tool to have, but it won't replace Hadoop.  </p>

<p><strong>EDIT</strong>:  As for setup complexity, I'd even say it's easier with Hadoop if you use AWS's EMR. Their tools are so mature that it's ridiculously easy to have your Hadoop job running. Tools and mechanisms surrounding Redshift's operation aren't that mature yet. For example, Redshift can't handle trickle loading and thus you have to come up with something that turns that into a batched load, which can add some complexity to your ETL. </p>
"
"['python', 'sklearn', 'pandas', 'pca']",Sklearn and PCA. Why is max n_row == max n_components?,"<p>Given m rows of n columns, I think it's natural to think of the data as n-dimensional. However the inherent dimension d of the data may be lower; d &lt;= n. d is the rank of the m x n matrix you could form from the data. The dimensionality of the data can be reduced to d with no loss of information, even. The same actually goes for rows, which is less intuitive but true; d &lt;= m. So, it always makes sense to reduce dimensionality to something &lt;= d since there's no loss; we typically reduce much further. This is why it won't let you reduce to more than the number of rows.</p>
"
"['r', 'visualization', 'markov-process', 'stata']",Data visualization of frequencies of state transitions (possibly in R?),"<p>How about a <a href=""https://bost.ocks.org/mike/sankey/"" rel=""nofollow"">Sankey diagram</a> with time on the x-axis and flow width representing state transition frequency. Here is a SO discussion on implementing <a href=""http://stackoverflow.com/questions/9968433/sankey-diagrams-in-r"">Sankey diagrams in R</a>.
<a href=""http://i.stack.imgur.com/PGUZn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PGUZn.png"" alt=""enter image description here""></a></p>

<p>One possible R package is <a href=""https://cran.r-project.org/web/packages/riverplot/riverplot.pdf"" rel=""nofollow"">{riverplot}</a>... here is code showing the first transition in your data:</p>

<pre><code>library(riverplot)
nodes &lt;- as.character(sapply(1:2, FUN = function(n){paste0(LETTERS[1:3],n)}))
edges &lt;- list(A1=list(C2=4), B1=list(A2=1,C2=1,B2=2), C1=list(C2=2))
r &lt;- makeRiver( nodes, edges, node_xpos= c( 1,1,1 ,2,2,2),
                node_labels= c( A1= ""A"", B1= ""B"", C1= ""C"", A2=""A"",B2=""B"",C2=""C"" ))
plot( r )
</code></pre>

<p>Will produce this:
<a href=""http://i.stack.imgur.com/KCCN2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KCCN2.png"" alt=""enter image description here""></a></p>
"
"['machine-learning', 'r', 'text-mining']",How to convert a text to lower case using tm package?,"<p>This seems like an encoding error. Try adding the line </p>

<pre><code>Encoding(movie_Clean)  &lt;- ""UTF-8""
</code></pre>

<p>before you lowercase the data. Check out this answer for a little context: <a href=""http://stackoverflow.com/a/28340080/4539807"">http://stackoverflow.com/a/28340080/4539807</a></p>
"
"['machine-learning', 'self-study']",Relationship between VC dimension and degrees of freedom,"<p>As stated by Prof Yaser Abu-Mostafa-</p>

<blockquote>
  <p>Degrees of freedom are an abstraction of the effective number of parameters. The effective number is based on how many <a href=""https://en.wikipedia.org/wiki/Dichotomy"">dichotomies</a> one can get, rather than how many real-valued parameters are used. In the case of 2-dimensional perceptron, one can think of slope and intercept (plus a binary degree of freedom for which region goes to +1), or one can think of 3 parameters w_0,w_1,w_2 (though the weights can be simultaneously scaled up or down without affecting the resulting hypothesis). The degrees of freedom, however, are 3 because we have the flexibility to shatter 3 points, not because of one way or another of counting the number of parameters.</p>
</blockquote>

<p><img src=""https://qph.is.quoracdn.net/main-qimg-c943643a81e326633b68e3c40387e793?convert_to_webp=true"" alt=""2-d perceptron""></p>
"
"['machine-learning', 'clustering', 'sequence']",Algorithm for segmentation of sequence data,"<p>Please see my comment above and this is my answer according to what I understood from your question:</p>

<p>As you correctly stated you do not need <em>Clustering</em> but <em>Segmentation</em>. Indeed you are looking for <em>Change Points</em> in your time series. The answer really depends on the complexity of your data. If the data is as simple as above example you can use the difference of vectors which overshoots at changing points and set a threshold detecting those points like bellow:
<img src=""http://i.stack.imgur.com/6BKAl.png"" alt=""enter image description here"">
As you see for instance a threshold of 20 (i.e. $dx&lt;-20$ and $dx&gt;20$) will detect the points. Of course for real data you need to investigate more to find the thresholds.</p>

<h2>Pre-processing</h2>

<p>Please note that there is a trade-off between accurate location of the change point and the accurate number of segments i.e. if you use the original data you'll find the exact change points but the whole method is to sensitive to noise but if you smooth your signals first you may not find the exact changes but the noise effect will be much less as shown in figures bellow:</p>

<p><img src=""http://i.stack.imgur.com/ceDxi.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/PLIpr.png"" alt=""enter image description here""></p>

<h2>Conclusion</h2>

<p>My suggestion is to smooth your signals first and go for a simple clustering mthod (e.g. using <a href=""http://www.autonlab.org/tutorials/gmm14.pdf"">GMM</a>s) to find an accurate estimation of the number of segments in signals. Given this information you can start finding changing points constrained by the number of segments you found from previous part.</p>

<p>I hope it all helped :)</p>

<p>Good Luck!</p>

<h2>UPDATE</h2>

<p>Luckily your data is pretty straightforward and clean. I strongly recommend dimensionality reduction algorithms (e.g. simple <a href=""http://sebastianraschka.com/Articles/2014_pca_step_by_step.html"">PCA</a>). I guess it reveals the internal structure of your clusters. Once you apply PCA to the data you can use k-means much much easier and more accurate.</p>

<h2>A Serious(!) Solution</h2>

<p>According to your data I see the generative distribution of different segments are different which is a great chance for you to segment your time series. See <a href=""http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2012_KhaleghiRMP12.pdf"">this</a> which is probably the best and most state-of-the-art solution to your problem. The main idea behind this paper is that if different segments of a time series are generated by different underlying distributions you can find those distributions, set tham as ground truth for your clustering approach and find clusters. </p>

<p>For example assume a long video in which the first 10 minutes somebody is biking, in the second 10 mins he is running and in the third he is sitting. you can cluster these three different segments (activities) using this approach.</p>
"
['machine-learning'],Why doesn't AlphaGo expect its opponent to play the best possible moves?,"<p>It appears that AlphaGo did <em>not</em> rate the move as a best possible move for Lee Sedol, just as one that was within its search space. To put into context the board is 19x19, so a 1 in 10000 chance of a move is much lower than chance of the square being picked at random. That likely makes the move that it ""found"" not worth exploring much deeper.</p>

<p>It is important to note too that the probabilities assigned to moves are equivalent to AlphaGo's rating for quality of that move - i.e. AlphaGo predicted that this was a bad choice for its opponent. Another way of saying this, is ""there is a probability p that this move is the best possible one, and therefore worth investigating further"". There is no separate quality rating - AlphaGo does not model ""opponent's chance of making a move"" separately from ""opponent's chance of gaining the highest score from this position if he/she makes that move"". There is just one probability covering both those meanings <sup>1</sup></p>

<p>As I understand it, AlphaGo rates the probabilities of <em>all possible moves</em> at each game board state that it considers (starting with the current board), and employs the most search effort for deeper searches on the highest rated ones. I don't know the ratios or how many nodes are visited in a typical search, but expect that a 1 in 10000 rating would not have been explored in much detail if at all. </p>

<p>It is not surprising to see the probability calculation in the system logs, as the logs likely contain the ratings for <em>all</em> legal next moves, as well as ratings for things that didn't actually happen in the game but AlphaGo considered in its deeper searches.</p>

<p>It is also not surprising that AlphaGo failed to rate the move correctly. The neural network is not expected to be a perfect oracle that rates all moves perfectly (if it was, then there would be no need to search). In fact, the opposite could be said to be the case - it is surprising (and of course an amazing feat of engineering) just how good the predictions are, good enough to beat a world-class champion. This is not the same as solving the game though. Go remains ""unsolved"", even if machines can beat humans, there is an unknown amount of additional room for better and better players - and in the immediate future that could be human or machine.</p>

<hr>

<ol>
<li>There are in fact two networks evaluating two different things - the ""policy network"" evaluates potential moves, and the output of that affects the Monte Carlo search. There is also a ""value network"" which assesses board states to score the end point of the search. It is the policy network that predicted the low probability of the move, which meant that the search had little or no chance of exploring game states past Lee Sedol's move (if it had, maybe the value network would of detected a poor end result from playing that through). In reinforcement learning, a <em>policy</em> is set of rules, based on known state, that decide between actions that an agent can take.</li>
</ol>
"
['nlp'],nlp - opinion mining vs sentiment analysis,"<p>I think the key is that most <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">Recurrent Neural Networks</a> problems are formulated in terms of either a regression (with low values indicating negative sentiment, and high values positive) or a binary classification (is this text positive?).</p>

<p>What you seem to be interested in is a much more nuanced definition of sentiment. This doesn't present any inherent problem, as the same algorithms might well work to predict more complex sentiments. The issue is simply labeled data. Because this kind of classification is difficult even for humans, it isn't easy to reliably gather data on, say, how stressed a writer is.</p>

<p>However if you're interested in assembling a dataset of that nature, you'd be able to apply the same methods (<a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">Recurrent Neural Networks</a> are a popular option) to do the classification. Many researchers in the field use <a href=""https://www.mturk.com/mturk/welcome"" rel=""nofollow"">Amazon Mechanical Turk</a> or something similar to gather labeled data at a reasonable cost.</p>
"
"['python', 'computer-vision', 'object-recognition']",Recognition human in images through HOG descriptor and SVM classifier performs poorly,"<p>You are using the training set that opencv is giving you which it doesn't correspond to the kind of images you are using. The data you are using comes from 'getDefaultPeopleDetector' and the kind of images that the default detector uses are pictures of many people, not a female model from a fashion ecommerce. </p>

<p>If you want to distinguish between models and garments you can try to train your own classifier with HOG or other features. </p>

<p>Another path you can take is to detect if there is a face or not. You could use haar cascades for that.</p>
"
"['r', 'clustering', 'similarity']",How to cluster Houses on the basis of similarity of features+location?,"<p>Check the ranges of your dimensions and consider scaling if you see a large difference.</p>

<p>I would interpret your described  behaviour due to much larger range if the index that the two other dimensions.
See also the <a href=""http://datascience.stackexchange.com/questions/6715/is-it-necessary-to-standardize-your-data-before-clustering/6722"">question</a>.</p>
"
"['machine-learning', 'pca', 'weighted-data']",How can give weight to feature before PCA,"<p>After standardizing your data you can multiply the features with weights to assign weights before the principal component analysis. Giving higher weights means the variance within the feature goes up, which makes it more important.</p>

<p>Standardizing (mean 0 and variance 1) is important for PCA because it is looking for a new orthogonal basis where the origin stays the same, so having your data centered around the origin is good. The first principal component is the direction with the most variance, by scaling a certain feature with a weight of more than 1 will increase the variance on this axis and thus give more weight to this axis, pulling the first principal component in it's direction.</p>
"
"['machine-learning', 'feature-selection', 'correlation', 'pca']",Feature Selection and PCA,"<p>PCA simply finds more compact ways of representing correlated data. PCA does not explicitly compact the data in order to better explain the target variable. In some cases, most of your inputs might be correlated with each other but have minimal relevance to your target variable. That's probably what is happening in your case.</p>

<p>Consider a toy example. Lets say I want to predict stock prices. Say I'm given four predictors: </p>

<ol>
<li>Year-over-year earnings growth (relevant)</li>
<li>Percent chance of rain (irrelevant)</li>
<li>Humidity (irrelevant)</li>
<li>Temperature (irrelevant)</li>
</ol>

<p>If I apply PCA to this data set, the first principle component would relate to weather since 75% of the predictors are weather related. Is this principle component relevant? It's not. </p>

<p>The two options you've highlighted boil down to using CFS or not using it. The option that uses CFS does better because it explicitly selects variables that have relevance to the target variable.</p>
"
"['terminology', 'data-wrangling', 'aggregation']",Analytics term for turning row values into column names and count its assigned values,"<p>Aggregation.
See  <a href=""https://en.wikipedia.org/wiki/Aggregate_function"" rel=""nofollow"">Aggregate function</a> </p>
"
"['feature-selection', 'dimensionality-reduction', 'pca']",feature redundancy,"<p>For the sake of training, features that are highly correlated offer little training ""value"" as the presence/state of one value can always (or almost always) be used to determine the presence/state of the other. If this is the case there's no reason to add both features as having both will have little impact on the predictions - if A ""on"" = B ""off"", and A ""off"" = B ""on"", then all states can be represented by just learning off either A or B. This is greatly simplified, but the same is true for other highly correlated values.</p>

<p>PCA can help reduce features, but in any case, if you've identified redundant or highly correlated features that will be of little use in training, it probably makes sense to eliminate them right away and then use PCA, or other feature importance metrics that can be generated by training off your full dataset, to further optimize your training feature set.</p>
"
"['r', 'dataset', 'data-cleaning', 'json']",How to read Several JSON files to a dataframe in R?,"<p>First, you can use the <code>full.names</code> parameter to <code>list.files()</code> to get 
the full path added to each file.</p>

<pre><code>temp &lt;- list.files(path, pattern=""*.json"", full.names=TRUE)
</code></pre>

<p>Next, there are issues with the data since they contain <code>NULL</code> values
which throws off a quick-and-dirty solution. So, we have to take each
list element and convert any <code>NULL</code> to <code>NA</code>.</p>

<p>Finally, we can use the handy <code>purrr::map_df()</code> to take the whole list
of lists and turn them into a <code>data.frame</code>:</p>

<pre><code>movies &lt;- purrr::map_df(temp, function(x) { 
  purrr::map(jsonlite::fromJSON(x), function(y) ifelse(is.null(y), NA, y)) 
})
</code></pre>
"
"['machine-learning', 'neuralnetwork', 'statistics', 'data']",IID violation in machine learning,"<p>Suppose you are investigating if heart rate can predict if a person smokes. You measure bpm for 30x1m consecutive times, and ask if the person smokes in order to build your training model data set.</p>

<p>What would contribute to a better predictor? 30 observations from a person who smokes, or 1 observation from 30 people who smoke? Given that one person's heart rate won't change much over 30m, it seems clear that you'd rather have 1 sample from 30 people than 30 samples from one person. 30 samples from one person are not worth as much as 1 sample from 30 people. </p>

<p>Because the samples from one person are not independent.</p>

<p>I think if you put non-independent samples into a neural net then it won't affect the predictive power too much as long as the non-independence is similar across all your training data. In one extreme, if all smokers and non-smokers have the same heart rate over the 30m period, then all you've done is repeated your input data precisely 30 times and nothing will be changed (except it will take 30x as long to run...).</p>

<p>However, if smokers' heart rates are constant, and non-smokers' vary, then you add 30 measurements of each smoker's heart rate to your model, and a bunch of random measurements correlating those rates with non-smoking. Your NN is very likely to predict anyone with one of those smokers' heart rates is a smoker. This is clearly wrong - those 30 measurements from each smoker are only worth one measurement, and putting them in a NN will train the network wrongly.</p>
"
"['python', 'regression', 'library', 'software-recommendation']",Multivariate linear regression in Python,"<p>You can still use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression"" rel=""nofollow"">sklearn.linear_model.LinearRegression</a>.  Simply make the output <code>y</code> a matrix with as many columns as you have dependent variables.  If you want something <a href=""http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions"" rel=""nofollow"">non-linear</a>, you can try different basis functions, use polynomial features, or use a different method for regression (like a NN).</p>
"
['scraping'],Connecting Authors with Published Papers,"<p>It's not an easy question to answer even for groups with a lot of leverage such as <a href=""https://www.researchgate.net/home"">Research Gate</a> (RG). RG has it's own (I assume proprietary) <code>author matching algorithm</code> <a href=""http://www.scilogs.com/in_scientio_veritas/researchgate-heal-thyself-please/"">which has caused problems in the past</a>.</p>

<p>They use the name of the author (in different combinations) to suggest <a href=""https://explore.researchgate.net/display/support/Authorship"">authorship to RG users</a> (so has you said, it does causes a lot of problems). Every once in a while users that are not the author accept the suggestions and, from the portal point of view, gain the equivalent reputation. It's a serious business that requires quite a bit of R&amp;D before making a decision.</p>

<p>That being said I can't answer with certainties only with reasonable possibilities. A few questions I would make (and hope to answer with a bit of data analysis):</p>

<ul>
<li><p>What is the probability that an author with a confirmed publication in an identified journal will publish in that journal again? - <strong>Journal Name</strong></p></li>
<li><p>What is the probability that an author that has partnered with other authors will repeat the same co-authoring combination? <strong>Co-author Names</strong></p></li>
<li><p>What is the expected time story for publishing for each author? Authors rarely publish articles 20 years apart. Typically they publish more and more, or less and less. <strong>Time frame</strong></p></li>
<li><p>How frequently do authors change the institution they belong to? <strong>Institution Name</strong></p></li>
<li><p>What are the preferred keywords for a given author? <strong>Key Names</strong></p></li>
<li><p>What are the preferred citations made by a given author? <strong>Bibliography</strong></p></li>
</ul>

<p>All of the questions above require quite a bit of <a href=""https://en.wikipedia.org/wiki/Text_mining"">Text Mining</a> and <a href=""https://en.wikipedia.org/wiki/Category:String_matching_algorithms"">String Matching</a>, as well as solid dataset to start your mining.</p>

<p>Some publishers have their own <a href=""http://api.elsevier.com/documentation/SCIDIRSearchAPI.wadl"">API</a> although I can't say much about permissiveness (never tried it myself). <a href=""https://www.researchgate.net/post/Is_there_any_Researchgate_API"">RG has been promising one for years</a> but, as far as I know, it still does not <a href=""https://github.com/researchgate"">exist</a>.</p>

<p>An unlikely thing I remember now is the inspiring story of <a href=""https://en.wikipedia.org/wiki/Aaron_Swartz"">Aaron Swartz</a>. This activist, along with other persons, successfully managed to create large open archives for books and articles. Should that information still exist it might be worth your time to take a look <a href=""https://openlibrary.org/"">there</a>.</p>

<p>Also if you have a list of the journals you are considering (you've mentioned only ""doctors"" which is a bit vague) you can try and see with the publisher if they have any way of accessing their database.</p>
"
"['data', 'tools']",Tool to Generate 2D Data via Mouse Clicking,"<p>I recently discovered this site: <a href=""http://www.librec.net/datagen.html"" rel=""nofollow"">http://www.librec.net/datagen.html</a></p>
"
['classification'],Classification when one class is other,"<p>As a first step you should try <a href=""https://en.wikipedia.org/wiki/Logistic_regression"" rel=""nofollow"">logistic regression</a> on your tf-idf vectors.  It is simple to implement and would provide a good baseline for comparison.  You can find an implementation in whatever language you're using.  You could also try some kind of (<a href=""https://www.cs.princeton.edu/~blei/papers/BleiMcAuliffe2007.pdf"" rel=""nofollow"">perhaps supervised</a>) <a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow"">topic modeling</a> to create a better feature space, but that would be more involved.</p>
"
"['machine-learning', 'algorithms', 'genetic']",Solving a system of equations with sparse data,"<p>If I understand you correctly, this is the case of <strong>multiple linear regression with sparse data</strong> (<em>sparse regression</em>). Assuming that, I hope you will find the following <strong>resources</strong> useful.</p>

<p>1) NCSU <strong>lecture slides on sparse regression</strong> with overview of algorithms, notes, formulas, graphics and references to literature: <a href=""http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect23sparse.pdf"">http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect23sparse.pdf</a></p>

<p>2) <code>R</code> ecosystem offers many <strong>packages</strong>, useful for sparse regression analysis, including:</p>

<ul>
<li><strong>Matrix</strong> (<a href=""http://cran.r-project.org/web/packages/Matrix"">http://cran.r-project.org/web/packages/Matrix</a>)</li>
<li><strong>SparseM</strong> (<a href=""http://cran.r-project.org/web/packages/SparseM"">http://cran.r-project.org/web/packages/SparseM</a>)</li>
<li><strong>MatrixModels</strong> (<a href=""http://cran.r-project.org/web/packages/MatrixModels"">http://cran.r-project.org/web/packages/MatrixModels</a>)</li>
<li><strong>glmnet</strong> (<a href=""http://cran.r-project.org/web/packages/glmnet"">http://cran.r-project.org/web/packages/glmnet</a>)</li>
<li><strong>flare</strong> (<a href=""http://cran.r-project.org/web/packages/flare"">http://cran.r-project.org/web/packages/flare</a>)</li>
</ul>

<p>3) A blog post with an <strong>example of sparse regression solution</strong>, based on <code>SparseM</code>: <a href=""http://aleph-nought.blogspot.com/2012/03/multiple-linear-regression-with-sparse.html"">http://aleph-nought.blogspot.com/2012/03/multiple-linear-regression-with-sparse.html</a></p>

<p>4) A blog post on using <strong>sparse matrices in R</strong>, which includes a <strong>primer</strong> on using <code>glmnet</code>: <a href=""http://www.johnmyleswhite.com/notebook/2011/10/31/using-sparse-matrices-in-r"">http://www.johnmyleswhite.com/notebook/2011/10/31/using-sparse-matrices-in-r</a></p>

<p>5) <strong>More examples and some discussion</strong> on the topic can be found on <strong>StackOverflow</strong>: <a href=""http://stackoverflow.com/questions/3169371/large-scale-regression-in-r-with-a-sparse-feature-matrix"">http://stackoverflow.com/questions/3169371/large-scale-regression-in-r-with-a-sparse-feature-matrix</a></p>

<p><strong>UPDATE</strong> (based on your comment):</p>

<p>If you're trying to solve an LP problem with constraints, you may find this <strong>theoretical paper</strong> useful: <a href=""http://web.stanford.edu/group/SOL/papers/gmsw84.pdf"">http://web.stanford.edu/group/SOL/papers/gmsw84.pdf</a>.</p>

<p>Also, check R package <strong>limSolve</strong>: <a href=""http://cran.r-project.org/web/packages/limSolve"">http://cran.r-project.org/web/packages/limSolve</a>. And, in general, check packages in <em>CRAN Task View</em> <strong>""Optimization and Mathematical Programming""</strong>: <a href=""http://cran.r-project.org/web/views/Optimization.html"">http://cran.r-project.org/web/views/Optimization.html</a>.</p>

<p>Finally, check the book <strong>""Using R for Numerical Analysis in Science and Engineering""</strong> (by Victor A. Bloomfield). It has a section on solving systems of equations, represented by <strong>sparse matrices</strong> (section 5.7, pages 99-104), which includes examples, based on some of the above-mentioned packages: <a href=""http://books.google.com/books?id=9ph_AwAAQBAJ&amp;pg=PA99&amp;lpg=PA99&amp;dq=r+limsolve+sparse+matrix&amp;source=bl&amp;ots=PHDE8nXljQ&amp;sig=sPi4n5Wk0M02ywkubq7R7KD_b04&amp;hl=en&amp;sa=X&amp;ei=FZjiU-ioIcjmsATGkYDAAg&amp;ved=0CDUQ6AEwAw#v=onepage&amp;q=r%20limsolve%20sparse%20matrix&amp;f=false"">http://books.google.com/books?id=9ph_AwAAQBAJ&amp;pg=PA99&amp;lpg=PA99&amp;dq=r+limsolve+sparse+matrix&amp;source=bl&amp;ots=PHDE8nXljQ&amp;sig=sPi4n5Wk0M02ywkubq7R7KD_b04&amp;hl=en&amp;sa=X&amp;ei=FZjiU-ioIcjmsATGkYDAAg&amp;ved=0CDUQ6AEwAw#v=onepage&amp;q=r%20limsolve%20sparse%20matrix&amp;f=false</a>. </p>
"
"['hadoop', 'distributed', 'knowledge-base']",What are the use cases for Apache Spark vs Hadoop,"<p>Hadoop means HDFS, YARN, MapReduce, and a lot of other things. Do you mean Spark vs <em>MapReduce</em>? Because Spark runs on/with Hadoop, which is rather the point.</p>

<p>The primary reason to use Spark is for speed, and this comes from the fact that its execution can keep data in memory between stages rather than always persist back to HDFS after a Map or Reduce. This advantage is very pronounced for iterative computations, which have tens of stages each of which is touching the same data. This is where things might be ""100x"" faster. For simple, one-pass ETL-like jobs for which MapReduce was designed, it's not in general faster.</p>

<p>Another reason to use Spark is its nicer high-level language compared to MapReduce. It provides a functional programming-like view that mimics Scala, which is far nicer than writing MapReduce code. (Although you have to either use Scala, or adopt the slightly-less-developed Java or Python APIs for Spark). <a href=""http://crunch.apache.org"">Crunch</a> and <a href=""http://cascading.org"">Cascading</a> already provide a similar abstraction on top of MapReduce, but this is still an area where Spark is nice.</p>

<p>Finally Spark has as-yet-young but promising subprojects for ML, graph analysis, and streaming, which expose a similar, coherent API. With MapReduce, you would have to turn to several different other projects for this (Mahout, Giraph, Storm). It's nice to have it in one package, albeit not yet 'baked'.</p>

<p>Why would you not use Spark? <a href=""https://www.quora.com/Apache-Spark/Assuming-you-have-a-system-with-both-Hadoop-and-Spark-installed-say-under-Yarn-is-there-any-reason-to-use-Hadoop-map-reduce-instead-of-the-equivalent-Spark-commands"">paraphrasing</a> myself:</p>

<ul>
<li>Spark is primarily Scala, with ported Java APIs; MapReduce might be friendlier and more native for Java-based developers</li>
<li>There is more MapReduce expertise out there now than Spark</li>
<li>For the data-parallel, one-pass, ETL-like jobs MapReduce was designed for, MapReduce is lighter-weight compared to the Spark equivalent</li>
<li>Spark is fairly mature, and so is YARN now, but Spark-on-YARN is still pretty new. The two may not be optimally integrated yet. For example until recently I don't think Spark could ask YARN for allocations based on number of cores? That is: MapReduce might be easier to understand, manage and tune</li>
</ul>
"
"['python', 'visualization']",Any usable libs to build and visualise SOM in python?,"<p>You could have a try on this package, 
<a href=""http://www.pymvpa.org/examples/som.html"" rel=""nofollow"">http://www.pymvpa.org/examples/som.html</a>. 
There is a working example on this page. </p>

<p>If what you are interested is the Manifold learning, 
you could also apply many package from sklearn, <a href=""http://scikit-learn.org/stable/modules/manifold.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/manifold.html</a>.</p>
"
"['bigdata', 'random-forest']",Random Forests with Big Data - number of trees v. number of observations,"<p>I would recommend using a combination of both options #1 and #2.</p>

<p>You could first try tuning your hyper-parameters to find out till what extent could you reduce the number of trees to a point where the random forest model's prediction starts deteriorating on the test set.</p>

<p>This is because changing the value of <code>mtry</code>, the randomly selected number of features for a new tree, is the only meaningful hyper-parameter that should impact accuracy of the model. Since averaging converges as the no. of trees increases, the no. of trees could be reduced to a point where its performance is not impacted as much. Hence, you need to iterate and choose a a limit beyond which very small number of trees may not produce a strong enough ensemble. A random forest needs works best by using more base learners for reducing the variance by averaging each individual tree's output.</p>

<p>It is not clear from your case whether you're using the Random Forest for a a classification or a regression problem. In case this is a classification problem, and if your data-set is imbalanced in terms of ratio of positive vs. negative classes; then you could reduce the size of the training set by under-sampling the majority class to bring it nearer to a 1:1 ratio. Since you have a large number of records, such class based sampling could improve accuracy as well as reduce data size for training.</p>

<p>Additionally, if you've got a fine tuned Random Forest with good performance, then you could also evaluate dropping features that are least important as determined by the algorithm on OOB samples. This would reduce the time taken to train the model.</p>
"
"['nosql', 'tools', 'processing', 'hadoop']",What is the difference between Hadoop and noSQL,"<p><strong>Hadoop is not a database</strong>, hadoop is an entire ecosystem.</p>

<p><img src=""http://i.stack.imgur.com/oOYp7.png"" alt=""the hadoop ecosystem""></p>

<p>Most people will refer to <a href=""http://de.wikipedia.org/wiki/MapReduce"" rel=""nofollow"">mapreduce</a> jobs while talking about hadoop. A mapreduce job splits big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.</p>

<hr>

<p>Let's assume you load into hadoop a set of <code>&lt;String, Integer&gt;</code> with the population of some neighborhoods within a city and you want to get the average population over the whole neighborhoods of each city(figure 1).</p>

<p><em>figure 1</em></p>

<pre><code>    [new york, 40394]
    [new york, 134]
    [la, 44]
    [la, 647]
    ...
</code></pre>

<p>Now hadoop will first map each value by using the keys (figure 2)</p>

<p><em>figure 2</em></p>

<pre><code>[new york, [40394,134]]
[la, [44,647]]
...
</code></pre>

<p>After the mapping it will reduce the values of each key to a new value (in this example the average over the value set of each key)(figure 3)</p>

<p><em>figure 3</em></p>

<pre><code>[new york, [20264]]
[la, [346]]
...
</code></pre>

<p>now hadoop would be done with everything. You can now load the result into the HDFS (hadoop distributed file system) or into any DBMS or file.</p>

<p>Thats just one <strong>very basic</strong> and <strong>simple</strong> example of what hadoop can do. You can run much more complicated tasks in hadoop.</p>

<p>As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS.</p>
"
"['data-mining', 'clustering']",Clustering customer data stored in ElasticSearch,"<p>One algorithm that can be used for this is the <a href=""http://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow"">k-means clustering algorithm</a>.</p>

<p>Basically:</p>

<ol>
<li>Randomly choose k datapoints from your set, m_1, ..., m_k.</li>
<li><p>""Until convergence"":</p>

<ol>
<li>Assign your data points to k clusters, where cluster i is the set of points for which m_i is the closest of your current means</li>
<li>Replace each m_i by the mean of all points assigned to cluster i.</li>
</ol></li>
</ol>

<p>It is good practice to repeat this algorithm several times, then choose the outcome that minimizes distances between the points of each cluster i and the center m_i.</p>

<p>Of course, you have to know k to start here; you can use cross-validation to choose this parameter, though.</p>
"
['r'],Unexpected Error Message in RStudio; while using 'twitterR' Package,"<p>It's because of the whitespace in the parameter for <code>geocode</code>.</p>

<p>It should be:</p>

<pre><code>my_tweets &lt;- searchTwitter('@jabhij', geocode=""20.593684,78.96288,2000km"")
</code></pre>
"
"['machine-learning', 'classification', 'algorithms', 'graphs']",Why are HMMs called linear-chain?,"<p>It is so called because it classifies the linear sequence, and not because the structure of the graph.</p>
"
"['python', 'pandas']",How to implement accurate counts or sums when you have different numbers of days of the week?,"<p>I would add a column that is a 3 if it's a weekday and a 4 if it's not using an apply, something like this:</p>

<pre><code>df['divide_by'] = df.apply(lambda x: 3 if x['Day_int']&lt;5 else 4, axis=1)
</code></pre>

<p>Assuming days are Monday 0 to Sunday 6. Then you can add the column as follows:</p>

<pre><code>df['adj_total'] = df.apply(lambda x: x['Total']/x['divide_by'], axis=1)
</code></pre>

<p>Then you can remove the divide_by column and you have the result</p>
"
"['machine-learning', 'neuralnetwork', 'bayesian-networks', 'probability']",What is difference between Bayesian Network and Belief Network?,"<p>Both are literally the same. A Belief network is the one, where we establish a <strong>belief</strong> that certain event A will occur, given B. The network assumes the structure of a directed graph. The term <strong>Bayesian</strong> was coined after the name of Thomas Bayes.</p>
"
"['machine-learning', 'python', 'deep-learning', 'scikit', 'keras']",Error in model.fit() method in Keras,"<p>In your <code>base_model</code> function, the <code>input_dim</code> parameter of the first <code>Dense</code> layer should be equal to the number of features and not to the number of samples, i.e. you should have <code>input_dim=X_train.shape[1]</code> instead of <code>input_dim=len(X_train)</code> (which is equal to <code>X_train.shape[0]</code>).</p>
"
"['machine-learning', 'classification', 'algorithms']",Classifying Java exceptions,"<p>First of all, some basics of classification (and in general any supervised ML tasks), just to make sure we have same set of concepts in mind. </p>

<p>Any supervised ML algorithm consists of at least 2 components: </p>

<ol>
<li>Dataset to train and test on.</li>
<li>Algorithm(s) to handle these data.</li>
</ol>

<p>Training dataset consists of a set of pairs <code>(x, y)</code>, where <code>x</code> is a <strong>vector of features</strong> and <code>y</code> is <strong>predicted variable</strong>. Predicted variable is just what you want to know, i.e. in your case it is exception type. Features are more tricky. You cannot just throw raw text into an algorithm, you need to extract meaningful parts of it and organize them as feature vectors first. You've already mentioned a couple of useful features - exception class name (e.g. <code>com.acme.PrintException</code>) and contained words (""Timeout""). All you need is to translate your row exceptions (and human-categorized exception types) into suitable dataset, e.g.: </p>

<pre><code>ex_class                  contains_timeout  ...   | ex_type
-----------------------------------------------------------
[com.acme.PrintException, 1                , ...] | Availability
[java.lang.Exception    , 0                , ...] | Network
 ...
</code></pre>

<p>This representation is already much better for ML algorithms. But which one to take? </p>

<p>Taking into account nature of the task and your current approach natural choice is to use <strong>decision trees</strong>. This class of algorithms will compute optimal decision criteria for all your exception types and print out resulting tree. This is especially useful, because you will have possibility to manually inspect how decision is made and see how much it corresponds to your manually-crafted rules. </p>

<p>There's, however, possibility that some exceptions with exactly the same features will belong to different exception types. In this case probabilistic approach may work well. Despite its name, <strong>Naive Bayes</strong> classifier works pretty well in most cases. There's one issue with NB and our dataset representation, though: dataset contains <em>categorical</em> variables, and Naive Bayes can work with <em>numerical</em> attributes only*. Standard way to overcome this problem is to use <a href=""http://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29"">dummy variables</a>. In short, dummy variables are binary variables that simply indicate whether specific category presents or not. For example, single variable <code>ex_class</code> with values <code>{com.acme.PrintException, java.lang.Exception, ...}</code>, etc. may be split into several variables <code>ex_class_printexception</code>, <code>ex_class_exception</code>, etc. with values <code>{0, 1}</code>:</p>

<pre><code>ex_class_printexception  ex_class_exception  contains_timeout | ex_type
-----------------------------------------------------------------------
[1,                    , 0                 , 1              ] | Availability
[0,                    , 1                 , 0              ] | Network
</code></pre>

<p>One last algorithm to try is <strong>Support Vector Machines (SVM)</strong>. It neither provides helpful visualisation, nor is probabilistic, but often gives superior results. </p>

<hr>

<p><em>*</em> - in fact, neither Bayes theorem, nor Naive Bayes itself state anything about variable type, but most software packages that come to mind rely on numerical features. </p>
"
"['bigdata', 'dataset', 'data-wrangling']",Tools to perform SQL analytics on 350TB of csv data,"<p>Talking about SQL world: from your requirements point of view parallel column based DB Vertica seems to fit well. It is not free of charge though. 
Even more powerful solution if you have good budget is Teradata. </p>

<p>You'd need to have at least 70 spindles to store your data on HDDs (5TB) on site +/- compression and redundancy. May be add more RAM or SDD if you can cache. That could fit in 3-10 boxes, depending on the processing power you require.</p>

<p>The rest is a software question on a trade-off between data management, feature set, optimization, your own time spent and the price.
Teradata and Vertica will surely reduce your time spent writing queries and optimizing them, managing data. In case you'd need joins that advantage will be much bigger yet.</p>
"
"['python', 'hadoop', 'map-reduce', 'java', 'distributed']",Do something with the output of reducers,"<p>I can safely conclude you cannot.</p>
"
['machine-learning'],Which machine learning approach/algorithm do I choose for path validation?,"<p>This is a good question but it's rather complicated. I can suggest two approaches:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Graphical_model"" rel=""nofollow"">Graphical models</a>; specifically <a href=""https://en.wikipedia.org/wiki/Bayesian_network"" rel=""nofollow"">Bayesian networks</a> since your graph is directed.</li>
<li><a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network"" rel=""nofollow"">Recurrent neural networks</a>. Here's a talk on popular recent model: <a href=""http://research.microsoft.com/apps/video/?id=239083"" rel=""nofollow"">Sequence to Sequence Learning with Neural Networks</a>.</li>
</ul>
"
"['machine-learning', 'python', 'tools', 'xgboost']",How to place XGBoost in a full stack for ML?,"<p>Yes, it is a full-strength Machine Learning paradigm.</p>

<p>XGBoost is basically Extreme Gradient Boosting.</p>

<p>It only takes in numeric matrix data. So, you might want to convert your data such that it is compatible with XGBoost.</p>

<p>The <a href=""http://xgboost.readthedocs.org/en/latest/parameter.html"" rel=""nofollow"">wide range of parameters</a> of the xgboost paradigm is what makes it so diverse. Boosting can be done on trees and linear models, and then more parameters can be defined depending on the model you have selected.</p>

<p>So, yes it is a complete paradigm in itself. But, when you want more than the limitations of xgboost like linear and tree models, then you can use the <a href=""https://en.wikipedia.org/wiki/Ensemble_learning"" rel=""nofollow"">concept of ensembling</a>. </p>

<p>In the case of ensembles, the tools/libraries which can be used depends on the data scientist who is conducting the experiment. It can be Keras or Theano or TensorFlow, or anything which he/she is comfortable with. (opinion-based)</p>
"
"['nlp', 'text-mining']",What are the main types of NLP annotators?,"<p>Here are the basic Natural Language Processing capabilities (or annotators) that are usually necessary to extract language units from textual data for sake of search and other applications:</p>

<p><a href=""http://en.wikipedia.org/wiki/Sentence_boundary_disambiguation"" rel=""nofollow"">Sentence breaker</a> - to split text (usually, text paragraphs) to sentences. Even in English it can be hard for some cases like ""Mr. and Mrs. Brown stay in room no. 20.""</p>

<p><a href=""http://en.wikipedia.org/wiki/Tokenization"" rel=""nofollow"">Tokenizer</a> - to split text or sentences to words or word-level units, including punctuation. This task is not trivial for languages with no spaces and no stable understanding of word boundaries (e.g. Chinese, Japanese)</p>

<p><a href=""http://en.wikipedia.org/wiki/POS_tagger"" rel=""nofollow"">Part-of-speech Tagger</a> - to guess part of speech of each word in the context of sentence; usually each word is assigned a so-called POS-tag from a tagset developed in advance to serve your final task (for example, parsing).</p>

<p><a href=""http://en.wikipedia.org/wiki/Lemmatization"" rel=""nofollow"">Lemmatizer</a> - to convert a given word into its canonical form (<a href=""http://en.wikipedia.org/wiki/Lemma_(morphology)"" rel=""nofollow"">lemma</a>). Usually you need to know the word's POS-tag. For example, word ""heating"" as gerund must be converted to ""heat"", but as noun it must be left unchanged.</p>

<p><a href=""http://en.wikipedia.org/wiki/Parser"" rel=""nofollow"">Parser</a> - to perform syntactic analysis of the sentence and build a syntactic tree or graph. There're two main ways to represent syntactic structure of sentence: via <a href=""http://en.wikipedia.org/wiki/Dependency_grammar#Dependency_vs._constituency"" rel=""nofollow"">constituency or dependency</a>.</p>

<p><a href=""http://en.wikipedia.org/wiki/Automatic_summarization"" rel=""nofollow"">Summarizer</a> - to generate a short summary of the text by selecting a set of top informative sentences of the document, representing its main idea. However can be done in more intelligent manner than just selecting the sentences from existing ones.</p>

<p><a href=""http://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow"">Named Entity Recognition</a> - to extract so-called named entities from the text. Named entities are the chunks of words from text, which refer to an entity of certain type. The types may include: geographic locations (countries, cities, rivers, ...), person names, organization names etc. Before going into NER task you must understand what do you want to get and, possible, predefine a taxonomy of named entity types to resolve.</p>

<p><a href=""http://en.wikipedia.org/wiki/Coreference_resolution"" rel=""nofollow"">Coreference Resolution</a> - to group named entities (or, depending on your task, any other text units) into clusters corresponding to a single real object/meaning. For example, ""B. Gates"", ""William Gates"", ""Founder of Microsoft"" etc. in one text may mean the same person, referenced by using different expressions.</p>

<p>There're many other interesting NLP applications/annotators (see <a href=""http://en.wikipedia.org/wiki/Category:Tasks_of_natural_language_processing"" rel=""nofollow"">NLP tasks category</a>), sentiment analysis, machine translation etc.). There're many books on this, the classical book: ""Speech and Language Processing"" by Daniel Jurafsky and James H. Martin., but it can be too detailed for you.</p>
"
"['data-mining', 'r', 'algorithms']",How does SQL Server Analysis Services compare to R?,"<p>In my opinion, it seems that SSAS makes <strong>more sense</strong> for someone who:</p>

<ul>
<li>has significantly invested in Microsoft's <em>technology stack</em> and <em>platform</em>;</li>
<li>prefer point-and-click <em>interface</em> (GUI) to command line;</li>
<li>focus on <em>data warehousing</em> (OLAP cubes, etc.);</li>
<li>has limited needs in terms of <em>statistical methods and algorithms variety</em>;</li>
<li>has limited needs in <em>cross-language integration</em>;</li>
<li>doesn't care much about <em>openness</em>, <em>cross-platform integration</em> and <em>vendor lock-in</em>.</li>
</ul>

<p>You can find useful <a href=""http://blog.samibadawi.com/2010/04/r-rapidminer-statistica-ssas-or-weka.html"" rel=""nofollow"">this blog post</a> by <a href=""http://twitter.com/Sami_Badawi"" rel=""nofollow"">Sami Badawi</a>. However, note that the post is <em>not recent</em>, so some information might be outdated. Plus, the post contains an <em>initial</em> review, which might be not very accurate or comprehensive. If you're thinking about data science, while considering staying within Microsoft ecosystem, I suggest you to take a look at Microsoft's own machine learning platform Azure ML. <a href=""https://dbolab.com/microsoft-azure-ml-vs-ssas-mining-structures"" rel=""nofollow"">This blog post</a> presents a brief comparison of (early) Azure ML and SSAS.</p>
"
"['data-mining', 'python', 'social-network-analysis']",Which book is the best for introduction to analysis social network using python3,"<p>Russell's book is fine. You might also like <a href=""http://shop.oreilly.com/product/0636920020424.do"" rel=""nofollow"">Social Network Analysis for Startups</a>. All the examples are in python. You can do all your analysis in that using packages like <code>networkx</code>. NodeXL is for the Excel crowd. Definitely not the ideal tool for the job; I would shy away from it.</p>

<p>The obvious book for NodeXL is <a href=""http://www.elsevier.com/books/analyzing-social-media-networks-with-nodexl/hansen/978-0-12-382229-1"" rel=""nofollow"">Analyzing Social Media Networks with NodeXL</a>, which is written by the authors of NodeXL.</p>
"
"['clustering', 'k-means']",Clustering not producing even clusters,"<p>Cluster analysis is not <em>supposed</em> to produce paritions of equal size. It is meant to discover structure in the data.</p>

<p>If the majority of objects is highly similar, then this majority is supposed to be in the majority cluster.</p>

<p>Consider all your data is identical. Any clustering algorithm producing more than one cluster has failed, in my opinion...</p>

<p>So you may be using the wrong class of algorithms for your problem.</p>
"
"['data-mining', 'time-series', 'anomaly-detection']",What are some good sources to learn fraud/anomaly detection in normal/time-series data?,"<p>Usually this is done as outliers analysis (fraud is an outlier vs normal usage). For this aspect, you can find more info in the <a href=""http://www.sciencedirect.com/science/book/9780123814791"" rel=""nofollow"">data mining: concepts and techniques</a> book, even if general purpose book.</p>

<p>I am convinced that learning this kind of basis is needed to understanding the domain specific methods.</p>
"
"['classification', 'python', 'neuralnetwork', 'image-classification']",Theano/Lasagne/Nolearn Neural Network Image Input,"<p>I also asked it in lasagne-users forum and Oliver Duerr helped me a lot with code sample:
<a href=""https://groups.google.com/forum/#!topic/lasagne-users/8ZA7hr2wKfM"" rel=""nofollow"">https://groups.google.com/forum/#!topic/lasagne-users/8ZA7hr2wKfM</a></p>
"
['hadoop'],Yarn timeline recovery not enabled error upgrading via ambari,"<p>Run the following from the ambari server's shell:</p>

<p><code>/var/lib/ambari-server/resources/scripts/configs.sh set mace gamma yarn-site ""yarn.timeline-service.recovery.enabled"" ""true""</code></p>

<p>Where mace is the ambari server's host name and gamma is the cluster name</p>
"
"['machine-learning', 'neuralnetwork', 'algorithms', 'tools', 'matlab']",Query related to Matlab Neural Network toolbox,"<p>I don't see it explicitly discussed in the documentation, but it appears that the numbers on the right side of the screen are the stopping criteria.  That run appears to have stopped because the error on the validation set increased six times (explained <a href=""http://www.mathworks.com/help/nnet/ref/plotperform.html"" rel=""nofollow"">here</a>).  I'm guessing it would also stop after 1000 epochs, when 0 MSE was achieved, the gradient was below 1e-6, or after 4 resets.</p>

<p>Thus the number to the left of the bar should be the MSE on the epoch with the best validation error*.  I'm guessing the value of the blue bar is the lowest that MSE got during the iterations that were actually increasing the validation error.</p>

<p>From the docs:
""In the default setup, the training stops after six consecutive increases in validation error, and the best performance is taken from the epoch with the lowest validation error.""</p>
"
['education'],How deep should ones linear algebra knowledge be before starting data science?,"<p>I think it truly depends on what you decide to specialize in. Data science is a very broad field, and you can actually work with data without knowing what eigenvalues and eigenvectors are. However, if you want to acquire a intermediate/advanced understanding of statistics or machine learning, you need at least an intermediate/advanced knowledge of linear algebra.</p>

<p>I suggest to take an introductory class on linear algebra on MOOC - just to have a more precise idea of what linear algebra is - and then study some other topics that you are interested in. Linear algebra is a useful tool, but it can be very boring, especially if you are an ""applied"" kind of guy. Moreover, I think that some concepts like eigenvalues or eigenvectors are easier to understand when seen in an applied context, e.g. principal component analysis.</p>
"
"['machine-learning', 'deep-learning', 'keras']",Similarity between bordermode and zero padding in Keras,"<p>I think I found the answer to this here <a href=""https://github.com/fchollet/keras/issues/1984"" rel=""nofollow"">https://github.com/fchollet/keras/issues/1984</a>. If the stride is 1, border_mode = 'same' does the job of padding to ensure that the output feature maps are the same size as the input. </p>
"
"['categorical-data', 'numerical']",How can I dynamically distinguish between categorical data and numerical data?,"<p>I'm not aware of a foolproof way to do this. Here's one idea off the top of my head:</p>

<ol>
<li>Treat values as categorical by default.</li>
<li>Check for various attributes of the data that would imply it is actually continuous. Weight these attributes based on how likely they are to correlate with continuous data. Here are some possible examples:

<ul>
<li>Values are integers: +.7</li>
<li>Values are floats: +.8</li>
<li>Values are normally distributed: +.3</li>
<li>Values contain a relatively small number of unique values: +.3</li>
<li>Values aren't all the same number of characters: +.1</li>
<li>Values don't contain leading zeros: +.1</li>
</ul></li>
<li>Treat any columns that sum to greater than 1 as being numerical. Adjust the factors and weights based on testing against different data sets to suit your needs. You could even build and train a separate machine learning algorithm just to do this.</li>
</ol>
"
"['clustering', 'k-means', 'unsupervised-learning']",What to do with stale centroids in K-means,"<p><em>k-means</em> finds only a local optima. Thus a wrong number of cluster or simply some random state of equilibrium in the attracting forces could lead to empty clusters. Technically k-means does not provide a procedure for that, but you can enrich the algorithm with no problem. </p>

<p>There are two approaches which I found that are useful:</p>

<ul>
<li>remove the stale cluster, choose a random instance from your data set and create a new cluster with centroid equal with the chosen random point</li>
<li>remove the stale cluster, choose the farthest distant point from any other centroids, create a new cluster with centroid in that point</li>
</ul>

<p>Both procedures can lead to indefinite running time, but if the number of this kind of adjustments is finite (and usually it is) that it will converge with no problem. To guard yourself from infinite running time you can set an upper bound for the number of <em>adjustments</em>. </p>

<p>The procedure itself is not practical if you have a huge data set a a large number of clusters. The running time can became prohibitive.</p>

<p>Another procedure to decrease the chances for that to happen is to use a better initialization procedure, like <em>k-means++</em>. In fact the second suggestion is an idea from <em>k-means++</em>. There are no guarantees, however.</p>

<p>Finally a note regarding implementation. If you can't change to code of the algorithm to make those improvements on the fly, your only option which comes to my mind is to start a new clustering procedure where you initialize the centroid positions for non-stale clusters, and follow procedures for stale clusters.</p>
"
"['machine-learning', 'r', 'time-series']",Time series data: How I measure influence of new product sales on existing product sales (statistically)?,"<p>You could build an ARIMAX model. This would permit to include autoregressive (AR) terms as well as well as the sales in product 5 as an Exogenous Input (X). This would give you a potential model where the sales for a product $i$ at time $t$ is given by $s^i_t$ then,</p>

<p>$s^1_t=\alpha_1 s^1_{t-1} + \alpha_2 s^1_{t-2} + \ldots + \beta_0 s^5_t + \beta_1 s^5_{t-1} + \ldots $</p>

<p>Note that you may need to make the series stationary first, but see more on that below. You could estimate this model with the <a href=""https://cran.r-project.org/web/packages/seasonal/"" rel=""nofollow"">seasonal</a> R package that relies on the <a href=""https://www.census.gov/srd/www/x13as/"" rel=""nofollow"">X-13ARIMA-SEATS software developed by the US Census Bureau</a>.</p>

<p>I would recommend to ensure that your time series are all stationary, see for example <a href=""http://stats.stackexchange.com/questions/27332/how-to-know-if-a-time-series-is-stationary-or-non-stationary"">this post</a> before you use X13. I would also run cointegration tests. See for more explanation this <a href=""http://www.econ.uiuc.edu/~econ508/R/e-ta8_R.html"" rel=""nofollow"">excellent post</a>.</p>

<p>Since you only have data on week 5 I would start modeling in week 5 but you could include autoregressive (AR) terms related to the sales of product 1 prior to week 5.</p>
"
"['bigdata', 'hadoop']",Can we use HDFS and big data Analytics for processing huge log files being processed through some application on some central server?,"<p>If the size of your logs are still growing then a distributed data system is definitely the right way to go. I have been using <a href=""http://mesos.apache.org/"" rel=""nofollow"">Mesos</a> in production for almost a year now and it solved the problem of <code>if this processing happens on some server it under or over utilizes resources</code>. </p>

<p>I probably would look into some stacks like this:</p>

<ul>
<li>Mesos as your fault-tolerant and elastic distributed systems </li>
<li><a href=""https://spark.apache.org/"" rel=""nofollow"">Spark</a> or some Hadoop-based solutions for log processing and store the output in a DFS like HDFS</li>
<li>Have your applications consume data stored in HDFS as final steps</li>
</ul>
"
"['scikit', 'xgboost', 'gbm']",Why is xgboost so much faster than sklearn GradientBoostingClassifier?,"<p>Since you mention ""numeric"" features, I guess your features are not categorical and have a high arity (they can take a lot of different values, and thus there are a lot of possible split points). In such a case, growing trees is difficult since there are [a lot of features $\times$ a lot of split points] to evaluate.</p>

<p>My guess is that the biggest effect comes from the fact that XGBoost uses an approximation on the split points. If you have a continuous feature with 10000 possible splits, XGBoost consider only ""the best"" 300 splits by default (this is a simplification). This behavior is controlled by the <code>sketch_eps</code> parameter, and you can read more about it <a href=""https://xgboost.readthedocs.org/en/latest/parameter.html#parameters-for-tree-booster"" rel=""nofollow"">in the doc</a>. You can try lowering it and check the difference it makes. Since there is no mention of it in the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"" rel=""nofollow"">scikit-learn documentation</a>, I guess it is not available. You can learn what XGBoost method is in the <a href=""http://arxiv.org/abs/1603.02754"" rel=""nofollow"">their paper (arxiv)</a>.</p>

<p>XGBoost also uses an approximation on the evaluation of such split points. I do not know by which criterion scikit learn is evaluating the splits, but it could explain the rest of the time difference.</p>

<hr>

<p><strong>Adressing Comments</strong></p>

<p>Regarding the evaluation of split points</p>

<blockquote>
  <p>However, what did you mean by ""XGBoost also uses an approximation on the evaluation of such split points""? as far as I understand, for the evaluation they are using the exact reduction in the optimal objective function, as it appears in eq (7) in the paper.</p>
</blockquote>

<p>In order to evaluate the split point, you would have to compute $L(y,H_{i-1}+h_i)$ where $L$ is the cost function, $y$ the target, $H_{i-1}$ the model built until now, and $h_i$ the current addition. Notice that this is not what XGBoost is doing; they are simplifying the cost function $L$ by a Taylor Expansion, which leads to a very simple function to compute. They have to compute the Gradient and the Hessian of $L$ with respect to $H_{i-1}$, and they can reuse those number for all potential splits at stage $i$, making the overral computation fast. You can check <a href=""http://stats.stackexchange.com/questions/202858/loss-function-approximation-with-taylor-expansion/204377"">Loss function Approximation With Taylor Expansion (CrossValidated Q/A)</a> for more details, or the derivation in their paper.</p>

<p>The point is that they have found a way to approximate $L(y,H_{i-1} + h_i)$ efficiently. If you were to evaluate $L$ fully, without insider knowledge allowing optimisation or avoidance or redundant computation, it would take more time per split. It this regard, it is an approximation. However, other gradient boosting implementations also use a proxy cost functions to evaluate the splits, and I do not know whether XGBoost approximation is quicker in this regards than the others.</p>
"
['nlp'],New to ML and NLP. Is topic/subject extraction a good place to start?,"<p>You could learn about <a href=""https://en.wikipedia.org/wiki/Word_embedding"" rel=""nofollow"">word embeddings</a>. These will provide you a natural path to topic models, and many other NLP tasks. Look up <a href=""https://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow"">word2vec</a>.</p>

<p>I think it is better not to learn two new subjects (NLP and ML) at once. I would start with ML on its own; take <a href=""https://www.coursera.org/learn/machine-learning"" rel=""nofollow"">this class</a> perhaps. A good place to start is <a href=""https://en.wikipedia.org/wiki/Linear_regression"" rel=""nofollow"">linear regression</a>, or <a href=""https://en.wikipedia.org/wiki/Binary_classification"" rel=""nofollow"">binary classification</a>.</p>
"
"['machine-learning', 'regression', 'categorical-data', 'linear-regression']",Categorical and ordinal feature data representation in regression analysis?,"<p>The distinction between ordinal and categorical does matter. If in truth the difference between white and red was drastically different from red and black, your (10,20,30) ordinal model would not have performed well.</p>

<p>One hot encoding can learn the relationship between the ordinal values more finely, but throws out the information that the variables are related. Similarly, with insufficient data it is more likely to overfit.</p>

<p>Ordinal variables lessen those  problems but at the cost of forcing you to define the interval. There are a number of methods for defining the values of your ordinal variables, like rologit.</p>
"
"['r', 'dataset', 'data-formats']",Appropriate way to store data in R,"<p>Without more information all I can say is that:</p>

<ol>
<li>the say you're storing it is fine in general </li>
<li>you can further transform/store your data depending on your use case</li>
</ol>

<p>To expand on #2, if I want to study Distance vs Energy across all subjects, then I would format my data like this:</p>

<pre><code>&gt; library(reshape2)
&gt; dt2 &lt;- dt[dt$measurement %in% c('Distance','Energy'),]
&gt; dt_cast &lt;- dcast(dt2, subject+Try~measurement+speed+condition, value.var='result')
</code></pre>

<p>The transformed data (dt_cast) would then look like:</p>

<pre><code>  subject  Try Distance_speed1_Control Distance_speed1_Experm Distance_speed2_Control
1       1 Try1                       1                      3                       5
2       1 Try2                       2                      4                       6
  Distance_speed2_Experm Energy_speed1_Control Energy_speed1_Experm Energy_speed2_Control
1                      7                    21                   23                    25
2                      8                    22                   24                    26
  Energy_speed2_Experm
1                   27
2                   28
</code></pre>

<p>Allowing me to, for example, look at the relationship between the Distance_speed1_Control vs Energy_speed1_Control columns.</p>

<p>Basically subset/aggregate your data and then use the dcast to get the rows and columns the computer needs.</p>
"
['python'],Querying DBpedia from Python,"<p>You do not need a wrapper for DBPedia, you need a library that can issue a SPARQL query to its SPARQL endpoint.  Here is <a href=""https://pypi.python.org/pypi/SPARQLWrapper/1.6.4"" rel=""nofollow"">an option for the library</a> and here is the URL to point it to: <a href=""http://dbpedia.org/sparql"" rel=""nofollow"">http://dbpedia.org/sparql</a></p>

<p>You need to issue a DESCRIBE query on the United_States resource page:</p>

<pre><code>PREFIX dbres: &lt;http://dbpedia.org/resource/&gt;

DESCRIBE dbres:United_States
</code></pre>

<p>Please note this is a <strong>huge</strong> download of resulting triplets.</p>

<p>Here is how you would issue the query:</p>



<pre><code>from SPARQLWrapper import SPARQLWrapper, JSON

def get_country_description():
    sparql = SPARQLWrapper(""http://dbpedia.org/sparql"")
    sparql.setReturnFormat(JSON)

    sparql.setQuery(query)  # the previous query as a literal string

    return sparql.query().convert()
</code></pre>
"
['algorithms'],Training a function that maps n-dim to n-dim,"<p>I would call a mapping between N dimensional input and N dimensional output a regression problem. </p>

<p>If you add more constraints about the relation between the input and output it might be called different names: linear filtering, nonlinear filtering, etc...</p>

<p>some examples on common techniques for that would be: neural networks, regression trees, regularised regressions...</p>
"
"['data-mining', 'bigdata', 'predictive-modeling']",Using a model for a different dataset,"<p>I think you answered yourself by ""values of B are not comparable"". Learning for predictions is based on a fundamental assumption which is the data for prediction has the same joint distribution as the data for learning. This is the link between those processes.</p>

<p>Now, if you want to handle that in a meaningful way you have to know somehow the source type. In your example the device type. One way would be to introduce the device type as a different column in your data set, so that the model can have the chance to differentiate between source type. Obviously you have to have training data for all the device types. Supposing you have 2 device types, A and B. Your training data should have some columns for the signal and also a factor column with type A and B. Also you have to have enough instances of data with type A and type B. </p>
"
"['classification', 'keras', 'weighted-data']",How to set class weights for imbalanced classes in Keras?,"<p>If you are talking about the regular case, where your network produces only one output your assumption is correct. In order to force your algorithm to treat every instance of <strong>class 1</strong> as 50 instances of <strong>class 0</strong> you have to:</p>

<ol>
<li><p>Define a dictionary with your labels and their associated weights</p>

<pre><code>class_weight = {0 : 1.,
    1: 50.,
    2: 2.}
</code></pre></li>
<li><p>Feed the dictionary as a parameter:</p>

<p>model.fit(X_train, Y_train, nb_epoch=5, batch_size=32, class_weight = class_weight)</p></li>
</ol>
"
['r'],"Depending upon how I download, I get two different files","<p>This code works for sites where you don't need to be logged on. The Kaggle link only gives you the file when you are logged on to Kaggle. The file that is created with the code only contains html / javascript code of the Kaggle page. </p>
"
"['machine-learning', 'scikit']","Using machine learning specifically for feature analysis, not predictions","<p>You don't need the linear regression to understand the effect of features in your random forest, you're better off looking at the partial dependence plots directly, this what you get when you hold all the variables fixed, and you vary one at a time. You can plot these using <code>sklearn.ensemble.partial_depence.plot_partial_dependence</code>. Take a look at the <a href=""http://scikit-learn.org/stable/modules/ensemble.html#partial-dependence"" rel=""nofollow"">documentation</a> for an example of how to use it.</p>

<p>Another type of model that can be useful for exploratory data analysis is a <code>DecisionTreeClassifier</code>, you can produce a graphical representation of this using <code>export_graphviz</code></p>
"
"['nlp', 'predictive-modeling', 'word-embeddings']",Predicting a word using Word2vec model,"<p>Word2vec works in two models CBOW and skip-gram. Let's take CBOW model, as your question goes in the same way that predict the target word, given the surrounding words.</p>

<p>Fundamentally, the model develops input and output weight matrices, which depends upon the input context words and output target word with the help of a hidden layer. Thus back-propagation is used to update the weights when the error difference between predicted output vector and the current output matrix.</p>

<p>Basically speaking, predicting the target word from given context words is used as an equation to obtain the optimal weight matrix for the given data.</p>

<p>To answer the second part, it seems a bit complex than just a linear sum.</p>

<ol>
<li>Obtain all the word vectors of context words</li>
<li>Average them to find out the hidden layer vector <code>h</code> of size <code>Nx1</code></li>
<li>Obtain the output matrix <code>syn1</code>(<code>word2vec.c</code> or <code>gensim</code>) which is of size <code>VxN</code></li>
<li>Multiply <code>syn1</code> by <code>h</code>, the resulting vector will be <code>z</code> with size <code>Vx1</code></li>
<li>Compute the probability vector <code>y = softmax(z)</code> with size <code>Vx1</code>, where the highest probability denotes the one-hot representation of the target word in vocabulary.
<code>V</code> denotes size of vocabulary and <code>N</code> denotes size of embedding vector.</li>
</ol>

<p>Source : <a href=""http://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf"" rel=""nofollow"">http://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf</a></p>
"
['beginner'],Beginner in programming and data science with 100 hours to spend learning the basics,"<p>Being good at a subject does not automatically make someone a good teacher, and it looks like Roman's answer on Quora has fallen into a trap of thinking everything he knows is simple and could be picked up quickly. Also making the potential student attempt things outside of a beginner level - effectively just by research on the web following a 2 paragraph pointer - is going to make progress slow and frustrating. Despite no doubt good intentions, the advice there is likely to give a very poor learning experience.</p>

<p>There are full structured tutorials available on free Massive Online Open Courses (MOOCs) available for learning data science topics. These have been put together by professionals who know how to teach, and the effort put into designing any of these courses, providing materials etc, totally dwarfs the effort that went into the advice on Quora.</p>

<p>Ignore the Quora answer, and sign up for one or more of these MOOCs. You can find them at <a href=""https://www.coursera.org/"" rel=""nofollow"">Coursera</a>, <a href=""https://www.udacity.com/"" rel=""nofollow"">Udacity</a>, <a href=""https://www.edx.org/"" rel=""nofollow"">edX</a> and other similar places. If you want to cram a lot of study into a short time, look for ""at your own pace"" courses where all the materials are available to you immediately.</p>

<p>The almost canonical start point to test the waters would be <a href=""https://www.coursera.org/learn/machine-learning"" rel=""nofollow"">Andrew Ng's Machine Learning course on Coursera</a>. Machine Learning is one of the more immediately accessible and fun parts of data science, and the course goes into theory and practice with additional sections for beginners at the start covering necessary maths and programming. Following that entire course is probably around 100 hours total effort. You won't come out of it knowing data science, but you will gain useful practical skills, and get a real taster for the machine learning side of the subject.</p>
"
['optimization'],Minimize absolute values of errors instead of squares,"<p>In what term ""calculating absolute values is much more efficient than calculating squares""? Compared to the complexity of any estimator/model used, I don't think it is significant - but I would be interested if anyone makes me wrong.</p>

<p>Again, why do you think it doesn't matter in practice? Working with a smooth and convex function is more convenient (in terms of time and results) than not-convex function.</p>

<p>Actually, you can choose whatever function to minimize you'd like; it is just a trade-off between : </p>

<ul>
<li>Which kind of value you want to penalize</li>
<li>Complexity of the function to solve (mathematically speaking : local or global solution)</li>
<li>Time consuming (related to the previous point)</li>
</ul>

<p><strong>1. Minimizing absolute values</strong> :</p>

<p>With absolute value, you penalize the distance between <em>y</em> and <em>f(x)</em> linearly. Roughly speaking, you might end up with a lot of data that will look like outliers as long as <em>enough</em> are well explained by your estimator <em>f</em>.</p>

<p>Then, to minimize a function, one generally looks for the root(s) of its derivative. However, the derivative of <em>|x|</em> is not smooth. You can work with subgradient and other more complex mathematical object that may result in a longer time process due to more calculation.</p>

<p><strong>2. Minimizing square values</strong> :  </p>

<p>In this case, the distance between <em>y</em> and <em>f(x)</em> is more penalized. You'll tend to have less <em>outliers</em> (relatively to <em>f(x)</em>). </p>

<p>What is interesting is that is a smooth function (i.e. a defined derivative) and convex (with a global minimum)</p>

<p>So I guess people believe that the square of errors is a good trade-off.</p>
"
"['neuralnetwork', 'tensorflow', 'rnn']","What is the meaning of ""The number of units in the LSTM cell""?","<p>As the helpful comments in that function say,</p>

<blockquote>
  <p>The definition of cell in this package differs from the definition used in the
    literature. In the literature, cell refers to an object with a single scalar
    output. The definition in this package refers to a horizontal array of such
    units.</p>
</blockquote>

<p>In essence, the layer will contain multiple parallel LSTM units, structurally identical but each eventually ""learning to remember"" some different thing.</p>
"
"['bigdata', 'software-recommendation']",What commercial software should a data scientist purchase?,"<p>If you are a data scientist, then there is very little use for pre-packaged, generally inflexible commercial tools. </p>

<p>Part of the reason OSS is so prevalent and useful in data science is that you will often need to combine and/or modify a procedure to fit the needs at hand -- and then <em>deploy</em> it without a bunch of lawyers and sales reps getting involved at every step. </p>

<p>Since data scientists are expected to be proficient programmers, you should be comfortable digging into the source code and adding functionality or making it more user friendly. </p>

<p>I've come close to recommending the purchase of non-free(as in GPL) a couple times only to find that some industrious person has set up a project in Git that provides most if not all of the functionality if commercial software. In the cases where it doesn't, it at least addresses the core issue and I can modify and extend from it. It's much easier to modify a prototype than start from scratch.</p>

<p>Bottom line: be wary of commercial software for data science unless you've done your due diligence in the OSS space and can honestly say that you could not find any projects that could be modified to suit your needs. 
Commercial software is not only less flexible but you're effectively in a business partnership with these folks and that means your fates are somewhat intertwined (at least for the projects that depend in this software).</p>
"
"['r', 'classification', 'logistic-regression']",How does Performance function classify predictions as positive or negative? Package:ROCR,"<p>I cant access an R console at the moment to check, but I'm quite certain the cutoff is 0.5: if your glm model does prediction, it first produces real values and then applies the link function on top. To the best of my knowledge, you can't change it inside the glm function, so your best bet is probably to check ROC, find what the optimal threshold is and use that as cutoff. </p>
"
['data-mining'],Can I consider this pattern of data as a linear and use parametric multiple linear regression?,"<p>I don't think ""can"" is the right question to ask; it's not going to give you a syntax error. The right question is ""what could go wrong?"". Any modeling technique will have assumptions that may be broken, and knowing how those assumptions impact the results will help you know what to look for (and how much to care when those assumptions are broken).</p>

<ol>
<li><p>The best test of whether or not linearity is appropriate is whether the residuals are white or structured. For example, it looks like X9 might have a nonlinear relationship with Y. But that might be an artifact of the interaction between X9 and other variables, especially categorical variables. Fit your full model, then plot the residuals against X9 and see what it looks like.</p></li>
<li><p>Treating it as continuous won't cause serious problems, but you might want to think about what this implies. Is the relationship between 1 and 2 in the same direction and half the strength as the relationship between 2 and 4? If not, you might want to transform this to a scale where you <em>do</em> think the differences are linear.</p></li>
<li><p>Same as 2, except it's even more reasonable to see time as linear.</p></li>
<li><p>Standardization is not necessary for most linear regression techniques, as they contain their own standardization. The primary exception is techniques that use <a href=""https://en.wikipedia.org/wiki/Regularization_(mathematics)"" rel=""nofollow"">regularization</a>, where the scale of the parameters is relevant.</p></li>
</ol>

<p>It's also worth pointing out that multivariate linear relationships, while they can capture general trends well, are very poor at capturing logical trends. For example, looking at X3 and X4, it could very well be that there are rules like Y>X3 and Y>X4 in place, which is hinted at but not captured by linear regression.</p>
"
"['feature-selection', 'feature-engineering', 'feature-construction']",Is it a good idea to train with a feature which value will be fixed in future predictions?,"<p>Given that in your training data this feature has different values and some predictive power, I think not keeping this feature would be a mistake (without looking into overfitting due to having too many features). You cannot just discard the feature from your training set if it does influence the target because then these would be from a different population than your predictions and it will be able to learn from the other features.</p>

<p>Extreme example where x_2 will always be 5 in the future:</p>

<pre><code>x_1  x_2  y
2    8    6
3    7    5
2.5  5    1.5
3    5    0.5
</code></pre>

<p>Just removing x_2 loses a lot of information and would create a significant bias towards higher targets.</p>
"
"['feature-selection', 'model-selection']","Any ""rules of thumb"" on number of features versus number of instances? (small data sets)","<p>Multiple papers have opined that </p>

<blockquote>
  <p>only in rare cases is there a known distribution of the error as a function of the number of features and sample size.</p>
  
  <p>The error surface for a given set of instances, and features, is a function of the correlation (or lack of) between features.</p>
</blockquote>

<p><a href=""https://bioinformatics.oxfordjournals.org/content/21/8/1509.full"" rel=""nofollow"">This paper</a> suggests the following:</p>

<ul>
<li>For uncorrelated features, the optimal feature size is $ N-1 $ (where $ N $ is sample size)</li>
<li>As feature correlation increases, and the optimal feature size becomes proportional to $ \sqrt N $ for highly correlated features.</li>
</ul>

<p>Another (empirical) approach that could be taken, is to draw the learning curves for different sample sizes from the same dataset, and use that to predict classifier performance at different sample sizes. Here's the <a href=""http://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/1472-6947-12-8#CR31_460"" rel=""nofollow"">link to the paper</a>.</p>
"
['apache-spark'],Reference of SVM Using Spark,"<p>The <a href=""http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms"" rel=""nofollow"">documentation</a> of Apache Spark's MLib library has a neat and clear reference to the implementation of linear SVM's in Python.</p>

<p>It supports two models: <code>SVMWithSGD</code> and <code>SVMModel</code></p>
"
['education'],Qualifications for PhD Programs,"<p>If I were you I would take a MOOC or two (e.g., <a href=""https://www.coursera.org/course/algs4partI"">Algorithms, Part I</a>, <a href=""https://www.coursera.org/course/algs4partII"">Algorithms, Part II</a>, <a href=""https://www.coursera.org/course/progfun"">Functional Programming Principles in Scala</a>), a good book on data structures and algorithms, then just code as much as possible. You could implement some statistics or ML algorithms, for example; that would be good practice for you and useful to the community.</p>

<p>For a PhD program, however, I would also make sure I were familiar with the type of maths they use. If you want to see what it's like at the deep end, browse the papers at the <a href=""http://jmlr.org/papers/"">JMLR</a>. That will let you calibrate yourself in regards to theory; can you sort of follow the maths?</p>

<p>Oh, and you don't need a PhD to work at top companies, unless you want to join research departments like his. But then you'll spend more time doing development, and you'll need good coding skills...</p>
"
['parsing'],Parsing data from a string,"<p>If you have R is quite simple</p>

<ol>
<li><p>Copy the lines into a file, let's say: ""mydata.json""</p></li>
<li><p>Be sure you have installed the rjson package</p>

<pre><code> install.packages(""rjson"")
</code></pre></li>
<li><p>Import your data</p>

<pre><code> library(""rjson"")
 json_data &lt;- fromJSON(file=""mydata.json"")
</code></pre></li>
</ol>
"
"['data-mining', 'classification', 'r', 'svm', 'categorical-data']",Choosing the right data mining method to find the effect of each parameter over the target,"<p>You can try Bayesian belief networks (BBNs). BBNs can easily handle categorical variables and give you the picture of the multivariable interactions. Furthermore, you may use sensitivity analysis to observe how each variable influences your class variable. </p>

<p>Once you learn the structure of the BBN, you can identify the Markov blanket of the class variable. The variables in the Markov blanket of the class variable is a subset of all the variables, and you may use optimization techniques to see which combination of values in this Markov blanket maximizes your class prediction.</p>
"
"['statistics', 'apache-spark']",How to convert a SQLContext Dataframe to RDD of vectors in Python?,"<p>The Dataframe Python API exposes the RDD of a Dataframe by calling the following :</p>

<pre><code>df.rdd # you can save it, perform transformations of course, etc. 
</code></pre>

<p>df.rdd returns the content as an pyspark.RDD of Row.</p>

<p>You can then map on that RDD of Row transforming every Row into a <code>numpy</code> vector. I can't be more specific about the transformation since I don't know what your vector represents with the information given. </p>

<p>Note 1: <code>df</code>is the variable define our Dataframe. </p>

<p>Note 2: this function is available since Spark 1.3</p>
"
"['machine-learning', 'data-mining', 'classification', 'random-forest', 'decision-trees']",Cross validation for C5.0 algorithm,"<p>I would say cross validation is unnecessary here since the multiple partitioning of the data and variables is already implicit in Random Forests.   But it's still a good practice to hold out a testing set that is distinct from the training set.  This is mostly because you may introduce changes in your random forest to improve the performance on the test sets overall, thereby introducing the bias that the random forests are trying to overcome.   So if you withheld a portion of your data and judged the final performance of the RF on that withheld set only in the predict step, then it's fine.</p>
"
"['classification', 'text-mining']",Newsgroup classification,"<p>Your use case boils down to categorizing news feed on an online forum and then finding out top-n categories. I would suggest you look at this <a href=""https://blog.monkeylearn.com/hacker-news-categorizer-with-monkeylearn/"" rel=""nofollow"">Hacker News Categorizer developed by MonekyLearn</a>. This way you can understand how to get started with such projects. </p>

<p>PS : I am <strong>not</strong> affiliated with MonkeyLearn.</p>
"
"['python', 'data']",python - Will this data mining approach work? Is it a good idea?,"<p>I am not sure if using a classifier is the best way to approach this problem.  If it is something which can be easily extracted using regex, then that is the best way to do it. If however, you want to use classifiers, here are two questions you need to ask yourself.</p>

<p>One, what does the unlabelled data look like and can you design good features from it? Depending on the kind of feature vector you design, the complexity of the classification task may range from very easy, to impossible. (A perceptron cannot solve XOR usually, except when you provide it with specific linear combinations of the input variable).</p>

<p>Two, what does the labelled data look like? Is it representative of the entire dataset or does it only contain very specific types of format? If it is the former, then your classifier will not work well on files which are not represented in the labelled data. </p>

<p>If you just want to test run a classifier first, you can solve the problem of having more features than training samples by using Regularization. Regularization forces the training algorithm of the classifier to accept the simplest possible solution (think occam's razor). </p>

<p>Almost all Machine Learning related packages in Python will have regularization options you can use, so enjoy. </p>
"
"['r', 'visualization', 'ggplot2']",Represent time-series data in much compact form,"<p>Simulate some data:</p>

<pre><code>library(ggplot2)
library(purrr)
library(ggthemes)

days &lt;- seq(as.Date(""2015-08-01""), as.Date(""2015-08-31""), by=""1 day"")
hours &lt;- sprintf(""%02d"", 0:23)

map_df(days, function(x) {
  map_df(hours, function(y) {
    data.frame(day=x, hour=y, val=sample(2500, 1), stringsAsFactors=FALSE)
  })
}) -&gt; df
</code></pre>

<p>Check it:</p>

<pre><code>ggplot(df, aes(x=hour, y=val, group=day)) +
  geom_line() +
  facet_wrap(~day) +
  theme_tufte(base_family=""Helvetica"") +
  labs(x=NULL, y=NULL)
</code></pre>

<p><a href=""http://i.stack.imgur.com/p6Oj2.png""><img src=""http://i.stack.imgur.com/p6Oj2.png"" alt=""enter image description here""></a></p>

<p>Since you're only trying to convey the scope of the variation, perhaps use a boxplot of the values of hours across days?</p>

<pre><code>ggplot(df, aes(x=hour, y=val)) +
  geom_boxplot(fill=""#2b2b2b"", alpha=0.25, width=0.75, size=0.25) +
  scale_x_discrete(expand=c(0,0)) +
  scale_y_continuous(expand=c(0,0)) +
  coord_flip() +
  theme_tufte(base_family=""Helvetica"") +
  theme(axis.ticks=element_blank()) +
  labs(x=NULL, y=NULL)
</code></pre>

<p><a href=""http://i.stack.imgur.com/B1EZD.png""><img src=""http://i.stack.imgur.com/B1EZD.png"" alt=""enter image description here""></a></p>

<p>That can be tweaked to fit into most publication graphics slots and the boxplot shows just how varied each day's readings are.</p>

<p>You could also use <code>boxplot.stats</code> to get the summary data and plot it on a line chart:</p>

<pre><code>library(dplyr)
library(tidyr)

bps &lt;- function(x) {
  cnf &lt;- boxplot.stats(x)$conf
  data.frame(as.list(set_names(cnf, c(""lwr"", ""upr""))), mean=mean(x))
}

group_by(df, hour) %&gt;% 
  do(bps(.$val)) %&gt;% 
  ggplot(aes(x=hour, y=mean, ymin=lwr, ymax=upr, group=1)) +
  geom_ribbon(fill=""#2b2b2b"", alpha=0.25) +
  geom_line(size=0.25) +
  theme_tufte(base_family=""Helvetica"") +
  theme(axis.ticks=element_blank()) +
  labs(x=NULL, y=NULL)
</code></pre>

<p><a href=""http://i.stack.imgur.com/RMQRa.png""><img src=""http://i.stack.imgur.com/RMQRa.png"" alt=""enter image description here""></a></p>
"
"['visualization', 'javascript']",What visualization technique to best describe a recommendation dataset?,"<p>I think you're looking for a <a href=""http://cgi.mtc.sri.com/Cluster-Lab/"">similarity matrix</a> (see bottom of the page). If you don't have data on similarity between certain pairs, you can always leave them as grey or white. Also, this will only work for data sets small enough to actually make out what's going on. I'd say 25 rows / columns maximum.</p>

<p>In a similarity matrix, x, and y coordinates correspond to the two things you're comparing, while a <a href=""http://en.wikipedia.org/wiki/False_color"">colormap</a> magnitude represents similarity </p>

<p><strong>EDIT</strong>:
One thing you could do to replace the colormap is the insert, say, circles of different sizes according to the similarity metric. Or you could insert the numbers themselves, again, varying the size of the number as the magnitude of that number varies. Size usually works best is business visualizations.</p>
"
['nlp'],Named Entity Recognition: NLTK using Regular Expression,"<p>You have a great idea going, and it might work for your specific project. However there are a few considerations you should take into account: </p>

<ol>
<li><p>In your first sentence, Obama in incorrectly classified as an organization, instead of a person. This is because the training model used my NLTK probably does not have enough data to recognize Obama as a PERSON. So, one way would be to update this model by training a new model with a lot of labeled training data. Generating labeled training data is one of the most expensive tasks in NLP - because of all the man hours it takes to tag sentences with the correct Part of Speech as well as semantic role. </p></li>
<li><p>In sentence 2, there are 2 concepts - ""Former Vice President"", and ""Dick Cheney"". You can use co-reference to identify the relation between the 2 NNPs. Both the NNP are refering to the same entity, and the same entity could be referenced as - ""former vice president"" as well as ""Dick Cheney"". Co-reference is often used to identify the Named entity that pronouns refer to. e.g. ""Dick Cheney is the former vice president of USA. He is a Republican"". Here the pronoun ""he"" refers to ""Dick Cheney"", and it should be identified by a co-reference identification tool. </p></li>
</ol>
"
"['machine-learning', 'dataset']",How to generate synthetic dataset using machine learning model learnt with original dataset?,"<p>The general approach is to do traditional statistical analysis on your data set to define a multidimensional random process that will generate data with the same statistical characteristics.  The virtue of this approach is that your synthetic data is independent of your ML model, but statistically ""close"" to your data. (see below for discussion of your alternative)</p>

<p>In essence, you are estimating the multivariate probability distribution associated with the process.  Once you have estimated the distribution, you can generate synthetic data through the Monte Carlo method or similar repeated sampling methods.  If your data resembles some parametric distribution (e.g. lognormal) then this approach is straightforward and reliable.  The tricky part is to estimate the dependence between variables. See: <a href=""https://www.encyclopediaofmath.org/index.php/Multi-dimensional_statistical_analysis"" rel=""nofollow"">https://www.encyclopediaofmath.org/index.php/Multi-dimensional_statistical_analysis</a>.</p>

<p>If your data is irregular, then non-parametric methods are easier and probably more robust.  <a href=""https://en.wikipedia.org/wiki/Multivariate_kernel_density_estimation"" rel=""nofollow"">Multivariate kernal density estimation</a> is a method that is accessible and appealing to people with ML background. For a general introduction and links to specific methods, see: <a href=""https://en.wikipedia.org/wiki/Nonparametric_statistics"" rel=""nofollow"">https://en.wikipedia.org/wiki/Nonparametric_statistics</a> .</p>

<p>To validate that this process worked for you, you go through the machine learning process again with the synthesized data, and you should end up with a model that is fairly close to your original.  Likewise, if you put the synthesized data into your ML model, you should get outputs that have similar distribution as your original outputs.</p>

<p>In contrast, you are proposing this:</p>

<blockquote>
  <p>[original data --> build machine learning model --> use ml model to generate synthetic data....!!!]</p>
</blockquote>

<p>This accomplishes something different that the method I just described.  This would solve the <em><a href=""https://en.wikipedia.org/wiki/Inverse_problem#Conceptual_understanding"" rel=""nofollow"">inverse problem</a></em>: ""what inputs could generate any given set of model outputs"". Unless your ML model is over-fitted to your original data, this synthesized data <em>will not</em> look like your original data in every respect, or even most.</p>

<p>Consider a linear regression model. The same linear regression model can have identical fit to data that have very different characteristics. A famous demonstration of this is through <a href=""http://en.wikipedia.org/wiki/Anscombe%27s_quartet"" rel=""nofollow"">Anscombe's quartet</a>.</p>

<p><img src=""http://i.stack.imgur.com/P9s4w.png"" alt=""All four sets are identical when examined using simple summary statistics, but vary considerably when graphed""></p>

<p>Thought I don't have references, I believe this problem can also arise in logistic regression, generalized linear models, SVM, and K-means clustering.</p>

<p>There are some ML model types (e.g. decision tree) where it's possible to inverse them to generate synthetic data, though it takes some work.  See: <a href=""http://csce.uark.edu/~cwt/DOCS/2008-06--IEEE-Internet-Computing--Reverse-Data-Mining--Eno-Thompson.pdf"" rel=""nofollow"">Generating Synthetic Data to Match Data Mining Patterns</a>.</p>
"
"['classification', 'python', 'svm', 'scikit', 'multilabel-classification']",One multilabel classifier or one for each type of label?,"<p>Both ways are valid and both are commonly used. Sometimes, a classifier that claims to be multilabel may just be separating the labels into multiple OneVsRest classifiers under-the-hood and conveniently joining the results together at the end.</p>

<p>However, there are cases where the methods are fundamentally different. For instance, in training a neural net with multiple targets (labels), you can setup the structure of the network such that there is shared structure. The shared nodes will end up learning features that are useful for all the targets, which could be very useful.</p>

<p>For example, if you're classes (labels) are ""cat-pet"", ""cat-big"", and ""dog"", you may want an algorithm that first learns to distinguish between any cat and any dog, and then in a later step learns to separate cats that are pets from cats that are big (like a lion!). This is called hierarchy, and if your classifier can exploit hierarchy you may gain better accuracy. If your classes are completely independent however, it may not make any difference.</p>

<p>I suggest you start with the method that is easiest (i.e. OneVsRest), and see if the performance is suitable to your needs, then move to more complicated methods (multilabel, hierarchical methods, etc) only once you need better performance.</p>
"
"['machine-learning', 'beginner', 'linear-regression']",What does it mean when people say a cost function is something you want to minimize?,"<p>No, it means you are trying to find the inputs that make the output of the cost function the smallest. It doesn't mean that you should ""minimize"" use it.</p>
"
"['python', 'pandas']",Pandas time series optimization problem: add year,"<p>Make a pandas Timedelta object then add with the += operator:</p>

<pre><code>x = pandas.Timedelta(days=365)
mydataframe.timestampcolumn += x
</code></pre>

<p>So the key is to store your time series as timestamps.  To do that, use the pandas <code>to_datetime</code> function:</p>

<pre><code>mydataframe['timestampcolumn'] = pandas.to_datetime(x['epoch'], unit='s')
</code></pre>

<p>assuming you have your timestamps as epoch seconds in the dataframe <code>x</code>.  That's not a requirement of course; see the <a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.tseries.tools.to_datetime.html"" rel=""nofollow"">to_datetime</a> documentation for converting other formats.</p>
"
"['bigdata', 'r']",Is the R language suitable for Big Data,"<p>Actually this is coming around. In the book R in a Nutshell there is even a section on using R with Hadoop for big data processing. There are some work arounds that need to be done because R does all it's work in memory, so you are basically limited to the amount of RAM you have available to you.</p>

<p>A mature project for R and Hadoop is <a href=""https://github.com/RevolutionAnalytics/RHadoop"">RHadoop</a></p>

<p>RHadoop has been divided into several sub-projects, rhdfs, rhbase, rmr2, plyrmr, and quickcheck (<a href=""https://github.com/RevolutionAnalytics/RHadoop/wiki"">wiki</a>).</p>
"
"['machine-learning', 'classification', 'text-mining', 'multiclass-classification']",Appropriate algorithm for string (not document) classification?,"<p>You might find it useful to treat n-grams of characters as your feature space. Then you could represent a string as a bag of substrings.  With <code>N = 4</code> or greater, you would capture things like "".com"" in emails, ""##.#"" in dates, etc.</p>

<p>It might also help to encode all single digits as one reserved number-only-character.</p>

<p>An easy way to to this might be to create all the n-gram substrings for each string in your dataset, then simply treat that list of words as a document.  Then you could use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"" rel=""nofollow"">term frequency</a> or <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow"">tf-idf</a> vectors in your supervised step.</p>

<p>For example to create the substring uni-, bi-, and tri-grams for ""whatever@gmail.com"":</p>

<pre><code>a = ""whatever@gmail.com"" 
b = set([a[x:x+y] for y in range(0,4) for x in range(0,len(a))]) 

set(['', 'co', 've', 'ai', 'eve', 'r@', 'at', '.co', 'gm', 'ev', 'tev', 'er', '@gm', 'ver', '@g', 'r@g', 'ail', 'il.', 'gma', '.', 'te', 'hat', '@', 'wha', 'om', 'wh', 'er@', 'mai', 'ma', 'ha', 'l.c', 'a', 'c', 'ate', 'e', 'g', 'i', 'h', '.c', 'm', 'l', 'o', 'l.', 'r', 't', 'w', 'v', 'com', 'il'])
</code></pre>
"
"['apache-spark', 'scala']",How to calculate the mean of a dataframe column and find the top 10%,"<p>This is the import you need, and how to get the mean for a column named ""RBIs"":</p>

<pre><code>import org.apache.spark.sql.functions._
df.select(avg($""RBIs"")).show()
</code></pre>

<p>For the standard deviation, see
<a href=""http://stackoverflow.com/questions/31789939/calculate-the-standard-deviation-of-grouped-data-in-a-spark-dataframe"">scala - Calculate the standard deviation of grouped data in a Spark DataFrame - Stack Overflow</a></p>

<p>For grouping by percentiles, I suggest defining a new column via a user-defined function (UDF), and using groupBy on that column.  See</p>

<ul>
<li><a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html#udf-registration-moved-to-sqlcontextudf-java--scala"">Spark SQL and DataFrames - Spark 1.5.1 Documentation - udf registration</a></li>
</ul>
"
"['time-series', 'correlation']",Normalized Euclidean Distance versus cross correlation?,"<p>These two metrics are <strong>not</strong> the same.</p>

<p>The normalized Euclidean distance is the distance between two normalized vectors that have been normalized to length one.  If the vectors are identical then the distance is <code>0</code>, if the vectors point in opposite directions the distance is <code>2</code>, and if the vectors are orthogonal (perpendicular) the distance is <code>sqrt(2)</code>.  It is a positive definite scalar value between <code>0</code> and <code>2</code>.</p>

<p><a href=""http://i.stack.imgur.com/QSIBn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QSIBn.png"" alt=""NED equation""></a></p>

<p>The normalized cross-correlation is the dot product between the two normalized vectors.  If the vectors are identical, then the correlation is <code>1</code>, if the vectors point in opposite directions the correlation is <code>-1</code>, and if the vectors are orthogonal (perpendicular) the correlation is <code>0</code>.  It is a scalar value between <code>-1</code> and <code>1</code>.</p>

<p><a href=""http://i.stack.imgur.com/XHjSu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/XHjSu.png"" alt=""NCC equation""></a></p>

<p>This all comes with the understanding that in time-series analysis the cross-correlation is a measure of similarity of two series as a function of the lag of one relative to the other.</p>
"
"['java', 'scala']",Scala vs Java if you're NOT going to use Spark?,"<p>This is a bit off topic for this SE, or maybe opinion-based, but, I work in this field and I'd recommend Scala.</p>

<p>No I would not characterize Scala as a ""stats-oriented"" Java. I'd describe it as what you get if you asked 3 people to design ""Java 11"" and then used all of their ideas at once.</p>

<p>Java 8 remains great, but Scala fully embraces just about all the good ideas from languages that you'd want, like type safety and closures and functional paradigms, and a more elaborate types/generics system, down to more syntactic sugar conveniences like case classes and lazy vals. Plus you get to run everything in the same JVM and interoperate with any Java libraries. </p>

<p>The price is complexity: understanding all of Scala is a lot more difficult than all of Java. And some bits of Scala maybe were a bridge too far. And, the tooling ecosystem isn't great IMHO in comparison to the standard Java tools. But there again you can for example use Maven instead of SBT. But you can mostly avoid the parts of Scala that are complex if you don't need them. Designing Scala libraries takes a lot of skill and know-how; just developing run-of-the-mill code in Scala does not.</p>

<p>From a productivity perspective, once you're used to Scala, you'll be more productive. If you're an experienced Java dev, I actually think you'll appreciate Scala. I have not liked any other JVM language, which tend to be sort of one-issue languages that change lots of stuff for marginal gains.</p>

<p>Certainly, for analytics, the existence of Spark argues for Scala. You'll use it eventually.</p>
"
"['r', 'statistics', 'time-series']",Impulse Response Function - Negative Shocks on R,"<p>Here is a simple example that should work:</p>

<pre><code>data(""Canada"")
var.2c=VAR(Canada,p=2,type=""const"")
irf.rw.e=irf(var.2c,impulse=""rw"",response=c(""e""))
n=length(irf.rw.e$irf$rw)
for(i in 1:n){irf.rw.e$irf$rw[i]=irf.rw.e$irf$rw[i]*2.0} 
plot(irf.rw.e)
</code></pre>

<p><a href=""http://r.789695.n4.nabble.com/Impulse-response-analysis-within-package-vars-td841596.html"" rel=""nofollow"">Source</a></p>
"
"['r', 'python', 'visualization']","Need some tips regarding starting out with the field's specific programming languages, with a heavy focus on data visualization","<p><a href=""https://www.r-project.org/"" rel=""nofollow"">R</a> is a more compact, target oriented, package. Good if you want to focus on very specific tasks (generally scientific). <a href=""https://www.python.org/"" rel=""nofollow"">Python</a>, on the other hand, is a general purpose language.</p>

<p>That being said, and obviously this is a matter of opinion, if you are an experienced developer go for Python. You'll have far more choices in libraries and a far bigger potential to build big software.</p>

<p><strong>Some examples of 2D scientific plotting libraries:</strong></p>

<ul>
<li><a href=""http://matplotlib.org/"" rel=""nofollow"">Matplotlib</a></li>
<li><a href=""http://bokeh.pydata.org/en/latest/docs/gallery.html"" rel=""nofollow"">Bokeh</a> (targeted D3.js)</li>
<li><a href=""http://code.enthought.com/projects/chaco/"" rel=""nofollow"">Chaco</a></li>
<li><a href=""http://ggplot.yhathq.com/"" rel=""nofollow"">ggplot</a></li>
<li><a href=""https://stanford.edu/~mwaskom/software/seaborn/"" rel=""nofollow"">Seaborn</a></li>
<li><a href=""http://www.pyqtgraph.org/"" rel=""nofollow"">pyQtGraph</a> (some significant 3D features)</li>
</ul>

<p><strong>Some examples of 3D scientific plotting libraries</strong></p>

<ul>
<li><a href=""http://vispy.org/"" rel=""nofollow"">Vispy</a></li>
<li><a href=""http://code.enthought.com/projects/mayavi/"" rel=""nofollow"">Mayavi</a></li>
<li><a href=""http://www.vtk.org/"" rel=""nofollow"">VTK</a></li>
<li><a href=""https://glumpy.github.io/"" rel=""nofollow"">Glumpy</a></li>
</ul>

<p><strong>Some examples of libraries typically used in Data Science in Python:</strong></p>

<ul>
<li><a href=""http://pandas.pydata.org/"" rel=""nofollow"">Pandas</a></li>
<li><a href=""http://www.numpy.org/"" rel=""nofollow"">Numpy</a></li>
<li><a href=""https://www.scipy.org/"" rel=""nofollow"">Scipy</a></li>
<li><a href=""http://scikit-learn.org/stable/"" rel=""nofollow"">Scikit-learn</a></li>
<li><a href=""http://scikit-image.org/"" rel=""nofollow"">Scikit-image</a></li>
</ul>

<p>Also check the list for other relevant <a href=""https://scikits.appspot.com/scikits"" rel=""nofollow"">Scikit packages</a>.</p>

<p>As for starting software I would advise you to use any of the already prepared Python distributions that already come with a bunch of scientific libraries inside as well as software such as IDEs. Some examples are:</p>

<ul>
<li><a href=""http://winpython.github.io/"" rel=""nofollow"">WinPython</a></li>
<li><a href=""https://python-xy.github.io/"" rel=""nofollow"">Python XY</a></li>
<li><a href=""https://www.continuum.io/why-anaconda"" rel=""nofollow"">Anaconda</a></li>
<li><a href=""https://www.enthought.com/products/canopy/"" rel=""nofollow"">Canopy</a></li>
</ul>

<p>Personally I'm a user of WinPython due to being portable (former user of Python XY, both are great). In any case these distributions will greatly simplify the task of having your scientific Python environment (so to speak) prepared. You just need to code. One IDE known to be specially good for scientists is <a href=""https://github.com/spyder-ide/spyder/"" rel=""nofollow"">Spyder</a>. Yet these ones also will work:</p>

<ul>
<li><a href=""http://www.pydev.org/"" rel=""nofollow"">PyDev</a></li>
<li><a href=""https://www.jetbrains.com/pycharm/"" rel=""nofollow"">PyCharm</a></li>
<li><a href=""http://www.wingware.com/"" rel=""nofollow"">WingIDE</a></li>
<li><a href=""http://komodoide.com/features/"" rel=""nofollow"">Komodo</a></li>
<li><a href=""https://www.visualstudio.com/en-us/features/python-vs.aspx"" rel=""nofollow"">Python Tools for Visual Studio</a></li>
</ul>

<p>As for data visualization tips you'll see that the most common functions in the libraries mentioned above are also the most widely used. For instance a library like Pandas let's you call plots directly from the object so there is already an intuitive approach to data visualization. A library like scikit-learn (check the site) already shows examples followed by data visualization of the results. I wouldn't be too concerned about this point. You'll learn just by roaming a bit on the libraries documentation (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN"" rel=""nofollow"">example</a>). </p>
"
"['data-mining', 'random-forest', 'data', 'decision-trees', 'preprocessing']","How to preprocess different kinds of data (continuous, discrete, categorical) before Decision Tree learning","<p>One of the benefits of decision trees is that ordinal (continuous or discrete) input data does not require any significant preprocessing. In fact, the results should be consistent regardless of any scaling or translational normalization, since the trees can choose equivalent splitting points. The best preprocessing for decision trees is typically whatever is easiest or whatever is best for visualization, as long as it doesn't change the relative order of values within each data dimension.</p>

<p>Categorical inputs, which have no sensible order, are a special case. If your random forest implementation doesn't have a built-in way to deal with categorical input, you should probably use a 1-hot encoding:</p>

<ul>
<li>If a categorical value has $n$ categories, you encode the value using $n$ dimensions, one corresponding to each category. </li>
<li>For each data point, if it is in category $k$, the corresponding $k$th dimension is set to 1, while the rest are set to 0.</li>
</ul>

<p>This 1-hot encoding allows decision trees to perform category equality tests in one split since inequality splits on non-ordinal data doesn't make much sense.</p>
"
"['python', 'csv']",Merging repeating data cells in csv,"<p><a href=""http://pandas.pydata.org/"" rel=""nofollow"">Pandas</a> is a python library that you will find very useful for these types of tasks.</p>

<p><a href=""http://stackoverflow.com/questions/29583312/pandas-sum-of-duplicate-attributes"">Here is a stack overflow post</a> that tells you how to do what you want to accomplish.</p>

<p>It boils down to three very pythonic lines with a <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"" rel=""nofollow"">groupby and transformation</a> followed by a <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow"">drop_duplicates</a>:</p>

<pre><code>import pandas
df = pandas.read_csv('csvfile.csv', header = 0)
df['Total'] = df.groupby(['Name', 'Age'])['Salary'].transform('sum')
df.drop_duplicates(take_last=True)
</code></pre>
"
"['machine-learning', 'graphs', 'distributed']",Large Graphs: NetworkX distributed alternative,"<p>Good , old and unsolved question! Distributed processing of large graphs as far as I know (speaking as a graph guy) has 2 different approaches, with the knowledge of Big Data frameworks or without it.</p>

<p><a href=""http://snap.stanford.edu/"" rel=""nofollow"">SNAP</a> library from Jure Leskovec group at  Stanford which is originally in C++ but also has a Python API (please check if you need to use C++ API or Python does the job you want to do). Using snap you can do many things on massive networks without any special knowledge of Big Data technologies. So I would say the easiest one.</p>

<p>Using Apache Graphx is wonderful only if you have experience in Scala because there is no Python thing for that. It comes with a large stack of built in algorithms including centrality measures. So the second easiest in case you know Scala.</p>

<p>Long time ago when I looked at GraphLab it was commercial. Now I see it goes open source so maybe you know better than me but from my out-dated knowledge I remember that it does not support a wide range of algorithms and if you need an algorithm which is not there it might get complicated to implement. On the other hand it uses Python which is cool. After all please check it again as my knowledge is for 3 years ago.</p>

<p>If you are familiar with Big Data frameworks and working with them, <a href=""http://giraph.apache.org/quick_start.html"" rel=""nofollow"">Giraph</a> and <a href=""https://github.com/dbs-leipzig/gradoop"" rel=""nofollow"">Gradoop</a> are 2 great options. Both do fantastic jobs but you need to know some Big Data architecture e.g. working with a hadoop platform.</p>

<h2>PS</h2>

<p>1) I have used simple NetworkX and multiprocessing to distributedly process DBLP network with 400,000 nodes and it worked well, so you need to know HOW BIG your graph is.</p>

<p>2) After all, I think SNAP library is a handy thing.</p>
"
"['dataset', 'deep-learning', 'word-embeddings', 'torch']",How to handle Memory issues in training Word Embeddings on Large Datasets?,"<p>I'm not familiar with Torch, but since basically word2vec and doc2vec are considered, These models learn from each sentences and so there is no need to have all the sentences in the memory. You could iterate via each sentence in the corpora and let the model learn from each of the sentence. And that is probably how people train on huge corpora with or without high computation machines.</p>

<p>A short example in python:</p>

<pre><code>class SentenceIterator(object):
   def __iter__(file_name):
      for line in open(file_name)
         yield line

sentences = SentenceIterator(""path/to/file"")
for line in sentences:
    model.train(line)
</code></pre>

<p>In this way, The memory is loaded with only one sentence at a time and when it is done the memory loads the next one. 
For building the vocabulary, you can do the whole iterating via all the documents to build the vocab first and then train the data, depending upon the word-embedding functions implemented.</p>
"
['r'],What is the difference between the 3 architectural components of Microsoft R Server?,"<p>RevoScaleR package &amp; ScaleR are same (I think after MS acquisition its became scaleR) 
A new file format especially designed for large files such as : 
XDF File Format (block based data format)
RxDataSource (designed to support the use of external memory algorithms with .xdf files.)
PEMA  based Algorithms
External memory implementations of the statistical algorithms most commonly used with large data sets such as : rxSummary, rxCrossTabs, rxLinMod. rxKMeans, rxGlm etc.
An extensible programming framework that allows R programmers to write their own external memory algorithms that can take advantage of ScaleR Enterprise’s new Big Data capabilities.</p>

<p>DistributedR for supporting big data framework
ConnectR It has all connectors to interact with other data sources
DeployR is used to integrate R with any application programming </p>
"
"['classification', 'apache-spark', 'multiclass-classification', 'naive-bayes-classifier']","SPARK, ML: Naive Bayes classifier often assigns 1 as probability prediction","<p>The reason NB is called ""Naive"" is that is makes the assumption that the predictive variables are all <strong>independent</strong>.
This assumption usually skews the model scores (which, under the above naive assumption are unbiased probability estimates) towards 0 or 1.</p>

<p>In your case, e.g., the presence of words <code>flower</code> and <code>petal</code> indicate <code>gardening</code> category, but, because the presence of these words is not independent (if one is present, the other is likely to be present too), the model will over-value their appearance. Taking the extreme case, if words <code>A</code> and <code>B</code> appear only together, then </p>

<pre><code>P(Category=X | A &amp; B) = P(Category=X | B) = P(Category=X | A)
</code></pre>

<p>and thus one should multiply by the odds ratio of <code>A&amp;B</code> once, not twice, as the Naive Bayes algorithm requires.</p>

<p>Your remedy is to use <em>calibration</em> (a separate model which maps model scores to probabilities).</p>
"
"['machine-learning', 'python', 'neuralnetwork', 'logistic-regression']","How to analyse move kindness in python (Logistic regression, neural network, etc.)?","<p>Okay, so from what I understand, you have a regression problem taking into account a variety of physical features. The reason I say that this is a regression problem, verses a classification problem is because the scale you are trying to predict is an ordinal scale.</p>

<p>There are a couple approaches to this. If your features are discriminative and linear enough, a simple least squares linear regression might work. If you believe the problem you have is to complicated for linear regressions, a simple vanilla neural network with one single output. I would recommend using the scikit-learn library in python for all models that are not neural networks. Here is a link to the generalized linear regression page:<br>
<a href=""http://scikit-learn.org/stable/modules/linear_model.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/linear_model.html</a><br>
 That link has code samples and mathematical explanations. If you decide to use neural networks, and you don't have a great amount of samples or a need to use the GPU, the pyBrain library is great. <a href=""http://pybrain.org/"" rel=""nofollow"">http://pybrain.org/</a></p>

<p>I wouldn't recommend using a logistic regression (since you mentioned it in your question), simply because a logistic regression is a classification problem, and I believe you would be better off approaching this from a regression standpoint.     </p>
"
"['machine-learning', 'python', 'java']",Why in deep learning or another AI technique python is used rather than other languages?,"<p>Short answers</p>

<ol>
<li><p><strong>Tooling.</strong> Python has fantastic math, statistics, and linear algebra libraries.</p></li>
<li><p><strong>Less Code, Same Result.</strong> Python provides quick and simple ways of achieving programming solutions compared to C#, Java, C++, etc. This means you'll write less code and achieve the same result.</p></li>
</ol>
"
"['r', 'python', 'predictive-modeling']",Is there an equivalent of the Bandit package in r (Used in implementation of Multi arm bandit algorithm) in python?,"<ul>
<li><p>Check this one from a <a href=""https://github.com/johnmyleswhite/BanditsBook"" rel=""nofollow"">book</a> on MAB</p></li>
<li><p>There's also a <a href=""http://pythonhosted.org/Flask-MAB/"" rel=""nofollow"">flask</a>implementation if you want a web service</p></li>
</ul>
"
['regression'],What does it mean when the coefficient of the squared term is negative in regression?,"<p>In this context probably the plain English way to put it is that 'returns increase with additional exposure to the ad, but there is a tapering effect at the upper end of exposures. Looking at this picture of a parabola here (i.e a graph of y= -ax^2+bx+c):<a href=""http://i.stack.imgur.com/ibAHt.png""><img src=""http://i.stack.imgur.com/ibAHt.png"" alt=""enter image description here""></a></p>

<p>Most likely your returns data are between 0 and the peak so you don't actually see  a decline, just a reduction in the rate of increase.</p>
"
"['r', 'python', 'statistics']",How to find a confidence level given the z value,"<p>OK, for a 95% confidence interval, you want to know how many standard deviations away from the mean your point estimate is (the ""z-score"").  To get that, you take off the 5% ""tails"".  Working in percentile form you have 100-.95 which yields a value 0.5  </p>

<p>Divide that in half to get 0.25 and then, in R, use the qnorm function to get the z-star (""critical value"").  Since you only care about one ""side"" of the curve (the values on either side are mirror images of each other) and you want a positive number, pass the argument lower.tail=FALSE.  </p>

<p>So, in the end, it would look like this:</p>

<pre><code>qnorm(.025,lower.tail=FALSE)
</code></pre>

<p>yielding a value of 1.959964</p>

<p>You then plug that value into the equation for the margin of error to finish things up.</p>

<p>If you want to go the other direction, from a ""critical value"" to a probability, use the pnorm function.  Something like:</p>

<pre><code>pnorm(1.959964,lower.tail=FALSE)
</code></pre>

<p>which will give you back 0.025</p>
"
"['python', 'image-classification']",Image classification in python,"<p>As this question highly overlaps with a similar question I have already answered, I would include that answer here (linked in the comments underneath the question):</p>

<p>In images, some frequently used techniques for feature extraction are <strong>binarizing</strong> and <strong>blurring</strong></p>

<p><strong>Binarizing:</strong> converts the image array into 1s and 0s. This is done while converting the image to a 2D image. Even gray-scaling can also be used. It gives you a numerical matrix of the image. Grayscale takes much lesser space when stored on Disc.</p>

<p>This is how you do it in Python:</p>

<pre><code>from PIL import Image

%matplotlib inline  

#Import an image
image = Image.open(""xyz.jpg"")

image
</code></pre>

<p>Example Image: </p>

<p><a href=""http://i.stack.imgur.com/mkf97.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mkf97.jpg"" alt=""enter image description here""></a></p>

<p>Now, convert into gray-scale:</p>

<pre><code>im = image.convert('L')

im
</code></pre>

<p>will return you this image:</p>

<p><a href=""http://i.stack.imgur.com/AGxy6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AGxy6.png"" alt=""enter image description here""></a></p>

<p>And the matrix can be seen by running this:</p>

<pre><code>array(im)
</code></pre>

<p>The array would look something like this:</p>

<pre><code>array([[213, 213, 213, ..., 176, 176, 176],
       [213, 213, 213, ..., 176, 176, 176],
       [213, 213, 213, ..., 175, 175, 175],
       ..., 
       [173, 173, 173, ..., 204, 204, 204],
       [173, 173, 173, ..., 205, 205, 204],
       [173, 173, 173, ..., 205, 205, 205]], dtype=uint8)
</code></pre>

<p>Now, use a histogram plot and/or a contour plot to have a look at the image features:</p>

<pre><code>from pylab import *

# create a new figure
figure()
gray()
# show contours with origin upper left corner
contour(im, origin='image')
axis('equal')
axis('off')


figure()


hist(im_array.flatten(), 128)

show()
</code></pre>

<p>This would return you a plot, which looks something like this:</p>

<p><a href=""http://i.stack.imgur.com/56A0K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/56A0K.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/tWpNy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tWpNy.png"" alt=""enter image description here""></a></p>

<p><strong>Blurring:</strong> Blurring algorithm takes weighted average of neighbouring pixels to incorporate surroundings color into every pixel. It enhances the contours better and helps in understanding the features and their importance better.</p>

<p>And this is how you do it in Python:</p>

<pre><code>from PIL import *


figure()
p = image.convert(""L"").filter(ImageFilter.GaussianBlur(radius = 2))
p.show()
</code></pre>

<p>And the blurred image is:</p>

<p><a href=""http://i.stack.imgur.com/0Dx8q.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0Dx8q.jpg"" alt=""enter image description here""></a></p>

<p>So, these are some ways in which you can do feature engineering. And for advanced methods, you have to understand the basics of Computer Vision and neural networks, and also the different types of filters and their significance and the math behind them.</p>

<hr>

<p>The entire analytics is done with the <a href=""http://www.pythonware.com/products/pil/"" rel=""nofollow"">PIL package</a>. I wouldn't claim that it's a one-stop shop for Image analytics, but for a starter to novice level, it is pretty much it.</p>
"
"['python', 'sklearn', 'cross-validation', 'model-selection']",Nested cross-validation and selecting the best regression model - is this the right SKLearn process?,"<p>Yours is not an example of nested cross-validation.</p>

<p>Nested cross-validation is useful to figure out whether, say, a random forest or a SVM is better suited for your problem. Nested CV only outputs a score, it does not output a model like in your code.</p>

<p>This would be an example of nested cross validation:</p>

<pre><code>from sklearn.datasets import load_boston
from sklearn.cross_validation import KFold
from sklearn.metrics import mean_squared_error
from sklearn.grid_search import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import numpy as np

params = [{'C': [0.01, 0.05, 0.1, 1]}, {'n_estimators': [10, 100, 1000]}]
models = [SVR(), RandomForestRegressor()]

df = load_boston()
X = df['data']
y = df['target']

cv = [[] for _ in range(len(models))]
for tr, ts in KFold(len(X)):
    for i, (model, param) in enumerate(zip(models, params)):
        best_m = GridSearchCV(model, param)
        best_m.fit(X[tr], y[tr])
        s = mean_squared_error(y[ts], best_m.predict(X[ts]))
        cv[i].append(s)
print(np.mean(cv, 1))
</code></pre>

<p>By the way, a couple of thoughts:</p>

<ul>
<li>I see no purpose to grid search for <code>n_estimators</code> for your random forest. Obviously, the more, the merrier. Things like <code>max_depth</code> is the kind of regularization that you want to optimize. The error for the nested CV of <code>RandomForest</code> was much higher because you did not optimize for the right hyperparameters, not necessarily because it is a worse model.</li>
<li>You might also want to try gradient boosting trees.</li>
</ul>
"
"['text-mining', 'algorithms', 'research']",Identify given patterns in unstructured data like text files,"<p>That is exactly what the <a href=""http://www.trifacta.com/"" rel=""nofollow"">Trifecta</a> product does (in addition to other features). It uses the Wrangle language which is a DSL (domain specific language) designed for data manipulation. There is a much earlier research project called <a href=""http://vis.stanford.edu/papers/wrangler"" rel=""nofollow"">Wrangler</a> from the same people. The Wrangler papers might give you ideas.</p>
"
['machine-learning'],Newbie: What is the difference between hypothesis class and models?,"<p>Your hypothesis class consists of all possible hypotheses that you are searching over, regardless of their form. For convenience's sake, the hypothesis class is usually constrained to be only one type of function or model at a time, since learning methods typically only work on one type at a time. This doesn't have to be the case, though: </p>

<ol>
<li>Hypothesis classes don't have to consist of only one type of function. If you're searching over all linear, quadratic, and exponential functions, then those are what your combined hypothesis class contains.</li>
<li>Hypothesis classes also don't have to consist of only simple functions. If you manage to search over all piecewise-$\tanh^2$ functions, then those functions are what your hypothesis class includes.</li>
</ol>

<p>The big tradeoff is that the larger your hypothesis class, the better the best hypothesis models the underlying true function, but the harder it is to find that best hypothesis. This is related to the <a href=""https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff"" rel=""nofollow"">bias–variance tradeoff</a>.</p>
"
"['neuralnetwork', 'matlab', 'confusion-matrix']",Multiple confusion matrix for multiple training instances. Which one to take?,"<p>I can't check at the moment (no Matlab at hand), but I suppose the differences come from the different random seeds used to initialize the neural networks (at least this is the only part which i can think of that has a random component). I would suggest predicting class probabilities, averaging those and then viewing the resulting confusion matrix of the ""averaged"" prediction. This way you - to a degree - mitigating the effect of randomness resulting from different initializations of the weights.</p>
"
"['time-series', 'linear-regression', 'feature-extraction', 'feature-construction']",Time-stamp for linear model,"<p>Welcome to Datascience.SE!</p>

<p>Like you said, you can extract the day of the week. Also extract the hour of the day, then encode these two variables using sines and cosines with their respective periodicities (7 and 24). Also create a column for the UNIX/epoch time. If there are ""special days"", such as holidays or sales, create boolean columns for them too.</p>
"
"['sampling', 'preprocessing']",Sampling for multi categorical variable,"<p>Let me give you some pointers (assuming that I'm right on this, which might not necessarily be true, so proceed with caution :-). First, I'd figure out the applicable terminology. It seems to me that your case can be categorized as <em>multivariate sampling</em> from a <em>categorical distribution</em> (see <a href=""https://en.wikipedia.org/wiki/Categorical_distribution#Sampling"" rel=""nofollow"">this section</a> on categorical distribution sampling). Perhaps, the simplest approach to it is to use <strong>R</strong> ecosystem's rich functionality. In particular, standard <code>stats</code> package contains <code>rmultinom</code> function (<a href=""http://stat.ethz.ch/R-manual/R-devel/library/stats/html/Multinom.html"" rel=""nofollow"">link</a>).</p>

<p>If you need more complex types of sampling, there are other packages that might be worth exploring, for example <code>sampling</code> (<a href=""https://cran.r-project.org/web/packages/sampling"" rel=""nofollow"">link</a>), <code>miscF</code> (<a href=""https://cran.r-project.org/web/packages/miscF"" rel=""nofollow"">link</a>), offering <code>rMultinom</code> function (<a href=""http://www.inside-r.org/packages/cran/miscF/docs/rMultinom"" rel=""nofollow"">link</a>). If your complex sampling is focused on survey data, consider reading <a href=""http://faculty.washington.edu/tlumley/survey-jsm-nup.pdf"" rel=""nofollow"">this interesting paper</a> ""Complex Sampling and R"" by Thomas Lumley.</p>

<p>If you use languages other than R, check <code>multinomial</code> function from Python's <code>numpy</code> package and, for Stata, <a href=""https://www.kai-arzheimer.com/sampling-from-a-multinomial-distribution-in-stata"" rel=""nofollow"">this blog post</a>. Finally, if you are interested in Bayesian statistics, the following two documents seems to be relevant: <a href=""https://theclevermachine.wordpress.com/tag/multivariate-sampling-methods"" rel=""nofollow"">this blog post</a> and <a href=""http://www.stat.ufl.edu/~aa/cda/bayes.pdf"" rel=""nofollow"">this survey paper</a>. Hope this helps.</p>
"
['bigdata'],Examples of the Three V's of Big Data?,"<p><strong>Volume:</strong><br>
Simply stated, big data is to big to work on one computer. This is a relative definition, as what can't work on today's computer will easily work on computers in the future.<br>
 - One Google search uses the computing power of the entire Apollo space mission.<br>
 - Excel used to hold up to 65k rows in a single spreadsheet. Now it holds over a million.</p>

<p><strong>Velocity:</strong><br>
Data is coming in extremely fast. Traditional scientific research methods of a few hundred cases could take weeks, months or even years to analyze and publish.<br>
 - Iris flower data set<br>
 - Statistical Programming Language R<br>
 - Twitter Firehose (6,000 tweets per second)      </p>

<p><strong>Variety:</strong><br>
Big Data that is contained in one specific data type or does not fit well within the format of a relational database. This data often comes in the form of unstructured text.<br>
 - Estimated 80% of all enterprise data is unstructured<br>
 - Open Data(Government)<br>
 - noSql Databases</p>

<p><strong>Iris Data Set:</strong> <a href=""https://en.wikipedia.org/wiki/Iris_flower_data_set"" rel=""nofollow"">https://en.wikipedia.org/wiki/Iris_flower_data_set</a><br>
<strong>Open Data:</strong> <a href=""https://www.data.gov/open-gov/"" rel=""nofollow"">https://www.data.gov/open-gov/</a>      </p>
"
"['bigdata', 'hadoop', 'apache-spark']",Is there a text on Apache Spark that attempts to be as comprehensive as White's Hadoop: The Definitive Guide'?,"<p><a href=""http://shop.oreilly.com/product/0636920028512.do"" rel=""nofollow"">Learning Spark: Lightning Fast Big Data Analytics</a> is a fairly comprehensive book covering the core concepts as well as the higher level components involved in the Spark stack. This is the book recommended by Databricks for their Spark Developer Certification as well.</p>

<p>If you are interested more about the use cases built using Spark, I suggest <a href=""http://shop.oreilly.com/product/0636920035091.do?green=FDE26D1E-4F6A-5328-9E23-46715C3B56EB&amp;intcmp=af-mybuy-0636920035091.IP"" rel=""nofollow"">Advanced Analytics with Spark</a></p>
"
"['machine-learning', 'sklearn', 'feature-engineering', 'encoding']",Encoding features in sklearn,"<p><code>LabelEncoder</code> converts strings to integers, but you have integers already. Thus, LabelEncoder will not help you anyway.</p>

<p>Wenn you are using your column with integers as it is, <code>sklearn</code> treats it as numbers. This means, for example, that distance between 1 and 2 is 1, distance between 1 and 4 is 3. Can you say the same about your activities (if you know the meaning of the integers)? What is the pairwise distances between, for example, ""exercise"", ""work"", ""rest"", ""leasure""? </p>

<p>If you think, that the pairwise distance between any pair of activities is 1, because those are just different activities, then <code>OneHotEncoder</code> is your choice.  </p>
"
"['visualization', 'terminology']","Why did Tufte call this a ""superbly produced duck""?","<p>If you assume <strong>duck</strong> to mean ""irrelevant decorative elements"" there are a few things that strike me as likely: (1) the ""squares"" only roughly indicate location of the target area; (2) squares represent volume, which is incongruent when overlaid on 2d geography; (3) shading of mountains/geographic features doesn't add detail.</p>

<p>Also problematic: comparison of h20 volume applied to crop types is difficult to discern using grids. Quick, which crop color uses more water?? </p>

<p><a href=""http://i.stack.imgur.com/KNGvw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KNGvw.png"" alt=""enter image description here""></a></p>

<p>If you want to <em>compare volume to crop type</em> then you should use a plain column chart, where it is easy to distinguish relative heights. This is analogous to why bar/column charts are preferable to pie charts - humans are better at comparing heights/lengths vs area/volume. If, on the other hand, the point of the graph is to <em>compare crop type irrigation by area</em>, you would want to display a column graph with regions next to each other, but grouped by crop type. </p>

<p>As far as a <strong>superbly produced</strong> duck, my guess is that the Applied Irrigation Water  is a duck, but a superb duck, at least when compared to the monstrous duck on the preceding page of the Visual Display of Quantitative Information 2nd Ed.</p>

<p><a href=""http://i.stack.imgur.com/dMmKm.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dMmKm.jpg"" alt=""horrible duck""></a></p>
"
"['hadoop', 'aws']",Processing data stored in Redshift,"<p>The new Amazon Machine Learning Service may work for you. It works directly with Redshift and might be a good way to start. <a href=""http://aws.amazon.com/machine-learning/"" rel=""nofollow"">http://aws.amazon.com/machine-learning/</a></p>

<p>If you're looking to process using EMR, then you can use Redshift's UNLOAD command to land data on S3. Spark on EMR can then access it directly without you having to pull it into HDFS.</p>

<p>Spark on EMR: <a href=""https://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923"" rel=""nofollow"">https://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923</a></p>
"
['python'],What is a good hardware setup for using Python across multiple users,"<p><strong>Is installing Python locally a good practice?</strong>  Yes, if you are going to develop in Python, it is always a good idea to have a local environment where you can break things safely.</p>

<p><strong>Is there value in setting up a Python ""server""?</strong>  Yes, but before doing so, be sure to be able to share your code with your colleagues using a <a href=""http://en.wikipedia.org/wiki/Revision_control"" rel=""nofollow"">version control system</a>.  My reasoning would be that, before you move things to a server, you can move a great deal forward by being able to test several different versions in the local environment mentioned above.  Examples of VCS are <a href=""http://git-scm.com"" rel=""nofollow"">git</a>, <a href=""https://subversion.apache.org"" rel=""nofollow"">svn</a>, and for the deep nerds, <a href=""http://darcs.net"" rel=""nofollow"">darcs</a>.</p>

<p>Furthermore, a ""Python server"" where you can deploy your software once it is integrated into a releasable version is something usually called ""<a href=""http://en.wikipedia.org/wiki/Staging_site"" rel=""nofollow"">staging server</a>"".  There is a whole philosophy in software engineering &mdash; <a href=""http://en.wikipedia.org/wiki/Continuous_integration"" rel=""nofollow"">Continuous Integration</a> &mdash; that advocates staging whatever you have in VCS daily or even on each change.  In the end, this means that some automated program, running on the staging server, checks out your code, sees that it compiles, runs all defined tests and maybe outputs a package with a version number.  Examples of such programs are <a href=""http://jenkins-ci.org"" rel=""nofollow"">Jenkins</a>, <a href=""http://buildbot.net"" rel=""nofollow"">Buildbot</a> (this one is Python-specific), and <a href=""https://travis-ci.org/recent"" rel=""nofollow"">Travis</a> (for cloud-hosted projects).</p>

<p><strong>What are the hardware requirements for such a box?</strong> None, as far as I can tell.  Whenever it runs out of disk space, you will have to clean up.  Having more CPU speed and memory will make concurrent builds easier, but there is no real minimum.</p>

<p><strong>Do I need to be concerned about any specific packages or conflicts between projects?</strong>  Yes, this has been identified as a problem, not only in Python, but in many other systems (see <a href=""http://en.wikipedia.org/wiki/Dependency_hell"" rel=""nofollow"">Dependency hell</a>).  The established practice is to keep projects isolated from each other as far as their dependencies are concerned.  This means, avoid installing dependencies on the system Python interpreter, even locally; always define a <a href=""http://virtualenv.readthedocs.org/en/latest/"" rel=""nofollow"">virtual environment</a> and install dependencies there.  Many of the aforementioned CI servers will do that for you anyway.</p>
"
"['visualization', 'regression', 'linear-regression']",How to visualize multivariate regression results,"<p>I personally like dotcharts of standardized regression coefficients, possibly with standard error bars to denote uncertainty. Make sure to standardize coefficients (and SEs!) appropriately so they ""mean"" something to your non-quantitative audience: ""As you see, an increase of 1 unit in Z is associated with an increase of 0.3 units in X.""</p>

<p>In R (without standardization):</p>

<pre><code>set.seed(1)
foo &lt;- data.frame(X=rnorm(30),Y=rnorm(30),Z=rnorm(30))
model &lt;- lm(X~Y+Z,foo)

coefs &lt;- coefficients(model)
std.errs &lt;- summary(model)$coefficients[,2]

dotchart(coefs,pch=19,xlim=range(c(coefs+std.errs,coefs-std.errs)))
lines(rbind(coefs+std.errs,coefs-std.errs,NA),rbind(1:3,1:3,NA))
abline(v=0,lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/0AwtB.png"" alt=""regression visualization""></p>
"
"['visualization', 'tableau', 'research']",Where can I find resources and papers regarding Data Science in the area of Public Health,"<p>I don't think that you will learn much about <em>data science</em> (meaning, acquire understanding and skills) by using software tools like Tableau. Such tools are targeting mainly <em>advanced users</em> (not <em>data scientists</em>), for example analysts and other subject matter experts, who use <em>graphical user interface (GUI)</em> to <em>analyze</em> and (mostly) <em>visualize</em> data. Having said that, software tools like Tableau might be good enough to perform initial phase of data science <strong>workflow</strong>: <em>exploratory data analysis (EDA)</em>.</p>

<p>In terms of data science <strong>self-education</strong>, there are several popular online courses (MOOCs) that you can choose from (most come in both free and paid versions). In addition to the one on Udacity that you've mentioned (<a href=""https://www.udacity.com/course/ud359"" rel=""nofollow"">https://www.udacity.com/course/ud359</a>), there are two data science courses on Coursera: <em>Introduction to Data Science</em> by University of Washington (<a href=""https://www.coursera.org/course/datasci"" rel=""nofollow"">https://www.coursera.org/course/datasci</a>) and a set of courses from <em>Data Science specialization</em> by Johns Hopkins University (<a href=""https://www.coursera.org/specialization/jhudatascience/1"" rel=""nofollow"">https://www.coursera.org/specialization/jhudatascience/1</a>). Note that you can take specialization's individual courses for free at your convenience. There are several other, albeit less popular, data science MOOCs.</p>

<p>In terms of <strong>data sources</strong>, I'm not sure what do you mean by ""Dummy Data"", but there is a wealth of <em>open data sets</em>, including many in the area of <em>public health</em>. You can review corresponding resources, listed on KDnuggets (<a href=""http://www.kdnuggets.com/datasets/index.html"" rel=""nofollow"">http://www.kdnuggets.com/datasets/index.html</a>) and choose ones that you're interested in. For a country-level analysis, the fastest way to obtain data is finding and visiting corresponding <em>open data</em> government websites. For example, for public health data in US, I would go to <a href=""http://www.healthdata.gov"" rel=""nofollow"">http://www.healthdata.gov</a> and <a href=""http://www.data.gov"" rel=""nofollow"">http://www.data.gov</a> (the latter - for corresponding non-medical data that you might want to include in your analysis).</p>

<p>In regard to <strong>research papers</strong> in the area of public health, I have two comments: 1) most <em>empirical research</em> in that (or any other) area IMHO can be considered a data science study/project; 2) you need to perform a <em>literature review</em> in the area or on the topic of your interest, so you're on your own in that sense.</p>

<p>Finally, a note on <strong>software tools</strong>. If you're serious about data science, I would suggest to invest some time in learning either R, or Python (if you don't know them already), as those are two most popular open source tools among data scientists nowadays. Both have a variety of feature-rich <em>development environments</em> as well as large <em>ecosystems</em> of packages/libraries and users/developers all over the world.</p>

<p>You might also find useful some of my other <strong>related answers</strong> here on Data Science StackExchange site. For example, I recommend you to read <a href=""http://datascience.stackexchange.com/a/742/2452"">this answer</a>, <a href=""http://datascience.stackexchange.com/a/843/2452"">this answer</a> and <a href=""http://datascience.stackexchange.com/a/759/2452"">this answer</a>. Good luck!</p>
"
"['tools', 'career', 'excel']",Do data scientists use Excel?,"<p>Most non-technical people often use Excel as a database replacement. I think that's wrong but tolerable. However, someone who is supposedly experienced in data analysis simply can not use Excel as his main tool (excluding the obvious task of looking at the data for the first time). That's because Excel was never intended for that kind of analysis and as a consequence of this, it is incredibly easy to make mistakes in Excel (that's not to say that it is not incredibly easy to make another type of mistakes when using other tools, but Excel aggravates the situation even more.)</p>

<p>To summarize what Excel doesn't have and is a must for any analysis:</p>

<ol>
<li>Reproducibility. A data analysis needs to be reproducible.</li>
<li>Version control. Good for collaboration and also good for reproducibility. Instead of using xls, use csv (still very complex and has lots of edge cases, but csv parsers are fairly good nowadays.) </li>
<li>Testing. If you don't have tests, your code is broken. If your code is broken, your analysis is worse than useless. </li>
<li>Maintainability.</li>
<li>Accuracy. Numerical accuracy, accurate date parsing, among others are really lacking in Excel.</li>
</ol>

<p>More resources:</p>

<p><a href=""http://www.eusprig.org/horror-stories.htm"" rel=""nofollow"">European Spreadsheet Risks Interest Group - Horror Stories</a></p>

<p><a href=""http://lemire.me/blog/archives/2014/05/23/you-shouldnt-use-a-spreadsheet-for-important-work-i-mean-it/"" rel=""nofollow"">You shouldn’t use a spreadsheet for important work (I mean it)</a></p>

<p><a href=""http://www.forbes.com/sites/timworstall/2013/02/13/microsofts-excel-might-be-the-most-dangerous-software-on-the-planet/"" rel=""nofollow"">Microsoft's Excel Might Be The Most Dangerous Software On The Planet</a></p>

<p><a href=""http://randyzwitch.com/excel-destroys-data/"" rel=""nofollow"">Destroy Your Data Using Excel With This One Weird Trick!</a></p>

<p><a href=""http://www.win-vector.com/blog/2014/11/excel-spreadsheets-are-hard-to-get-right/"" rel=""nofollow"">Excel spreadsheets are hard to get right</a></p>
"
['bigdata'],Learning resources for data science to win political campaigns?,"<p>This is an interesting and <a href=""http://www.datanami.com/2014/11/04/data-analytics-shaped-election-day-2014"" rel=""nofollow"">relevant question</a>. I think that from <em>data science perspective</em>, it should not be, in principle, any different from any other similar data science tasks, such as prediction, forecasting or other analyses. Similarly to any data science work, the quality of applying data science to politics very much depends on understanding not only data science <strong>approaches, methods and tools</strong>, but, first and foremost, the <strong>domain</strong> being analyzed, that is politics domain.</p>

<p>Rapidly rising popularity of data science and machine learning (ML), in general, certainly has a significant impact on particular verticals and politics is not an exception. This impact can be seen not only in increased <strong>research interest</strong> in applying data science and ML to political science (for example, see <a href=""http://imai.princeton.edu/talk/files/SML11.pdf"" rel=""nofollow"">this presentation</a>, <a href=""http://www.stat.columbia.edu/~jakulin/Politics"" rel=""nofollow"">this paper</a>, <a href=""http://stanford.edu/~jgrimmer/bd_2.pdf"" rel=""nofollow"">this overview paper</a> and <a href=""http://www.oxfordjournals.org/our_journals/polana/virtualissue4.html"" rel=""nofollow"">this whole virtual/open issue in a prominent Oxford journal</a>), but in <strong>practical applications</strong>. Moreover, a new term - <strong>political informatics</strong> or <em>poliInformatics</em> or <em>poli-informatics</em> - has been coined to name an <a href=""http://poliinformatics.org"" rel=""nofollow"">interdisciplinary field</a>, which stated goal is to study and use data science, big data and ML in the government and politics domains. As I've said earlier, the interest in applying data science to politics goes beyond research and often results in politics-focused startups, such as <a href=""https://gigaom.com/2012/06/25/can-a-big-data-product-level-the-playing-field-in-politics"" rel=""nofollow"">PoliticIt</a> or <a href=""http://www.parabellumlabs.com"" rel=""nofollow"">Para Bellum Labs</a>. Following the unfortunate, but established trend in startup ecosystem, many of those ventures fail. For example, read <a href=""http://www.poliana.com"" rel=""nofollow"">the story</a> of one of such startups.</p>

<p>I am pretty sure that you will be able to find neither <em>proprietary algorithms</em> that political startups or election data science teams used and use, nor the their <strong>data sets</strong>. However, I am rather positive that you can get some understanding about typical data sets as well as data collection and analysis methods via the resources that I have referenced above. Hope this helps.</p>
"
"['distributed', 'state-of-the-art']",Possibility of working on KDDCup data in local system,"<p>I think that you have, at least, the following <strong>major options</strong> for your <em>data analysis scenario</em>:</p>

<ol>
<li><p>Use <em>big data-enabling</em> <code>R</code> packages on your <strong>local system</strong>. You can find most of them via the corresponding <em>CRAN Task View</em> that I reference in <a href=""http://datascience.stackexchange.com/a/780/2452"">this answer</a> (see point #3).</p></li>
<li><p>Use the same packages on a <strong>public cloud</strong> infrastructure, such as <em>Amazon Web Services (AWS) EC2</em>. If your analysis is non-critical and tolerant to potential restarts, consider using <em>AWS Spot Instances</em>, as their pricing allows for significant financial savings.</p></li>
<li><p>Use the above mention <strong>public cloud</strong> option with <code>R</code> <em>standard platform</em>, but on <strong>more powerful instances</strong> (for example, on AWS you can opt for memory-optimized EC2 instances or general purpose on-demand instances with more memory).</p></li>
</ol>

<p>In some cases, it is possible to <strong>tune</strong> a <em>local system</em> (or a <em>cloud on-demand instance</em>) to enable <code>R</code> to work with big(ger) data sets. For some help in this regard, see <a href=""http://datascience.stackexchange.com/a/3731/2452"">my relevant answer</a>.</p>

<p>For both above-mentioned cloud (AWS) options, you can find more convenient to use <code>R</code>-focused <strong>pre-built VM images</strong>. See <a href=""http://datascience.stackexchange.com/a/4927/2452"">my relevant answer</a> for details. You may also find useful <a href=""https://github.com/onurakpolat/awesome-bigdata"" rel=""nofollow"">this excellent comprehensive list of big data frameworks</a>.</p>
"
['machine-learning'],Correlation threshold for Neural Network features selection,"<p>Given non-linearity of neural networks, I believe correlation analysis isn't a good way to estimate importance of variables. For example, imagine that you have 2 input variables - <code>x1</code> and <code>x2</code> - and following conditions hold: </p>

<ul>
<li><code>cor(x2, y) = 1</code> if <code>x1 = 1</code></li>
<li><code>cor(x2, y) = 0</code> otherwise</li>
<li><code>x1 = 1</code> in 10% of cases</li>
</ul>

<p>That is, <code>x2</code> is a very good predictor for <code>y</code>, but only given that <code>x1 = 1</code>, which is the case only in 10% of data. Taking into account correlations of <code>x1</code> and <code>x2</code> separately won't expose this dependency, and you will most likely drop out both variables. </p>

<p>There are other ways to perform feature selection, however. Simplest one is to train your model with all possible sets of variables and check the best subset. This is pretty inefficient with many variables, though, so many ways to improve it exist. For a good introduction in best subset selection see chapter 6.1 of <a href=""http://www-bcf.usc.edu/~gareth/ISL/"" rel=""nofollow"">Introduction to Statistical Learning</a>. </p>
"
['time-series'],Finding unpredictability or uncertainty in a time series,"<p>Since you have looked at Kolmogorov-Smirnov and Shannon entropy measures, I would like to suggest some other hopefully relevant options. First of all, you could take a look at the so-called <a href=""http://en.wikipedia.org/wiki/Approximate_entropy"">approximate entropy $ApEn$</a>. Other potential statistics include <em>block entropy</em>, <em>T-complexity</em> (<em>T-entropy</em>) as well as <em>Tsallis entropy</em>: <a href=""http://members.noa.gr/anastasi/papers/B29.pdf"">http://members.noa.gr/anastasi/papers/B29.pdf</a></p>

<p>In addition to the above-mentioned potential measures, I would like to suggest to have a look at available statistics in <em>Bayesian inference-based</em> model of <em>stochastic volatility</em> in time series, implemented in <code>R</code> package <code>stochvol</code>: <a href=""http://cran.r-project.org/web/packages/stochvol"">http://cran.r-project.org/web/packages/stochvol</a> (see detailed <a href=""http://cran.r-project.org/web/packages/stochvol/vignettes/article.pdf"">vignette</a>). Such statistics of uncertainty include <em>overall level of volatility</em> $\mu$, <em>persistence</em> $\phi$ and <em>volatility of volatility</em> $\sigma$: <a href=""http://simpsonm.public.iastate.edu/BlogPosts/btcvol/KastnerFruwhirthSchnatterASISstochvol.pdf"">http://simpsonm.public.iastate.edu/BlogPosts/btcvol/KastnerFruwhirthSchnatterASISstochvol.pdf</a>. A <strong>comprehensive</strong> <em>example</em> of using stochastic volatility model approach and <code>stochvol</code> package can be found in the excellent blog post <a href=""http://www.themattsimpson.com/2014/02/22/exactly-how-volatile-is-bitcoin"">""Exactly how volatile is bitcoin?""</a> by Matt Simpson.</p>
"
"['r', 'visualization', 'dimensionality-reduction']",Optimal projection for data visualization,"<p>There are a few interesting plots and transformations you could start with, each dependent upon the purpose of your analysis. Below are some first steps I might take.</p>

<ol>
<li><p>If simply visually looking for clusters:<br>
If clusters are what you are after, then I would recommend applying a Principal Component Analysis to the dataset and then plotting the dataset with the first 2 principal components as the axes. However, the major downside to PCA is that you will have to go ""unpack"" the principal components to find the original variables. In other words, you'll be may be able to identify cool clusters, but it will be a little harder to tie the findings back to your 32 variables.</p></li>
<li><p>If looking for quick relationships between variables to build on:<br>
With ""only"" 32 variables you could do a pairwise plot. However, a smarter way may be to first identify the relationships mathematically (e.g. correlation) and then plotting those variables.</p></li>
<li><p>If looking for quick relationships in each variable to build on:
Or, look at 32 histograms to start off with. Look out for clear bi-modal (or more) to begin piecing together an understanding of how your variables may contribute to an unsupervised model. If you end up look at 32 unimodal histograms then you can conclude early that no matter how you cluster, you will simply end up with a blob.</p></li>
</ol>

<p>Actually, in usual analytics workflows I would go 3 > 2 > 1. But if I was fishing for clusters and just wanted to see if clusters will appear or not, PCA would be a good shortcut.</p>

<p>Also, feel free to sample your dataset. 90k points on your screen will likely do more harm than good.</p>
"
"['machine-learning', 'python', 'scikit', 'decision-trees']","Extract the ""path"" of a data point through a decision tree in sklearn","<p>Looks like this is easier to do in R, using the <code>rpart</code> library in combination with the <code>partykit</code> library. I'd ideally like to find a way to do this in python, but here's the code, for anyone who is interested (taken from <a href=""http://stackoverflow.com/questions/29618490/get-decision-tree-rule-path-pattern-for-every-row-of-predicted-dataset-for-rpart"">here</a>):</p>

<pre><code>pathpred &lt;- function(object, ...){
    ## coerce to ""party"" object if necessary
    if(!inherits(object, ""party"")) object &lt;- as.party(object)

    ## get standard predictions (response/prob) and collect in data frame
    rval &lt;- data.frame(response = predict(object, type = ""response"", ...))
    rval$prob &lt;- predict(object, type = ""prob"", ...)

    ## get rules for each node
    rls &lt;- partykit:::.list.rules.party(object)

    ## get predicted node and select corresponding rule
    rval$rule &lt;- rls[as.character(predict(object, type = ""node"", ...))]

    return(rval)
}
</code></pre>

<p>Illustration using the iris data and rpart():</p>

<pre><code>library(""rpart"")
library(""partykit"")
rp &lt;- rpart(Species ~ ., data = iris)
rp_pred &lt;- pathpred(rp)
rp_pred[c(1, 51, 101), ]
</code></pre>

<p>Yielding,</p>

<pre><code>       response prob.setosa prob.versicolor prob.virginica
 1       setosa  1.00000000      0.00000000     0.00000000
 51  versicolor  0.00000000      0.90740741     0.09259259
 101  virginica  0.00000000      0.02173913     0.97826087
                                           rule
 1                          Petal.Length &lt; 2.45
 51   Petal.Length &gt;= 2.45 &amp; Petal.Width &lt; 1.75
 101 Petal.Length &gt;= 2.45 &amp; Petal.Width &gt;= 1.75
</code></pre>

<p>Which looks to be something I could at least use to derive shared parent node information.</p>
"
"['machine-learning', 'recommendation', 'evaluation']",Difference between using RMSE and nDCG to evaluate Recommender Systems,"<p>nDCG is used to evaluate a golden ranked list (typically human judged) against your output ranked list. The more is the correlation between the two ranked lists, i.e. the more similar are the ranks of the relevant items in the two lists, the closer is the value of nDCG to 1.</p>

<p>RMSE (Root Mean Squared Error) is typically used to evaluate regression problems where the output (a predicted scalar value) is compared with the true scalar value output for a given data point.</p>

<p>So, if you are simply recommending a score (such as recommending a movie rating), then use RMSE. Whereas, if you are recommending a list of items (such as a list of related movies), then use nDCG.  </p>
"
"['machine-learning', 'classification', 'bioinformatics', 'data']",What kind of research can be done with genomic data?,"<ul>
<li>Determine the function of genes and the elements that regulate genes
throughout the genome.</li>
<li>Find variations in the DNA sequence among people and determine their
significance. The most common type of genetic variation is known as a
single nucleotide polymorphism or SNP (pronounced “snip”).  These
small differences may help predict a person’s risk of particular
diseases and response to certain medications.</li>
<li>Discover the 3-dimensional structures of proteins and identify their
functions.</li>
<li>Explore how DNA and proteins interact with one another and with the
environment to create complex living systems.</li>
<li>Develop and apply genome-based strategies for the early detection,
diagnosis, and treatment of disease.</li>
<li>Sequence the genomes of other organisms, such as the rat, cow, and
chimpanzee, in order to compare similar genes between species.</li>
<li>Develop new technologies to study genes and DNA on a large scale and
store genomic data efficiently.</li>
<li>Continue to explore the ethical, legal, and social issues raised by
genomic research.</li>
<li><a href=""http://ghr.nlm.nih.gov/handbook/genomicresearch?show=all"" rel=""nofollow"">Source</a></li>
</ul>
"
"['neuralnetwork', 'data-stream-mining']",Designing a ConvNet to facilitate game playing,"<p>I've recently started using OpenCV's python implementation, and I found some good OpenCV tutorials on this website: <a href=""http://www.pyimagesearch.com/"" rel=""nofollow"">http://www.pyimagesearch.com/</a> that I really liked. OpenCV allows you to do Haar Cascades for fast facial recognition (by default it doesn't use a convoluted neural network but an optimized implementation of Ada Boosting that evaluates frames in stages for faster processing). OpenCV converts each frame into a multidimensional numpy tensor/matrix that you can then feed into into your ML algorithm (e.g., in TensorFlow or some other library), although I think most people just use the built-in OpenCV face classifiers. In any case, I believe OpenCV can process up to 70 frames per second, so it should be fast enough for you. </p>

<p>The original paper that invented Haar Cascades:
<a href=""https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf"" rel=""nofollow"">https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf</a></p>

<p>The OpenCV documentation that further explains Haar Cascades:
<a href=""http://docs.opencv.org/3.1.0/d7/d8b/tutorial_py_face_detection.html#gsc.tab=0"" rel=""nofollow"">http://docs.opencv.org/3.1.0/d7/d8b/tutorial_py_face_detection.html#gsc.tab=0</a></p>
"
['r'],group_by: How to Transform my Data,"<pre><code>ManufMktConc &lt;- read.csv(""Downloads/Manufacturing.csv"",
                         stringsAsFactors = FALSE)

ManufMktConc %&lt;&gt;% 
  tbl_df %&gt;% 
  slice(-1) %&gt;% 
  filter(CONCENFI.display.label!=""All companies"") %&gt;%
  select(-GEO.id,-GEO.id2,-GEO.display.label,-COMPANY,-RCPTOT,-YEAR.id)

##  use melt and cast from reshape2 
require(reshape2)

# melt makes the data set tall and thin using id variables and measure variables
# 
ManufMktConc_molten &lt;- as.data.frame(ManufMktConc) %&gt;%
  melt(id.vars=1:4, measure.vars=5:6) %&gt;%
  filter(value!=""X"")
ManufMktConc_molten[1:5,]

    NAICS.id       NAICS.display.label CONCENFI.id CONCENFI.display.label  variable value
1      311        Food manufacturing         856    4 largest companies CCORCPPCT  16.3
2      311        Food manufacturing         857    8 largest companies CCORCPPCT  24.2
3      311        Food manufacturing         858   20 largest companies CCORCPPCT  38.4
4      311        Food manufacturing         859   50 largest companies CCORCPPCT  50.9
5     3111 Animal food manufacturing         856    4 largest companies CCORCPPCT  30.2



# make a new column with the eventual column header. (Note this is different that your example)
ManufMktConc_molten$label &lt;- paste0(ManufMktConc_molten$variable,
                                    trimws(substr(ManufMktConc_molten$CONCENFI.display.label,1,2)))

# cast it into multiple columns (something like a pivot in Excel).
ManufMktConc_result  &lt;- ManufMktConc_molten %&gt;%
  cast(NAICS.id + NAICS.display.label ~ label) %&gt;%
  select(1,2,4,6,3,5,7)     ## reorder columns

ManufMktConc_result[1:5,]

      NAICS.id             NAICS.display.label CCORCPPCT4 CCORCPPCT8 CCORCPPCT20 CCORCPPCT50 VSHERFI50
    1      311              Food manufacturing       16.3       24.2        38.4        50.9     110.7
    2     3111       Animal food manufacturing       30.2       40.7        57.8        71.5     368.6
    3    31111       Animal food manufacturing       30.2       40.7        57.8        71.5     368.6
    4   311111  Dog and cat food manufacturing       67.8       80.6        89.6        96.5    2019.4
    5   311119 Other animal food manufacturing       24.3       36.2        51.5        68.2     228.3
</code></pre>
"
"['azure-ml', 'powerbi']",Does MS PowerBI directly connect with Azure's ML workbench?,"<p>Yes, they can connect natively. You can manage data and then put it all in different services like it is showed next. As you can see you can use SQL database, blob storate and also PowerBI.
<a href=""http://i.stack.imgur.com/gbSIy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gbSIy.png"" alt=""enter image description here""></a></p>

<p><a href=""https://azure.microsoft.com/en-gb/documentation/articles/stream-analytics-power-bi-dashboard/"" rel=""nofollow"">Here</a> you can find a tutorial on how to do streaming analytics with Azure and PowerBI.</p>
"
['similarity'],Similarity measure based on multiple classes from a hierarchical taxonomy?,"<p>While I don't have enough expertise to advise you on selection of the best <em>similarity measure</em>, I've seen a number of them in various papers. The following <strong>collection</strong> of research papers hopefully will be useful to you in determining the optimal measure for your research. Please note that I intentionally included papers, using both <em>frequentist</em> and <em>Bayesian</em> approaches to hierarchical classification, including class information, for the sake of more <em>comprehensive</em> coverage.</p>

<p><strong>Frequentist approach:</strong></p>

<ul>
<li><a href=""http://arxiv.org/pdf/cmp-lg/9709008.pdf"">Semantic similarity based on corpus statistics and lexical taxonomy</a></li>
<li><a href=""http://www.sciencedirect.com/science/article/pii/S0048733313000115"">Can’t see the forest for the leaves: Similarity and distance measures for hierarchical taxonomies with a patent classification example</a> (also see <a href=""http://mcnameephd.pbworks.com/w/page/11389167/Research%20Policy%20-%20Similarity%20and%20distance%20measures%20for%20hierarchical%20taxonomies"">additional results and data</a>)</li>
<li><a href=""http://research.microsoft.com/pubs/192155/hier_embd.pdf"">Learning hierarchical similarity metrics</a></li>
<li><a href=""http://arxiv.org/ftp/arxiv/papers/1211/1211.4709.pdf"">A new similarity measure for taxonomy based on edge counting</a></li>
<li><a href=""http://www.siam.org/meetings/sdm01/pdf/sdm01_22.pdf"">Hierarchical classification of real life documents</a></li>
<li><a href=""http://users.cis.fiu.edu/~taoli/pub/Li-jiis.pdf"">Hierarchical document classification using automatically generated hierarchy</a></li>
<li><a href=""http://www.cs.ucla.edu/~weiwang/paper/SSDBM09.pdf"">Split-Order distance for clustering and classification hierarchies</a></li>
<li><a href=""http://ccis2k.org/iajit/PDF/vol.8,no.3/791.pdf"">A hierarchical k-NN classifier for textual data</a></li>
</ul>

<p><strong>Bayesian approach:</strong></p>

<ul>
<li><a href=""http://ba.stat.cmu.edu/journal/2007/vol02/issue01/shahbaba.pdf"">Improving classification when a class hierarchy is available using a hierarchy-based prior</a></li>
<li><a href=""http://gfx.cs.princeton.edu/pubs/DeCoro_2007_BAF/bayesgenre.pdf"">Bayesian aggregation for hierarchical genre classification</a></li>
<li><a href=""http://oro.open.ac.uk/12991/1/Hierarchical_Classification.pdf"">Hierarchical classification for multiple, distributed web databases</a></li>
</ul>
"
"['statistics', 'data-cleaning']",Correcting Datasets with artificially low starting values,"<p>Yes, the general idea is to add a baseline small count to every category. The technical term for this is <a href=""http://en.wikipedia.org/wiki/Additive_smoothing"" rel=""nofollow"">Laplace smoothing</a>. Really it's not so much of a hack, as encoding the idea that you think there is some (uniform?) prior distribution of the events occurring.</p>
"
"['machine-learning', 'python', 'neuralnetwork', 'nlp', 'deep-learning']",Approaches for implementing Domain specific Question answering System,"<p>While the actual ""best system"" depends heavily on a number of factors including your goals and your resources, it is possible to discuss some general pros and cons to each system.</p>

<p><strong>1. IR Based approaches:</strong> Information retrieval approaches allow the algorithm to make effective use of what we might call ""explicit knowledge"" or hardcoded facts. They can work very well when the answer to the query exists in some form already and where the algorithm simply needs to find this answer amongst a set of all the other possible answers. Such systems typically rely on either an explicit dataset of possible answers or facts, or by treating the web as such a dataset and querying it in some way. IR systems are often the only reasonable choice when the answer cannot be generated from abstract principles. For example, if we built a system to answer trivia questions about movie stars, we would need to have explicit knowledge of which features are associated with which movie stars (i.e. Brad Pitt has brown hair and starred in <em>Fight Club</em>.) No machine learning algorithm can be expected to generate this without the knowledge already present (although machine learning systems with access to such knowledge may serve as effective information retrieval systems.) Due to the external nature of the data with respect to the algorithm, IR systems tend to be very effective for large and rapidly changing data sets, which would otherwise require retraining of the machine learning model or manual reconfiguration of the ontology to handle. It is also worth noting that more complex information retrieval systems may include other systems such as NLP engines to translate the data (ex. natural language text) into a form usable by the IR system.</p>

<p><strong>2. Ontology based approaches:</strong> Ontology methods take the most time to implement and are usually the most finicky. They require the iterative hypothesizing of a structured representation of the domain, testing of the representation, and modification of the representation based on discovered flaws. For complex real-world domains with many possible contingencies, this process can take a very long time and be quite frustrating. Because everything is hard-coded, even the best ontology-based systems display a relative degree of inflexibility compared to their machine learning and IR counterparts. Ontologies can be effective, however, when you do not have access to a data set for information retrieval or machine learning training, as the domain knowledge are hard-coded into ontology systems by hand. They can also be useful when the domain is relatively well-structured and where it is important to understand and plan every decision made by the system such as in automated attendant phone systems.</p>

<p><strong>3. Machine learning based approaches:</strong> As machine learning approaches extract patterns from data, they are relatively robust to subtleties present in complex domains that were not explicitly built into an algorithm, model, or ontology. They work best when a prediction needs to be made about which ""class"" or category a given input (or query in your case) belongs to, or when a quantitative prediction needs to be produced from data. For example, if I have a large collection of animal pictured labeled with the type of animal present in these images, I can train a machine learning model to predict which animal type is present in future images, as long as the model has seen enough pictures of this animal that it can extract the patterns in the image that correspond to the image likely representing a lion or a tiger or a bear, etc. This works because there are properties in the image itself that predict which animal it represents and these patterns can be extracted from labeled data (there are also machine learning algorithms that pick out patterns in unlabeled data called unsupervised algorithms, but these are not relevant for your domain of interest.)  Traditionally, machine learning systems are relatively poor at modeling domains where the answer depends not on recognizing a pattern, but having access to knowledge like in the movie stars example above. Machine learning systems are also great because they require relatively little manpower to implement. As you mentioned, they also require access to a dataset, which may need to be quite large depending on the complexity of the domain.</p>

<p>We can <em>roughly</em> summarize the advantages and disadvantages by ranking the methods according to a few criteria:
<a href=""http://i.stack.imgur.com/9seU8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9seU8.png"" alt=""enter image description here""></a></p>

<p>All of the best systems (ex: IBM Watson) use a hybrid approach, taking advantage of each method for its relative strengths and substituting other methods to address their weaknesses. Depending on the performance you want, a QA system can be built by a single person with some knowledge of any of the above, or may require a team of upwards of 100 engineers. </p>
"
"['beginner', 'self-study']",How to self-learn data science?,"<p>Welcome to the site, Martin! That's a pretty broad question, so you're probably going to get a variety of answers. Here's my take.</p>

<ol>
<li><em>Data Science</em> is an interdisciplinary field generally thought to combine classical statistics, machine learning, and computer science (again, this depends on who you ask, but other might include business intelligence here, and possible information visualization or knowledge discovery as well; for example, <a href=""https://en.wikipedia.org/wiki/Data_science"">the wikipedia article on data science</a>). A good data scientist is also skilled at picking up on the domain-specific characteristics of the domain in which they working, as well. For example, a data scientist working on analytics for hospital records is much more effective if they have a background in Biomedical Informatics.</li>
<li>There are many options here, depending on the type of analytics you're interested in. <a href=""https://www.coursera.org/learn/machine-learning"">Andrew Ng's coursera course is the first resource mentioned by most</a>, and rightly so. If you're interested in machine learning, that's a great starting place. If you want an in-depth exploration of the mathematics involved, <a href=""http://rads.stackoverflow.com/amzn/click/0387848576"">Tibshirani's The Elements of Statistical Learning</a> is excellent, but fairly advanced text. There are many online courses available on coursera in addition to Ng's, but you should select them with a mind for the type of analytics you want to focus on, and/or the domain in which you plan on working.</li>
<li><a href=""https://www.kaggle.com/"">Kaggle</a>. Start with kaggle, if you want to dive in on some real-world analytics problems. Depending on your level of expertise, it might be good to start of simpler, though. <a href=""https://projecteuler.net/"">Project Euler</a> is a great resource for one-off practice problems that I still use as warm-up work.</li>
<li>Again, this probably depends on the domain you wish to work in. However, I know Coursera offers a data science certificate, if you complete a series of data science-related courses. This is probably a good place to start.</li>
</ol>

<p>Good luck! If you have any other specific questions, feel free to ask me in the comments, and I'll do my best to help!</p>
"
['neuralnetwork'],How are neural nets related to Fourier transforms?,"<p>They are not related in any meaningful sense. Sure, you can use them both to extract features, or do any number of things, but the same can be said about a many techniques. I would have asked ""what kind of neural network?"" to see if the interviewer had something specific in mind.</p>
"
"['data-mining', 'r']",Error using twitter R package's userTimlien,"<p>It seems to be working well on my configuration:</p>

<p>Ubuntu Vivid and R:</p>

<pre><code>&gt; sessionInfo()
R version 3.1.2 (2014-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] twitteR_1.1.8

loaded via a namespace (and not attached):
 [1] bit_1.1-12     bit64_0.9-4    bitops_1.0-6   DBI_0.3.1      httr_0.6.1    
 [6] magrittr_1.5   RCurl_1.95-4.6 rjson_0.2.15   stringi_0.4-1  stringr_1.0.0 
[11] tools_3.1.2  
</code></pre>

<p>Maybe you should update packages versions?</p>
"
"['machine-learning', 'classification', 'clustering', 'decision-trees', 'naive-bayes-classifier']",How to identify similar data points?,"<p>Well, thats a question that institutes at univiersities, among others, are trying to solve since several decades and where millions of money currencies are spent in each country every year.</p>

<p>Short answer: for now, nothing <em>general</em> is known to perform good. </p>

<p>Long answer: it depends completely on your image domain. Do you want to recognize suns in a picture? That possible up to some false discovery rates. Do you want recognize familiar faces? Possible, see automated passport controls at airports. Emotion recognition? Good luck with that! Grouping by persons? Possible, but statues will be included. etc...</p>

<p>Something really general:  Grouping by colors? Totally viable! But will not mean much. Something that includes some hint to the structure? Well, the easiest way is to compute color histograms and use them as feature vectors for (like suggested) nearest neighbor calculation - (watch out for normalization, color space, and other sources of information distortion)</p>

<p>Basic answer: take some Computer Vision courses either online (e.g. <a href=""https://youtu.be/skaQfPQFSyY?list=PL7v9EfkjLswLfjcI-qia-Z-e3ntl9l6vp"" rel=""nofollow"">https://youtu.be/skaQfPQFSyY?list=PL7v9EfkjLswLfjcI-qia-Z-e3ntl9l6vp</a>) </p>

<p>There is also an intersting TED talk to that topic <a href=""https://youtu.be/40riCqvRoMs"" rel=""nofollow"">https://youtu.be/40riCqvRoMs</a></p>

<p>On the other side: don't let yourself down! Understand the #basics of data processing and break the structures! Develop that algorithm we are all hoping to use some day!</p>
"
"['bigdata', 'scalability']",Memory efficient structure for membership checking without false positive,"<p>For web crawler scale why not use a distributed database like Apache Cassandra? Lookups on indexes are efficient and no false positives.</p>
"
"['r', 'predictive-modeling', 'random-forest', 'probability']",What is the appropriate evaluation metric for RandomForest with probability in R?,"<p>You should select some threshold, let's say 0.5 and treat customers with probability below threshold as not buy and above as buy. Based on this you can compute accuracy of your model. You can also check the ROC curve of the model. You can check various metric for binary classifier <a href=""https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers"" rel=""nofollow"">here</a></p>
"
"['deep-learning', 'convnet']",LeNet for Convolution network?,"<p><a href=""http://yann.lecun.com/exdb/lenet/"" rel=""nofollow"">LeNet</a> is a family (LeNet-1, LeNet-4, LeNet-5) of convolutional neural network designed by Yann LeCun et al. The name is a play on his name and the French language, where ""le"" means ""the"", hence LeNet means ""the network"". I believe it was originally devised to recognize handwritten numbers on checks (cheques). LeNet is only one early instance of a convolutional neural network; many others exist today.</p>
"
"['machine-learning', 'neuralnetwork', 'backpropagation']",How to update bias and bias's weight using backpropagation algorithm,"<p>There should be a bias weight for each virtual neuron as it controls the threshold at which the neuron responds to combined input. So if your hidden layer has 100 neurons, that is 100 bias weights for that layer. Same applies to each layer.</p>

<p>There are usually two different approaches taken when implementing bias. You can do one or the other:</p>

<ol>
<li><p>As a separate vector of bias weights for each layer, with different (slightly cut down) logic for calculating gradients.</p></li>
<li><p>As an additional column in the weights matrix, with a matching column of 1's added to input data (or previous layer outputs), so that the exact same code calculates bias weight gradients and updates as for connection weights.</p></li>
</ol>

<p>In both cases, you only do backpropagation calculation from neuron activation deltas to the bias weight deltas, you don't need to calculate the ""activation"" delta for bias, because it is not something that can change, it is always 1.0. Also the bias does not contribute deltas back further to anything else.</p>
"
"['sklearn', 'linear-regression', 'weighted-data']",Sklearn Linear Regression examples,"<p>There is an application of tf-idf on the <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">sklearn website</a>. </p>

<p>sklearn handles sparse matrices for you, so I wouldn't worry about it too much:</p>

<blockquote>
  <p>Fortunately, most values in X will be zeros since for a given document less than a couple thousands of distinct words will be used. For this reason we say that bags of words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.
  scipy.sparse matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures.</p>
</blockquote>

<p>Regarding your point about inserting the weight, I guess you have already performed tf-idf on your training corpus, but you don't know how to apply it to your test corpus? If so you could so as follows (taken from above link)</p>

<pre><code>from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(data)

# Perform tf-idf
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

docs_new = ['God is love', 'OpenGL on the GPU is fast'] # New test documents
X_test_counts = count_vect.transform(docs_new) # Count vectorise the new documents
X_test_tfidf = tfidf_transformer.transform(X_new_counts) # transfrom the test counts 
</code></pre>
"
"['machine-learning', 'text-mining', 'word-embeddings', 'word2vec']",Doc2Vec - How to label the paragraphs (gensim),"<p>Both are possible. You can give every document a unique ID (such as a sequential serial number) as a doctag, or a shared string doctag representing something else about it, or both at the same time. </p>

<p>The TaggedDocument constructor takes a list of tags. (If you happen to limit yourself to to plain ints ascending from 0, the Doc2Vec model will use those as direct indexes into its backing array, and you'll save a lot of memory that would otherwise be devoted to a string -> index lookup, which could be important for large datasets. But you can use string doctags or even a mixture of int and string doctags.)</p>

<p>You'll have to experiment with what works best for your needs. </p>

<p>For some classification tasks, an approach that's sometimes worked better than I would have expected is skipping per-text IDs entirely, and just training the Doc2Vec model with known-class examples, with the desired classes as the doctags. You then get 'doc vectors' just for the class doctags – not every document – a potentially much smaller model. Later inferring vectors for new texts results in vectors meaningfully close to related class doc vectors.</p>
"
"['python', 'nlp']",Sentiment retriving from text (Russian),"<p>If you are seeking a working solution, I know of an API that supports many languages, including Russian: <a href=""https://indico.io/docs#sentiment"" rel=""nofollow"">indico.io Text Analysis sentiment()</a></p>

<pre><code>&gt;&gt;&gt; import indicoio
&gt;&gt;&gt; indicoio.config.api_key = YOUR_API_KEY
&gt;&gt;&gt; indicoio.sentiment(u""Это круто, убивает!  Хочу."", language='ru')
0.6978093435482927
&gt;&gt;&gt; indicoio.sentiment(u""Ты кто такой?  Давай досвидания"", language='ru')
0.13258737684773209
</code></pre>

<p>Note that the <code>language</code> parameter is optional</p>

<p>(Obviously it's not a lib, but they offer a Python client and their free tier is generous enough.)</p>
"
"['feature-extraction', 'sequence']",Unsupervised sequence identification,"<p>You may use <code>ngram</code>s to get the frequent sequences of you events.</p>

<p>Here a little example</p>

<pre><code> library(tau)
 seq &lt;- ""ababcdcde""
 textcnt(seq, method=""ngram"", n=3L, decreasing=TRUE)

   _   a  ab   b   c  cd   d  _a _ab aba abc  ba bab  bc bcd cdc cde  dc dcd  de de_   e  e_ 
   2   2   2   2   2   2   2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
</code></pre>

<p>Select the <em>longest</em> and <em>most frequent</em> ngram, here <strong>ab</strong> and <strong>cd</strong>.
You may also trade off the length and frequency to get maximal compression. The <strong>n</strong> parameter limits the length of the ngram.</p>
"
"['data-mining', 'classification', 'text-mining']",Error Analysis for misclassification of text documents,"<p>It seems to me that both of your questions could be answered by storing the retrieved neighbours on your test set and giving them a thorough analysis. Assuming you are using a unigram + tf-idf text representation and a cosine similarity distance metric for your K-NN retrieval, it would be trivial once you have a classified document to display the K neighbours and analyze their common unigrams and their respective tf-idf weights in order to see what influenced the classification. Moreover, doing it on your misclassified documents could help you understand which features caused the error.</p>

<p>I'd be interested to know if there is a more systematized approach to those issues.</p>
"
"['hadoop', 'r']",What are R's memory constraints?,"<p>As Konstantin has pointed, R performs all its computation in the system's memory i.e. RAM. Hence, RAM capacity is a very important constraint for computation intensive operations in R. Overcoming this constraint, data is being stored these days in HDFS systems, where data isn't loaded onto memory and program is run instead, program goes to the data and performs the operations, thus overcoming the memory constraints.  RHadoop (<a href=""https://github.com/RevolutionAnalytics/RHadoop/wiki"">https://github.com/RevolutionAnalytics/RHadoop/wiki</a>) is the connector you are looking for.</p>

<p>Coming to the impact on algorithms which are computation intensive, Random Forests/Decision Trees/Ensemble methods on a considerable amount of data (minimum 50,000 observations in my experience) take up a lot of memory and are considerably slow. To speed up the process, parallelization is the way to go and parallelization is inherently available in Hadoop! That's where, Hadoop is really efficient.</p>

<p>So, if you are going for ensemble methods which are compute intensive and are slow, you would want to try out on the HDFS system which gives a considerable performance improvement.</p>
"
"['r', 'databases', 'efficiency', 'tools']",Running an R script programmatically,"<p>For windows, use the task scheduler to set the task to run for example daily at 4:00 AM</p>

<p>It gives you many other options regarding frequency etc.
<a href=""http://en.wikipedia.org/wiki/Windows_Task_Scheduler"">http://en.wikipedia.org/wiki/Windows_Task_Scheduler</a></p>
"
"['machine-learning', 'classification', 'random-forest']",What's a good machine learning algorithm for low frequency trading?,"<p>Random forests, GBM or even the newer and fancier xgboost are not the best candidates for binary classification (predicting ups and down) of stocks predictions or forex trading or at least not as the main algorithm. The reason is that, for this particular problem, they require a huge amount of trees (and tree depth in case of GBM or xgboost) to obtain reasonable accuracy (Breiman suggested using at least 5000 trees and to ""not be stingy"" and in fact his main ML paper on RF he used 50,000 trees per run).</p>

<p>However, some quants use random forests as feature selectors while others use it to generate new features. It all depends on the characteristics of the data.</p>

<p>I would suggest you read this <a href=""https://quant.stackexchange.com/questions/9313/machine-learning-vs-regression-and-or-why-still-use-the-latter/9317#9317"">question and answers on quant.stackexchange</a> where people discuss what methods are the best and when to use them, among them ISOMAP, Laplacian eigenmaps, ANNs, swarm optimization. </p>

<p>Check out the <a href=""https://quant.stackexchange.com/tags/machine-learning/hot"">machine-learning tag on the same site</a>, there you might find information related to your particular dataset.</p>
"
"['machine-learning', 'nosql']",Any Master Thesis Topics related to NoSQL and Machine Learning or Business Intelligence?,"<p>The question is a bit broad and opinion-based for StackExchange, but I'll have a quick go anyway: I don't see good topics in this area. For academic machine learning, how you store your data is largely irrelevant, either because the research is on small data anyway and will be read into memory, or because the research is pretty theoretical to begin with.</p>

<p>There are certainly interesting issues to explore here for distributed ML. However NoSQL stores would not in general be helpful. They specialize in random access and random updates to keyed data. ML generally needs high-throughput sequential access to data without updating it.</p>

<p>BI + ML is too broad. Yes there are topics in there somewhere but hard to discuss at that level.</p>
"
"['open-source', 'dataset', 'crawling']",Publicly available social network datasets/APIs,"<p>A couple of words about social networks APIs. About a year ago I wrote a review of popular social networks’ APIs for researchers. Unfortunately, it is in Russian. Here is a summary:</p>

<p><strong>Twitter</strong> (<a href=""https://dev.twitter.com/docs/api/1.1"">https://dev.twitter.com/docs/api/1.1</a>)</p>

<ul>
<li>almost all data about tweets/texts and users is available;</li>
<li>lack of sociodemographic data;</li>
<li>great streaming API: useful for real time text processing;</li>
<li>a lot of wrappers for programing languages;</li>
<li>getting network structure (connections) is possible, but time-expensive (1 request per 1 minute).</li>
</ul>

<p><strong>Facebook</strong> (<a href=""https://developers.facebook.com/docs/reference/api/"">https://developers.facebook.com/docs/reference/api/</a>)</p>

<ul>
<li>rate limits: about 1 request per second;</li>
<li>well documented, sandbox present;</li>
<li>FQL (SQL-like) and «regular Rest» Graph API;</li>
<li>friendship data and sociodemographic features present;</li>
<li>a lot of data is beyond <em>event horizon</em>: only friends' and friends' of friends data is more or less complete, almost nothing could be investigated about random user;</li>
<li>some strange API bugs, and looks like nobody cares about it (e.g., some features available through FQL, but not through Graph API synonym).</li>
</ul>

<p><strong>Instagram</strong> (<a href=""http://instagram.com/developer/"">http://instagram.com/developer/</a>)</p>

<ul>
<li>rate limits: 5000 requests per hour;</li>
<li>real-time API (like Streaming API for Twitter, but with photos) - connection to it is a little bit tricky: callbacks are used;</li>
<li>lack of sociodemographic data;</li>
<li>photos, filters data available;</li>
<li>unexpected imperfections (e.g., it’s possible to collect only 150 comments to post/photo).</li>
</ul>

<p><strong>Foursquare</strong> (<a href=""https://developer.foursquare.com/overview/"">https://developer.foursquare.com/overview/</a>)</p>

<ul>
<li>rate limits: 5000 requests per hour;</li>
<li>kingdom of geosocial data :)</li>
<li>quite closed from researches because of privacy issues. To collect checkins data one need to build composite parser working with 4sq, bit.ly, and twitter APIs at once;</li>
<li>again: lack of sociodemographic data.</li>
</ul>

<p><strong>Google+</strong> (<a href=""https://developers.google.com/+/api/latest/"">https://developers.google.com/+/api/latest/</a>)</p>

<ul>
<li>about 5 requests per second (try to verify);</li>
<li>main methods: activities and people;</li>
<li>like on Facebook, a lot of personal data for random user is hidden;</li>
<li>lack of user connections data.</li>
</ul>

<p>And out-of-competition: I reviewed social networks for Russian readers, and #1 network here is <a href=""http://en.wikipedia.org/wiki/VK_%28social_network%29""><strong>vk.com</strong></a>. It’s translated to many languages, but popular only in Russia and other CIS countries. API docs link: <a href=""http://vk.com/dev/"">http://vk.com/dev/</a>. And from my point of view, it’s the best choice for homebrew social media research. At least, in Russia. That’s why:</p>

<ul>
<li>rate limits: 3 requests per second;</li>
<li>public text and media data available;</li>
<li>sociodemographic data available: for random user availability level is about 60-70%;</li>
<li>connections between users are also available: almost all friendships data for random user is available;</li>
<li>some special methods: e.g., there is a method to get online/offline status for exact user in realtime, and one could build schedule for his audience.</li>
</ul>
"
"['nlp', 'text-mining', 'freebase']",How to grow a list of related words based on initial keywords?,"<p>The <a href=""https://code.google.com/p/word2vec/"">word2vec algorithm</a> may be a good way to retrieve more elements for a list of similar words. It is an unsupervised ""deep learning"" algorithm that has previously been demonstrated with Wikipedia-based training data (helper scripts are provided on the Google code page).</p>

<p>There are currently <a href=""https://code.google.com/p/word2vec/"">C</a> and <a href=""http://radimrehurek.com/gensim/models/word2vec.html"">Python</a> implementations. This <a href=""http://radimrehurek.com/2014/02/word2vec-tutorial"">tutorial</a> by <a href=""http://radimrehurek.com/"">Radim Řehůřek</a>, the author of the <a href=""http://radimrehurek.com/gensim/"">Gensim topic modelling library</a>, is an excellent place to start.</p>

<p>The <a href=""http://radimrehurek.com/2014/02/word2vec-tutorial#single"">""single topic""</a> demonstration on the tutorial is a good example of retreiving similar words to a single term (try searching on 'red' or 'yellow'). It should be possible to extend this technique to find the words that have the greatest overall similarity to a set of input words.</p>
"
['tools'],does anyone use Julia programming language?,"<p>I personally have used <code>Julia</code> for a good number of professional projects, and while, as Dirk mentioned, this is purely conjecture, I can give some insights on where Julia really stands out. The question of whether or not these reasons will prove enough to have <code>Julia</code> succeed as a language is anyone's guess.</p>

<ul>
<li><strong>Distributed Systems</strong>: Julia is the easiest language I've ever dealt with in terms of building distributed systems. This is becoming more and more relevant in computing, and will potentially become a deciding factor, but the question of whether or not <code>Julia</code>'a relative ease decides this is up for debate</li>
<li><strong>JIT Performance</strong>: Julia's JIT compiler is extremely fast, and while there is a lot of debate as to how accurate these benchmark numbers are, the <a href=""http://julialang.org/"">Julia Website</a> shows a series of relevant benchmarks</li>
<li><strong>Community</strong>: This is an area where <code>Julia</code> just isn't quite there. The community that is there is generally supportive, but not quite as knowledgable as the <code>R</code> or <code>python</code> communities, which is a definite minus.</li>
<li><strong>Extensibility</strong>: This is another place where <code>Julia</code> is currently lacking, there is a large disconnect between the implies code patterns that <code>Julia</code> steers you toward and what it can actually support. The type system is currently overly bulky and difficult to use effectively.</li>
</ul>

<p>Again, can't say what this means for the future, but these are just a couple of relevant points when it comes to evaluating <code>Julia</code> in my opinion.</p>
"
"['python', 'apache-spark', 'cross-validation', 'pyspark']",Merging multiple data frames row-wise in PySpark,"<p>Stolen from: <a href=""http://stackoverflow.com/questions/33743978/spark-union-of-multiple-rdds"">http://stackoverflow.com/questions/33743978/spark-union-of-multiple-rdds</a></p>

<p>Outside of chaining unions this is the only way to do it for DataFrames.</p>

<pre><code>from functools import reduce  # For Python 3.x
from pyspark.sql import DataFrame

def unionAll(*dfs):
    return reduce(DataFrame.unionAll, dfs)

unionAll(td2, td3, td4, td5, td6, td7, td8, td9, td10)
</code></pre>

<p>What happens is that it takes all the objects that you passed as parameters and reduces them using unionAll (this reduce is from Python, not the Spark reduce although they work similarly) which eventually reduces it to one DataFrame.</p>

<p>If instead of DataFrames they are normal RDDs you can pass a list of them to the union function of your SparkContext</p>

<p>EDIT: For your purpose I propose a different method, since you would have to repeat this whole union 10 times for your different folds for crossvalidation, I would add labels for which fold a row belongs to and just filter your DataFrame for every fold based on the label</p>
"
"['machine-learning', 'books', 'reinforcement-learning']",Books on Reinforcement Learning,"<p>Here you have some good references on Reinforcement Learning:</p>

<p><strong>Classic</strong></p>

<p><em>Sutton RS, Barto AG. Reinforcement Learning: An Introduction. Cambridge, Mass: A Bradford Book; 1998. 322 p.</em> </p>

<p>The draft for the second edition is available for free: <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html"" rel=""nofollow"">https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html</a></p>

<p>Russell/Norvig Chapter 21:</p>

<p><em>Russell SJ, Norvig P, Davis E. Artificial intelligence: a modern approach. Upper Saddle River, NJ: Prentice Hall; 2010.</em> </p>

<p><strong>More technical</strong></p>

<p><em>Szepesvári C. Algorithms for reinforcement learning. Synthesis Lectures on Artificial Intelligence and Machine Learning. 2010;4(1):1–103.</em>  <a href=""http://www.ualberta.ca/~szepesva/RLBook.html"" rel=""nofollow"">http://www.ualberta.ca/~szepesva/RLBook.html</a></p>

<p><em>Bertsekas DP. Dynamic Programming and Optimal Control. 4th edition. Belmont, Mass.: Athena Scientific; 2007. 1270 p.</em> 
Chapter 6, vol 2 is available for free: <a href=""http://web.mit.edu/dimitrib/www/dpchapter.pdf"" rel=""nofollow"">http://web.mit.edu/dimitrib/www/dpchapter.pdf</a></p>

<p><strong>For more recent developments</strong></p>

<p><em>Wiering M, van Otterlo M, editors. Reinforcement Learning. Berlin, Heidelberg: Springer Berlin Heidelberg; 2012</em> Available from: <a href=""http://link.springer.com/10.1007/978-3-642-27645-3"" rel=""nofollow"">http://link.springer.com/10.1007/978-3-642-27645-3</a></p>

<p><em>Kochenderfer MJ, Amato C, Chowdhary G, How JP, Reynolds HJD, Thornton JR, et al. Decision Making Under Uncertainty: Theory and Application. 1 edition. Cambridge, Massachusetts: The MIT Press; 2015. 352 p.</em> </p>

<p><strong>Multi-agent reinforcement learning</strong></p>

<p><em>Buşoniu L, Babuška R, Schutter BD. Multi-agent Reinforcement Learning: An Overview. In: Srinivasan D, Jain LC, editors. Innovations in Multi-Agent Systems and Applications - 1 . Springer Berlin Heidelberg; 2010 p. 183–221.</em> Available from: <a href=""http://link.springer.com/chapter/10.1007/978-3-642-14435-6_7"" rel=""nofollow"">http://link.springer.com/chapter/10.1007/978-3-642-14435-6_7</a></p>

<p><em>Schwartz HM. Multi-agent machine learning : a reinforcement approach. Hoboken, New Jersey: Wiley; 2014.</em> </p>

<p><strong>Videos / Courses</strong></p>

<p>I would also suggest David Silver course in YouTube: <a href=""https://www.youtube.com/playlist?list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa"" rel=""nofollow"">https://www.youtube.com/playlist?list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa</a></p>
"
['sequence'],Classification of DNA Sequences,"<p>One way would be to create 20 features (each feature representing a codon). In this way, you would have a dataset with 3190 instances and 20 categorical features. There is no need to treat the sequence as a Markov chain. </p>

<p>Once the dataset has been featurized as suggested above, any supervised classifier can work well. I would suggest using a gradient boosting machine as it might be better suited to handle categorical features. </p>
"
"['machine-learning', 'clustering', 'dbscan']",How are clusters from DBSCAN sometimes non-convex?,"<p>A cluster in DBSCAN consists of <em>multiple</em> core points.</p>

<p>The radius is the area covered by a single core point, but together with neighbor core points the shape will be much more complex. In particular, they can be much larger than epsilon, so you should choose a small value, and rely on this ""cover"" functionality.</p>

<p><a href=""https://en.m.wikipedia.org/wiki/DBSCAN#/media/File:DBSCAN-density-data.svg"" rel=""nofollow"">Wikipedia has an example of a non-convex cluster</a></p>
"
"['machine-learning', 'r', 'neuralnetwork', 'predictive-modeling', 'performance']",What are the performance measures in the neural networks field?,"<p>Typical predictive performance measures used to compare accuracy of ANN models are:</p>

<ul>
<li>RMSE - (root mean squared error) measures the distance between estimated and actual outcomes. Other metrics that measure the same concept are MSE, MAE or MPE.</li>
<li>R square - (R^2 or coefficient of determination) measures the reduction of variance when using the model.</li>
</ul>

<p>When comparing two different ANN models for performance, metrics that take into account the complexity of the model may be used, such as AIC or BIC.</p>
"
"['data-mining', 'feature-extraction', 'pandas']",Pivoting a two-column feature table in Pandas,"<p>If there is no value column - introduce it yourself!</p>

<pre><code>df[""value""]=1
pd.pivot_table(df, values=""value"", index=[""city""], columns=""cuisine"", fill_value=0) 
</code></pre>

<p>For your example I got (after fixing the misprint in 'Japanse' to 'Japanese')</p>

<pre><code>cuisine  Chinese  French  German  Japanese
city                                      
NY             1       0       0         1
SF             0       1       1         1
</code></pre>
"
"['machine-learning', 'data-mining', 'classification']",What should I care about while stacking as an ensemble method?,"<p>To directly answer your question about stacking: you should care about minimizing 1) bias, and 2) variance. This is obvious, but in practice this often comes down to simply having models which are <a href=""http://dl.acm.org/citation.cfm?id=640232"" rel=""nofollow"">""diverse""</a>. (I apologize that link is behind a paywall, but there are a few others like it and you may well find it other ways)</p>

<p>You don't want ensembles of like-minded models - they will make the same mistakes and reinforce each other. </p>

<p>In the case of stacking, what is happening? You are letting the outputs of the probabilistic classifiers on the actual feature input become the new features. A diverse set of classifiers which can in any way give signals about edge cases is desirable. If classifier 1 is terrible at classes A, B, and C but fantastic at class D, or a certain edge case, it is still a good contribution to the ensemble. </p>

<p>This is why neural nets are so good at what they do in image recognition - deep nets are in fact recursive logistic regression stacking ensembles! Nowadays people don't always use the sigmoid activation and there are many layer architectures, but it's the same general idea. </p>

<p>What I would recommend is trying to maximize the diversity of your ensemble by using some of the similarity metrics on the classifiers' prediction output vectors (ie, Diettrich's Kappa statistic) in training. <a href=""http://www.sandia.gov/~wpk/pubs/publications/infoFusion.pdf"" rel=""nofollow"">Here is another good reference</a>.  </p>

<p>Hope that helps.</p>
"
"['python', 'statistics', 'pandas']",t test or anova,"<p>This is typical statistics problem.  When you have multiple 'classes' that you assume are normally distributed you first run an ANOVA.  Then, IFF (if-and-only-if) the ANOVA is significant, then run post-hoc pairwise t-tests with an appropriate correction (e.g. Bonferroni).</p>
"
['computer-vision'],Open cv and computer vision,"<p>Depends on your programming skills. Here is the summary:</p>

<p>OpenCV is a great tool originally developed in C++ and after a while a Python interface was added. In industry (and also many academic research group) C++ is the popular language for Computer Vision as the nature of data needs efficiency in computation. So starting with python is an easy way to learn Computer Vision and OpenCV capabilities but when you are applying for a job in industry or academia, they most probably need you to get your hands dirty with C++.</p>
"
"['neuralnetwork', 'deep-learning', 'image-classification']",Deconvolutional Network in Semantic Segmentation,"<p>There are two main functions they undo.</p>

<p>The <a href=""http://deeplearning.net/tutorial/lenet.html#maxpooling"" rel=""nofollow"">pooling layers</a> in the convolutional neural network downsample the image by (usually) taking the maximum value within the receptive field.  Each <code>rxr</code> image region is downsampled to a single value.  What this implementation does is store which unit had the maximum activation in each of these pooling steps.  Then, at each ""unpooling"" layer in the deconvolutional network, they upsample back to a <code>rxr</code> image region, only propagating the activation to the location that produced the original max-pooled value.</p>

<p>Thus, ""the output of an unpooling layer is an enlarged, yet sparse activation map.""</p>

<p>Convolutional layers learn a filter for each image region that maps from a region of size <code>r x r</code> to a single value, where <code>r</code> is the receptive field size. The point of the deconvolutional layer is to learn the opposite filter.  This filter is a set of weights that projects an <code>rxr</code> input into a space of size <code>sxs</code>, where s is the size of the next convolutional layer.  These filters are learned in the same way that as regular convolutional layers are.</p>

<p>As the mirror image of a deep CNN, the low-level features of the network are really high-level, class-specific features.  Each layer in the network then localizes them, enhancing class-specific features while minimizing noise.</p>
"
"['bigdata', 'hadoop']",Can we access HDFS file system and YARN scheduler in Apache Spark?,"<p>Yes.</p>

<p>There are examples on spark official document: <a href=""https://spark.apache.org/examples.html"" rel=""nofollow"">https://spark.apache.org/examples.html</a>
Just put your HDFS file uri in your input file path as below (scala syntax).</p>

<pre><code>val file = spark.textFile(""hdfs://train_data"")
</code></pre>
"
['machine-learning'],Amplifying a Locality Sensitive Hash,"<p>I think I've worked something out. Basically I'm looking for an approach that works in a map/reduce type environment and I think this approach does it.</p>

<p>So,</p>

<ul>
<li>suppose I have b bands of r rows and I want to add another AND stage, say another c ANDs.</li>
<li>so instead of b * r bits I need hashes of b * r * c bits</li>
<li>and I run my previous procedure c times, each time on b * r bits</li>
<li>If x and y are found to be a candidate pair by any of these procedures it emits a key value pair ((x, y), 1), with the tuple of IDs (x,y) as the key and the value 1</li>
<li>At the end of the c procedures I group these pairs by key and sum</li>
<li>Any pair (x,y) with a sum equal to c was a candidate pair in each of the c rounds, and so is a candidate pair of the entire procedure.</li>
</ul>

<p>So now I have a workable solution, and all I need to do is work out whether using 3 steps like this will actually help me get a better result with fewer overall hash bits or better overall performance...</p>
"
"['algorithms', 'similarity']",Score matrix string similarity,"<p>As I understood, Document 1 and Document 2 may have different number of keys. And you wand to get final similarity evaluation between 0 and 1. If so, I would propose following algorithm:</p>

<ol>
<li>Sum of max. vals is equal to 0.</li>
<li>Select maximum value from doc-doc matrix and add it to Sum of max. vals.</li>
<li>Remove row and column with maximum value from the matrix.</li>
<li>Repeat steps 2-3 until rows or columns are ended.</li>
<li>Denominate Sum of max. vals by average number of key words in two texts.</li>
</ol>

<p>Final estimation would be equal to 1, if both documents have identical length, and every word from Doc 1 has equivalent in Doc 2.</p>

<p>You haven't mentioned software, you are using, but here is <strong>R</strong> example of function, computing such similarity (it takes object of class matrix as input):</p>

<pre><code>eval.sim &lt;- function(sim.matrix){
  similarity &lt;- 0
  denominator &lt;- sum(dim(sim.matrix)) / 2
  for(i in 1:(min(c(nrow(sim.matrix), ncol(sim.matrix))) - 1)){
    extract &lt;- which(sim.matrix == max(sim.matrix), arr.ind=T)[1, ]
    similarity &lt;- similarity + sim.matrix[extract[1], extract[2]]
    sim.matrix &lt;- sim.matrix[-extract[1], -extract[2]]
  }
  similarity &lt;- similarity + max(sm.copy)
  similarity &lt;- similarity / denominator
}
</code></pre>

<p>In python - </p>

<pre><code>import numpy as np

def score_matrix(sim_matrix):
    similarity = 0
    denominator = sum(sim_matrix.shape) / 2
    for i in range(min(sim_matrix.shape)):
        x, y = np.where(sim_matrix == np.max(sim_matrix))[0][0], np.where(sim_matrix == np.max(sim_matrix))[1][0]
        similarity += sim_matrix[x, y]
        sim_matrix = np.delete(sim_matrix,(x),axis=0)
        sim_matrix = np.delete(sim_matrix,(y),axis=1)
    return similarity / denominator
</code></pre>
"
['nlp'],how to get the Polysemes of a word in wordnet or any other api?,"<p>There are several third-party Java APIs for WordNet listed here: <a href=""http://wordnet.princeton.edu/wordnet/related-projects/#Java"" rel=""nofollow"">http://wordnet.princeton.edu/wordnet/related-projects/#Java</a></p>

<p>In the past, I've used JWNL the most: <a href=""http://sourceforge.net/projects/jwordnet/"" rel=""nofollow"">http://sourceforge.net/projects/jwordnet/</a></p>

<p>The documentation for JWNL isn't great, but it should provide the functionality you need.</p>
"
['r'],Sort a data frame when column name is assigned in a variable,"<p>Your first attempt was more accurate. Since I do not have your data I will just use the iris data set,</p>

<pre><code> iris_ordered_on_Sepal_Length &lt;- iris[order(iris$Sepal.Length),]
</code></pre>

<p>or if you use <a href=""https://cran.r-project.org/web/packages/data.table/"" rel=""nofollow"">data.table</a>,</p>

<pre><code>iris_dt &lt;- data.table(iris)
iris_dt[order(Sepal.Length)]
</code></pre>

<p>data.table will make renaming easier too,</p>

<pre><code>setnames(input, c(""colname_11, ""colname_17"", ""colname_23""), validoutcomes[1 : 3])
</code></pre>
"
"['nlp', 'word-embeddings']",Shall I use the Euclidean Distance or the Cosine Similarity to compute the semantic similarity of two words?,"<p>First of all, if GloVe gives you <em>normalized</em> unit vectors, then the two calculations are equivalent. In general, I would use the cosine similarity since it removes the effect of document length. For example, a postcard and a full-length book may be about the same topic, but will likely be quite far apart in pure ""term frequency"" space using the Euclidean distance. They will be right on top of each other in cosine similarity.</p>
"
"['machine-learning', 'classification', 'r', 'visualization', 'svm']",Visualizing Support Vector Machines (SVM) with Multiple Explanatory Variables,"<p>Does it matter that the model is created in the form of SVM?</p>

<p>If no, I have seen a clever 6-D visualization. Its varieties are becoming popular in medical presentations. 
3 dimensions are shown as usual, in orthographic projection.
Dimension 4 is color (0..255)
Dimension 5 is thickness of the symbol 
Dimension 6 requires animation. It is a frequency of vibration of a dot on the screen. 
In static, printed versions, one can replace frequency of vibration by blur around the point, for a comparable visual perception.</p>

<p>If yes, and you specifically need to draw separating hyperplanes, and make them look like lines\planes, the previous trick will not produce good results. Multiple 3-D images are better.</p>
"
"['python', 'time-series', 'markov-process']",Python library to implement Hidden Markov Models,"<p>The <a href=""http://ghmm.org/"" rel=""nofollow"">ghmm</a> library might be the one which you are looking for. </p>

<p>As it is said in their website:</p>

<blockquote>
  <p>It is used for implementing efficient data structures and algorithms
  for basic and extended HMMs with discrete and continuous emissions. It
  comes with Python wrappers which provide a much nicer interface and
  added functionality.</p>
</blockquote>

<p>It also has a nice documentation and a step-by-step tutorial for getting your feet wet.</p>
"
"['machine-learning', 'python', 'neuralnetwork']",Python + Neural Nets + Large dimension Large Dataset,"<p>I experimented with PyBrain and PyLearn2
I rather find Keras [<a href=""http://keras.io/"" rel=""nofollow"">http://keras.io/</a> ] as an excellent choice. It way ahead of PyBrain for sure. PyBrain development has been stopped</p>
"
"['data-mining', 'classification', 'binary']",Why might several types of models give almost identical results?,"<p>This results means that whatever method you use, you are able to get reasonably close to the optimal decision rule (aka <a href=""http://en.wikipedia.org/wiki/Admissible_decision_rule#Bayes_rules_and_generalized_Bayes_rules"">Bayes rule</a>). The underlying reasons have been explained in Hastie, Tibshirani and Friedman's <a href=""http://statweb.stanford.edu/~tibs/ElemStatLearn/"">""Elements of Statistical Learning""</a>. They demonstrated how the different methods perform by comparing Figs. 2.1, 2.2, 2.3, 5.11 (in my first edition -- in section on multidimensional splines), 12.2, 12.3 (support vector machines), and probably some others. If you have not read that book, you need to drop everything <strong>RIGHT NOW</strong> and read it up. (I mean, it isn't worth losing your job, but it is worth missing a homework or two if you are a student.)</p>

<p>I don't think that observations to variable ratio is the explanation. In light of my rationale offered above, it is the relatively simple form of the boundary separating your classes in the multidimensional space that all of the methods you tried have been able to identify.</p>
"
['classification'],change in variable importance,"<p>Probably you can use multiple decision trees - each for slice of dataset. Then combine the results from each decision tree. You can also weigh the results from each decision tree. Eg the dataset for 2015 can be given more weight than dataset for 2014. I do not see much harm in spliting dataset and training different trees as it helps to account for the predictor values in a more accurate manner. If you could more details about the data or the ""variable"" you mentioned, people would be able to help more.</p>
"
"['machine-learning', 'nlp', 'text-mining']",Wordnet Lemmatization,"<p>The exception list files are used to help the processor find base forms from 'irregular inflections' according to the <a href=""http://wordnet.princeton.edu/wordnet/man/wndb.5WN.html"" rel=""nofollow"">man page</a>.  They mean that some words, when plural or a different tense, can't be algorithmically processed to find the base/root word.  More details can be found in the <a href=""https://wordnet.princeton.edu/man/morphy.7WN.html#sect3"" rel=""nofollow"">morphy man</a>.  I'm not a language processing expert, but this is likely a result of English words that 'break the rules'.  If you think about the code like a human trying to learn English: the student learns rules to use (algorithm) and then has to memorize exceptions for the rules (exception lists).  An over-simplified analogy that does not involve endings/conjugation would a spell checking program.  An algorithm might check for 'i before e, except after c' but would first have to check the word against an exception list to make sure it isn't 'weird' or 'caffeine' - please don't start a linguistics fight about this rule, I am not commenting on the validity of it/that's not the point I'd like to make.</p>
"
"['feature-selection', 'featurization']",Is automatic feature detection feasible?,"<p>It is a good idea to do automatic feature vector selection for classification, and is widely done for decision trees.</p>

<p>If you are trying to find the relative importance of variables in classification schemes, a popular way of selecting variables is by performing entropy minimization.  This a technique that is often used for constructing decision trees.  Entropy is simply a way of numerically quantifying disorder of a system:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=H=%5CSigma_x-p_x*log_2%28p_x%29"" alt=""Entropy""></p>

<p>For binary classification, it's simple to calculate entropy for every variable, moving the variables which minimize entropy to the top, </p>

<p>As an example, consider a simple case where a game exists when one must choose between a right and left door, one of which is red, one of which is blue.    The color of the door changes from instance to instance (sometime left is red, sometimes right is red), but the prize is always behind the left door.  Suppose also that over many trials, both left-right, and red-blue have equal number of people choosing those options.  The entropy of the original system (P(prize)=.5, P(!prize)=.5) is 1.  Supposing that we select a door based on color, we have:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=H=p_%7Bred%7D*(H_%7Bred%7D)%2Bp_%7Bblue%7D*(H_%7Bblue%7D)=0.5*(-.5*log_2(.5)-.5*log_2(.5))%2B.5*(-.5*log_2(.5)-.5*log_2(.5))"" alt=""RBEntropy""></p>

<p>Here the probability of red and blue are both .5, and the entropy of each system is 1, so the Entropy of the new system is still 1.  This means that no information was gained from choosing red or blue.  Suppose instead we select on left right:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=H=p_%7Bleft%7D*%28H_%7Bleft%7D%29%2Bp_%7Bright%7D*%28H_%7Bright%7D%29=0.5*%28-1*log_2%281%29%29%2B.5*%28-1*log_2%281%29%29"" alt=""LREntropy""></p>

<p>And since log_2(1)=0, there is no disorder (i.e. a perfect predictor).</p>

<p>For continuous variables, it becomes a bit more complicated because you then have to select based on a floating threshold (or a few for a single variable).  While the basic entropy calculations hold fast, tuning many thresholds on many variables becomes resource intensive, so at that point you'll want to look at something like spark's mllib. </p>
"
"['data-mining', 'association-rules', 'market-basket-analysis']",Does the count of items in a transaction matter to apriori?,"<p>No it is not important and highly recommended to remove the duplicate items and sort the items in lexicographical order in each transaction. This is to improve the performance.</p>

<p>In association rule mining, an item is frequent iff it is repeated in multiple transactions not in a single transaction. This is why you don't need to have duplicate items in a each transaction.</p>
"
"['python', 'random-forest', 'pandas', 'unbalanced-classes', 'ensemble-modeling']",EasyEnsemble explaination,"<p>The toolbox only manage the sampling so this is slightly different from the algorithm from the paper.</p>

<p>What it does is the following: it creates several subset of data which are balanced. These subsets are created by randomly under-sampling the majority class. That is what you are getting from the toolbox.</p>

<p>To obtain what in the paper, you need to train an AdaBoost classifier for each subset. Thus, what you get is an ensembles of ensembles.</p>

<p>Hope that's help.</p>
"
"['neuralnetwork', 'predictive-modeling']",Is there an R package which uses neural networks to explicitly model count data?,"<p>I skimmed over a paper recently that aims to use Neural Networks as Poisson regression. The method they propose is basically a standard Multi-Layer Perceptron where they use a different loss function, namely:</p>

<p>$$E = -\sum_{n=1}^N[-t_n + y_nlog(t_n)]$$</p>

<p>This is a version without regularization to prevent the overfitting, they use regular weight decay.</p>

<p>They mention that they wrote it in R and Matlab but I don't have a clue if it's available online somewhere, but any neural network package where you can pass your own loss function should suffice.</p>

<p><a href=""http://www.mathstat.dal.ca/~hgu/Neural%20Comput%20&amp;%20Applic.pdf"" rel=""nofollow"">http://www.mathstat.dal.ca/~hgu/Neural%20Comput%20&amp;%20Applic.pdf</a></p>
"
['predictive-modeling'],Modeling and Predicting Co-Occuring Values,"<p>I would recommend a <a href=""https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine"" rel=""nofollow"">Restricted Boltzmann machine</a>, which is a type of neural network that can model probability distributions. What you are trying to do is to estimate the <a href=""https://en.wikipedia.org/wiki/Marginal_distribution"" rel=""nofollow"">marginal distribution</a> of the <a href=""https://en.wikipedia.org/wiki/Missing_data"" rel=""nofollow"">missing features</a>. In a probabilistic model, this is done through <em>marginalization</em>. The beauty of this method is that it provides the full distribution over the missing features rather than mere <a href=""https://en.wikipedia.org/wiki/Point_estimation"" rel=""nofollow"">point estimates</a>.</p>

<p><a href=""http://deeplearning.net/tutorial/rbm.html"" rel=""nofollow"">Here</a> <a href=""http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/"" rel=""nofollow"">are</a> two tutorials.</p>
"
"['data', 'beginner']",How to handle the CEO expectations from a company that's new to data science?,"<p>I tell you what has worked for me: practical examples. They have probably already read about what data science is in general and which are the standard procedures. What they have not seen is someone in front of them explaining how innovative (and useful for business!!!) the data science really is. Follow the ""Say what you would say to your granny to tell her how cool your work is"" advice.</p>

<p>People want to know what you have actually done, indipendently from the algorithms or procedures you have used.</p>

<p>Good luck!</p>
"
"['r', 'csv']",Reading a CSV using R,"<p>Sorry, never mind, I just realized that I thought I was running the command to set my working directory to the correct file and it turns out I wasn't so when I was importing the file I was importing the wrong one.  All fixed.</p>
"
"['machine-learning', 'data-mining', 'tools', 'beginner']","What initial steps should I use to make sense of large data sets, and what tools should I use?","<p>I will try to answer your questions, but before I'd like to note that using term ""large dataset"" is misleading, as ""large"" is a <em>relative</em> concept. You have to provide more details. If you're dealing with <strong>bid data</strong>, then this fact will most likely affect selection of preferred <em>tools</em>, <em>approaches</em> and <em>algorithms</em> for your <strong>data analysis</strong>. I hope that the following thoughts of mine on data analysis address your sub-questions. Please note that the numbering of my points does not match the numbering of your sub-questions. However, I believe that it better reflects general <strong>data analysis workflow</strong>, at least, how I understand it.</p>

<p>1) Firstly, I think that you need to have at least some kind of <strong>conceptual model</strong> in mind (or, better, on paper). This model should guide you in your <em>exploratory data analysis (EDA)</em>. A presence of a <em>dependent variable (DV)</em> in the model means that in your <em>machine learning (ML)</em> phase later in the analysis you will deal with so called supervised ML, as opposed to unsupervised ML in the absence of an identified DV.</p>

<p>2) Secondly, <strong>EDA</strong> is a crucial part. IMHO, EDA should include <strong>multiple iterations</strong> of producing <em>descriptive statistics</em> and <em>data visualization</em>, as you refine your understanding about the data. Not only this phase will give you valuable insights about your datasets, but it will feed your next important phase - <strong>data cleaning and transformation</strong>. Just throwing your raw data into a statistical software package won't give much - for any <strong>valid</strong> statistical analysis, data should be <em>clean, correct and consistent</em>. This is often the most time- and effort-consuming, but absolutely necessary part. For more details on this topic, read these nice papers: <a href=""http://vita.had.co.nz/papers/tidy-data.pdf"">http://vita.had.co.nz/papers/tidy-data.pdf</a> (by Hadley Wickham) and <a href=""http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf"">http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf</a> (by Edwin de Jonge and Mark van der Loo).</p>

<p>3) Now, as you're hopefully done with <em>EDA</em> as well as data cleaning and transformation, your ready to start some more statistically-involved phases. One of such phases is <em>exploratory factor analysis (EFA)</em>, which will allow you to extract the underlying <strong>structure</strong> of your data. For datasets with large number of variables, the positive side effect of EFA is <em>dimensionality reduction</em>. And, while in that sense EFA is similar to <em>principal components analysis (PCA)</em> and other dimensionality reduction approaches, I think that EFA is more important as it allows to refine your conceptual model of the phenomena that your data ""describe"", thus making sense of your datasets. Of course, in addition to EFA, you can/should perform <strong>regression analysis</strong> as well as apply <strong>machine learning techniques</strong>, based on your findings in previous phases.</p>

<p>Finally, a note on <strong>software tools</strong>. In my opinion, current state of statistical software packages is at such point that practically any major software packages have comparable offerings feature-wise. If you study or work in an organization that have certain policies and preferences in term of software tools, then you are <em>constrained</em> by them. However, if that is not the case, I would heartily recommend <strong>open source</strong> statistical software, based on your comfort with its specific <em>programming language</em>, <em>learning curve</em> and your <em>career perspectives</em>. My current platform of choice is <strong>R Project</strong>, which offers mature, powerful, flexible, extensive and open statistical software, along with amazing ecosystem of packages, experts and enthusiasts. Other nice choices include <em>Python</em>, <em>Julia</em> and specific open source software for processing <strong>big data</strong>, such as <em>Hadoop</em>, <em>Spark</em>, <em>NoSQL</em> databases, <em>WEKA</em>. For more examples of open source software for <strong>data mining</strong>, which include general and specific statistical and ML software, see this section of a Wikipedia page: <a href=""http://en.wikipedia.org/wiki/Data_mining#Free_open-source_data_mining_software_and_applications"">http://en.wikipedia.org/wiki/Data_mining#Free_open-source_data_mining_software_and_applications</a>.</p>

<p>UPDATE: Forgot to mention <em>Rattle</em> (<a href=""http://rattle.togaware.com"">http://rattle.togaware.com</a>), which is also a very popular open source R-oriented GUI software for data mining.</p>
"
"['bigdata', 'definitions', 'terminology', 'history']","Original Meaning of ""Intelligence"" in ""Business Intelligence""","<p>Howard Dresner, in 1989, is believed to have coined the term ""business intelligence"", to describe ""concepts and methods to improve business decision making by using fact-based support systems."". When he was at Gartner Group. This is a common mantra, spread over the Web. I have not been able to trace the exact source for this origin yet. Many insist on he was not at Gartner group in 1989, which is confirmed in the <a href=""http://www.informationbuilders.com/new/magazine/v11-2/pdf/DRESNERI.PDF"">following interview</a>. In his 2008 book, Performance Management Revolution: Improving Results Through Visibility and Actionable Insight, the termed is defined as:</p>

<blockquote>
  <p>BI is knowledge gained through the access and analysis of business
  information.</p>
</blockquote>

<p>He says, at the beginning, that</p>

<blockquote>
  <p>In 1989, for example, I started-some might say incited-the BI
  revolution with the premise that all users have a fundamental right to
  access information without the help of IT.</p>
</blockquote>

<p>No apparent claim of the invention of the term on his side. Indeed, one can find older roots in H. P. Luhn, <a href=""http://dx.doi.org/10.1147/rd.24.0314"">A Business Intelligence System</a>, IBM Journal of Research and Development, 1958, Vol. 2, Issue 4, p. 314--319. </p>

<blockquote>
  <p>Abstract: An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the ""action points"" in an organization. Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points. This paper shows the flexibility of such a system in identifying known information, in finding who needs to know it and in disseminating it efficiently either in abstract form or as a complete document.</p>
</blockquote>

<p>The author claims that:</p>

<blockquote>
  <p>The techniques proposed here to make these things possible are:</p>
  
  <ol>
  <li>Auto-abstracting of documents;</li>
  <li>Auto-encoding of documents;</li>
  <li>Automatic creation and updating of action-point profiles.</li>
  </ol>
  
  <p>All of these techniques are based on statistical procedures which can
  be performed on present-day data processing machines. Together with
  proper communication facilities and input-output equipment a
  comprehensive system may be assembled to accommodate all information
  problems of an organization. We call this a Business Intelligence
  System.</p>
</blockquote>

<p>He also gives the explanation of the terms ""<strong>business</strong>"" and ""<strong>intelligence</strong>"": </p>

<blockquote>
  <p>In this paper, business is a collection of activities carried on for
  whatever purpose, be it science, technology, commerce, industry, law,
  government, defense, et cetera. The communication facility serving the
  conduct of a business (in the broad sense) may be referred to as an
  intelligence system. The notion of intelligence is also defined here,
  in a more general sense, as ""the ability to apprehend the
  interrelationships of presented facts in such a way as to guide action
  towards a desired goal.""</p>
</blockquote>

<p>So the idea of ""linking the facts"" is already present in H. P. Luhn paper. To many sources, Howard Dresner has re-invented ""Business Intelligence"" to re-brand decision support system (DSS) and executive information system (EIS) when at DEC, and the term became famous throught the influence of the Gartner group. </p>

<p>Apparently, the term has already been used way before, as in the book <a href=""http://www.worldcat.org/title/wholesale-business-intelligence-and-southern-and-western-merchants-pocket-directory-to-the-principal-mercantile-houses-in-the-city-of-philadelphia-for-the-year-1839/oclc/35144726"">Wholesale Business Intelligence and Southern and Western Merchants' Pocket Directory to the Principal Mercantile Houses in the City of Philadelphia, for the Year 1839</a>.</p>

<p>As I could not fetch this source, I will stick to the Luhn/Dresner acception. It relates to the <a href=""http://www.etymonline.com/index.php?term=intelligence"">etymology of intelligence</a>: </p>

<blockquote>
  <p>late 14c., ""faculty of understanding,"" from Old French intelligence
  (12c.), from Latin intelligentia, intellegentia ""understanding, power
  of discerning; art, skill, taste,"" from intelligentem (nominative
  intelligens) ""discerning,"" present participle of intelligere ""to
  understand, comprehend,"" from inter- ""between"" (see inter-) + legere
  ""choose, pick out</p>
</blockquote>

<p>In Business Intelligence for Dummies (Scheps, 2008), the definition chapter plays on Military Intelligence:</p>

<blockquote>
  <p>Business Intelligence Defined: No CIA Experience Required So what the
  heck is business intelligence, anyway? In essence, BI is any activity,
  tool, or process used to obtain the best information to support the
  process of making decisions.</p>
  
  <p>For our purposes, BI revolves around putting computing power (highly
  pecialized software in concert with other more common technology
  assets) to work, to help make the best choices for your organization.</p>
  
  <p>Business intelligence is essentially timely, accurate, high-value, and
  actionable business insights, and the work processes and technologies
  used to obtain them.</p>
</blockquote>

<p>I would thus bend toward ""Gathering data and information about the business"", maybe more ""to better conduct business"". Additional historical comments can be found in <a href=""http://monashbi.blogspot.fr/2006/11/father-of-bi-is-he-having-laugh.html"">Father of BI? Is he having a laugh?!</a></p>
"
"['r', 'preprocessing']",Apply function on every four rows,"<p>You could use <code>tapply</code> or <code>aggregate</code>:</p>

<pre><code>set.seed(1)
X &lt;- matrix(rnorm(30), nrow=100, ncol=6)
id &lt;- ceiling(seq_len(nrow(X))/4)

tapply(X[, 3], id, sum)
#       1       2       3       4       5       6       7       8       9      10 
# -0.2137 -1.0629 -0.5030  0.2687  1.4961 -0.9343  2.0076  3.1162 -1.3511 -1.6868

aggregate(X[, 3], by=list(id), FUN=sum)
#    Group.1       x
# 1        1 -0.2137
# 2        2 -1.0629
# 3        3 -0.5030
# 4        4  0.2687
# 5        5  1.4961
# 6        6 -0.9343
# 7        7  2.0076
# 8        8  3.1162
# ...

# check results
sum(X[1:4, 3])
# [1] -0.2137
sum(X[5:8, 3])
# [1] -1.063
</code></pre>
"
"['classification', 'dataset', 'nlp', 'text']",How to classify support call texts?,"<p>Rather than use an external dictionary of keywords that are indicative of target class, you may want to take your raw data (or rather a random subset of it), then hand-label your instances (assign each row a label, BEST_EFFORT or URGENT). This becomes your <a href=""http://datascience.stackexchange.com/questions/10531/create-training-data/10532#10532"">training data</a> - each row of data can be transformed into a bag-of-words vector indicating the presence/absence of the word in that particular text. You can train a classifier on this data, for example a naive bayes classifier, which can then be tested on the held out unseen test data. The advantages of the  proposed approach are: (1) automated computation of features vs. hand created dictionary; (2) probabilistic/weighted indicators of class vs. binary dictionary indicators.</p>
"
"['python', 'nlp']",Add Custom Labels to NLTK Information Extractor,"<p>Once you have your own lists of named entities, and you're only interested in extracting the relations, I believe there are simpler solutions (although I never tried relation extraction in NLTK, so I might be wrong):</p>

<ul>
<li><p><a href=""http://reverb.cs.washington.edu/"" rel=""nofollow"">ReVerb</a> - a tool written in Java. Once it produces the results, you can simply keep the rows, where your labels are present as objects of the relation.</p></li>
<li><p><a href=""http://knowitall.github.io/openie/"" rel=""nofollow"">OpenIE</a> - the successor of ReVerb (also written in Java). The authors claim better perfomance, and the output might be more informative.</p></li>
<li><p><a href=""http://iepy.readthedocs.org/en/latest/index.html"" rel=""nofollow"">IEPY</a> - a relation extraction tool in Python. You should be able to provide your own labels/named entities using <a href=""http://iepy.readthedocs.org/en/latest/gazettes.html"" rel=""nofollow"">gazetees</a>.</p></li>
<li><p><a href=""https://github.com/mit-nlp/MITIE"" rel=""nofollow"">MITIE</a> - this library has bindings in Python, and it offers <a href=""https://github.com/mit-nlp/MITIE/blob/master/examples/python/ner.py"" rel=""nofollow"">relation extraction</a> functionality.</p></li>
</ul>
"
"['machine-learning', 'r', 'logistic-regression', 'apache-spark']",Why does logistic regression in Spark and R return different models for the same data?,"<p>A quick glance at the <a href=""https://spark.apache.org/docs/1.2.1/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS"" rel=""nofollow"">docs for LogisticRegressionWithLBFGS</a> indicates that it uses feature scaling and L2-Regularization by default. I suspect that R's <code>glm</code> is returning a maximum likelihood estimate of the model while Spark's <code>LogisticRegressionWithLBFGS</code> is returning a regularized model estimate. Note how the estimated model weights of the Spark model are all smaller in magnitude than those in the R model.</p>

<p>I'm not sure whether or not <code>glm</code> in R is implementing feature scaling, but this would also contribute to different model values.</p>
"
"['python', 'clustering', 'scikit']",How do I obtain the weight and variance of a k-means cluster?,"<p>It is valid to use the k-means to initialize the EM for Mixture of Gaussian modeling. As you said, the mean of each component will be the average of all samples belong to the same cluster (it depends on the used clustering algorithm, some times the centroid is not the average of the cluster but is one of the samples). for the weight you can use the following: the weight of cluster x = the number of samples belong to cluster x divided by the total number of samples. thus, the cluster with the highest number of samples is the cluster with the highest weight. for the variance: just find the variance of all samples belong to the same cluster.</p>
"
"['r', 'k-means']","k-means in R, usage of nstart parameter?","<p>Stability of the clusters is highly dependent on your dataset, for clear cut cases running it multiple times is a waste of resources. I think that is the rationale behind the default value of 1. But I agree that for most smaller cases setting it much higher makes a lot of sense.</p>
"
"['k-means', 'distance']",How to evaluate distance in k-means clusters?,"<p><strong>There are several important points to keep in mind in considering your questions:</strong></p>

<ol>
<li><p>You should always <a href=""http://www.dataminingblog.com/standardization-vs-normalization/"" rel=""nofollow"">normalize or standardize your data</a> before applying k-means clustering. This is true of most other clustering algorithms also. If you are clustering in more than one dimension, the distance metric is meaningless unless each dimension has the same weight, so normalization is essential.  Imagine clustering people by body weight and income. Without normalization the results will depend on whether you think in pounds and dollars, kilograms and pesos, or moles and euros. The lack of normalization introduces non-determinism.</p></li>
<li><p>Strictly speaking, the stability of the k-means algorithm has been <a href=""https://papers.nips.cc/paper/989-convergence-properties-of-the-k-means-algorithms.pdf"" rel=""nofollow"">shown for Euclidean distance metrics</a> and there is no assurance of convergence with other distance metrics.  More practically, most sensible metrics attain convergence and its not much of an issue, but its worth putting that warning out there.</p></li>
<li><p>k-means isn't a clustering algorithm that readily lends itself to statistical analysis within the cluster. Every point in the space is a member of one of the k clusters regardless of how much of an outlier the point is. There are other clustering methods that are more adept at finding and ignoring outliers.  <a href=""http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html"" rel=""nofollow"">DBSCAN</a> is one such algorithm that is very good and finding clusters and ignoring noise.</p></li>
</ol>

<p>Now, answering your questions:</p>

<p><strong>Is it relevant to center / scale euclidean distance on each cluster ? (and then consider outliers as the ones with the highest scaled distance)</strong></p>

<p>Yes, you can certainly do this. Combining k-means with outlier detection is certainly possible but is probably not the most elegant or efficient algorithm.  It kind of sounds like poor-mans's DBSCAN.  Euclidean distance works fine, but just do a second set of normalizations using the centroids and the standard deviation of the cluster.</p>

<p><strong>Are there other kind of distance to consider?</strong></p>

<p>There are lots of other metrics that are useful for many different reasons.  As stated, the k-means convergence proofs hold only for Euclidean distance. For outlier detection Euclidean seems best, but there may be cases where Cosine Similarity metrics could be useful. People may suggest L1 (Manhattan) distance metrics, but I find this is only useful when there is significant linear dependence in your data, which can be remedied with dimensionality reduction.</p>

<p><strong>Short answer:</strong></p>

<p>Give it a try as Euclidean should work fine, but also take a look at clustering via DBSCAN, which has outlier detection built into it.</p>

<p>Hope this helps!</p>
"
"['machine-learning', 'random-forest', 'ensemble-modeling']",Where does the random in Random Forests come from?,"<p>For each tree you randomly select from the variables that you can use to split tree nodes. Generally you randomly select 1/3 of the variables per tree. </p>
"
"['classification', 'definitions']",What is the definition of precision for no positive classifications?,"<p>I just found that <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"" rel=""nofollow""><code>sklearn.metrics.precision_score</code></a> handles it like this:</p>

<pre><code>&gt;&gt;&gt; sklearn.metrics.precision_score(y_true, y_pred)
/home/moose/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074:
  UndefinedMetricWarning: Precision is ill-defined and being
    set to 0.0 due to no predicted samples.
    'precision', 'predicted', average, warn_for)
0.0
</code></pre>

<p>So they give a warning and set it to 0.</p>
"
"['r', 'visualization']",Heat maps in R with more than 2 categorical variables,"<p>Here is one way of visualizing the data you have presented. However, I have used some liberties and assumptions to create this plot. </p>

<p>First, create a year_quarter variable that simply concatenates the year and quarter to present the time on the X axis. The following piece of code can do this in R</p>

<pre><code>year_quarter = paste(dat$year, dat$Quarter, sep=""-"")
</code></pre>

<p>Now, the dataset you have will look like: </p>

<pre><code>&gt; dat
</code></pre>

<p><code>Variable  value year Quarter Location year_quarter
1        A 48.235 2011      Q1    North      2011-Q1
2        B 65.444 2011      Q2    North      2011-Q2
3        C 77.453 2011      Q3    North      2011-Q3
4        D 44.678 2011      Q4    North      2011-Q4
5        A 88.542 2012      Q1    South      2012-Q1
6        B 66.566 2012      Q2    South      2012-Q2
7        C 55.443 2012      Q3    South      2012-Q3
8        D 78.990 2012      Q4    South      2012-Q4</code> </p>

<p>Finally, using <code>ggplto2</code>, you can create the plot such that the colour represents the value, the shape represents the Location and the size represents the Variable. </p>

<p>This simple one liner can help you produce such a plot:</p>

<p><code>p = ggplot(dat, aes(x = year_quarter, y = value, colour = value)) + geom_point(aes(shape = Location,size = Variable))</code></p>

<p>This is how the output plot looks like: </p>

<p><a href=""http://i.stack.imgur.com/gouOb.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gouOb.png"" alt=""enter image description here""></a></p>

<p>Note that you can also add <code>geom_line</code> with <code>interaction</code> if you would like the lines to be connected based on Location and Variable. </p>
"
"['classification', 'deep-learning']",Example of binary classifier with numerical features using deep learning,"<p>I used Binary classification for sentiment analysis of texts. I converted sentences into vectors by taking appropriate vectorizer and classified using OneVsRest classifier. </p>

<p>On another approach, my words were converted into vectors and there, I used a CNN based approach to classify. Both when tested on my datasets were giving comparable results as of now.</p>

<p>If you have vectors, there are already really good approaches available for binary classification which you can try. <a href=""http://arxiv.org/pdf/1509.03891v1.pdf"" rel=""nofollow"">On Binary Classification with Single–Layer
Convolutional Neural Networks</a> is a good read for you for classification using CNNs for starters. </p>

<p><a href=""http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"" rel=""nofollow"">This</a> is one of the first blogs I read to gain more knowledge about this and doesn't require much of pre-requisites to understand(I am assuming you know the basics about convolution and Neural Networks). </p>
"
"['machine-learning', 'neuralnetwork', 'gradient-descent', 'backpropagation']",How to update weights in a neural network using gradient descent with mini-batches?,"<p>Let us say that the output of one neural network given it's parameters is $$f(x;w)$$
Let us define the loss function as the squared L2 loss (in this case).
$$L(X,y;w) = \frac{1}{2n}\sum_{i=0}^{n}[f(X_i;w)-y_i]^2$$
In this case the batchsize will be denoted as $n$. Essentially what this means is that we iterate over a finite subset of samples with the size of the subset being equal to your batch-size, and use the gradient normalized under this batch. We do this until we have exhausted every data-point in the dataset. Then the epoch is over. The gradient in this case:
$$\frac{\partial L(X,y;w)}{\partial w} = \frac{1}{n}\sum_{i=0}^{n}[f(X_i;w)-y_i]\frac{\partial f(X_i;w)}{\partial w}$$
Using batch gradient descent normalizes your gradient, so the updates are not as sporadic as if you have used stochastic gradient descent. </p>
"
"['random-forest', 'feature-selection', 'scikit']",Extremely dominant feature?,"<p>Actually, it shouldn't really matter what classification algorithm you use. The whole point of machine learning is that the algorithm learns how to combine the available features to achieve the desired result. If one feature has the ability to 'turn the others off,' the algorithm will learn that (It'll also learn lots of things that you probably aren't aware of). </p>

<p>So in short, no, modifying the data this way probably won't affect classification performance. Not needing to incorporate these kinds of things into the training set is part of what makes machine learning so cool!</p>
"
"['machine-learning', 'svm']",Can 2 dimensional input be applied to SVM?,"<p>If I understand your question correctly. Yes, SVM can take multiple inputs. My suggestion for handling a vector as a feature would be to expand it out. For example,</p>

<pre><code>x0 = (1,2)       x0 = 1
x1 = .4  -----&gt;  x1 = 2
x2 = 0           x2 = .4
                 x3 = 0
</code></pre>

<p>If this does not capture all of the characteristics of the vector that are important, then you may want to add other features (like magnitude of the vector) as well. </p>
"
['bigdata'],What is the best Big-Data framework for stream processing?,"<p>It really depends on what you are looking to do. I love Apache Spark, but Storm has some history. I am sure as the streaming capability in Spark is built out that it will become a competitive solution. However, until Spark has some heavy hitting users (for streaming) there will remain unknown bugs. </p>

<p>You can also consider the community. Spark has a great community. I am not sure the level of the Storm community as I am usually the one receiving the data not handling the ingest. I can say we have used Storm on projects and I have been impressed with the real-time analysis and volumes of streaming data.</p>
"
"['machine-learning', 'feature-selection', 'svm', 'feature-extraction', 'matlab']",Different runs of feature selection algorithm giving different set of selected feature. How to choose the best set among them?,"<p>You can set a <code>seed</code> or a <code>random state</code> for the splitting process. This helps in generating a fixed random number everytime, which helps you get the same data everytime you do the CV split.</p>

<p>This can be done by <code>set_seed(...)</code> in R, and adding the <code>random_state = ...</code> in relevant functions parameters in Python. So, please add the relevant parameter for Matlab, and your CV would do good.</p>
"
"['nosql', 'relational-dbms']","the data on our relational DBMS is getting big, is it the time to move to NoSQL?","<p>A few gigabytes is not very ""<strong>big</strong>"". It's more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don't get TB's of data a day).</p>

<p>Most professionals working in a big data environment consider <strong>> ~5TB</strong> as the <em>beginning</em> of the term big data. But even then it's not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.</p>

<p>i.e. if you do alot of searches in you database it would probably be better to run a solr instance/cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance.</p>
"
"['machine-learning', 'data-mining', 'statistics', 'python', 'correlation']",Correlations - Get values in the way we want,"<p>Several kernel functions can serve as similarity functions (=scores). See a list, for example, <a href=""https://www.otexts.org/1560"" rel=""nofollow"">here</a>. You can try several of them and see which suits you the best.</p>

<p>You need something that drops fast at low distances. You can try
$$ score = 1/(1+distance)^2$$
and adjust coefficient in front of distance so that the score fits between 0 and 1
<img src=""http://i.stack.imgur.com/oufxf.png"" alt=""enter image description here"">  </p>

<p>About your picture: what are axis labels? and what are x-ticks?</p>
"
"['r', 'optimization', 'xgboost', 'hyperparameter', 'weighted-data']",How can the process of hypertuning of XGBoost parameters be automated?,"<p>In general, if you want to automate fine tuning a model's hyper parameters, its best to use a well tested package such as caret or MLR.</p>

<p>I've used the caret package extensively. <a href=""http://topepo.github.io/caret/modelList.html"" rel=""nofollow"" title=""here"">Here</a> is a reference of the parameters supported by caret for tuning a xgboost model.</p>

<p>To automatically select parameters using caret, do the following:</p>

<ol>
<li>First define a range of values of each parameter you would want caret to search. Define this in the tuning grid.</li>
<li>Start model training using caret after specifying a measure to optimize, e.g. accuracy or Kappa statistic, etc.</li>
<li>Plot or print the performance comparison for various parameter values, refine and repeat if required.</li>
</ol>

<p>Refer to the caret guide <a href=""http://topepo.github.io/caret/"" rel=""nofollow"" title=""here"">here</a> to get step-by-step instructions on using it.</p>

<p>For handling class imbalance, I've found from my experience that adjusting weights is not as helpful as under-sampling majority class and over-sampling the minority class, or a combination of the two. However, it all depends on the size of data available and the case at hand.</p>

<p>In case you need to tune some parameters which are not supported by caret, then, you could write your own iterative loop to train and test the model for different values of that parameter and then choose one that works best. I think most of the really relevant parameters have already been included in caret.</p>

<p>You would need to adjust these parameters in case the population itself changes over time. Or, the methods to gather data and their accuracy may change which could result in performance deterioration. You could run a simple check by comparing the performance of your model over the current dataset vs. a 6 month older dataset. If the performance is similar, then you may not need to update the model in the future.</p>
"
"['machine-learning', 'predictive-modeling']",Why Is Overfitting Bad in Machine Learning?,"<p>Overfitting is <em>empirically</em> bad.  Suppose you have a data set which you split in two, test and training.  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on additional (new) test datasets than models which are not overfitted.  </p>

<p>One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data (including future data).</p>

<p>Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.</p>

<p>P.S. On the ""ability to generalize"" part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail.</p>
"
"['python', 'time-series', 'anomaly-detection', 'sequential-pattern-mining']","If a time series has random time events, how to detect patterns?","<p>As a first step, to segregate the messages that appear to be a bot, you could first try binning by message size. For example, if messages sent by bots are likely to be around 128 bytes to 140 bytes, assign these to a unique bin.</p>

<p>Next, create a time series based on this bin. Try to decompose the time series using an additive or multiplicative method such as Holt Winters. A strong seasonal component would help you identify regular and repetitive messages which are being generated automatically.</p>
"
['career'],Career switch to Big Data Analytics,"<p>Due to high demand, it is possible to start a career in data science without a formal degree. My experience is that having a degree is often a 'requirement' in job descriptions, but if the employer is desperate enough, then that won't matter. In general, it's harder to get into large corporations with formalized job application processes than smaller companies without them. ""Knowing people"" can get you a long way, in either case.</p>

<p>Regardless of your education, no matter how high demand is, you must have the skills to do the job.</p>

<p>You are correct in noting that advanced statistics and other mathematics are very hard to learn independently. It is a matter of how badly you want to make the career change. While some people do have 'natural talent' in mathematics, everybody does have to do the work to learn. Some may learn more quickly, but everybody has to take the time to learn. </p>

<p>What it comes down to is your ability to show potential employers that you have a genuine interest in the field, and that you will be able to learn quickly on the job. The more knowledge you have, the more projects you can share in a portfolio, and the more work experience under your belt, the higher level jobs that will be available to you. You may have to start  in an entry level position first.</p>

<p>I could suggest ways to study mathematics independently, but that isn't part of your question. For now, just know that it's hard, but possible if you are determined to make a career change. Strike while the iron is hot (while demand is high).</p>
"
"['python', 'regression', 'linear-regression']",Is there a library that would perform segmented linear regression in python?,"<p>No, currently there isn't a package in Python that does segmented linear regression as thoroughly as those in R (e.g. <a href=""http://www.r-bloggers.com/r-for-ecologists-putting-together-a-piecewise-regression/"" rel=""nofollow"">R packages listed in this blog post</a>). Alternatively, you can use a Bayesian Markov Chain Monte Carlo algorithm in Python to create your segmented model.</p>

<p>Segmented linear regression, as implemented by all the R packages in the above link, doesn't permit extra parameter constraints (i.e. priors), and because these packages take a frequentist approach, the resulting model doesn't give you probability distributions for the model parameters (i.e. breakpoints, slopes, etc). Defining a segmented model in <a href=""http://statsmodels.sourceforge.net/"" rel=""nofollow"">statsmodels</a>, which is frequentist, is even more restrictive because the model requires a fixed x-coordinate breakpoint.</p>

<p>You can design a segmented model in Python using the Bayesian Markov Chain Monte Carlo algorithm <a href=""http://dan.iel.fm/emcee/current/"" rel=""nofollow"">emcee</a>. Jake Vanderplas wrote a useful <a href=""http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/"" rel=""nofollow"">blog post</a> and <a href=""http://arxiv.org/pdf/1411.5018.pdf"" rel=""nofollow"">paper</a> for how to implement emcee with comparisons to PyMC and PyStan.</p>

<p>Example:</p>

<ul>
<li>Segmented model with data:</li>
</ul>

<p><a href=""http://i.stack.imgur.com/QoGKH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QoGKH.png"" alt=""Segmented regression""></a></p>

<ul>
<li>Probability distributions of fit parameters:</li>
</ul>

<p><a href=""http://i.stack.imgur.com/iJ5br.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iJ5br.png"" alt=""enter image description here""></a></p>

<ul>
<li><p><a href=""https://github.com/stharrold/ARCHIVED_2015_Harrold_SDSSJ1600/blob/master/code/utils.py#L1241-L1546"" rel=""nofollow"">Link to code for segmented model</a>.</p></li>
<li><p><a href=""https://gist.github.com/stharrold/33120aaf182b66983ee8"" rel=""nofollow"">Link to (large) ipython notebook</a>.</p></li>
</ul>
"
['apache-spark'],Scan-based operations Apache Spark,"<p>Scan based operations are basically all the operations that require evaluating the predicate on an RDD. </p>

<p>In other terms, each time your create an RDD or a DataFrame in which you need to compute a <em>predicate</em> like performing a filter, map on a case class, per example, or even explain method will be considered as a scan based operation. </p>

<p>To be more clear, let's review the definition of a predicate.</p>

<p>A predicate or a functional predicate is a logical symbol that may be applied to an object term to produce another object term. </p>

<p>Functional predicates are also sometimes called <em>mappings</em>, but that term can have other meanings as well.</p>

<p>Example :</p>

<pre><code>// scan based transformation
rdd.filter(!_.contains(""#"")) // here the predicate is !_.contains(""#"")

// another scan based transformation
rdd.filter(myfunc) // myfunc is a boolean function

// a third also trivial scan based transformation followed by a non scan based one.
rdd.map(myfunc2) 
   .reduce(myfunc3)
</code></pre>

<p>If you want to understand how spark internals work, I suggest that you watch the <a href=""https://spark-summit.org/2014/talk/a-deeper-understanding-of-spark-internals"" rel=""nofollow"">presentation</a> made by Databricks about the topics</p>
"
"['machine-learning', 'pgm']",Is there any domain where Bayesian Networks outperform neural networks?,"<p>One of the areas where Bayesian approaches are often used, is where one needs interpretability of the prediction system. You don't want to give doctors a Neural net and say that it's 95% accurate. You rather want to explain the assumptions your method makes, as well as the decision process the method uses. </p>

<p>Similar area is when you have a strong prior domain knowledge and want to use it in the system. </p>
"
"['machine-learning', 'python', 'svm', 'scikit']",Face Recognition using eigenfaces and SVM,"<p>Take a look at the code that you linked to:</p>

<pre><code># Download the data, if not already on disk and load it as numpy arrays

lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
</code></pre>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html#sklearn.datasets.fetch_lfw_people"" rel=""nofollow""><code>fetch_lfw-poeple</code></a> is a routine that loads the data and is detailed <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html#sklearn.datasets.fetch_lfw_people"" rel=""nofollow"">here</a>.</p>

<p>Hope this helps!</p>
"
"['sampling', 'sas']","SAS. How to write ""OR""","<p>i'm assuming this will be migrated to stack overflow, but instead of trying to do <code>if var1=1 or 2</code> wouldn't it be better to use <code>if var1 in (1, 2)</code>?</p>

<p>...and somebody with enough reputation should probably create an sas tag (and an spss tag while you're at it) unless data scientists only use open source languages like <code>r</code> and <code>python</code> now...</p>
"
"['machine-learning', 'reinforcement-learning']",Value Updation Dynamic Programming Reinforcement learning,"<p>The probabilities you describe refer only to the go-north action. It means that if you want to go north, you have 80% chance of actually going north and 20% of going left or right, making the problem more difficult (non-deterministic). This rule applies to every direction. Also, the formula does not tell which action to chose, just how to update the values. In order to select an action, assuming a greedy-policy, you'd select the one with the highest expected value ($V(s')$).</p>

<p>The formula says to sum the values for all possible outcomes from the best action. So, supposing go-north is indeed the best action, you have: </p>

<p>$$.8 * (-.1 + 0) + .1 * (-.1 + 0) + .1 * (-.1 + 0) = -.1$$</p>

<p>But let us suppose that you still don't know which is the best action and want to select one greedily. Then you must compute the sum for each possible action (north, south, east, west). Your example has all values set to 0 and the same reward and so is not very interesting. Let's say you have a +1 reward to east (-0.1 for the remaining directions) and that south already has V(s) = 0.5 (0 for the remaining states). Then you compute the value for each action (let $\gamma = 1$, since it is a user-adjusted parameter):</p>

<ul>
<li>North: $.8 * (-.1 + 0) + .1 * (-.1 + 0) + .1 * (1 + 0) = -.08 - .01 + .1 = .01$</li>
<li>South: $.8 * (-.1 + .5) + .1 * (-.1 + 0) + .1 * (1 + 0) = 0.32 - .01 + .1 = .41$</li>
<li>East: $.8 * (1 + 0) + .1 * (-.1 + 0) + .1 * (-.1 + .5) = .8 - .01 + .04 = .83$</li>
<li>West: $.8 * (-.1 + 0) + .1 * (-.1 + 0) + .1 * (-.1 + .5) = -.08 - .01 + .04 = -.05$</li>
</ul>

<p>So you would update your policy to go <strong>East</strong> from the current state, and update the current state value to <strong>0.83</strong>.</p>
"
"['machine-learning', 'r', 'nlp']",Identifying templates with parameters in text fragments,"<p>The basic rationale behind the following suggestion is to associate ""eigenvectors"" and ""templates"".</p>

<p>In particular one could use LSA on the whole corpus based on a a bag-of-words. The resulting eigenvectors would serve as surrogate templates; these eigenvectors should not be directly affected by the number of words in each template. Subsequently the scores could be used to cluster the documents together following a standard procedure (eg. $k$-means in conjunction with AIC).  As an alternative to LSA one could use NNMF. Let me point out that the LSA (or NNMF) would probably need to be done to the transformed TF-IDF rather than the raw word-counts matrix.</p>
"
"['machine-learning', 'python', 'deep-learning', 'scikit', 'random-forest']",Parameters in GridSearchCV in scikit-learn,"<p>I would say that you have to remove <code>random_state</code> from the parameter grid. That, or put something like [7,X] which will work but is nonsense I think. If you want to use fixed <code>random_state=7</code> you should write it when you instantiate the estimator just as another hyperparameter (next to <code>n_estimators</code>).</p>

<p>I can't test it right now but I'd say thats the problem.</p>
"
"['cross-validation', 'performance']",Cross-validation strategy,"<p>The natural choice would be the total squared error across the N predicted values, averaged across all examples. This is the simple extension of mean squared error from the univariate case. If you're using multivariate linear regression, this is in fact what you want to optimize in order to get the maximum likelihood estimate of the parameters as well.</p>
"
"['neuralnetwork', 'deep-learning', 'image-classification']",How to prepare colored images for neural networks?,"<p>Your R,G, and B pixel values can be broken into 3 separate channels (and in most cases this is done for you). These channels are treated no differently than feature maps in higher levels of the network. Convolution extends naturally to more than 2 dimensions.</p>

<p>Imagine the greyscale, single-channel example. Say you have N feature maps to learn in the first layer. Then the output of this layer (and therefore the input to the second layer) will be comprised of N channels, each of which is the result of convolving a feature map with each window in your image. Having 3 channels in your first layer is no different. </p>

<p>This tutorial does a nice job on convolution in general.</p>

<p><a href=""http://deeplearning.net/tutorial/lenet.html"" rel=""nofollow"">http://deeplearning.net/tutorial/lenet.html</a></p>
"
"['logistic-regression', 'theano']",Early-Stopping for logistic regression. Theano,"<p><code>Patience</code> is the number of training batches to do before you stop. <code>iter</code> is the number of training batches you've seen. Each iteration, you decide whether or not your validation is lower than your previous best. The improvement is only stored if the score is lower than <code>improvement_threshold * validation_score</code>.  </p>

<p>It appears that <code>patience_increase</code> is a multiplier.  Every time you have a new lowest score, you up the total number or training batches to <code>iter*patience_increase</code>, but not below the current value of <code>patience</code>.</p>

<p><code>validation_frequency</code> is just the number of batches between times you check the validation score.</p>
"
['hadoop'],"Good books for Hadoop, Spark, and Spark Streaming","<p>There's such an overwhelming amount of literature that with programming, databases, and Big Data I like to stick to the O'reilly series as my go-to source. O'reilly books are extremely popular in the industry and I've been very satisfied.</p>

<p>A current version of </p>

<ol>
<li><a href=""http://shop.oreilly.com/product/0636920021773.do"" rel=""nofollow"">Hadoop: The Definitive Guide</a>, </li>
<li><a href=""http://shop.oreilly.com/product/0636920025122.do"" rel=""nofollow"">MapReduce Design Patterns</a>, and </li>
<li><a href=""http://shop.oreilly.com/product/0636920028512.do"" rel=""nofollow"">Learning Spark</a> </li>
</ol>

<p>might suit your needs by providing high quality, immediately useful information and avoiding information overload -- all are published by O'reilly.</p>

<p>Spark Streaming is covered in Chapter 13 of ""Learning Spark"".</p>
"
"['sklearn', 'pandas', 'cross-validation']","Sklearn: How to adjust data set proportion during training, but not testing","<p>Try to use predictor option class_weight='balanced' or auto. It worked really well for me for <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"" rel=""nofollow"">SGDClassifier</a> in a similar situation.</p>
"
['clustering'],Filter Data for clustering,"<p>If you can afford to do the full join once, do it and learn which columns are useful through <a href=""https://en.wikipedia.org/wiki/Feature_selection"" rel=""nofollow"">feature selection</a>. Then you can only SELECT these columns for subsequent iterations, when the database is updated.</p>

<p>Here's a survey: <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.295.8115&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">Feature Selection for Clustering: A Review</a></p>
"
"['python', 'random-forest', 'pandas']","ValueError: Input contains NaN, infinity or a value too large for dtype('float32')","<p>With <code>np.isnan(X)</code> you get a boolean mask back with True for positions containing <code>NaN</code>s.</p>

<p>With <code>np.where(np.isnan(X))</code> you get back a tuple with i, j coordinates of <code>NaN</code>s.</p>

<p>Finally, with <code>np.nan_to_num(X)</code> you ""replace nan with zero and inf with finite numbers"".</p>

<p>Alternatively you can use </p>

<ul>
<li>sklearn.preprocessing.Imputer for mean / median imputation of missing values, or</li>
<li>pandas' pd.DataFrame(X).fillna(), if you need something other than filling it with zeros.</li>
</ul>
"
"['python', 'sklearn']",Difference between fit and fit_transform in scikit_learn models?,"<p>To <a href=""https://en.wikipedia.org/wiki/Standard_score"" rel=""nofollow"">center the data</a> (make it have zero mean and unit standard error) you subtract the mean, and divide the result by standard deviation.</p>

<p>$$x' = \frac{x-\mu}{\sigma^2}$$</p>

<p>You do that on the training set of data. But then you have to apply the same transformation to your testing set (e.g. in cross-validation), or to newly obtained examples before forecast. But you have to use the same two parameters $\mu$ and $\sigma$ (values) that you used for centering the training set.</p>

<p>Hence, every sklearn's transform's <code>fit()</code> just calculates the parameters (e.g. $\mu$ and $\sigma$ in case of <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""nofollow"">StandardScaler</a>) and saves them as an internal objects state. Afterwards, you can call its <code>transform()</code> method to apply the transformation to a particular set of examples.</p>

<p><code>fit_transform()</code> joins these two steps and is used for the initial fitting of parameters on the training set $x$, but it also returns a transformed $x'$. Internally, it just calls first <code>fit()</code> and then <code>transform()</code> on the same data.</p>
"
"['bigdata', 'apache-spark', 'pyspark']",Machine Learning in Spark,"<p>Probability can be found for the test dataset once you trained the model and transformed for the test dataset e.g: if your trained Naive Bayes model is <strong>model</strong>  then  <code>model.transform(test)</code> contains a node of <strong>probability</strong>, for more details please check the below code, going to show you the probability node and others useful nodes also for <strong>iris</strong> dataset.</p>

<p>Partition dataset randomly into Training and Test sets. Set seed for reproducibility</p>

<pre><code>(trainingData, testData) = irisdf.randomSplit([0.7, 0.3], seed = 100)

trainingData.cache()
testData.cache()

print trainingData.count()
print testData.count()
</code></pre>

<p>Output:</p>

<pre><code>103
47
</code></pre>

<p>Next, we will use the VectorAssembler() to merge our feature columns into a single vector column, which we will be passing into our Naive Bayes model. Again, we will not transform the dataset just yet as we will be passing the VectorAssembler into our ML Pipeline.</p>

<pre><code>from pyspark.ml.feature import VectorAssembler
vecAssembler = VectorAssembler(inputCols=[""SepalLength"", ""SepalWidth"", ""PetalLength"", ""PetalWidth""], outputCol=""features"")
</code></pre>

<p>For <strong>iris dataset</strong>, it has three classes namely <strong>setosa</strong>, <strong>versicolor</strong> and <strong>virginica</strong>. So let's create a <strong>Multiclass Naive Bayes Classifier</strong> using pysaprk library <code>ml</code>.</p>

<pre><code>from pyspark.ml.classification import NaiveBayes
from pyspark.ml import Pipeline

# Train a NaiveBayes model
nb = NaiveBayes(smoothing=1.0, modelType=""multinomial"")

# Chain labelIndexer, vecAssembler and NBmodel in a pipeline
pipeline = Pipeline(stages=[labelIndexer, vecAssembler, nb])

# Run stages in pipeline and train model
model = pipeline.fit(trainingData)
</code></pre>

<p>Analyse the created mode <strong>model</strong>, from which we can make predictions.</p>

<pre><code>predictions = model.transform(testData)
# Display what results we can view
predictions.printSchema()
</code></pre>

<p>Output </p>

<pre><code>root
 |-- SepalLength: double (nullable = true)
 |-- SepalWidth: double (nullable = true)
 |-- PetalLength: double (nullable = true)
 |-- PetalWidth: double (nullable = true)
 |-- Species: string (nullable = true)
 |-- label: double (nullable = true)
 |-- features: vector (nullable = true)
 |-- rawPrediction: vector (nullable = true)
 |-- probability: vector (nullable = true)
 |-- prediction: double (nullable = true)
</code></pre>

<p>You can also select a particular node to view for some dataset as:</p>

<pre><code># DISPLAY Selected nodes only
display(predictions.select(""label"", ""prediction"", ""probability""))
</code></pre>

<p>Above will show you in tabular formate.</p>

<p>Reference: </p>

<ol>
<li><a href=""http://spark.apache.org/docs/latest/mllib-naive-bayes.html"" rel=""nofollow"">spark</a></li>
<li><a href=""https://spark.apache.org/docs/latest/ml-classification-regression.html"" rel=""nofollow"">Models using pipeline</a></li>
<li><a href=""https://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/"" rel=""nofollow"">https://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/</a></li>
<li><a href=""http://stackoverflow.com/questions/31028806/how-to-create-correct-data-frame-for-classification-in-spark-ml"">http://stackoverflow.com/questions/31028806/how-to-create-correct-data-frame-for-classification-in-spark-ml</a></li>
</ol>
"
"['machine-learning', 'beginner', 'similarity', 'supervised-learning', 'recommender-system']",Which supervised learning algorithms are available for matching?,"<p>You can try to frame this problem as a recommender systems situation. Where you have your users (prospective students) and items (alumni) and want to recommend to the users one item. </p>

<p>It's not a perfect fit as you want just one item for each user and you don't have previous match data for each user. However you could investigate this idea a bit further. I'm applying these techniques to the recruitment problem, I'm matching users with job offers and I'm having some success. </p>

<p>Try to read a bit about recommender systems, to start I recommend chapter 9 of mining massive data sets, it's really introductory, but gives a good overview of the most common techniques. </p>
"
"['random-forest', 'sklearn', 'decision-trees']",Why is the number of samples smaller than the number of values in my decision tree?,"<p>Yes, it seems to display unique samples, the others have been duplicated by the bootstrap sampling.</p>

<p>There's the 0.632 rule - when you have N items and you take a random sample of size N with replacement (as bootstrap does), you only get 63.2% of the samples from N, the rest are duplicates.</p>

<p>That roughly matches what you've seen: 0.632 * (507+545) = 665 unique samples.</p>

<p>You can also try it with some Python code:</p>

<pre><code>samples = np.arange(507 + 545)

bootstrap_sample = np.random.choice(samples, size=len(samples), replace=True)

print(len(np.unique(bootstrap_sample)))
</code></pre>

<p>This always prints values closely around 665.</p>
"
"['r', 'convnet', 'software-recommendation']",Convolutional Neural Networks in R,"<p>I guess there is no package for cnn but you can write your own convolutional layer. mxnet or h2o will be useful for it. </p>

<p>check this out:</p>

<p><a href=""http://dmlc.ml/rstats/2015/11/03/training-deep-net-with-R.html"" rel=""nofollow"">http://dmlc.ml/rstats/2015/11/03/training-deep-net-with-R.html</a></p>
"
"['dataset', 'search', 'google']",Gathering the number of Google results from a large amount of searches.,"<p>With any search engine you will be limited by number of requests and any way of outcoming those limits will be a gray zone of violation of end user agreement (and, eventually, you will get banned for some time, of course). You should be looking into Search APIs of known search engines, for example, Bing gives you 5000 searches per month for free which - for a proof of concept research - might be enough. Also, 5k/month will give you some 20-30k until summer, so your data set will become bigger while you will be polishing your idea.</p>

<p>Also, Google's free tier search is limited to 100 requests per day. Which gives you completely legal 3k per month as well. Combined (given, you treat Google and Bing results as equal) you get 8k per month.</p>
"
"['machine-learning', 'classification', 'svm', 'accuracy', 'random-forest']",How to increase accuracy of classifiers?,"<p><strong>Dimensionality Reduction</strong></p>

<p>Another important procedure is to compare the error rates on training and test dataset to see if you are overfitting (due to the ""curse of dimensionality""). E.g., if your error rate on the test dataset is much larger than the error on the training data set, this would be one indicator.<br>
In this case, you could try dimensionality reduction techniques, such as PCA or LDA.</p>

<p>If you are interested, I have written about PCA, LDA and some other techniques here: <a href=""http://sebastianraschka.com/index.html#machine_learning"">http://sebastianraschka.com/index.html#machine_learning</a> and in my GitHub repo here: <a href=""https://github.com/rasbt/pattern_classification"">https://github.com/rasbt/pattern_classification</a></p>

<p><strong>Cross validation</strong></p>

<p>Also you may want to take a look at cross-validation techniques in order to evaluate the performance of your classifiers in a more objective manner</p>
"
"['python', 'tools', 'version-control']",Tools and protocol for reproducible data science using Python,"<p>The topic of <em>reproducible research</em> (RR) is <strong>very popular</strong> today and, consequently, is <strong>huge</strong>, but I hope that my answer will be <strong>comprehensive enough</strong> as an answer and will provide enough information for <strong>further research</strong>, should you decide to do so.</p>

<p>While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more <strong>universal tools</strong> (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let's take a look what tools are available per your list.</p>

<p>1) <strong>Tools for data version control</strong>. Unless you plan to work with (very) <em>big data</em>, I guess, it would make sense to use the same <code>git</code>, which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful: <a href=""http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git"">http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git</a>.</p>

<p>2) <strong>Tools for managing RR workflows and experiments</strong>. Here's a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity):</p>

<ul>
<li><p><em>Taverna Workflow Management System</em> (<a href=""http://www.taverna.org.uk"">http://www.taverna.org.uk</a>) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal <em>myExperiment</em> (<a href=""http://www.myexperiment.org"">http://www.myexperiment.org</a>), where user can store and share their RR workflows. Web-based RR portal, fully compatible with <em>Taverna</em> is called <em>Taverna Online</em>, but it is being developed and maintained by totally different organization in Russia (referred there to as <em>OnlineHPC</em>: <a href=""http://onlinehpc.com"">http://onlinehpc.com</a>).</p></li>
<li><p><em>The Kepler Project</em> (<a href=""https://kepler-project.org"">https://kepler-project.org</a>)</p></li>
<li><p><em>VisTrails</em> (<a href=""http://vistrails.org"">http://vistrails.org</a>)</p></li>
<li><p><em>Madagascar</em> (<a href=""http://www.reproducibility.org"">http://www.reproducibility.org</a>)</p></li>
</ul>

<p><strong>EXAMPLE</strong>. Here's an interesting article on scientific workflows with an example of the <strong>real</strong> workflow design and data analysis, based on using <em>Kepler</em> and <em>myExperiment</em> projects: <a href=""http://f1000research.com/articles/3-110/v1"">http://f1000research.com/articles/3-110/v1</a>.</p>

<p>There are many RR tools that implement <em>literate programming</em> paradigm, exemplified by <code>LaTeX</code> software family. Tools that help in report generation and presentation is also a large category, where <code>Sweave</code> and <code>knitr</code> are probably the most well-known ones. <code>Sweave</code> is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort (<a href=""http://stackoverflow.com/questions/2161152/sweave-for-python"">http://stackoverflow.com/questions/2161152/sweave-for-python</a>). I think that <code>knitr</code> might be a better option, as it's modern, has extensive support by popular tools (such as <code>RStudio</code>) and is language-neutral (<a href=""http://yihui.name/knitr/demo/engines"">http://yihui.name/knitr/demo/engines</a>).</p>

<p>3) <strong>Protocol and suggested directory structure</strong>. If I understood correctly what you implied by using term <em>protocol</em> (<em>workflow</em>), generally I think that standard RR data analysis workflow consists of the following sequential phases: <em>data collection</em> => <em>data preparation</em> (cleaning, transformation, merging, sampling) => <em>data analysis</em> => <em>presentation of results</em> (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps.</p>

<p>For sample directory structure, you may take a look at documentation for R package <code>ProjectTemplate</code> (<a href=""http://projecttemplate.net"">http://projecttemplate.net</a>), as an attempt to automate data analysis workflows and projects:</p>

<p><img src=""http://i.stack.imgur.com/0B2vo.png"" alt=""enter image description here""></p>

<p>4) <strong>Automated build/run tools</strong>. Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is <code>make</code>. Read the following article for some reasons to use <code>make</code> as the preferred RR workflow automation tool: <a href=""http://bost.ocks.org/mike/make"">http://bost.ocks.org/mike/make</a>. Certainly, there are other <strong>similar</strong> tools, which either improve some aspects of <code>make</code>, or add some additional features. For example: <code>ant</code> (officially, Apache Ant: <a href=""http://ant.apache.org"">http://ant.apache.org</a>), <code>Maven</code> (""next generation <code>ant</code>"": <a href=""http://maven.apache.org"">http://maven.apache.org</a>), <code>rake</code> (<a href=""https://github.com/ruby/rake"">https://github.com/ruby/rake</a>), <code>Makepp</code> (<a href=""http://makepp.sourceforge.net"">http://makepp.sourceforge.net</a>). For a comprehensive list of such tools, see Wikipedia: <a href=""http://en.wikipedia.org/wiki/List_of_build_automation_software"">http://en.wikipedia.org/wiki/List_of_build_automation_software</a>.</p>
"
"['statistics', 'glm', 'regression']",Extrapolating GLM coefficients for year a product was sold into future years?,"<p>I believe that this is a case for applying <em>time series analysis</em>, in particular <em>time series forecasting</em> (<a href=""http://en.wikipedia.org/wiki/Time_series"" rel=""nofollow"">http://en.wikipedia.org/wiki/Time_series</a>). Consider the following resources on <strong>time series regression</strong>:</p>

<ul>
<li><a href=""http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471363553.html"" rel=""nofollow"">http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471363553.html</a></li>
<li><a href=""http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf"" rel=""nofollow"">http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf</a> (especially section
4.6)</li>
<li><a href=""http://arxiv.org/abs/0802.0219"" rel=""nofollow"">http://arxiv.org/abs/0802.0219</a> (Bayesian approach)</li>
</ul>
"
"['machine-learning', 'random-forest']",Do Random Forest overfit?,"<p>Every ML algorithm with high complexity can overfit. However, the OP is asking whether an RF will not overfit when increasing the number of trees in the forest.</p>

<p>In general, ensemble methods reduces the prediction variance to almost nothing, improving the accuracy of the ensemble. If we define the variance of the expected generalization error of an individual randomized model as:</p>

<p><img src=""http://i.stack.imgur.com/ZUORL.gif"" alt=""""></p>

<p>From <a href=""http://arxiv.org/abs/1407.7502"" rel=""nofollow"">here</a>, the variance of the expected generalization error of an ensemble corresponds to:</p>

<p><img src=""http://i.stack.imgur.com/5Zf9e.gif"" alt=""""></p>

<p>where <code>p(x)</code> is the Pearson’s correlation coefficient between the predictions of two randomized models trained on the same data from two independent seeds. If we increase the number of DT's in the RF, larger <code>M</code>, the variance of the ensemble decreases when <code>ρ(x)&lt;1</code>. Therefore, the variance of an ensemble is strictly smaller than the variance of an individual model. </p>

<p>In a nutshell, increasing the number of individual randomized models in an ensemble will never increase the generalization error.</p>
"
"['neuralnetwork', 'image-classification', 'computer-vision', 'convnet']",What is the depth of an image in Convolutional Neural Network?,"<p>It means that the number of filters (<a href=""http://stats.stackexchange.com/a/188216/12359"">a.k.a. kernels, or  feature detector</a>) in the previous convolutional layer is 96. You may want to watch the <a href=""https://youtu.be/LxfUGhug-iQ?t=29m55s"" rel=""nofollow"">video</a> of the lecture, and in particular <a href=""http://cs231n.stanford.edu/slides/winter1516_lecture7.pdf"" rel=""nofollow"">this slide</a>, which mentions that a filter is applied to the full depth of your previous layer:</p>

<p><a href=""http://i.stack.imgur.com/AuqKy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AuqKy.png"" alt=""enter image description here""></a></p>
"
['visualization'],Library/package/tool for geographical data visualizations,"<p>Assuming you want your visualizations to be available on the internet, there are many options for this:</p>

<ul>
<li><a href=""https://developers.google.com/maps/"" rel=""nofollow"">Google Maps</a> (and <a href=""http://tables.googlelabs.com/"" rel=""nofollow"">Google Fusion Tables</a>)</li>
<li><a href=""https://www.mapbox.com/"" rel=""nofollow"">Mapbox</a> (and the related <a href=""http://leafletjs.com/"" rel=""nofollow"">LeafletJS</a>)</li>
<li><a href=""http://cartodb.com/"" rel=""nofollow"">CartoDB</a></li>
<li><a href=""http://bost.ocks.org/mike/map/"" rel=""nofollow"">D3</a></li>
</ul>

<p>It really comes down to exactly what you want to do and how much control you want to have over the map.  Additionally you should consider what sort of interactivity you need.  </p>

<p>Google Maps is probably the most user friendly for very basic map functions but is somewhat limited in what you can do stylistically.  </p>

<p>Mapbox and CartoDB are both user friendly and offer good options for styling and displaying different varieties of data.  However, they also both tend to require subscription fees for anything other than small maps. Also, the last time I checked, CartoDB explicitly handles animation and time-series data where Mapbox does not.</p>

<p>D3 will probably give you the most control over display, animation, and interactivity but also has a long learning curve. Even if you don't want the map to be available on the internet, this is still a very good tool for making interactive visualizations that run in the browser.</p>

<p>If you don't care as much about the visualizations being online, you can get a lot of work done in open GIS software like <a href=""http://www.qgis.org/en/site/"" rel=""nofollow"">QGIS</a> or <a href=""http://grass.osgeo.org/"" rel=""nofollow"">GrassGIS</a>, though I don't know if user interactivity is really an option there.</p>

<p>As I said though, it really comes down to the specifics of exactly what you're trying to do and how comfortable you are with various aspects of mapping and coding.</p>
"
['data-mining'],What kind of research can be done with an email data set?,"<p>You're learning, are you? Try to find something easy and interesting to start. Why don't you start off something easy like building a Bayesian model to predict which email will get deleted. You should glance over those deleted emails, are they spams? are they just garbage?</p>

<p>Here, you have a simply supervised model where the data-set already labels the emails for you (deleted or not). Think of something easy like words, titles, length of the email etc, see if you can build a model that predicts email deletion.</p>
"
['data-mining'],Creating Strings corresponding to Location Co-ordinates,"<p>Your question has nothing to do with NLP or text-mining (as you claim in the attached tags), or data science in general. It's a pure programming question best suitable for <a href=""http://stackoverflow.com"">StackOverflow</a>.</p>

<p>Moreover, you don't really need any libraries to do, what you want to do. A simple function will do. NOTE: I am using <code>map</code> and <code>reduce</code> functions on purpose to include at least a little bit of data science-related stuff (WINK):</p>

<pre><code>coord = (1.23, 4.56)

def get_coord(coord):
   coord_str = map(lambda x: str(x).replace('.', ''), coord)
   for level in range(1, len(coord_str[0])+1):
      yield reduce(lambda x, y: (x[:level]+'_'+y[:level]), coord_str)

print list(get_coord(coord))
</code></pre>

<p>Running this code will result in printing:</p>

<pre><code>['1_4', '12_45', '123_456']
</code></pre>

<p>You're not discussing any corner cases in your question, so I assume there are none.</p>
"
['cross-validation'],Which observation to use when doing k-fold validation or boostrap?,"<p>I recommend using the second option you presented.  I would use $T$ with 10-fold CV to select my modeling technique and optimal tuning parameters.  Take a look at what performed the best (""best"" being the model that gives us the best error, but also doesn't have the error fluctuate too much from fold to fold).  After selecting a model, you can use the model on $V$ to get a realistic error rate.</p>

<p>The reason I don't recommend the first option is: There are varying degrees of over fitting that can occur when going through model selection and model tuning, then using that same data to get an error rate.  CV is a great way to limit this overfitting and it gives us a sense of performance variance which is great, but a classic hold-out validation set is the gold standard for model performance.  In your case the first option might not be wrong (depends a lot on data/techniques), but if a hold-out validation set is available I would go for that.</p>
"
"['python', 'statistics', 'anomaly-detection']",change detection,"<p>Consider how an algorithm might detect a change. You're observing instances of some random variable, $X_1,X_2,\dots,X_{k-1}$. Suddenly (and unknown to you) at $X_k$ something about the distribution of $X$ changes. Now your observations $X_k,\dots,X_n$ are different in some way. You want to know what $k$ is based on your observations alone.</p>

<p>In order to detect the change, you have to have some idea of what 'before' might look like so you can have confidence that 'after' is really different. So, yes, <em>all</em> change detection algorithms will use some run length before and after the true change to make a decision. Anything else would be an even wilder kind of predicting the future.</p>

<p>It seems like the real concern might be the latency of signal detection. You'd like the sensor to detect it after just a few instances of the data after the true change point. </p>

<p>So my question is, do you really need it to work <em>now</em>? It seems reasonable to me that you're not interested in the number of data points, but the time it takes to gather them. If you have a sensor that updates 100,000 times a second, 100 data points isn't a huge deal.</p>
"
"['classification', 'bigdata']",Classifying transactions as malicious,"<p>This problem is popularly called the ""<strong><a href=""http://www.azleg.gov/ars/13/02105.htm"">Credit Card Fraud Detection</a></strong>""</p>

<p>There are several classification algorithms, which aim to tackle this problem. </p>

<p>With the knowledge of the dataset you possess, the Decision Trees algorithm can be employed for detecting malicious transactions from the non-malicious ones. This <a href=""http://www.iaeng.org/publication/IMECS2011/IMECS2011_pp442-447.pdf"">paper</a> is a nice resource to learn and develop the intuition about fraud detection and the usage of basic classification algorithms like the Decision Trees and the SVMs for solving the problem.</p>

<p>There are <a href=""https://scholar.google.co.in/scholar?hl=en&amp;q=Fraud%20Detection&amp;btnG="">several other papers</a> which solve this problems employing algorithms like <a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=4280163&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4280076%2F4280077%2F04280163.pdf%3Farnumber%3D4280163"">Neural Networks</a>, Logistic Regression, Genetic Algorithms, etc. However, the paper which uses the decision trees algorithm is a nice place to start learning.</p>

<p><strong>what would be a good place to host the 600 or so GB of data for cheaps?</strong></p>

<p><a href=""http://what%20would%20be%20a%20good%20place%20to%20host%20the%20600%20or%20so%20GB%20of%20data%20for%20cheaps?"">Aws S3</a> would be a nice, cheap way to do that. It also integrates nicely with Redshift, in case you want to do complex analytics on the data.</p>
"
"['python', 'classification', 'neuralnetwork', 'scikit', 'image-classification']",Image Segmentation with a challenging background,"<p>It would be appreciated if you could explain precisely what your goal is:</p>

<ol>
<li>you want to identify what animal is in your picture ?</li>
<li>you want to count the number of animals ?</li>
<li>you want to get the position of each animal in the picture ?</li>
</ol>

<p>In any case, I know that you can get some already trained neural nets from google or anywhere else. This neural net can be used with caffe as it is the case in this google deepdream stuff on github (look at ):</p>

<p><a href=""https://github.com/google/deepdream/blob/master/dream.ipynb"" rel=""nofollow"">https://github.com/google/deepdream/blob/master/dream.ipynb</a></p>

<p>Then, if you want to highlight or identify the positions of your animals, you'll find this article inspiring:</p>

<p><a href=""http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf"" rel=""nofollow"">http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf</a></p>

<p>It explain how to reverse convolutional networks to identify what part of your image helped to recognize what is inside. The found projection get you something similar to your second picture (called a mask), but depending on the neural net you use, you can get better results. </p>
"
"['statistics', 'time-series', 'experiments', 'ab-test']",How to determine if a company decision was successful or not?,"<p>You could use time series approach to model the ""what if not"" scenario and compare it with your values after the new program introduction. Check the causal_impact packet from Google and you might also find helpful this tutorial for probabilistic programming. <a href=""http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Prologue/Prologue.ipynb"" rel=""nofollow"">http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Prologue/Prologue.ipynb</a></p>
"
"['nlp', 'word-embeddings', 'terminology', 'word2vec']",Are Word2Vec and Doc2Vec both distributional representation or distributed representation?,"<p>The reply from Andrey Kutuzov via google groups felt satisfactory</p>

<blockquote>
  <p>I would say that word2vec algorithms are based on both. </p>
  
  <p>When people say <code>distributional representation</code>, they usually mean the 
  linguistic aspect: meaning is context, know the word by its company and 
  other famous quotes. </p>
  
  <p>But when people say <code>distributed representation</code>, it mostly doesn't have 
  anything to do with linguistics. It is more about computer science 
  aspect. If I understand Mikolov and other correctly, the word 
  <code>distributed</code> in their papers means that each single component of a 
  vector representation does not have any meaning of its own. The 
  interpretable features (for example, word contexts in case of word2vec) 
  are hidden and <code>distributed</code> among uninterpretable vector components: 
  each component is responsible for several interpretable features, and 
  each interpretable feature is bound to several components. </p>
  
  <p>So, word2vec (and doc2vec) uses distributed representations technically, 
  as a way to represent lexical semantics. And at the same time it is 
  conceptually based on distributional hypothesis: it works only because 
  distributional hypothesis is true (word meanings do correlate with their 
  typical contexts). </p>
  
  <p>But of course often the terms <code>distributed</code> and <code>distributional</code> are 
  used interchangeably, increasing misunderstanding :) </p>
</blockquote>
"
"['machine-learning', 'classification']",What is the actual output of Principal Component Analysis?,"<p>I agree with dpmcmlxxvi's answer that the common ""output"" of PCA is computing and finding the eigenvectors for the principal components and the eigenvalues for the variances, but I can't add comments yet and would still like to contribute. </p>

<p>Once you hit this step of calculating the eigenvectors and eigenvalues of the principal components, you can do many types of analyses depending on your needs.</p>

<p>I believe the ""output"" you are specifically asking about in your question is the resultant data set of applying a transformation or projection of the original data set into the desired linear subspace (of n-dimensions). This is taking the output of PCA and applying it on your original data set. </p>

<p>This <a href=""http://sebastianraschka.com/Articles/2014_pca_step_by_step.html"" rel=""nofollow"" title=""PCA step by step example"">PCA step by step example</a> may help. The ultimate output of this 6 step analysis was the projection of a 3 dimensional data set into 2 dimensions. Here are the high level steps:</p>

<blockquote>
  <ol>
  <li>Taking the whole dataset ignoring the class labels</li>
  <li>Compute the d-dimensional mean vector</li>
  <li>Computing the scatter matrix (alternatively, the covariance matrix)</li>
  <li>Computing eigenvectors and corresponding eigenvalues</li>
  <li>Ranking and choosing k eigenvectors</li>
  <li>Transforming the samples onto the new subspace</li>
  </ol>
</blockquote>

<p>Ultimately, step 4 is the ""output"" since that is where the common requirements for performing PCA are fulfilled. We can make different decisions at steps 5 and 6 and produce alternative output there.  </p>

<p>A few more possibilities:</p>

<ul>
<li>You could decide to project the observations <em>with outliers removed</em></li>
<li>Another possible outcome here would be to calculate the proportion of
variance explained by one or any combination of principal components. For example, the proportion of variance explained by the first two principal components of K components is <code>(λ1+λ2)/(λ1+λ2+. . .+λK)</code>.</li>
<li>After plotting the projected observations into the first two principal components (as in the given example), you can impose a plot of the loadings of each of the original dimensions into the subspace (scaled by the standard deviation of the principal components). This way, we can see the contribution of the original dimensions (in your case a - e) to principal component 1 and 2. The biplot is another common product of PCA.</li>
</ul>
"
"['machine-learning', 'data-mining', 'r', 'predictive-modeling', 'random-forest']",How to avoid overfitting in random forest?,"<p>Relative to other models, Random Forests are less likely to overfit but it is still something that you want to make an explicit effort to avoid. Tuning model parameters is definitely one element of avoiding overfitting but it isn't the only one. In fact I would say that your training features are more likely to lead to overfitting than model parameters, especially with a Random Forests. So I think the key is really having a reliable method to evaluate your model to check for overfitting more than anything else, which brings us to your second question.</p>

<p>As alluded to above, running cross validation will allow to you avoid overfitting. Choosing your best model based on CV results will lead to a model that hasn't overfit, which isn't necessarily the case for something like out of the bag error. The easiest way to run CV in R is with the <code>caret</code> package. A simple example is below:</p>

<pre><code>&gt; library(caret)
&gt; 
&gt; data(iris)
&gt; 
&gt; tr &lt;- trainControl(method = ""cv"", number = 5)
&gt; 
&gt; train(Species ~ .,data=iris,method=""rf"",trControl= tr)
Random Forest 

150 samples
  4 predictor
  3 classes: 'setosa', 'versicolor', 'virginica' 

No pre-processing
Resampling: Cross-Validated (5 fold) 

Summary of sample sizes: 120, 120, 120, 120, 120 

Resampling results across tuning parameters:

  mtry  Accuracy  Kappa  Accuracy SD  Kappa SD  
  2     0.96      0.94   0.04346135   0.06519202
  3     0.96      0.94   0.04346135   0.06519202
  4     0.96      0.94   0.04346135   0.06519202

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 
</code></pre>
"
['machine-learning'],"Text-Classification-Problem, what is the right approach?","<p>I think perhaps the first thing to decide that will help clarify some of your other questions is whether you want to perform binary classification or multi-class classification. If you're interested in classifying each instance in your dataset into more than one class, then this brings up a set of new concerns regarding setting up your data set, the experiments you want to run, and how you plan to evaluate your classifier(s). My hunch is that you could formulate your task as a binary one where you train and test one classifier for each class you want to predict, and simply set up the data matrix so that there are two classes to predict - (1) the one you're interested in classifying and (2) everything else.</p>

<p>In that case, instead of your training set looking like this (where each row is a document and columns 1-3 contain features for that document, and the class column is the class to be predicted): </p>

<pre><code>1           2           3           class
feature1    feature2    feature3    politics
feature1    feature2    feature3    law
feature1    feature2    feature3    president
feature1    feature2    feature3    politics
</code></pre>

<p>it would look like the following in the case where you're interested in detecting the politics class against everything else:</p>

<pre><code>1           2           3           class
feature1    feature2    feature3    politics
feature1    feature2    feature3    non-politics
feature1    feature2    feature3    non-politics
feature1    feature2    feature3    politics
</code></pre>

<p>You would need to do this process for each class you're interested in predicting, and then train and test one classifier per class and evaluate each classifier according to your chosen metrics (usually accuracy, precision, or recall or some variation thereof).</p>

<p>As far as choosing features, this requires quite a bit of thinking. Features can be highly dependent on the type of text you're trying to classify, so be sure to explore your dataset and get a sense for how people are writing in each domain. Qualitative investigation isn't enough to decide once and for all what are good features, but it is a good way to get ideas. Also, look into <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">TF-IDF</a> weighting of terms instead of just using their frequency within each instance of your dataset. This will help you pick up on (a) terms that are prevalent within a document (and possibly a target class) and (b) terms that distinguish a given document from other documents. I hope this helps a little.</p>
"
"['accuracy', 'methods']",Can distribution values of a target variable be used as features in cross-validation?,"<p>After speaking with some experienced statisticians, this is what I got.</p>

<blockquote>
  <p>As for technical issues regarding the paper, I'd be worried about data leakage or using future information in the current model. This can also occur in cross validation. You should make sure each model trains only on past data, and predicts on future data. I wasn't sure exactly how they conducted CV, but it definitely matters. It's also non-trivial to prevent all sources of leakage. They do claim unseen examples but it's not explicit exactly what code they wrote here. I'm not saying they are leaking for sure, but I'm saying it could happen.</p>
</blockquote>
"
"['machine-learning', 'neuralnetwork']",Machine learning toolkit for Excel,"<p>As far as I know, currently there not that many projects and products that allow to perform serious <em>machine learning (ML)</em> work from within Excel. However, the situation seems to be changing rapidly due to active Microsoft's efforts in popularizing its ML cloud platform <em>Azure ML</em> (along with <em>ML Studio</em>). The <a href=""http://blogs.microsoft.com/blog/2015/01/23/microsoft-acquire-revolution-analytics-help-customers-find-big-data-value-advanced-statistical-analysis"">recent acquisition</a> of R-focused company Revolution Analytics by Microsoft (which appears to me as more of <em>acqui-hiring</em> to a large extent) is an example of the company's aggressive data science <strong>market strategy</strong>.</p>

<p>In regard to <em>ML toolkits for Excel</em>, as a confirmation that we should expect most Excel-enabled ML projects and products to be <strong>Azure ML-focused</strong>, consider the following two <em>projects</em> (the latter is an open source):</p>

<ul>
<li><strong>Excel DataScope</strong> (Microsoft Research): <a href=""http://research.microsoft.com/en-us/projects/exceldatascope"">http://research.microsoft.com/en-us/projects/exceldatascope</a></li>
<li><strong>Azure ML Excel Add-In</strong> (seems to be Microsoft sponsored): <a href=""https://azuremlexcel.codeplex.com"">https://azuremlexcel.codeplex.com</a></li>
</ul>
"
['regression'],One-hot encoding,"<p>Yes you turn it into three different variables, however this is not called multivariate regression, that indicates multiple output variables, not inputs. (Thanks to Simon for correcting me)</p>
"
"['machine-learning', 'python', 'k-means']",K-means Clustering algorithm problems,"<p>Wikipedia says: ""Assign each observation to the cluster whose mean yields the least within-cluster sum of squares (WCSS)""</p>

<p>I think in your case, this is translatable to: $c_i$ is assigned to the closest centroid by euclidean distance.</p>

<p>For your second question, the centroid should $\mu$ should have the same number of dimensions as each training point $x_i$. They are both points in the co-ordinate system.</p>

<p>You can use a high number of features with K-means, for example, text analytics might reduce a corpora of news articles to 10,000+ dimensions. Depending on the package you use these might be represented as a sparse matrix.</p>
"
"['r', 'neuralnetwork', 'time-series', 'matlab']",Time Series Forecasting with Neural Network (mixed data types),"<p>I guess I answered the question in the comments, so here goes.</p>

<p>Most ML models cannot deal with categorical values. A common way to solve this is to use one-hot encoding, also known as dummy variables. For very possible value of your categorical variable you create a column which is 0 unless this row has this category, then it is 1. It is possible to remove one of the categories since it is a linear combination of the other dummy variables (if all are 0, the last one must be 1).</p>

<p>The downside of this method is that it increases the dimensionality of your feature space. If you have enough data to support this or not that many categories that is not a problem. There are other alternatives, like taking the average feature of every category and adding that to your features as opposed to the categorical feature.</p>
"
"['machine-learning', 'recommendation', 'multiclass-classification']",Multiclass Classification with large number of categories,"<p>You might have more luck with a Naive Bayes Classifier.  It can handle a large number of target classes, and is relatively fast to train, since you largely just calculate a bunch of univariate stats to plug into at prediction time.  It won't capture fancy interactions as much as a random forest though, so if you are concerned about ""they only buy shoelaces if they bought shoes but NOT shoeshine"" vs ""they often buy shoelaces if they've bought shoes"" then it may disappoint.  You may also want to incorporate a time component, but I'm not sure what you're doing.  </p>

<p><a href=""https://en.wikipedia.org/wiki/Association_rule_learning"" rel=""nofollow"">https://en.wikipedia.org/wiki/Association_rule_learning</a> may also be relevant.</p>
"
"['r', 'logistic-regression']","For logistic regression, Predict.glm() outputs $p$ or $ln(p/1-p)$?","<p>It returns the log odds. You can see that with this basic example,</p>

<pre><code># Create a perfect correlated data.
data &lt;- data.frame(x = c(1,0,1,0), y = c(T, F, T, F))

# Estimate the model.
model &lt;- glm(y ~ x, data)

# Show predictions.
predict(model)
##        1         2         3         4 
## 23.56607 -23.56607  23.56607 -23.56607

# Compute the inverse of the log odd to come to your prediction. 
boot::inv.logit(predict(model))
##           1            2            3            4 
##1.000000e+00 5.826215e-11 1.000000e+00 5.826215e-11 
</code></pre>

<p>If logit would return probabilities you could not take the inverse to come to your predicted values.</p>
"
"['machine-learning', 'scikit', 'gaussian']",Gaussian Mixture Models EM algorithm use average log likelihood to test convergence,"<p>It makes unit testing easier; invariant to the size of the sample.</p>

<p>Reference: <a href=""https://github.com/scikit-learn/scikit-learn/issues/4178"" rel=""nofollow"">the github discussion that led to the change</a>.</p>
"
"['machine-learning', 'r', 'hadoop', 'map-reduce']",Difference Between Hadoop Mapreduce(Java) and RHadoop mapreduce,"<p><a href=""https://github.com/RevolutionAnalytics/RHadoop"" rel=""nofollow"">rhadoop</a> (the part you are interested in is now called <a href=""https://github.com/RevolutionAnalytics/rmr2"" rel=""nofollow"">rmr2</a>) is simply a client API for MapReduce written in R. You invoke MapReduce using R package APIs, and send an R function to the workers, where it is executed by an R interpreter locally. But it is otherwise exactly the same MapReduce.</p>

<p>You can call anything you like in R this way, but no R functions are themselves parallelized to use MapReduce in this way. The point is simply that you can invoke M/R from R. I don't think it somehow lets you do anything more magical than that.</p>
"
['k-means'],Boundary conditions for clustering,"<p><strong>Since you accepted another answer, which says this can't be done, I am editing this to include an example of it being done.  Hope this helps!</strong></p>

<p><strong>Original Answer:</strong></p>

<p>The most logical way to transform hour is into two variables that swing back and forth out of sink. Imagine the position of the end of the hour hand of a 24-hour clock. The <code>x</code> position swings back and forth out of sink with the <code>y</code> position. For a 24-hour clock you can accomplish this with <code>x=sin(2pi*hour/24)</code>,<code>y=cos(2pi*hour/24)</code>.  </p>

<p>You need both variables or the proper movement through time is lost.  This is due to the fact that the derivative of either sin or cos changes in time where as the <code>(x,y)</code> position varies smoothly as it travels around the unit circle.</p>

<p>This method works really well for clustering and for keeping the distance between 15 minutes after midnight and 5 minutes before midnight ""close"" in Euclidean space.  All of the modulo suggestions don't accomplish this and the cyclic representation that they do accomplish is pretty clumsy.</p>

<p>Finally, consider whether it is worthwhile to add a third feature to trace linear time, which can be constructed by hours (or minutes or seconds) from the start of the first record or a Unix time stamp or something similar.  These three features then provide proxies for both the cyclic and linear progression of time e.g. you can pull out cyclic phenomenon like sleep cycles in people's movement and also linear growth like population vs. time.</p>

<p>Hope this helps!</p>

<p><strong>Example of if being accomplished:</strong></p>

<pre><code># Enable inline plotting
%matplotlib inline

#Import everything I need...

import numpy as np
import matplotlib as mp

import matplotlib.pyplot as plt
import pandas as pd

# Grab some random times from here: https://www.random.org/clock-times/
# put them into a csv.
from pandas import DataFrame, read_csv
df = read_csv('/Users/angus/Machine_Learning/ipython_notebooks/times.csv',delimiter=':')
df['hourfloat']=df.hour+df.minute/60.0
df['x']=np.sin(2.*np.pi*df.hourfloat/24.)
df['y']=np.cos(2.*np.pi*df.hourfloat/24.)

df
</code></pre>

<p><a href=""http://i.stack.imgur.com/1UTYj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1UTYj.png"" alt=""enter image description here""></a></p>

<pre><code>def kmeansshow(k,X):

    from sklearn import cluster
    from matplotlib import pyplot
    import numpy as np

    kmeans = cluster.KMeans(n_clusters=k)
    kmeans.fit(X)

    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_
    #print centroids

    for i in range(k):
        # select only data observations with cluster label == i
        ds = X[np.where(labels==i)]
        # plot the data observations
        pyplot.plot(ds[:,0],ds[:,1],'o')
        # plot the centroids
        lines = pyplot.plot(centroids[i,0],centroids[i,1],'kx')
        # make the centroid x's bigger
        pyplot.setp(lines,ms=15.0)
        pyplot.setp(lines,mew=2.0)
    pyplot.show()
    return centroids
</code></pre>

<p>Now lets try it out:</p>

<pre><code>kmeansshow(6,df[['x', 'y']].values)
</code></pre>

<p><a href=""http://i.stack.imgur.com/MkVNg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MkVNg.png"" alt=""enter image description here""></a></p>

<p>You can just barely see that there are some after midnight times included with the before midnight green cluster.  Now lets reduce the number of clusters and show that before and after midnight can be connected in a single cluster in more detail:</p>

<pre><code>kmeansshow(3,df[['x', 'y']].values)
</code></pre>

<p><a href=""http://i.stack.imgur.com/bWg2B.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bWg2B.png"" alt=""enter image description here""></a></p>

<p>See how the blue cluster contains times that are from before and after midnight that are clustered together in the same cluster...</p>

<p>QED!</p>
"
"['neuralnetwork', 'deep-learning']",Understanding convolutional pooling sizes (deep learning),"<p>In the given example from the e-book, the number 4 comes from (12-5+1)/2, where 12 is the input image size (12*12) of the second constitutional layer; 5 is the filter size (5*5) used in that layer; and 2 is the poolsize.<br>
This is similar to how you get the number 12 from the first constitutional layer: 12=(28-5+1)/2. It's well explained in your linked chapter.</p>

<p>Regarding your ""For instance"" code, your 6th line is not correct:</p>

<blockquote>
  <p>ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 1),   </p>
</blockquote>

<p>The number 12 should be replaced by (81-5+1)/2 which unfortunately is not an integer. You may want to change the filter_shape in the first layer to (6,1) to make it work. In that case, your 6th line should be: </p>

<blockquote>
  <p>ConvPoolLayer(image_shape=(mini_batch_size, 20, 38, 1),   </p>
</blockquote>

<p>and your 11th line should be: </p>

<blockquote>
  <p>n_in=40*17*1, n_out=1000, activation_fn=ReLU, p_dropout=0.5),</p>
</blockquote>
"
['databases'],What makes columnar databases suitable for data science?,"<p>A column-oriented database (=columnar data-store) stores the data of a table column by column on the disk, while a row-oriented database stores the data of a table row by row.</p>

<p>There are two main advantages of using a column-oriented database in comparison
with a row-oriented database. The first advantage relates to the amount of data one’s
need to read in case we perform an operation on just a few features. Consider a simple
query:</p>

<pre><code>SELECT correlation(feature2, feature5)
FROM records
</code></pre>

<p>A traditional executor would read the entire table (i.e. all the features):</p>

<p><a href=""http://i.stack.imgur.com/4EZHo.png""><img src=""http://i.stack.imgur.com/4EZHo.png"" alt=""enter image description here""></a></p>

<p>Instead, using our column-based approach we just have to read the columns
which are interested in:</p>

<p><a href=""http://i.stack.imgur.com/nGTKQ.png""><img src=""http://i.stack.imgur.com/nGTKQ.png"" alt=""enter image description here""></a></p>

<p>The second advantage, which is also very important for large databases, is that column-based storage allows better compression, since the data in
one specific column is indeed homogeneous than across all the columns.</p>

<p>The main drawback of a column-oriented approach is that manipulating (lookup, update or delete) an entire given row is inefficient. However the situation should
occur rarely in databases for analytics (“warehousing”),
which means most operations are read-only, rarely read many attributes in the same
table and writes are only appends.</p>

<p>Some RDMS offer a column-oriented storage engine option. For example, PostgreSQL
has natively no option to store tables in a column-based fashion, but Greenplum has
created a closed-source one (DBMS2, 2009). Interestingly, Greenplum is also behind
the open-source library for scalable in-database analytics, MADlib (Hellerstein et al.,
2012), which is no coincidence. More recently, CitusDB, a startup working on high speed, analytic database, released their own open-source columnar store extension for
PostgreSQL, CSTORE (Miller, 2014). Google’s system for large scale machine learning
Sibyl also uses column-oriented data format (Chandra et al., 2010). This trend
reflects the growing interest around column-oriented storage for large-scale analytics.
Stonebraker et al. (2005) further discuss the advantages of column-oriented DBMS.</p>

<p>Two concrete use cases: <a href=""http://qr.ae/RoQxgp"">How are most datasets for large-scale machine learning stored?</a></p>

<p>(most of the answer comes from Appendix C of: <a href=""http://francky.me/doc/2015beatdbwaveletsgp.pdf"">BeatDB: An end-to-end approach to unveil saliencies from massive signal data sets. Franck Dernoncourt, S.M, thesis, MIT Dept of EECS</a>)</p>
"
"['machine-learning', 'clustering', 'correlation']",Devices behavior in one continuous variable vs events rate,"<p>Done with K-means clustering with descriptive statistics as features:</p>

<p>In short, I've tried the idea described in the question, even if I was thinking it won't work. Let the experience talk...</p>

<p>I initially had a list of devices data. Each element of the list were 2 columns, R rows matrix, and R was different for each device. So, per device:</p>

<pre><code>[
    [mesureValue, timestamp],
    ..., 
    [mesureValue, timestamp],
]
</code></pre>

<p>Since I'm only interested in the measureValue distribution, I've transformed the inital data to a 8 columns, N rows matrix, where N = number of devices.</p>

<p>The columns are, computed on the correponding device's measure value:</p>

<ul>
<li>Arithmetic mean</li>
<li>Median</li>
<li>First quartille</li>
<li>Third quartille</li>
<li>Minimum</li>
<li>Maximum</li>
<li>Range</li>
<li>Standard deviation</li>
</ul>

<p>With this matrix, I've applied K-means clustering using scikit learn (python).</p>

<p>I made the link between the matrix line and the physical device by using pandas Data Frames (python) who's line index are in fact the serial number of the device.</p>

<p>I've tried with 5 clusters, and it works.</p>

<p>Just in case of, if I need improvements in the future, I'm planning to add other statistics in the columns, especially for deviation vs. normality. So, for example Kurstosis and normal q-q plot p value.</p>

<p>Best regards.</p>
"
"['python', 'distance', 'cosine-distance']",Cosine Distance > 1 in scipy,"<p>The cosine distance formula is:</p>

<p><a href=""http://i.stack.imgur.com/UmTFw.png""><img src=""http://i.stack.imgur.com/UmTFw.png"" alt=""enter image description here""></a></p>

<p>And the formula used by the <code>cosine</code> function of the <code>spatial</code> class of scipy is:</p>

<p><a href=""http://i.stack.imgur.com/KLk4n.png""><img src=""http://i.stack.imgur.com/KLk4n.png"" alt=""enter image description here""></a> </p>

<p>So, the actual cosine similarity metric is: -0.9998.</p>

<p>So, it signifies complete dissimilarity.</p>
"
"['r', 'classification', 'neuralnetwork', 'visualization']",Use the output of 2 hidden neurons in the last hidden layer of a NN to visualize the result of a 4-class classification task,"<p>I'm somewhat new to the topic as well, but I think what you are looking for is an autoencoder.  I have only used it in the h2o deeplearning package, but it seems to work well.</p>

<p>The idea behind an auto-encoder is to begin with your inputs, encode down to less nodes (two is nice for visual representation), then decode back to your original outputs (ass accurately as possible).</p>

<p><a href=""http://i.stack.imgur.com/Zwsmz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Zwsmz.png"" alt=""Autoencoder""></a></p>

<p>If the auto-encoder can decode back to an accurate representation of your inputs, then there was sufficient important information withheld in those two nodes.  You can then plot the features that those two nodes held in a two-dimensional plot that would be analogous to a bounded PCA plot.</p>

<p>Also - the auto-encoder is not constrained to linearity like the PCA is.  Below is a quick snippet of some code, hope it helps.</p>

<pre><code>library(h2o)
localH2O = h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE, min_mem_size = ""3g"", max_mem_size = ""4g"", nthreads = -1)
dat_h2o &lt;- as.h2o(train.df)

unsupervised &lt;- 
  h2o.deeplearning(x = 24:356,  # column numbers to use
                   training_frame = dat_h2o, # data in H2O format
                   autoencoder = TRUE, ## unsupervised autoencoding
                   activation = ""Tanh"", # or 'Tanh' 'Rectifier' 'WithDropout' node activation function, Tanh seems to work best for autoencoding
                   hidden = c(5,2,5), # three layers of nodes, with 5/2/5 nodes, respectively
                   epochs = 1) # max. no. of epochs 

## layer 1 corresponds to hidden[1], so it will reduce to 5 variables
## below is roughly similar to using predict(pca.object, newdata)
training_data &lt;- h2o.deepfeatures(unsupervised, dat_h2o, layer = 1)
testing_data &lt;- h2o.deepfeatures(unsupervised, test_h2o, layer = 1)

## explore the second layer with 2 nodes, similar to exploring PC1 vs PC2
train_supervised_features2 = h2o.deepfeatures(unsupervised, dat_h2o, layer=2)

plotdata2 = as.data.frame(train_supervised_features2)
plotdata2$label = as.character(as.vector(dat_h2o[,364]))

## L2 corresponds to layer 2, so use L2
qplot(DF.L2.C1, DF.L2.C2, data = plotdata2, color = label, main = 'Neural network: 5-2-5')
</code></pre>
"
"['machine-learning', 'reinforcement-learning']",What is the Q function and what is the V function in reinforcement learning?,"<p>Q-values are a great way to the make actions explicit so  you can deal with problems where the transition function is not available (model-free). However, when your action-space is large, things are not so nice and Q-values are not so convenient. Think of a huge number of action or even continuous action-spaces.</p>

<p>From a sampling perspective, the dimensionality of Q(s,a) is higher than V(s) so it might get harder to get enough (s,a) samples in comparison with (s). If you have access to the transition function sometimes V is good. </p>

<p>There are also other uses where both are combined. For instance, the advantage function where A(s,a)=Q(s,a)-V(s). If you are interested, you can find a recent example using advantage functions here:</p>

<p>Wang Z, de Freitas N, Lanctot M. Dueling Network Architectures for Deep Reinforcement Learning. arXiv:151106581</p>

<p>Available from: <a href=""http://arxiv.org/abs/1511.06581"" rel=""nofollow"">http://arxiv.org/abs/1511.06581</a></p>
"
"['machine-learning', 'dimensionality-reduction']",Dimensionality and Manifold,"<blockquote>
  <p>What is a dimension?</p>
</blockquote>

<p>To put it simply, if you have a tabular data set with m rows and n columns, then the dimensionality of your data is n</p>

<blockquote>
  <p>What is a manifold? </p>
</blockquote>

<p>The simplest example is our planet Earth. For us it looks flat, but it reality it's a sphere. So it's sort of a 2d manifold embedded in the 3d space. </p>

<blockquote>
  <p>What is the difference? </p>
</blockquote>

<p>To answer this question, consider another example of a manifold: </p>

<p><img src=""http://i.stack.imgur.com/FrBXu.png"" alt=""enter image description here""></p>

<p>This is so-called ""swiss roll"". The data points are in 3d, but they all lie on 2d manifold, so the dimensionality of the manifold is 2, while the dimensionality of the input space is 3.</p>

<p>There are many techniques to ""unwrap"" these manifolds. One of them is called <a href=""https://www.google.com/search?q=locally+linear+embedding"" rel=""nofollow"">Locally Linear Embedding</a>, and this is how it would do that:</p>

<p><img src=""http://i.stack.imgur.com/pagFb.png"" alt=""enter image description here""></p>

<p>Here's a scikit-learn snippet for doing that:</p>



<pre><code>from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(n_neighbors=k, n_components=2)
X_lle = lle.fit_transform(data)
plt.scatter(X_lle[:, 0], X_lle[:, 1], c=color)
plt.show()
</code></pre>
"
['hive'],How to paste string and int from map to an array in hive?,"<p>Assuming your map is called ""M"" and you want your array field to be called ""A""</p>

<pre><code>SELECT 
...
array(concat_ws("":"",""string1"",M[1]),
      concat_ws("":"",""string2"",M[2]),
      concat_ws("":"",""string3"",M[3]) as A
....
FROM table;
</code></pre>
"
"['text-mining', 'multiclass-classification']",Making a labelled training data set,"<p>Generally with multiple classes you have to make a distinction between exclusive and inclusive groups. The simplest cases are ""all classes are exclusive"" (predict only one class), and ""all classes are compatible"" (predict list of classes that apply).</p>

<p>Either way, label the classes as you would want your trained model to predict them. If you expect your classifier to predict an example is in both <code>garbage</code> and <code>footpath</code>, then you should label such an example with both. If you want it to disambiguate between them, then label with a single correct class.</p>

<p>To train a classifier to predict multiple target classes at once, it is usually just a matter of picking the correct objective function and a classifier with architecture that can support it. </p>

<p>For example, with a neural network, you would avoid using a ""softmax"" output which is geared towards predicting a single class - instead you might use a regular ""sigmoid"" function and predict class membership on a simple threshold on each output.</p>

<p>You can get also more sophisticated perhaps with a pipeline model if your data can be split into several exclusive groups - predict the group in the first stage, and have multiple group-specific models predicting the combination of classes in each group in a second stage. This may be overkill for your problem, although it may still be handy if it keeps your individual models simple (e.g. they could all be logistic regression, and the first stage may gain some accuracy if the groups are easier to separate).</p>
"
['time-series'],How can you determine the growth rate you need to achieve a certain customer base?,"<p>A growth rate is an exponential growth, you multiply your userbase by a certain fraction, which is above 1 in case of growth, else it would be decline. This effect compounds because the amount of users is bigger while being multiplied by the same multiplier. Let's call this multiplier <strong>a</strong>. We apply this 12 times, once for every month. We are interested in:</p>

<p>$$12000 \cdot a^{12} = 1500000$$</p>

<p>This equals:</p>

<p>$$a^{12} = 125$$</p>

<p>We can take the 12th root on both sides:</p>

<p>$$a = 1.495$$</p>

<p>This means you would need to grow approximately 50% every month.</p>

<p>For growing faster first and then plateauing you can do similar equations, although a bit more difficult. You have to express the growth at the first level in terms of the other level because else there are two unknowns which make for infinite solutions. Let's say the first 3 months the growth <strong>b</strong> is five times as high as the growth <strong>a</strong> last 9 months. This means $b = 5(a-1)+1$. This leads to the following equation.</p>

<p>First we have three months of growth rate <strong>b</strong>, which means after three months our userbase is $12000\cdot b^3$ and after that we have nine months of growth <strong>a</strong> which leads to the final user base being $12000\cdot b^3\cdot a^9$. Since $b=5(a-1)+1$ this comes down to $12000\cdot (5(a-1)+1)^3\cdot a^9$, which according to Wolfram Alpha comes down to <strong>a</strong> being around 1.2785 which means <strong>b</strong> is around 2.3925. This means the first three months a growth of 139% and then nine months of 27.85% growth.</p>
"
"['python', 'r', 'spss']",Assigning numerical IDs to variable values in a data file,"<p>SPSS has an AUTORECODE command which will do the whole job with one command. for example:</p>

<pre><code>AUTORECODE vr1 to vr100 /into Kvr1 to Kvr100/PRINT.
</code></pre>

<p>This will take text variables vr1 to vr100 and recode them into new numerical variables Kvr1 to Kvr100 in which each textual category in the old variable is now automatically numbered in the new variable, with the textual category now used as a value label.<br>
The <code>PRINT</code> sub-command will show you in the output window a list of all the number codes chosen for text categories in each variable.<br>
Please note - using the <code>TO</code> convention (as in ""vr1 to vr100"") only works when the variables are consecutively ordered in the file. If they are not, you have to name them separately.</p>
"
"['python', 'scikit']",Does using unimportant features hurt accuracy?,"<p>I used the gradient boosting classifier in a project a while ago. We had about 130 features and since the system had to be implemented on a microcontroller I tried to reduce the number of features as far as possible.</p>

<p>Here is a plot showing the performance (F1-score) of GBC models trained with the top n most important features:</p>

<p><a href=""http://i.stack.imgur.com/YDDlt.png""><img src=""http://i.stack.imgur.com/YDDlt.png"" alt=""enter image description here""></a></p>

<p>From that image it looks like ""good-enough performance"" is already reached with only 30 out of 130 features, while the other 100 (unimportant) features don't seem to have much positive nor negative influence.</p>

<p>Only when you zoom in a bit on the Y-axis you see this:</p>

<p><a href=""http://i.stack.imgur.com/A3raC.png""><img src=""http://i.stack.imgur.com/A3raC.png"" alt=""enter image description here""></a></p>

<p>This shows that the performance reaches its peak (0.93) at 90 out of 130 features. So 40 features are unimportant if you want the best model.</p>

<p>To answer your question ""<strong>does this have an impact other than computation time?</strong>"": it does have an impact on performance, since the performance decreases again with more than 90 features, but only slightly.</p>

<p>To answer your second question ""<strong>If it does hurt accuracy, how should I select which features to get rid of?</strong>"":</p>

<p>I did the following:</p>

<ol>
<li>Select the top n features</li>
<li>Use a cross-validated grid search to train a GBC model on the top n features</li>
<li>Repeat with the top n+1,... features</li>
<li>Select the model / features with the best score</li>
</ol>

<p>How to select the top n features in step 1):</p>

<p>I actually didn't use GBC for feature selection, but random forests. I don't know why, but my experiments have shown that selecting features with RF and then training a GBC model on them works better than selecting features with GBC and then training a GBC model on them.</p>
"
['apache-spark'],What version of spark in latest Cloudera QuickStart VirtualBox?,"<p>QuickStart VM latest 5.7 has spark version 1.6.0 by default. </p>

<p><a href=""http://i.stack.imgur.com/po32P.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/po32P.png"" alt=""enter image description here""></a></p>
"
"['machine-learning', 'data-mining']",How to connect data-mining with machine learner process,"<p>I am not 100% if a message queue library will be the right tool for this job but so far it looks to me so.</p>

<p>With a messaging library like:</p>

<ul>
<li><a href=""http://nsq.io"" rel=""nofollow"">nsq</a></li>
<li><a href=""http://zeromq.org"" rel=""nofollow"">zeromq</a></li>
<li><a href=""http://mqtt.org"" rel=""nofollow"">mqtt</a> (?)</li>
</ul>

<p>You can connect different processes operating on different environment through a TCP based protocol. As these systems run distributed it is possible to connect multiple nodes.</p>

<p>For <strong>nsq</strong> we even have a library in Python <strong>and</strong> Go!</p>
"
"['machine-learning', 'data-mining', 'algorithms', 'recommendation']",Item based and user based recommendation difference in Mahout,"<p>You are correct that both models work on the same data without any problem. Both items operate on a matrix of user-item ratings.</p>

<p>In the user-based approach the algorithm produces a rating for an item <code>i</code> by a user <code>u</code> by combining the ratings of other users <code>u'</code> that are similar to <code>u</code>. Similar here means that the two user's ratings have a high Pearson correlation or cosine similarity or something similar.</p>

<p>In the item-based approach we produce a rating for <code>i</code> by <code>u</code> by looking at the set of items <code>i'</code> that are similar to <code>i</code> (in the same sense as above except now we'd be looking at the ratings that items have received from users) that <code>u</code> has rated and then combines the ratings by <code>u</code> of <code>i'</code> into a predicted rating  by <code>u</code> for <code>i</code>.</p>

<p>The item-based approach was invented at Amazon (<a href=""http://dl.acm.org/citation.cfm?id=642471"" rel=""nofollow"">http://dl.acm.org/citation.cfm?id=642471</a>) to address their scale challenges with user-based filtering. The number of things they sell is much less and much less dynamic than the number of users so the item-item similarities can be computed offline and accessed when needed.</p>
"
"['machine-learning', 'beginner', 'bayesian', 'bernoulli-distribution']","Understanding Bernoulli Trials, Bayesian Setting","<blockquote>
  <p>What is a Bayesian setting and why is Bayesian thing everywhere?</p>
</blockquote>

<p>In very simple terms:</p>

<blockquote>
  <p>Bayesian is a statistical setting, where the likelihood of an event
  happening (called the posterior) depends on the prior trials or
  observations (called the prior(s)).</p>
</blockquote>

<p>Bayesian networks is an extension of the above, forming a chain or a network of inferencing.</p>

<blockquote>
  <p>Are there any theorems/theories that I must know?</p>
</blockquote>

<p>For understanding the Bayesian paradigm, you need to know the Bayesian theorem/relation, which is basically:</p>

<p>$$P(\theta|d) = \dfrac{P(d|\theta)P(\theta)}{P(d)}.$$</p>

<blockquote>
  <p>Any book/notes/resources where I could learn about these stuff in a
  relatable way (I have very little maths background, but I can learn)?</p>
</blockquote>

<p>I would highly recommend <a href=""http://store.elsevier.com/Doing-Bayesian-Data-Analysis/John-Kruschke/isbn-9780124059160/"" rel=""nofollow"">""Doing Bayesian Analysis"" by John Krushke</a></p>
"
"['classification', 'sampling']",Poor performance shown on Rare event modeling,"<p>If you are willing to use the caret package in R and use random forests, you can use the method in the following blog post for downsampling with unbalanced datasets: <a href=""http://appliedpredictivemodeling.com/blog/2013/12/8/28rmc2lv96h8fw8700zm4nl50busep"" rel=""nofollow"">http://appliedpredictivemodeling.com/blog/2013/12/8/28rmc2lv96h8fw8700zm4nl50busep</a></p>

<p>Basically, you just add a single line to your <em>train</em> call. Here is the relevant part: </p>

<pre><code>&gt; rfDownsampled &lt;- train(Class ~ ., data = training,
+                        method = ""rf"",
+                        ntree = 1500,
+                        tuneLength = 5,
+                        metric = ""ROC"",
+                        trControl = ctrl,
+                        ## Tell randomForest to sample by strata. Here, 
+                        ## that means within each class
+                        strata = training$Class,
+                        ## Now specify that the number of samples selected
+                        ## within each class should be the same
+                        sampsize = rep(nmin, 2))
</code></pre>

<p>I have had some success with this approach in your type of situation.</p>

<p>For some more context, here is an in-depth post about experiments with unbalanced datasets: <a href=""http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/"" rel=""nofollow"">http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/</a></p>
"
['data-mining'],What is a XML dataset?,"<p>XML is a markup language similar to html. One uses tags with attributes to build data structures. For example,</p>

<pre><code>&lt;sampleXML&gt;
  &lt;Menu&gt;
    &lt;Food&gt;
      &lt;item1&gt;Spaghetti Bolognese&lt;/item1&gt;
      &lt;item2&gt;Spaghetti Carbonara&lt;/item2&gt;
    &lt;/Food&gt;
    &lt;Drinks&gt;
      &lt;item1 class = 'drinks'&gt;Sprite&lt;/item1&gt;
    &lt;/Drinks&gt;
  &lt;/Menu&gt;
&lt;/sampleXML&gt;
</code></pre>

<p>As you can see XML employs tags such as <code>&lt;Food&gt;&lt;/Food&gt;</code> and attributes such as <code>class = 'drinks'</code> which is exactly what HTML has. To access the XML data in java you got couple choices. <a href=""http://www.mkyong.com/java/how-to-read-xml-file-in-java-dom-parser/"" rel=""nofollow"">You can read it in as a string and parse it using built in DOM parser</a>. Or you can use JAXB to <a href=""https://docs.oracle.com/javase/tutorial/jaxb/intro/"" rel=""nofollow"">map XML directly to Java objects</a>. </p>

<p>Surely you can convert XML to a csv file. There are free websites online for that. Just google ""XML to csv converter.""</p>

<p>Binary files are not XML, but can be. This needs little explanation. The letters and words you see here are ASCII characters. This is human readable text. Each ASCII character has a binary representation. For example, j in binary is 1101010. A binary file is any file in computer language (0 an 1). A binary file can also be a combination of text and binary. You can convert binary to ASCII and those files you download may indeed be XML I described above. To convert binary to ASCII just google it.</p>
"
"['classification', 'logistic-regression', 'decision-trees']",Decision tree or logistic regression?,"<p><strong>Long story short</strong>: <em>do what @untitledprogrammer said, try both models and cross-validate to help pick one.</em></p>

<p>Both decision trees (depending on the implementation, e.g. C4.5) and logistic regression should be able to handle continuous and categorical data just fine. For logistic regression, you'll want to <a href=""https://stats.stackexchange.com/questions/115049/why-do-we-need-to-dummy-code-categorical-variables"">dummy code your categorical variables</a>.</p>

<p>As @untitledprogrammer mentioned, it's difficult to know a priori which technique will be better based simply based on the types of features you have, continuous or otherwise. It really depends on your specific problem and the data you have. (See <a href=""http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=585893"">No Free Lunch Theorem</a>)</p>

<p>You'll want to keep in mind though that a logistic regression model is searching for a single linear decision boundary in your feature space, whereas a decision tree is essentially partitioning your feature space into half-spaces using <em>axis-aligned</em> linear decision boundaries. The net effect is that you have a non-linear decision boundary, possibly more than one. </p>

<p>This is nice when your data points aren't easily separated by a single hyperplane, but on the other hand, decisions trees are so flexible that they can be prone to overfitting. To combat this, you can try pruning. Logistic regression tends to be less susceptible (but not immune!) to overfitting.</p>

<p>Lastly, another thing to consider is that decision trees can automatically take into account interactions between variables, e.g. $xy$ if you have two independent features $x$ and $y$. With logistic regression, you'll have to manually add those interaction terms yourself.</p>

<p>So you have to ask yourself: </p>

<ul>
<li>what kind of decision boundary makes more sense in your particular problem?</li>
<li>how do you want to balance bias and variance?</li>
<li>are there interactions between my features?</li>
</ul>

<p>Of course, it's always a good idea to just try both models and do cross-validation. This will help you find out which one is more like to have better generalization error.</p>
"
"['bigdata', 'definitions']","What is ""data science""?","<p>I get asked this question all the time, so earlier this year I wrote an article (<a href=""http://oss4ds.com/what-is-data-science.html"" rel=""nofollow""><strong>What is Data Science?</strong></a>) based on a presentation I've given a few times. Here's the gist...</p>

<p>First, a few definitions of data science offered by others:</p>

<p><a href=""https://twitter.com/josh_wills/status/198093512149958656"" rel=""nofollow""><strong>Josh Wills</strong> from <strong>Cloudera</strong> says</a> a data scientist is someone ""who is better at statistics than any software engineer and better at software engineering than any statistician.""</p>

<p><a href=""https://twitter.com/nivertech/status/180109930139893761"" rel=""nofollow"">A frequently-heard <strong>joke</strong></a> is that a ""Data Scientist"" is a Data Analyst who lives in California.</p>

<p><a href=""https://twitter.com/BigDataBorat/status/372350993255518208"" rel=""nofollow"">According to <strong>Big Data Borat</strong></a>, Data Science is statistics on a Mac.</p>

<p>In <a href=""http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram"" rel=""nofollow""><strong>Drew Conway's</strong> famous Data Science Venn Diagram</a>, it's the intersection of Hacking Skills, Math &amp; Statistics Knowledge, and Substantive Expertise.</p>

<p>Here's another good definition I found on the <a href=""http://www.itproportal.com/2014/02/11/how-to-pick-a-data-scientist-the-right-way/"" rel=""nofollow""><strong>ITProPortal</strong> blog</a>:</p>

<blockquote>
  <p>""A data scientist is someone who understands the domains of programming, machine learning, data mining, statistics, and hacking""</p>
</blockquote>

<p>Here's how we define Data Science at <strong>Altamira</strong> (my current employer):</p>

<p><img src=""http://i.stack.imgur.com/YAUZy.png"" alt=""data science diagram""></p>

<p>The bottom four rows are the <strong>table stakes</strong> -- the cost of admission just to play the game. These are foundational skills that all aspiring data scientists must obtain. Every data scientist must be a <strong>competent programmer</strong>. He or she must also have a solid grasp of math, statistics, and <strong>analytic methodology</strong>. Data science and ""<strong>big data</strong>"" go hand-in-hand, so all data scientists need to be familiar with frameworks for distributed computing. Finally, data scientists must have a basic understanding of the domains in which they operate, as well as excellent communications skills and the ability to <strong>tell a good story with data</strong>.</p>

<p>With these basics covered, the next step is to develop <strong>deep expertise</strong> in one or more of the vertical areas. ""Data Science"" is really an umbrella term for a collection of interrelated techniques and approaches taken from a variety of disciplines, including mathematics, statistics, computer science, and software engineering. The goal of these diverse methods is to <strong>extract actionable intelligence</strong> from data of all kinds, enabling clients to make better <strong>data-driven decisions</strong>. No one person can ever possibly master all aspects of data science; doing so would require multiple lifetimes of training and experience. The best data scientists are therefore ""<strong>T-shaped</strong>"" individuals -- that is, they possess a breadth of knowledge across all areas of data science, along with deep expertise in at least one. Accordingly, the best <strong>data science teams</strong> bring together a set of individuals with complementary skillsets spanning the <strong>entire spectrum</strong>.</p>
"
"['machine-learning', 'tensorflow']",Implement MLP in tensorflow,"<p>You made three mistakes:</p>

<ol>
<li>You omitted the offset terms before the nonlinear transformations (variables b_1 and b_out). This increases the representative power of the neural network.</li>
<li>You omitted the softmax transformation at the top layer. This makes the output a probability distributions, so you can calculate the cross-entropy, which is the usual cost function for classification.</li>
<li>You used the binary form of the cross-entropy when you should have used the multi-class form.</li>
</ol>

<p>When I run this I get accuracies over 90%:</p>

<pre><code>import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)

x = tf.placeholder(tf.float32, shape=[None, 784])
y = tf.placeholder(tf.float32, shape=[None, 10])

W_h1 = tf.Variable(tf.random_normal([784, 512]))
b_1 = tf.Variable(tf.random_normal([512]))
h1 = tf.nn.sigmoid(tf.matmul(x, W_h1) + b_1)

W_out = tf.Variable(tf.random_normal([512, 10]))
b_out = tf.Variable(tf.random_normal([10]))
y_ = tf.nn.softmax(tf.matmul(h1, W_out) + b_out)

# cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(y_, y)
cross_entropy = tf.reduce_sum(- y * tf.log(y_), 1)
loss = tf.reduce_mean(cross_entropy)
train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)

correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# train
with tf.Session() as s:
    s.run(tf.initialize_all_variables())

    for i in range(10000):
        batch_x, batch_y = mnist.train.next_batch(100)
        s.run(train_step, feed_dict={x: batch_x, y: batch_y})

        if i % 1000 == 0:
            train_accuracy = accuracy.eval(feed_dict={x: batch_x, y: batch_y})
            print('step {0}, training accuracy {1}'.format(i, train_accuracy))
</code></pre>
"
"['machine-learning', 'neuralnetwork']",(Why) do activation functions have to be monotonic?,"<p>The monotonicity criterion helps the neural network to converge easier into an more accurate classifier. 
See this <a href=""http://cs.stackexchange.com/questions/45281/should-activation-function-be-monotonic-in-neural-networks"">stackexchange answer</a> and <a href=""https://en.wikipedia.org/wiki/Activation_function"">wikipedia article</a> for further details and reasons.</p>

<p>However, the monotonicity criterion is not mandatory for an activation function - It is also possible to train neural nets with non-monotonic activation functions. It just gets harder to optimize the neural network. 
See <a href=""https://www.quora.com/Can-non-monotonic-activation-function-neural-networks-be-trained-using-the-same-backpropagation-algorithm-as-monotonic-activation-function-neural-networks"">Yoshua Bengio's answer</a>.</p>
"
"['machine-learning', 'linear-regression']","What is this formula, related to simple linear regression, called?","<p>I do not know other terms than the  average of inverse slope or the inverse of the harmonic mean of slopes. This is also the negative of the average slope of the perpendiculars.</p>

<p>It gives you the inverse of the average slope of lines passing through $(0,0)$ and $(x_i,y_i)$. If the $(x,y)$ are almost aligned with $(0,0)$, this is an estimate of the inverse of the slope of a line passing through the points. </p>

<p>Linear regression is quite different, as it involves cross-products of sums of $x$ or $y$.</p>
"
"['machine-learning', 'classification', 'data-mining', 'dataset']",Characterisitcs of the data set for a binary classification problem,"<p>Unfortunately you're not going to be able to do much without at least 200-300 records. You're going to be limited to simple (i.e. mostly linear) models until your dataset expands to at least 1,000. Anything less than 1,000 will require very thorough cross validation, and if you're not careful you'll be at risk of building a model that easily overfits. </p>

<p>@EricLecoutre makes a great point that you should use Amazon's Mechanical Turk. It usually costs just a penny or two per record and could save you a lot of time. </p>
"
"['r', 'random-forest', 'performance', 'accuracy']",Understanding ROCs in imbalanced data-sets,"<p>So your data-set of 155000 records has 403 records where B=1, and B=0 for the remaining 154597 records.</p>

<p>You could try splitting your data-set into 2:1 training/test sets sampled by each class of B. After you've done this, then only for the training set use SMOTE to over-sample the records with B=1 along with under-sampling the B=0 training records to bring the class ratio to something like 4:1.</p>

<p>Over/under sampling for the test set is not required as it is supposed to mimic real world uncertainty to test your model's performance.</p>

<p>Your model's AUC will definitely get reduced since (as rightly pointed out by stmax), you've leaked test records into the training set by over sampling B=1 cases before splitting the train-test sets.</p>

<p>The answers to each of your questions are:</p>

<ol>
<li>Yes, class imbalance does effect a random forest model's accuracy. How severely depends on the severity of the imbalance as well as the nature of the data-set itself.</li>
<li>Yes, you are biasing the training by over sampling the minority class, but if these are large enough samples in the original set, then hopefully they are close representations to the entire population.</li>
<li>I would recommend two metrics/techniques you could use here: Kappa Statistic (refer to <a href=""https://en.wikipedia.org/wiki/Cohen&#39;s_kappa"" rel=""nofollow"" title=""this article"">this article</a>) and the precision-recall curves to compare different models.</li>
</ol>
"
"['bigdata', 'data-mining', 'python', 'dataset']",Getting GitHub repository information by different criteria,"<p>My limited understanding, based on brief browsing <em>GitHub API</em> documentation, is that currently there is NO <strong>single API request</strong> that supports <strong>all</strong> your listed criteria <strong>at once</strong>. However, I think that you could use the following sequence in order to achieve the goal from your example (at least, I would use this approach):</p>

<p>1) <strong>Request</strong> information on all public repositories (API returns <em>summary representations</em> only): <a href=""https://developer.github.com/v3/repos/#list-all-public-repositories"" rel=""nofollow"">https://developer.github.com/v3/repos/#list-all-public-repositories</a>;</p>

<p>2) <strong>Loop</strong> through the list of all public repositories retrieved in step 1, requesting individual resources, and save it as new (detailed) list (this returns <em>detailed representations</em>, in other words, all attributes): <a href=""https://developer.github.com/v3/repos/#get"" rel=""nofollow"">https://developer.github.com/v3/repos/#get</a>;</p>

<p>3) <strong>Loop</strong> through the detailed list of all repositories, filtering corresponding fields by your criteria. For your example request, you'd be interested in the following attributes of the <strong>parent</strong> object: <em>stargazers_count</em>, <em>forks_count</em>. In order to filter the repositories by number of committers, you could use a separate API: <a href=""https://developer.github.com/v3/repos/#list-contributors"" rel=""nofollow"">https://developer.github.com/v3/repos/#list-contributors</a>.</p>

<p>Updates or comments from people more familiar with GitHub API are welcome!</p>
"
"['bigdata', 'python']",Is Python suitable for big data,"<p>To clarify, I feel like the original question references by OP probably isn't be best for a SO-type format, but I will certainly represent <code>python</code> in this particular case.</p>

<p>Let me just start by saying that regardless of your data size, <code>python</code> shouldn't be your limiting factor. In fact, there are just a couple main issues that you're going to run into dealing with large datasets:</p>

<ul>
<li><strong>Reading data into memory</strong> - This is by far the most common issue faced in the world of big data. Basically, you can't read in more data than you have memory (RAM) for. The best way to fix this is by making atomic operations on your data instead of trying to read everything in at once.</li>
<li><strong>Storing data</strong> - This is actually just another form of the earlier issue, by the time to get up to about <code>1TB</code>, you start having to look elsewhere for storage. AWS S3 is the most common resource, and <code>python</code> has the fantastic <code>boto</code> library to facilitate leading with large pieces of data.</li>
<li><strong>Network latency</strong> - Moving data around between different services is going to be your bottleneck. There's not a huge amount you can do to fix this, other than trying to pick co-located resources and plugging into the wall.</li>
</ul>
"
"['databases', 'nosql', 'neo4j']",Is this Neo4j comparison to RDBMS execution time correct?,"<p>Looking at this document called <a href=""https://www.facebook.com/notes/facebook-data-team/anatomy-of-facebook/10150388519243859"">Anatomy of Facebook</a> I note that the median is 100. Looking at the cumulative function plot I can bet that the average is higher, near 200. So 50 seems to not be the best number here. However I think that this is not the main issue here. </p>

<p>The main issue is the lack of information on how the database was used.</p>

<p>It seems reasonable that a data storage designed specially for graph structures to be more efficient than traditional RDBMs. However, even if the RDBMs are not in the latest trends as a data storage of choice, these systems evolved continuously in a race with the data set dimensions. There are various types of possible designs, various ways of indexing data, improvements related with concurrency and so on. </p>

<p>To conclude I think that regarding reproducibility, the study lack a proper description of how the database schema was designed. I do not expect that a database to dominate on such king of interrogations, however I would expect that with a well-tuned design the differences to not be such massive.</p>
"
"['bigdata', 'tools', 'data-stream-mining']","Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?","<p>I'd suggest <a href=""http://kafka.apache.org"" rel=""nofollow"">Apache Kafka</a> as message store and any stream processing solution of your choice like <a href=""https://camel.apache.org"" rel=""nofollow"">Apache Camel</a> or <a href=""https://github.com/apache/incubator-storm"" rel=""nofollow"">Twitter Storm</a></p>
"
['statistics'],Is there a replacement for small p-values in big data?,"<p>There is no replacement in the strict sense of the word.  Instead you should look at other measures.</p>

<p>The other measures you look at depend on what you type of problem you are solving.  In general, if you have a small p-value, also consider the magnitude of the effect size.  It may be highly statistically significant but in practice meaningless.  It is also helpful to report the confidence interval of the effect size.</p>

<p>I would consider <a href=""http://galitshmueli.com/system/files/Print%20Version.pdf"">this paper</a> as mentoned in DanC's answer to <a href=""http://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive"">this question</a>.</p>
"
"['bigdata', 'statistics']",When are p-values deceptive?,"<p>You are asking about <a href=""http://en.wikipedia.org/wiki/Data_dredging"">Data Dredging</a>, which is what happens when testing a very large number of hypotheses against a data set, or testing hypotheses against a data set that were suggested by the same data.  </p>

<p>In particular, check out <a href=""http://en.wikipedia.org/wiki/Multiple_comparisons"">Multiple hypothesis hazard</a>, and <a href=""http://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data"">Testing hypotheses suggested by the data</a>.</p>

<p>The solution is to use some kind of correction for <a href=""http://en.wikipedia.org/wiki/False_discovery_rate"">False discovery rate</a> or <a href=""http://en.wikipedia.org/wiki/Familywise_error_rate"">Familywise error rate</a>, such as <a href=""http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method"">Scheffé's method</a> or the (very old-school) <a href=""http://en.wikipedia.org/wiki/Bonferroni_correction"">Bonferroni correction</a>.</p>

<p>In a somewhat less rigorous way, it may help to filter your discoveries by the confidence interval for the odds ratio (OR) for each statistical result.  If the 99% confidence interval for the odds ratio is 10-12, then the OR is &lt;= 1 with some <em>extremely</em> small probability, especially if the sample size is also large.  If you find something like this, it is probably a strong effect even if it came out of a test of millions of hypotheses.  </p>
"
['neuralnetwork'],What does the activation of a neuron mean?,"<p>The activation of a neuron is mathematically nothing but a function of its input. Consider a neural network with one hidden layer and one input vector $\mathbf{x}$. The input $a_j$ of neuron $j$ can be written as:  </p>

<p>$a_j = w_j^T \mathbf{x} + b_j$. </p>

<p>The activation of neuron $j$ is then a transformation $g: \mathbf{R} \rightarrow \mathbf{R}$ of the input. For exmaple, one can use the sigmoid activiation function</p>

<p>$sig(a_j) = \dfrac{1}{1 + \exp(-a_j)} $</p>

<p>The weights $w$ are chosen to minimize a loss function and not for the sake of their interpretation. Nevertheless, one can try to interpret the activation of a neuron as <em>internal representation of the input</em>. One of the nicest examples I've seen comes from the <a href=""http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html"" rel=""nofollow"">Rummelhart et al (1986)</a>, Figure 4. In that paper, two family trees were fed into a neural network. The family trees represented an Italian and an English speaking family comprising three generations. Among other things, when activating one name of the tree, the neuronal activities represented whether the person was from the English or Italian tree and which generation the person embodied.   </p>
"
"['python', 'logistic-regression', 'gradient-descent']",Trying to understand Logistic Regression Implementation,"<p>There are several issues I see with the implementation. Some are just unnecessarily complicated ways of doing it, but some are genuine errors. </p>

<p><strong>Primary takeaways</strong></p>

<p>A: <em>Try to start from the math behind the model.</em> The logistic regression is a relatively simple one. Find the two equations you need and stick to them, replicate them letter by letter.</p>

<p>B: <em>Vectorize.</em> It will save you a lot of unnecessary steps and computations, if you step back for a bit and think of the best vectorized implementation.</p>

<p>C: <em>Write more comments in the code.</em> It will help those trying to help you. It will also help you understand each part better and maybe uncover errors yourself.</p>

<p>Now let's go over the code step by step.</p>

<p><strong>1. The sigmoid function</strong></p>

<p>Is there a reason for such a complicate implementation in <code>phi(t)</code>? Assuming that <code>t</code> is a vector (a numpy array), then all you really need is:</p>

<pre><code>def phi(t):
   1. / (1. + np.exp(-t))
</code></pre>

<p>As <code>np.exp()</code> operates element-wise over arrays. Ideally, I'd implement it as a function that can also return its derivative (not necessary here, but might be handy if you try to implement a basic neural net with sigmoid activations):</p>

<pre><code>def phi(t, dt = False):
   if dt:
      phi(t) * (1. - phi(t))
   else:
      1. / (1. + np.exp(-t))
</code></pre>

<p><strong>2. Cost function</strong></p>

<p>Usually, the logistic cost function is defined as a log cost in the following way (vectorized): $ \frac{1}{m} (-(y^T \log{(\phi(X\theta))})-(1-y^T)(\log{(1 - \phi(X\theta))}) + \frac{\lambda}{2m} \theta^{1T}\theta $  where $\phi(z)$ is the logistic (sigmoid) function, $\theta$ is the full parameter vector (including bias weight), $\theta^1$ is parameter vector with $\theta_1=0$ (by convention, bias is not regularized) and $\lambda$ is the regularization parameter. </p>

<p>What I really don't understand is the part where you multiply <code>y * z</code>. Assuming <code>y</code> is your label vector $y$, why are you multiplying it with your <code>z</code> before applying the sigmoid function? And why do you need to split the cost function into zeros and ones and calculate losses for either sample separately? 
I think the problem in your code really lies in this part: you must be erroneously multiplying $y$ with $X\theta$ before applying $\phi(.)$.</p>

<p>Also, this bit here: <code>X.dot(w) + c</code>. So <code>c</code> is your bias parameter, right? Why are you adding it to every element of $X\theta$? It shouldn't be added - it should be the first element of the vector $X\theta$. Yes, you don't regularize it, but you need to use in the ""prediction"" part of the loss function.</p>

<p>In your code, I also see the cost function as being overly complicated. Here's what I would try:</p>

<pre><code>def loss(X,y,w,lam):
   #calculate ""prediction""
   Z = np.dot(X,w)
   #calculate cost without regularization
   #shape of X is (m,n), shape of w is (n,1)
   J = (1./len(X)) * (-np.dot(y.T, np.log(phi(Z))) * np.dot((1-y.T),np.log(1 - phi(Z))))
   #add regularization
   #temporary weight vector
   w1 = copy.copy(w) #import copy to create a true copy
   w1[0] = 0
   J += (lam/(2.*len(X))) * np.dot(w1.T, w)
   return J
</code></pre>

<p><strong>3. Gradient</strong></p>

<p>Again, let's first go over the formula for the gradient of the logistic loss, again, vectorized: $\frac{1}{m} ((\phi(X\theta) - y)^TX)^T + \frac{\lambda}{m}\theta^1$.
This will return a vector of derivatives (i.e. gradient) for all parameters, regularized properly (without the bias term regularized).</p>

<p>Here again, you've multiplied by $y$ way too soon: <code>phi(y * z)</code>. In fact, you shouldn't have multiplied by $y$ in gradient at all. </p>

<p>This is what I would do for the gradient:</p>

<pre><code>def gradient(X, y, w, lam):
   #calculate the prediction
   Z = np.dot(X,w)
   #temporary weight vector
   w1 = copy.copy(w) #import copy to create a true copy
   w1[0] = 0
   #calc gradient
   grad = (1./len(X)) * (np.dot((phi(Z) - y).T,X).T) + (lam/len(X)) * w1
   return grad
</code></pre>

<p>The actual gradient descent implementation seems ok to me, but because there are errors in the gradient and cost function implementations, it fails to deliver :/</p>

<p>Hope this will help you get on track.</p>
"
"['data-mining', 'ensemble-modeling']","Is SuperLearning actually different to stacking,or are they essentially the same thing?","<p>So Ensemble Learning is essentially using multiple learning algorithms and providing the best predictive performance considering all of them. <a href=""http://www.stat.berkeley.edu/~ledell/docs/dlab_ensembles.pdf"" rel=""nofollow"">This</a> gives a better detailed description. Now, Ensemble Learning can be broadly divided into multiple types like </p>

<ul>
<li>Boosting</li>
<li>Bagging</li>
<li>Stacking / Super Learning</li>
</ul>

<p>Stacking contains a bunch of algorithms along with a learner to ensemble a group of base learners. Going deeper, the term <a href=""http://statistics.berkeley.edu/sites/default/files/tech-reports/367.pdf"" rel=""nofollow"">Stacking</a> was used way before Super Learning. Later, when the algorithm was actually developed theoretically and made popular in 2007, it was given the name 'Super Learner'. </p>
"
['neuralnetwork'],Neural Network data type conversion - float from int?,"<p>Technically with most languages you could pass in integer features for the input layer, since the weights will be floats, and multiplying a float by an integer will give you a float. Also, you don't usually care about partial derivatives of the input data, so it doesn't matter that the values are discrete.</p>

<p>However:</p>

<ul>
<li><p>For all weights and neuron activations, if you are using a method based on backpropagation for training updates, then you need a data type that approximates real numbers, so that you can apply fractional updates based on differentiation. Best weight values are often going to be fractional, non-whole numbers. Non-linearities such as sigmoid are also going to output floats. So after the input layer you have matrices of float values anyway. There is not much speed advantage multiplying integer matrix with float one (possibly even slightly slower, depending on type casting mechanism). So the input may as well be float.</p></li>
<li><p>In addition, for efficient training, the neural network inputs should be normalised to a specific roughly unit range (-1.0 to 1.0) or to mean 0, standard deviation 1.0. Both of these require float representation. If you have input data in 0-255 range - float or not - you will usually find the network will learn less effectively.</p></li>
</ul>

<p>There are exceptions to these rules in some architectures and learning algorithms, where perhaps an integer-based input would work, but for the most common NN types, including MLP and ""deep"" feed-forward networks, it is simpler and easier to use float data type.</p>
"
"['python', 'logistic-regression', 'scikit', 'gradient-descent']",Scikit-learn: Getting SGDClassifier to predict as well as a Logistic Regression,"<p>The comments about iteration number are spot on. The default <code>SGDClassifier</code> <code>n_iter</code> is <code>5</code> meaning you do <code>5 * num_rows</code> steps in weight space. The <a href=""http://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use"">sklearn rule of thumb</a> is ~ 1 million steps for typical data. For your example, just set it to 1000 and it might reach tolerance first. Your accuracy is lower with <code>SGDClassifier</code> because it's hitting iteration limit before tolerance so you are ""early stopping""</p>

<p>Modifying your code quick and dirty I get:</p>

<pre><code># Added n_iter here
params = [{}, {""loss"": ""log"", ""penalty"": ""l2"", 'n_iter':1000}]

for param, Model in zip(params, Models):
    total = 0
    for train_indices, test_indices in kf:
        train_X = X[train_indices, :]; train_Y = Y[train_indices]
        test_X = X[test_indices, :]; test_Y = Y[test_indices]
        reg = Model(**param)
        reg.fit(train_X, train_Y)
        predictions = reg.predict(test_X)
        total += accuracy_score(test_Y, predictions)

    accuracy = total / numFolds
    print ""Accuracy score of {0}: {1}"".format(Model.__name__, accuracy)

Accuracy score of LogisticRegression: 0.96
Accuracy score of SGDClassifier: 0.96
</code></pre>
"
"['r', 'clustering', 'time-series', 'bioinformatics']",R: Comparing dissimilarity between metabolic models with discrete wavelet transformation,"<p>Take a look at the <code>TSclust</code> package. Here how you would apply it to your sample data. </p>

<pre><code>require(TSclust)

#read in the data
model_a &lt;- read.csv(""~/Desktop/Model A.csv"", header = TRUE, stringsAsFactors = FALSE)
model_b &lt;- read.csv(""~/Desktop/Model B.csv"", header = TRUE, stringsAsFactors = FALSE)

#data must be in rows rather than columns
model_a &lt;- as.data.frame(t(model_a))

model_b &lt;- as.data.frame(t(model_b))

#calculate dissimlarities between metabolites in models 1 and 2
met1_DWT.diss &lt;- as.numeric(diss.DWT(rbind(model_a['Met1', ], model_b['Met1', ])))
met1_DWT.diss
[1] 90.80332

met2_DWT.diss &lt;- as.numeric(diss.DWT(rbind(model_a['Met2', ], model_b['Met2', ])))
met2_DWT.diss
[1] 1.499241
</code></pre>
"
"['neuralnetwork', 'supervised-learning', 'matlab', 'accuracy', 'training']",Query regarding neural network model,"<p>To debug this case, I suggest you try the following steps:</p>

<ol>
<li>Reduce the features step-by-step until you end up with using just 1 feature and see whether the accuracy changes or not.</li>
<li>Add a sine-wave and a random noise to the feature set and see whether it effects any of these optimization algorithms.</li>
<li>Re-evaluate how you selected or derived these features, check if these are highly correlated.</li>
<li>Are your classification targets highly imbalanced? If so, then under/over sample them to achieve a more balanced training set. Then check the performance of you algorithm after training over this balanced dataset.</li>
</ol>

<p>As already highlighted by Jan van der Vegt, its extremely odd that changing the no of features from 8 to 40 has no impact on test set accuracy.</p>
"
"['machine-learning', 'time-series', 'feature-selection']",Machine learning - features engineering from date/time data,"<p>I would start by graphing the time variable vs other variables and looking for trends.  </p>

<h2>For example</h2>

<p><img src=""http://i.stack.imgur.com/QGYUC.png"" alt=""enter image description here""></p>

<p>In this case there is a periodic weekly trend and a long term upwards trend.  So you would want to encode two time variables:</p>

<ul>
<li><code>day_of_week</code></li>
<li><code>absolute_time</code></li>
</ul>

<h2>In general</h2>

<p>There are several common time frames that trends occur over:</p>

<ul>
<li><code>absolute_time</code></li>
<li><code>day_of_year</code></li>
<li><code>day_of_week</code></li>
<li><code>month_of_year</code></li>
<li><code>hour_of_day</code></li>
<li><code>minute_of_hour</code></li>
</ul>

<p>Look for trends in all of these.  </p>

<h2>Weird trends</h2>

<p>Look for weird trends too.  For example you may see rare but persistent time based trends:</p>

<ul>
<li><code>is_easter</code></li>
<li><code>is_superbowl</code></li>
<li><code>is_national_emergency</code></li>
<li><code>etc.</code></li>
</ul>

<p>These often require that you cross reference your data against some external source that maps events to time.  </p>

<h2>Why graph?</h2>

<p>There are two reasons that I think graphing is so important.</p>

<ul>
<li><p><em>Weird trends</em><br>
While the general trends can be automated pretty easily (just add them
every time), weird trends will often require a human eye and knowledge
of the world to find.  This is one reason that graphing is so
important.</p></li>
<li><p><em>Data errors</em><br>
All too often data has serious errors in it.  For example, you may find that the dates were encoded in two formats and only one of them has been correctly loaded into your program.  There are a myriad of such problems and they are surprisingly common.  This is the other reason I think graphing is important, not just for time series, but for any data. </p></li>
</ul>
"
['classification'],What software is being used in this image recognition system?,"<p>I'm pretty sure that the software you're referring to is a some kind of <strong>internal</strong> <em>research project</em> software, developed by Enlitic (<a href=""http://www.enlitic.com"" rel=""nofollow"">http://www.enlitic.com</a>), where Jeremy Howard works as a founder and CEO. By ""internal research project software"" I mean either a <em>proof-of-concept</em> software, or a <em>prototype</em> software.</p>
"
"['machine-learning', 'feature-selection', 'optimization']",Machine Learning for hedging/ portfolio optimization?,"<p>That is a rather broad question, and there is tons of literature about quantitative analysis and stock market prediction using machine learning.</p>

<p>The most classical example of predicting the stock market is employing neural networks; you can use whatever feature you think might be relevant for your prediction, for example the unemployment rate, the oil price, the gold price, the interest rates, and the timeseries itself, i. e. the volatility, the change in the last 2,3,7,..., days etc. - a more classical approach is the input-output-analysis in econometrics, or the autoregression analysis, but all of it can be modeled using neural networks or any other function approximator / regression in a very natural way.</p>

<p>But, as said, there are tons of other possibilities to model the market, to name a few: Ant Colony Optimization (ACO), Classical regression analysis, genetic algorithms, decision trees, reinforcement learning etc. you name it, almost EVERYTHING has probably been applied to the stock market prediction problem.</p>

<p>There are different fond manager types on the markets. There are still the Quants which are doing a quantitative analysis using classical financial maths and maths borrowed from the physics to describe the market movements. There are still the most conservative ones which do a long-term, fundamental analysis of the corporation, that is, looking in how the corporation earns money and where it spends money. Or the tactical analysts who just look for immediate signals to buy / sell a stock in the short term. And those quantitative guys who employ machine learning amongst other methods.</p>
"
['neuralnetwork'],How would you teach multiplications to a neural network?,"<p>You need to limit the network model to one which naturally would generalise your problem.</p>

<p>For a simple multiplication, this would be a single linear neuron. You can also train bias so that the network represents <code>y = Ax + B</code>.</p>

<p>If you take that very simple network give it just 2 or 3 examples to learn from, and train it using least squares error, it will quickly converge to the function you want. A single layer linear network like this can generalise to <code>y = Ax + B</code> where x, y and B are vectors, and A is a matrix - i.e. it can solve simultaneous linear equations provided you supply enough examples to do so (usually number of examples equals number of dimensions of the vectors).</p>

<p>This is actually really trivial, and if you train such a simple network using any modern NN library, it will be very fast to learn your linear function. However, in the general case of having a specific non-linear function and wanting the network to learn it in a general sense, it is not possible. The network can be made to learn to generalise reasonably well near where you have provided example inputs and outputs, but will output incorrect results when you give it an <code>x</code> value far away from the training examples.</p>

<p>It is important to note that the network does not learn the math formula for your function. It finds the best approximation given its own internal model. But by picking a network where the internal model is a good match to the function you want to learn, it is going to generalise well to it, and may be able to get it exactly if there is no noise in the examples and enough of them.</p>
"
"['machine-learning', 'clustering']",NNDSVD to initialize Convex-NMF,"<p>I've played before with a library for (""classic"") NMF from University of Vienna: <a href=""http://www.univie.ac.at/rlcta/software/"" rel=""nofollow"">libNMF</a></p>

<p>While it wasn't useful for me at first, since they don't implement Convex-NMF, I had a look at their NNDSVD implementation.
They simply <strong>replace all negative values</strong> and values smaller than the machine epsilon <strong>with zero</strong>.</p>

<p>I tried that, but it fails, since the update rules for Convex-NMF are such that I get a division by zero. Since they <strong>add 0.2 to all values</strong> in their proposed initialization scheme (for ""smoothing""), I just did the same here.</p>

<p>It works, the precision is worse than expected (a bit worse than k-means, which is my best approach for now), but I'll tweak it a bit more.</p>
"
"['machine-learning', 'statistics']",Dividing percentage,"<p>If this is about splitting your data into training and testing data, then 80/20 is a common rule of thumb. An ""optimal"" split (which would need to be operationalized) would likely depend on your sample size, distributions and relationships between your variables.</p>

<p>It is also common to split your data <em>three</em> ways (e.g., 60/20/20 - again rules of thumb), into a training set that you train your models on and a test set which you test your model on. You will iterate training and testing until you like the result. Then, <em>and only then</em> you apply the final model (trained on both the training and test set) on the third validation set. This avoids ""overfitting on the test set"".</p>

<p>However, <a href=""http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""nofollow"">cross-validation</a> is much better than a simple data split. Your textbook should also cover cross-validation. If it doesn't, get a better textbook.</p>
"
"['machine-learning', 'classification']",Kaggle Titanic Survival Table an example of Naive Bayes?,"<p>Naive Bayes is just one of the several approaches that you may apply in order to solve the Titanic's problem. The aim of the Kaggle's Titanic problem is to build a <strong>classification</strong> system that is able to predict one outcome (whether one person survived or not) given some input data. The survival table is a <strong>training dataset</strong>, that is, a table containing a set of examples to train your system with. </p>

<p>As I mentioned before, you could apply Naive Bayes to build your classification system to solve the Titanic problem. Naive Bayes is one of the simplest classification algorithms out there. It assumes that the data in your dataset has a very specific structure. Sometimes Naive Bayes can provide you with results that are good enough. Even if that is not the case, Naive Bayes may be useful as a first step; the information you obtain by analyzing Naive Bayes' results, and by further data analysis, will help you to choose which classification algorithm you could try next. Other examples of classification methods are k-nearest neighbours, neural networks, and logistic regression, but this is just a short list. </p>

<p>If you are new to Machine Learning, I recommend you to take a look to this course from Stanford: <a href=""https://www.coursera.org/course/ml"" rel=""nofollow"">https://www.coursera.org/course/ml</a></p>
"
['graphs'],Facebook's Huge Database,"<p>Strange as it sounds, graphs and graph databases are typically implemented as <a href=""http://docs.oracle.com/javase/7/docs/api/java/util/LinkedList.html"">linked lists</a>. As alluded to <a href=""http://docs.neo4j.org/chunked/stable/cypher-cookbook-newsfeed.html"">here</a>, even the most popular/performant graph database out there (neo4j), is secretly using something akin to a doubly-linked list.</p>

<p>Representing a graph this way has a number of significant benefits, but also a few drawbacks. Firstly, representing a graph this way means that you can do edge-based insertions in near-constant time. Secondly, this means that traversing the graph can happen extremely quickly, if we're only looking to either step up or down a linked list.</p>

<p>The biggest drawback of this though comes from something sometimes called The Justin Bieber Effect, where nodes with a large number of connections tend to be extremely slow to evaluate. Imagine having to traverse a million semi-redundant links every time someone linked to Justin Bieber.</p>

<p>I know that the awesome folks over at Neo4j are working on the second problem, but I'm not sure how they're going about it, or how much success they've had.</p>
"
"['nlp', 'efficiency', 'clustering']",How to implement Brown Clustering Algorithm in O(|V|k^2),"<p>I have managed to resolve this. There is an excellent and thorough explanation of the optimization steps in the following thesis: <a href=""http://cs.stanford.edu/~pliang/papers/meng-thesis.pdf"" rel=""nofollow"">Semi-Supervised Learning for Natural Language by Percy Liang</a>.</p>

<p>My mistake was trying to update the quality for all potential clusters pairs. Instead, you should initialize a table with the quality changes of doing each merge. Use this table to find the best merge, and the update the relevant terms that make up the table entries.</p>
"
['bigdata'],What is an 'old name' of data scientist?,"<p>In reverse chronological order: data miner, statistician, (applied) mathematician.</p>
"
"['machine-learning', 'classification', 'predictive-modeling', 'supervised-learning']",Equipment failure prediction,"<p>You should include data when the phone was serviced to create a survival model. These models are commonly used in reliability engineering as well as treatment efficacy. For reliability engineering it is very common to fit your data to a Weibull distribution. Even aircraft manufacturers consider the model to be reliable after calibrating with three to five data points. I can highly recommend the R package 'flexsurv' package.</p>

<p>You cannot use typical linear or logistic regressions since some phones in your population will leave your observation period without ever being serviced. Survival models allow for this sort of missing information (this is called censoring). </p>

<p>Typically you would have the following data</p>

<pre><code>|ID| Type       | serviced  | # months_since_purchase
|1 | iphone     | 1         | 12
|2 | iphone     | 0         | 15
|3 | android    | 1         | 2
|4 | android    | 0         | 10
|5 | blackberry | 0         | 5.5
</code></pre>

<p>With that data you could create the following model in R</p>

<pre><code>require(survival)
model &lt;- survfit(Surv(months_since_purchase, serviced) ~ strata(Type) +
 cluster(ID), data = phone_repairs)
</code></pre>

<p>The <code>survfit.formula</code> <code>Surv(months_since_purchase, serviced) ~ strata(Type) + cluster(ID)</code>
indicates that <code>months_since_purchase</code> is the time at which an observation was
made, <code>serviced</code> is 1 if the phone was serviced and 0 otherwise, <code>strata(Type)</code>
will make sure that you create a different survival model for each phone,
<code>cluster(ID)</code> will make sure that events relating to the same ID are considered
as a cluster. </p>

<p>You could extend this model with Joint Models such as <code>JM</code>.</p>
"
"['databases', 'search', 'parsing', 'data-indexing-techniques']","Looking for a 'CITY, STATE' within a body of text (from a CITY-STATE database)","<p>The only thing I can see would be to separate both city and state lists and treat the problem as an automaton: parse your text, run through the n-grams, whenever you detect a CITY token (meaning a n-gram present in your list of cities or close to it in a similarity sense, as there might be misspellings) then look for a STATE token in its neighbourhood (similarly by looking into a list of states, using an edit distance metric to allow for misspellings). If you find one, then you can tag your text with that geographical location.</p>

<p>Of course, allowing for misspellings will bring some false positives but you could easily bypass that by doing a quick lookup through your corpus to see that ""SALAMI, OREGANO"" is different from ""SALEM, OREGON"" (because the frequency of the latter will hopefully be higher than the former)</p>
"
"['python', 'pandas', 'sklearn']",Struggling to integrate sklearn and pandas in simple Kaggle task,"<p>Here is an example of how to get pandas and sklearn to play nice </p>

<p>say you have 2 columns that are both strings and you wish to vectorize - but you have no idea which vectorization params will result in the best downstream performance.</p>

<p>create the vectorizer </p>

<pre><code>to_vect = Pipeline([('vect',CountVectorizer(min_df =1,max_df=.9,ngram_range=(1,2),max_features=1000)),
                    ('tfidf', TfidfTransformer())])
</code></pre>

<p>create the DataFrameMapper obj.</p>

<pre><code>full_mapper = DataFrameMapper([
        ('col_name1', to_vect),
        ('col_name2',to_vect)
    ])
</code></pre>

<p>this is the full pipeline</p>

<pre><code>full_pipeline  = Pipeline([('mapper',full_mapper),('clf', SGDClassifier(n_iter=15, warm_start=True))])
</code></pre>

<p>define the params you want the scan to consider</p>

<pre><code>full_params = {'clf__alpha': [1e-2,1e-3,1e-4],
                   'clf__loss':['modified_huber','hinge'],
                   'clf__penalty':['l2','l1'],
                   'mapper__features':[[('col_name1',deepcopy(to_vect)),
                                        ('col_name2',deepcopy(to_vect))],
                                       [('col_name1',deepcopy(to_vect).set_params(vect__analyzer= 'char_wb')),
                                        ('col_name2',deepcopy(to_vect))]]}
</code></pre>

<p>Thats it! - note however that mapper_features are a single item in this dictionary - so use a for loop or itertools.product to generate a FLAT list of all to_vect options you wish to consider - but that is a separate task outside the scope of the question.</p>

<p>Go on to create the optimal classifier or whatever else your pipeline ends with</p>

<pre><code>gs_clf = GridSearchCV(full_pipe, full_params, n_jobs=-1)
</code></pre>
"
['data-mining'],Dealing with events that have not yet happened when building a model,"<p>This setting is common in reliability, health care, and mortality. The statistical analysis method is called <a href=""http://en.wikipedia.org/wiki/Survival_analysis"">Survival Analysis</a>. All users are coded according to their start date (or week or month).  You use the empirical data to estimate the survival function, which is the probability that the time of defection is later than some specified time <strong><em>t</em></strong>.</p>

<p>Your baseline model will estimate survival function for all users.  Then you can do more sophisticated modeling to estimate what factors or behaviors might predict defection (churn), given your baseline survival function.  Basically, any model that is predictive will yield a survival probability that is significantly lower than the baseline.</p>

<hr>

<p>There's another approach which involves attempting to identify precursor events patterns or user behavior pattern that foreshadow defection. Any given event/behavior pattern might occur for users that defect, or for users that stay. For this analysis, you may need to censor your data to only include users that have been members for some minimum period of time. The minimum time period can be estimated using your estimate of survival function, or even simple histogram analysis of the distribution of membership period for users who have defected. </p>
"
"['machine-learning', 'statistics', 'time-series']",Linearly increasing data with manual reset,"<p>I thought this was an interesting problem, so I wrote a sample data set and a linear slope estimator in R.  I hope it helps you with your problem.  I'm going to make some assumptions, the biggest is that you want to estimate a constant slope, given by some segments in your data.  Another assumption to separate the blocks of linear data is that the natural 'reset' will be found by comparing consecutive differences and finding ones that are X-standard deviations below the mean. (I chose 4 sd's, but this can be changed)</p>

<p>Here is a plot of the data, and the code to generating it is at the bottom.
<img src=""http://i.stack.imgur.com/2dC1w.png"" alt=""Sample Data""></p>

<p>For starters, we find the breaks and fit each set of y-values and record the slopes.</p>

<pre><code># Find the differences between adjacent points
diffs = y_data[-1] - y_data[-length(y_data)]
# Find the break points (here I use 4 s.d.'s)
break_points = c(0,which(diffs &lt; (mean(diffs) - 4*sd(diffs))),length(y_data))
# Create the lists of y-values
y_lists = sapply(1:(length(break_points)-1),function(x){
  y_data[(break_points[x]+1):(break_points[x+1])]
})
# Create the lists of x-values
x_lists = lapply(y_lists,function(x) 1:length(x))
#Find all the slopes for the lists of points
slopes = unlist(lapply(1:length(y_lists), function(x) lm(y_lists[[x]] ~ x_lists[[x]])$coefficients[2]))
</code></pre>

<p>Here are the slopes:
(3.309110, 4.419178, 3.292029, 4.531126, 3.675178, 4.294389)</p>

<p>And we can just take the mean to find the expected slope (3.920168).</p>

<hr>

<p><strong>Edit: Predicting when series reaches 120</strong></p>

<p>I realized I didn't finish predicted when series reaches 120.  If we estimate the slope to be m and we see a reset at time t to a value x (x&lt;120), we can predict how much longer it would take to reach 120 by some simple algebra.</p>

<p><img src=""http://i.stack.imgur.com/DixZv.gif"" alt=""enter image description here""></p>

<p>Here, t is the time it would take to reach 120 after a reset, x is what it resets to, and m is the estimated slope.  I'm not going to even touch the subject of units here, but it's good practice to work them out and make sure everything makes sense.</p>

<hr>

<p><strong>Edit: Creating The Sample Data</strong></p>

<p>The sample data will consist of 100 points, random noise with a slope of 4 (Hopefully we will estimate this).  When the y-values reach a cutoff, they reset to 50.  The cutoff is randomly chosen between 115 and 120 for each reset.  Here is the R code to create the data set.</p>

<pre><code># Create Sample Data
set.seed(1001)
x_data = 1:100 # x-data
y_data = rep(0,length(x_data)) # Initialize y-data
y_data[1] = 50 
reset_level = sample(115:120,1) # Select initial cutoff
for (i in x_data[-1]){ # Loop through rest of x-data
  if(y_data[i-1]&gt;reset_level){ # check if y-value is above cutoff
    y_data[i] = 50             # Reset if it is and
    reset_level = sample(115:120,1) # rechoose cutoff
  }else {
    y_data[i] = y_data[i-1] + 4 + (10*runif(1)-5) # Or just increment y with random noise
  }
}
plot(x_data,y_data) # Plot data
</code></pre>
"
"['python', 'sklearn', 'error-handling']",Problems with accuracy.score sklearn,"<p>You have not defined the variable <code>predictions</code> anywhere. You will need to get them from your classifier somehow. You have fit your <code>nordan_tree</code> on your training data, now you can use the fitted <code>nordan_tree</code> to generate the predictions, for example like this:</p>

<pre><code>predictions = nordan_tree.predict(X_test)
</code></pre>

<p>Then your line of:</p>

<pre><code>print(accuracy_score(y_test, predictions))
</code></pre>

<p>should work.</p>
"
"['xgboost', 'evaluation']",XGBoost increase the error when changing evaluation function,"<p>If your goal is to minimize the RMSLE, the easier way is to transform the labels directly into log scale and use <code>reg:linear</code> as objective (which is the default) and <code>rmse</code> as evaluation metric. This way XGBoost will be minimizing the RMSLE direclty. You can achieve this by setting:</p>

<pre><code>dtrain = DMatrix(X_train, label=np.log1p(y_train))
</code></pre>

<p>where <code>np.log1p(x)</code> is equal to <code>np.log(x+1)</code>.</p>

<p>When you want to make your prediction in the original space, you will need to compute the inverse transform of <code>np.log1p</code>, that is to say <code>np.expm1</code>:</p>

<pre><code>predictions = np.expm1(bst.predict(dtest))
</code></pre>

<p>If you are just interested into monitoring the RMSLE through the training of your XGBoost which actually is minimizing the RMSE, then you should expect to see the RMSLE behave a little strangely as it is not what you are minimizing.</p>
"
"['machine-learning', 'python', 'neuralnetwork', 'statistics', 'tensorflow']",Neural networks: which cost function to use?,"<p>This answer is on the <em>general</em> side of cost functions, not related to TensorFlow, and will mosty adress the ""some explanation about this topic"" part of your question.</p>

<p>In most examples/tutorial i followed, the cost function used was somewhat arbitrary. The point was more to introduce the reader to a specific method, not to the cost function specifically. It should not stop you to follow the tutorial to be familiar with the tools, but my answer should help you on how to choose the cost function for your own problems.</p>

<p>If you want answers regarding Cross-Entropy, Logit, L2 norms, or anything specific, I advise you to post multiple, more specific questions. This will increase the probability that someone with the specific knowledge will see your question.</p>

<hr>

<p>Choosing the right cost function for achieving the desired result is a critical point of machine learning problems. The basic approach, if you do not know exactly what you want out of your method, is to use <a href=""https://en.wikipedia.org/wiki/Mean_squared_error"">Mean Square Error (Wikipedia)</a> for regression problems and Percentage of error for classification problems. However, if you want <em>good</em> results out of your method, you need to <em>define good</em>, and thus define the adequate cost function. This comes from both domain knowledge (what is your data, what are you trying to achieve), and knowledge of the tools at your disposal. </p>

<p>I do not believe I can guide you through the costs functions already implemented in TensorFlow, as I have very little knowledge of the tool, but I can give you an example on how to write and assess different cost functions.</p>

<hr>

<p>To illustrate the various differences between cost functions, let us use the exemple of the binary classification problem, where we want, for each sample $x_n$, the class $f(x_n) \in \{0,1\}$.</p>

<p>Starting with <strong>computational properties</strong>; how two functions measuring the ""same thing"" could lead to different results. Take the following, simple cost function; the percentage of error. If you have $N$ samples, $f(y_n)$ is the predicted class and $y_n$ the true class, you want to minimize</p>

<ul>
<li>$\frac{1}{N} \sum_n \left\{
\begin{array}{ll}
1 &amp; \text{ if } f(x_n) \not= y_n\\
0 &amp; \text{ otherwise}\\
\end{array} \right. = \sum_n y_n[1-f(x_n)] + [1-y_n]f(x_n)$.</li>
</ul>

<p>This cost function has the benefit of being easily interpretable. However, it is not smooth; if you have only two samples, the function ""jumps"" from 0, to 0.5, to 1. This will lead to inconsistencies if you try to use gradient descent on this function. One way to avoid it is to change the cost function to use probabilities of assignment; $p(y_n = 1 | x_n)$. The function becomes</p>

<ul>
<li>$\frac{1}{N} \sum_n y_n p(y_n = 0 | x_n) + (1 - y_n) p(y_n = 1 | x_n)$.</li>
</ul>

<p>This function is smoother, and will work better with a gradient descent approach. You will get a 'finer' model. However, it has other problem; if you have a sample that is ambiguous, let say that you do not have enough information to say anything better than $p(y_n = 1 | x_n) = 0.5$. Then, using gradient descent on this cost function will lead to a model which increases this probability as much as possible, and thus, maybe, overfit.</p>

<p>Another problem of this function is that if $p(y_n = 1 | x_n) = 1$ while $y_n = 0$, you are certain to be right, but you are wrong. In order to avoid this issue, you can take the log of the probability, $\log p(y_n | x_n)$. As $\log(0) = \infty$ and $\log(1) = 0$, the following function does not have the problem described in the previous paragraph:</p>

<ul>
<li>$\frac{1}{N} \sum_n y_n \log p(y_n = 0 | x_n) + (1 - y_n) \log p(y_n = 1 | x_n)$.</li>
</ul>

<p>This should illustrate that in order to optimize the <em>same thing</em>, the percentage of error, different definitions might yield different results if they are easier to make sense of, computationaly.</p>

<p><strong>It is possible for cost functions $A$ and $B$ to measure the <em>same concept</em>, but $A$ might lead your method to better results than $B$.</strong></p>

<hr>

<p>Now let see how different costs function can measure different concepts. In the context of information retrieval, as in google search (if we ignore ranking), we want the returned results to</p>

<ul>
<li>have high <em>precision</em>, not return irrelevant information</li>
<li>have high <em>recall</em>, return as much relevant results as possible</li>
<li><a href=""https://en.wikipedia.org/wiki/Precision_and_recall"">Precision and Recall (Wikipedia)</a></li>
</ul>

<p>Note that if your algorithm returns <em>everything</em>, it will return every relevant result possible, and thus have high recall, but have very poor precision. On the other hand, if it returns only <em>one</em> element, the one that it is the most certain is relevant, it will have high precision but low recall.</p>

<p>In order to judge such algorithms, the common cost function is the <a href=""https://en.wikipedia.org/wiki/F1_score"">$F$-score (Wikipedia)</a>. The common case is the $F_1$-score, which gives equal weight to precision and recall, but the general case it the $F_\beta$-score, and you can tweak $\beta$ to get</p>

<ul>
<li>Higher recall, if you use $\beta &gt; 1$</li>
<li>Higher precision, if you use $\beta &lt; 1$.</li>
</ul>

<p>In such scenario, <strong>choosing the cost function is choosing what tradeoff your algorithm should do</strong>.</p>

<p>Another example that is often brought up is the case of medical diagnosis, you can choose a cost function that punishes more false negatives or false positives depending on what is preferrable:</p>

<ul>
<li>More healthy people being classified as sick (But then, we might treat healthy people, which is costly and might hurt them if they are actually not sick)</li>
<li>More sick people being classified as healthy (But then, they might die without treatment)</li>
</ul>

<hr>

<p>In conclusion, defining the cost function is defining the goal of your algorithm. The algorithm defines how to get there.</p>

<hr>

<p>Sidenote: Some cost functions have nice algorithm ways to get to their goals. For example, a nice way to the minimum of the <a href=""https://en.wikipedia.org/wiki/Hinge_loss"">Hinge loss (Wikipedia)</a> exists, by solving the dual problem in <a href=""https://en.wikipedia.org/wiki/Support_vector_machine"">SVM (Wikipedia)</a></p>
"
"['machine-learning', 'data-mining', 'python', 'scikit']",sklearn.naive_bayes VS categorical variables,"<p><a href=""https://en.wikipedia.org/wiki/One-hot"" rel=""nofollow"">Hot encode</a> the categorical variables and use Bernoulli naive Bayes. Hot encoding is usually the trick one uses in representing categorical variables.</p>
"
"['nlp', 'unsupervised-learning', 'nltk']",Inferring Relational Hierarchies of Words,"<p>Look up taxonomy/ontology construction/induction. Relevant papers:</p>

<ul>
<li>Automatic Taxonomy Construction from Keywords via Scalable Bayesian Rose Trees</li>
<li>Topic Models for Taxonomies</li>
<li>OntoLearn Reloaded. A Graph-Based Algorithm for Taxonomy Induction</li>
<li>Ontology Population and Enrichment: State of the Art</li>
<li>Probabilistic Topic Models for Learning Terminological Ontologies</li>
</ul>
"
"['machine-learning', 'clustering']",Algorithm for deriving mutiple clusters,"<p>I don't think that EM clustering algorithms like k-means and Gaussian mixture models are quite what you're looking for. There are definitely other algorithms that don't require one to pick a number of clusters. My personal favorite (most of the time) is called mean-shift-clustering. You could find a great little blog post about it <a href=""http://spin.atomicobject.com/2015/05/26/mean-shift-clustering/"" rel=""nofollow"">here</a>, and it has a good implementation in python's scikit-learn library.</p>
"
"['machine-learning', 'python', 'clustering', 'k-means', 'sklearn']",K Means giving poor results,"<p>K-means is based on the assumption that the data is ""translation invariant"" (more precisely: variance does, and k-means is variance minimization).</p>

<p>In other words, it assumes that a difference of d=(x-y)^2 is of the <em>same</em> importance everywhere. Because of this, <strong>k-means does not work on skewed data</strong>. Furthermore, because of the square, it is <strong>sensitive to outliers and other extreme values</strong>.</p>

<p>For salaries and other monetary values, this usually does not hold. The difference between \$0 and \$1000 is massive, and not the same as a salary difference of \$100000 to \$101000. Salaries are usually rather skewed, and you often have some extreme values.</p>

<p><strong>Converting the ""user"" attribute to a numerical value is outright statistical nonsense</strong>. What's variance worth in this attribute? <strong>K-means is for continuous numerical data only</strong>, and converting data does not chnage the <em>nature</em>, only the encoding - it's still inappropriate.</p>
"
"['machine-learning', 'cross-validation', 'model-selection']",On coursera what exactly does Andrew Ng say in videos Lectures 60 & 61 of machine learning?,"<p><strong>No</strong>, he actually says the opposite:</p>

<blockquote>
  <p>One final note: I should say that in the machine learning as of this practice today, there <strong>are</strong> many people that will do that early thing that I talked about, and said that, you know...​</p>
</blockquote>

<p>Then he says (the ""early thing"" he talked about):</p>

<blockquote>
  <p>selecting your model as a test set and then using the same test set to report the error
  ...
  unfortunately many people do that</p>
</blockquote>

<hr>

<p>In this lesson he explains about separating the data set:</p>

<ol>
<li><em>training set</em> to train the model;</li>
<li><em>cross validation set</em> to find the right parameters;</li>
<li><em>test set</em> to find the final generalization error (of the function with the best parameter values found during using the <em>cross validation set</em>).</li>
</ol>

<p>So Andrew Ng is complaining that many people us the same data set to find the right parameters, and then report the error of that data set as final <em>generalization error</em>.</p>
"
"['nlp', 'text-mining']",Extract most informative parts of text from documents,"<p>What you're describing is often achieved using a simple combination of <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"">TF-IDF</a> and <a href=""http://en.wikipedia.org/wiki/Automatic_summarization#Extraction-based_summarization"">extractive summarization</a>.</p>

<p>In a nutshell, TF-IDF tells you the relative importance of each word in each document, in comparison to the rest of your corpus. At this point, you have a score for each word in each document approximating its ""importance."" Then you can use these individual word scores to compute a composite score for each sentence by summing the scores of each word in each sentence. Finally, simply take the top-N scoring sentences from each document as its summary.</p>

<p>Earlier this year, I put together an iPython Notebook that culminates with an implementation of this in Python using NLTK and Scikit-learn: <a href=""https://github.com/charlieg/A-Smattering-of-NLP-in-Python"">A Smattering of NLP in Python</a>.</p>
"
"['machine-learning', 'convnet']",Is Maxout the same as max pooling?,"<p>They are almost identical:</p>

<blockquote>
  <p>The second key reason that maxout performs well is
  that it improves the bagging style training phase of
  dropout. Note that the arguments in section 7 motivating
  the use of maxout also apply equally to rectified
  linear units (Salinas &amp; Abbott, 1996; Hahnloser, 1998;
  Glorot et al., 2011). <strong>The only difference between maxout
  and max pooling over a set of rectified linear units
  is that maxout does not include a 0 in the max</strong>.</p>
</blockquote>

<p>Source: <a href=""http://arxiv.org/pdf/1302.4389v4.pdf"" rel=""nofollow"">Maxout Networks</a>.</p>
"
"['r', 'data-cleaning']",removing words based on a predefined vector,"<pre><code>texts &lt;- c(""This is the first document."", 
       ""Is this a text?"", 
       ""This is the second file."", 
       ""This is the third text."", 
       ""File is not this."") 

test_stopword &lt;- as.data.frame(texts)
ordinal_stopwords  &lt;- c(""first"",""primary"",""second"",""secondary"",""third"")

(newdata &lt;- as.data.frame(sapply(texts, function(x)   gsub(paste(ordinal_stopwords, collapse = '|'), '', x))))
</code></pre>

<p>The output is getting skewed when added in a code block (<a href=""http://meta.stackexchange.com/q/270069/302377"">maybe a bug in SE</a>). But, you would get the desired output.</p>
"
['machine-learning'],"Why is there no end user Application, yet?","<p>Listing 2 examples:
<a href=""https://www.ibm.com/marketplace/cloud/watson-analytics/us/en-us"" rel=""nofollow"">IBM Watson Analytics</a></p>

<p><a href=""https://www.ibm.com/marketplace/cloud/watson-analytics/us/en-us"" rel=""nofollow"">Amazon ML use case</a></p>

<p>Preparing the data for supervised learning is require skills. Not all data came labeled and in form to be used as in need to solving problems. </p>

<p>Also many more platforms/Api are in market now but for sure you can't solve a problem only with 1 algorithm, is needed much more ... Hope it help. </p>
"
"['classification', 'definitions', 'accuracy', 'sampling']",How to define a custom resampling methodology,"<p>Random subsampling seems appropriate, bootstrapping is a bit more generic, but also correct.</p>

<p>Here are some references and synonyms: <a href=""http://www.frank-dieterle.com/phd/2_4_3.html"" rel=""nofollow"">http://www.frank-dieterle.com/phd/2_4_3.html</a></p>
"
"['machine-learning', 'neuralnetwork', 'feature-selection', 'feature-extraction']",How to choose the features for a neural network?,"<p>A very strong correlation between the new feature and an existing feature is a fairly good sign that the new feature provides little new information.  A low correlation between the new feature and existing features is likely preferable.</p>

<p>A strong linear correlation between the new feature and the predicted variable is an good sign that a new feature will be valuable, but the absence of a high correlation is not necessary a sign of a poor feature, because neural networks are not restricted to linear combinations of variables.   </p>

<p>If the new feature was manually constructed from a combination of existing features, consider leaving it out.  The beauty of neural networks is that little feature engineering and preprocessing is required -- features are instead learned by intermediate layers.  Whenever possible, prefer learning features to engineering them.</p>
"
['hadoop'],Cloudera QuickStart VM Error,"<p>Go into the other link from home - to the cloudera manager.</p>

<p>From there, you'll see Hue can be restarted, but there is probably an alert that needs to be resolved in there first.</p>

<p>If I remember right there's some initial configuration that needs to be done on the quickstart VM that's spelled out as soon as you log into the manager application.</p>
"
"['data-mining', 'svm', 'state-of-the-art']","Are Support Vector Machines still considered ""state of the art"" in their niche?","<p>SVM is a powerful classifier. It has some nice advantages (which I guess were responsible for its popularity)... These are:</p>

<ul>
<li>Efficiency: Only the support vectors play a role in determining the classification boundary. All other points from the training set needn't be stored in memory.</li>
<li>The so-called power of kernels: With appropriate kernels you can transform feature space into a higher dimension so that it becomes linearly separable. The notion of kernels work with arbitrary objects on which you can define some notion of similarity with the help of inner products... and hence SVMs can classify arbitrary objects such as trees, graphs etc.</li>
</ul>

<p>There are some significant disadvantages as well.</p>

<ul>
<li>Parameter sensitivity: The performance is highly sensitive to the choice of the regularization parameter C, which allows some variance in the model.</li>
<li>Extra parameter for the Gaussian kernel: The radius of the Gaussian kernel can have a significant impact on classifier accuracy. Typically a grid search has to be conducted to find optimal parameters. LibSVM has a support for grid search.</li>
</ul>

<p>SVMs generally belong to the class of ""Sparse Kernel Machines"". The sparse vectors in the case of SVM are the support vectors which are chosen from the maximum margin criterion. Other sparse vector machines such as the <strong>Relevance Vector Machine</strong> (RVM) perform better than SVM. The following figure shows a comparative performance of the two. In the figure, the x-axis shows one dimensional data from two classes y={0,1}. The mixture model is defined as P(x|y=0)=Unif(0,1) and P(x|y=1)=Unif(.5,1.5) (Unif denotes uniform distribution). 1000 points were sampled from this mixture and an SVM and an RVM were used to estimate the posterior. The problem of SVM is that the predicted values are far off from the true log odds.  </p>

<p><img src=""http://i.stack.imgur.com/zNYbt.png"" alt=""RVM vs. SVM""> </p>

<p>A very effective classifier, which is very popular nowadays, is the <strong>Random Forest</strong>. The main advantages are:</p>

<ul>
<li>Only one parameter to tune (i.e. the number of trees in the forest)</li>
<li>Not utterly parameter sensitive</li>
<li>Can easily be extended to multiple classes</li>
<li>Is based on probabilistic principles (maximizing mutual information gain with the help of decision trees)</li>
</ul>
"
['text-mining'],Commercial Text Summarization Tools,"<p>There are a couple of open source options I know of - </p>

<p>LibOTS - <a href=""http://libots.sourceforge.net/"" rel=""nofollow"">http://libots.sourceforge.net/</a></p>

<p>DocSum - <a href=""http://docsum.sourceforge.net/docsum/web/about.php"" rel=""nofollow"">http://docsum.sourceforge.net/docsum/web/about.php</a></p>

<p>A couple of commercial solutions - </p>

<p>Intellix Summarizer Pro - <a href=""http://summarizer.intellexer.com/order_summarizer_pro.php"" rel=""nofollow"">http://summarizer.intellexer.com/order_summarizer_pro.php</a></p>

<p>Copernic Summarizer - <a href=""http://www.copernic.com/en/products/summarizer/"" rel=""nofollow"">http://www.copernic.com/en/products/summarizer/</a></p>

<p>And this one is a web service - </p>

<p>TextTeaser - <a href=""http://www.textteaser.com/"" rel=""nofollow"">http://www.textteaser.com/</a></p>

<p>I'm sure there are plenty of others out there.  I have used Copernic a good deal and it's pretty good, but I was hoping it could be automated easily, which it can't - at least it couldn't when I used it.  </p>
"
"['neuralnetwork', 'parameter', 'matlab']",How can the performance of a neural network vary considerably without changing any parameters?,"<p>In general, there is no guarantee that ANNs such as a multi-layer Perceptron network will converge to the global minimum squared error (MSE) solution. The final state of the network can be heavily dependent on how the network weights are initialized. Since most initialization schemes (including Nguyen-Widrow) use random numbers to generate the initial weights, it is quite possible that some initial states will result in convergence on local minima, whereas others will converge on the MSE solution.</p>
"
"['algorithms', 'search']",Algorithm for multiple extended string matching,"<p>I would recommend regexp pattern matching. I know that usual implementations are slow but you have to study Thompson's construction algorithm for nondeterministic automaton. See the wikipedia dedicated <a href=""http://en.wikipedia.org/wiki/Thompson%27s_construction_algorithm"" rel=""nofollow"">article</a>. However here the wikipedia fails to present this treasure properly. I would strongly recommend to study carefuly this blog article: <a href=""http://swtch.com/~rsc/regexp/regexp1.html"" rel=""nofollow"">Regular expressions can be simple and fast</a>. For implementations you have pointers in the given article (for example awk and grep uses this implementation). </p>
"
"['machine-learning', 'convnet']",How to architect ConvNet to ignore top half of image,"<p>You could be right that ignoring top part of image would benefit the CNN. However, there is very little point in trying to <em>architect</em> this - if your premise that the CNN will ignore irrelevant details in the top half is correct, then that will occur anyway and there is no standard NN architecture that will help that other than disconnecting the top half of network, which is going to be logically exactly the same as programmatically slicing the image, with the disadvantage of storing and calculating with twice as many parameters.</p>

<p>You should either programatically cut the image in half or do nothing to the image and rely on the CNN's inherent ability to give low weights to irrelevant details. If you do the latter, you may be able to get around the learning of incorrect details in the top half by augmenting your data - e.g. add some noise to images*, especially in the irrelevant top half. Perhaps horizontally flipping a few images (and reverse relevant targets for the control class) might be another useful augmentation. Some augmentations could also be useful if you just take the lower half of the image.</p>

<p>* Noise should be something close to variations that could be seen when in use. E.g. slurring pixels left or right might be reasonable. Inserting ""static"" probably is not.</p>
"
"['neuralnetwork', 'graphs', 'reference-request']",Application of ideas from graph theory in machine learning,"<p>Graphs are a very flexible form of data representation, and therefore have been applied to machine learning in many different ways in the past. You can take a look to the papers that are submitted to specialized conferences like S+SSPR (The joint IAPR International Workshops on Structural and Syntactic Pattern Recognition and Statistical Techniques in Pattern Recognition) and GBR (Workshop on Graph-based Representations in Pattern Recognition) to start getting a good idea of potential applications. Some examples:</p>

<ul>
<li>Within the Computer Vision field, graphs have been used to extract structure information that can later on be used on several applications, like for instance object recognition and detection, image segmentation and so on. </li>
<li>Spectral clustering is an example of clustering method based on graph theory. It makes use of the eigenvalues of the similarity matrix to combine clustering and dimensionality reduction. </li>
<li>Random walks may be used to predict and recommend links in social networks or to rank webpages by relevance. </li>
</ul>
"
"['machine-learning', 'search', 'ranking']",Ranking Bias in Learning to Rank,"<p>Have you seen this paper?
<a href=""http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf"" rel=""nofollow"">Optimizing Search Engines using Clickthrough Data</a></p>

<p>I stumbled upon this the other day, and I'm still reading through it, but the author attempts to deal with the problem you describe. You may also find <a href=""http://research.microsoft.com/en-us/um/people/sdumais/sigir2006-fp345-ranking-agichtein.pdf"" rel=""nofollow"">Improving Web Search Ranking by Incorporating User Behavior Information</a> useful.</p>
"
"['r', 'logistic-regression']",Best or recommended R package for logit and probit regression,"<p>Unless you have some very specific or exotic requirements, in order to perform <em>logistic</em> (<em>logit</em> and <em>probit</em>) regression analysis in <code>R</code>, you can use standard (built-in and loaded by default) <code>stats</code> package. In particular, you can use <code>glm()</code> function, as shown in the following nice <em>tutorials</em> from UCLA: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"">logit in R tutorial</a> and <a href=""http://www.ats.ucla.edu/stat/r/dae/probit.htm"">probit in R tutorial</a>.</p>

<p>If you are interested in <em>multinomial logistic regression</em>, <a href=""http://www.ats.ucla.edu/stat/r/dae/mlogit.htm"">this UCLA tutorial</a> might be helpful (you can use <code>glm()</code> or packages, such as <code>glmnet</code> or <code>mlogit</code>). For the above-mentioned very specific or exotic requirements, many other R packages are available, for example <code>logistf</code> (<a href=""http://cran.r-project.org/web/packages/logistf"">http://cran.r-project.org/web/packages/logistf</a>) or <code>elrm</code> (<a href=""http://cran.r-project.org/web/packages/elrm"">http://cran.r-project.org/web/packages/elrm</a>).</p>

<p>I also recommend another nice <a href=""http://data.princeton.edu/R/glms.html"">tutorial on GLMs</a> from Princeton University (by Germán Rodríguez), which discusses some modeling aspects, not addressed in the UCLA materials, in particular <em>updating models</em> and <em>model selection</em>.</p>
"
['algorithms'],Smith-Waterman-Gotoh Algorithm - how to determine an overall similarity percentage,"<p>I don't know how to write math algebra like on the <a href=""https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm"" rel=""nofollow"">Wikipedia page for Smith-Waterman</a>, so i'll use pseudo code. I found the logic at SimMetrics in the <a href=""https://github.com/Simmetrics/simmetrics/blob/master/simmetrics-core/src/main/java/org/simmetrics/metrics/SmithWatermanGotoh.java"" rel=""nofollow"">SmithWatermanGotoh java code</a>. </p>

<pre><code>str1 = PELICAN
str2 = COELACANTH
matchValue = 1 #in the comparisons below, when characters are equal, assign this value
mismatchValue = -2 #in the comparisons below, when characters are not equal, assign this value
gapValue = -0.5 #the gap penalty used in smith-waterman 

# get the maxDistance which is the smallest number of characters between str1 and str2 multiplied by 
# the largest of matchValue and gapValue 
maxDistance = min(length(str1), length(str2)) x max(matchValue, gapValue);

# function to compare character at index aIndex of string a with character at index bIndex of string b 
function compareCharacters(a, aIndex, b, bIndex, matchValue, mismatchValue) {
  if a[aIndex] === b[bIndex] 
    return matchValue 
  else 
    return mismatchValue 
}

v0 = an array 
v1 = an array 

lengthOfStr1 = number of characters in str1 
lengthOfStr2 = number of characters in str2 

# do the smith waterman similarity measure (currentMax)
currentMax = v0[0] = max(0, gapValue, compareCharacters(str1, 0, str2, 0, matchValue, mismatchValue))

for (j = 1; j &lt; lengthOfStr2; j++) {
  v0[j] = max(0, v0[j - 1] + gapValue,
            compareCharacters(str1, 0, str2, j, matchValue, mismatchValue))

  currentMax = max(currentMax, v0[j])
}

for (i = 1; i &lt; lengthOfStr1; i++) {
  v1[0] = max(0, v0[0] + gapValue, compareCharacters(str1, i, str2, 0, matchValue, mismatchValue))

  currentMax = max(currentMax, v1[0])

  for (j = 1; j &lt; lengthOfStr2; j++) {
    v1[j] = max(0, v0[j] + gapValue, v1[j - 1] + gapValue,
            v0[j - 1] + compareCharacters(str1, i, str2, j, matchValue, mismatchValue))

    currentMax = max(currentMax, v1[j])
  }

  for (j = 0; j &lt; lengthOfStr2; j++) {
    v0[j] = v1[j]
  }
}

# calculate the overallSimilarity between the strings 
overallSimilarity = currentMax / maxDistance #&lt;- 0.4767 for COELACANTH vs PELICAN 
</code></pre>
"
"['machine-learning', 'statistics', 'visualization', 'data-wrangling', 'methods']",How do you define the steps to explore the data?,"<p>As far as working with data depends on one's education, expertise, goal and favorite tools, I would answer it within my narrow scope - and trying to keep your track.</p>

<ul>
<li><p><strong>Framing the problem</strong> is an important starting point a lot of people neglect. Even-though it is only the beginning, this should result in first strategies to explore the data. </p>

<ol>
<li>Translate <em>""What I want to do""</em> to <em>""What are the implicit information I need to have to achieve it""</em></li>
<li>Given the information you need, find your way to get it (by decomposing it into tasks and sub-tasks) and the corresponding data to extract it (specific task implies specific signal(s) : structured data, pictures, movies, sounds, texts ...) </li>
<li>Along with 1. and 2., you should have a clearer idea of the data you'll deal with and thus the tools you might use (NLP, image processing, time-series, ...)</li>
</ol></li>
<li><p><strong>Collecting the data</strong> is now easier as it is a implied by the previous task. However, classify mentally your data in the following graph to know what to start with according to your personal trade-off: <a href=""http://i.stack.imgur.com/87vAS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/87vAS.png"" alt=""enter image description here""></a></p>

<ol>
<li>Direct data are those that can be obtained easily. Indirect are those that require some pre-process (scrapping websites, cropping images, counting number of clicks, ...)</li>
<li>The simplicity / complexity of use depends on the data : generally speaking, structured data within arrays are easier to deal with that images.</li>
<li>The size of the dot is the reward obtained if you achieve to work on these data, regarding your whole project</li>
</ol></li>
<li><p><strong>Exploring and Cleaning the data</strong> : there are levels of complexity here. I usually start with standard processes to clean the data (mean/median for missing values, normalization and centering when needed, ...). Meanwhile, I start looking deeper into the data by getting histograms of values, evolution of the mean for time series, word frequency for texts, ... This is task specific but exploration is here to give you hints about your data. Once inspecting them, you should mature your cleaning process.</p></li>
<li><p><strong>Working on the data</strong> : As you said, here comes the fun part. You can choose your favorite tools, or start improving your skills by looking for new concepts ( as a future good data scientist ), to process your data. One reason you don't know what to start with may be that you went too fast over the previous dots - implying what you have to do is still unclear. Get back to them, write the process on a paper until you clearly identify the inputs and outputs you need. Again, generally speaking, it involves the following :</p>

<ol>
<li>Dimensionality reduction (especially for images) and feature design (one-hot encoder, floats or ints, ordinal or cardinal category, ...)</li>
<li>Choose of your estimator / model by tuning the hyper-parameters</li>
<li>Training with validation methods (cross-validation, Leave One Out, ...)</li>
<li>Testing and improving your results</li>
</ol></li>
<li><p><strong>Report the results</strong>. Not as easy as it sounds, as mentioned <a href=""http://datascience.stackexchange.com/questions/12557/measuring-value-in-data-science"">here</a>. If is it for your own, having a whole project, started from scratch, is a good reward. Moreover, you may remember your scores when testing the model and how you improve it (which hyperparameters, which model, ...). If it is a well-discussed subject, you can compare to top teams in the world on well-known datasets. Finally, if it is for an employer, I would recommend to start this discussion before getting into the subject - would same time and trouble.</p></li>
</ul>
"
"['feature-selection', 'correlation', 'feature-engineering']",Detecting redundancy with Pearson correlation in continuous features,"<p>If the correlation between two features $x_1$ and $x_2$ is 1 that means that you can write $x_1 = c\cdot x_2 + a$. The only knew knowledge there is are those two constants, the individual values can be retrieved knowing this. I highly doubt there is anything a machine learning algorithm can learn from this and it is a fact that for some having this kind of correlation between features can hurt your performance quite a bit, so I would test it a bit but I would say it's very likely you can remove one of the two, and which one is not going to matter.</p>
"
"['machine-learning', 'data-mining', 'bigdata', 'statistics', 'hadoop']","Is our data ""Big Data"" (Startup)","<p>Yes, this is a how-long-is-a-piece-of-string question. I think it's good to beware of over-engineering, while also making sure you engineer for where you think you'll be in a year.</p>

<p>First I'd suggest you distinguish between processing and storage. Storm is a (stream) processing framework; NoSQL databases are a storage paradigm. These are not alternatives. The Hadoop ecosystem has HBase for NoSQL; I suspect Azure has some kind of stream processing story.</p>

<p>The bigger difference in your two alternatives is consuming a cloud provider's ecosystem vs Hadoop. The upside to Azure, or AWS, or GCE, is that these services optimize for integrating with each other, with billing, machine management, etc. The downside is being locked in to the cloud provider; you can't run Azure stuff anywhere but Azure. Hadoop takes more work to integrate since it's really a confederation of sometimes loosely-related projects. You're investing in both a distribution, and a place to run that distribution. But, you get a lot less lock-in, and probably more easy access to talent, and a broader choice of tools.</p>

<p>The Azure road is also a ""big data"" solution in that it has a lot of the scalability properties you want for big data, and the complexity as well. It does not strike me as an easier route. Do you need to invest in distributed/cloud anything at this scale? given your IoT-themed use case, I believe you will need to soon, if not now, so yes. You're not talking about gigabytes, but many terabytes in just the first year.</p>

<p>I'd give a fresh team 6-12 months to fully productionize something based on either of these platforms. That can certainly be staged as a POC, followed by more elaborate engineering.</p>
"
"['algorithms', 'graphs', 'apache-spark', 'parallel']",What makes a graph algorithm a good candidate for concurrency?,"<p>Two words: associative and commutative</p>

<p>In other words, the operations that the algorithm does need to be independent of how you order or group your data...this minimizes the need for cross-talk in the algorithm and leads to more efficiency.</p>
"
['neuralnetwork'],Pricing decisions using neural network,"<p><strong>Question 1) Is this a good idea in general?</strong></p>

<p>Solving this problem as a supervised learning regression problem is a fantastic idea and is the type of solution that will greatly benefit your company since it will translate to other similar and dissimilar problems much more easily than deterministic methods. </p>

<p>However using a neural network to solve this supervised learning regression problem is probably a very bad idea. Neural networks can add great value to certain problems, are among the very best algorithms for complex problems where computing power is not an issue, and have fascinating implications for the future of machine learning and artificial intelligence. But... they can be very tricky to train, require a lot of computing power, require a lot of data, and perform poorly in many cases.</p>

<p>I suggest you scope the problem using linear regression and then try a support vector regressor (SVM, SVR) or naive Bayes regressor. The analogue methods in SVR work very well with limited data and provide surprisingly accurate results.</p>

<p><strong>Question 2) If yes, what type of NN would be most suited for such a problem?</strong></p>

<p>If you must use a neural network then try playing with the problem.  Start with a feed forward neural network. Note that it will likely underperform an SVR.  Then think about moving toward a convolution neural network. Again, this is probably a very bad way to go. Try linear regression followed by other methods first.</p>

<p><strong>Moving forward</strong></p>

<p>The documentation for either <a href=""http://scikit-learn.org/stable/index.html"" rel=""nofollow"">Scikit-Learn</a> in python, <a href=""http://www.h2o.ai/product/"" rel=""nofollow"">H20</a> in Java, or <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow"">Weka</a> provide very shallow learning curves to jump on the merry-go-round and take a spin or two. Please make sure that you thoroughly understand cross validation and scoring metrics as this is essential to continued, adequate progress.</p>

<p>Hope this helps!</p>
"
"['python', 'pandas', 'correlation']",How/What Correlation for Position Ranking?,"<p>Person correlation assumes data is coming from a Normal distribution and there is a linear relationship. An alternative is the Spearman correlation or Kendall's tau for ranked data. </p>

<p>As an edit, here are the links to how you would calculate the Spearman Correlation coefficient and Kendall's Tau, respectively.</p>

<p><a href=""http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.spearmanr.html"" rel=""nofollow"">http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.spearmanr.html</a></p>

<p><a href=""http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.kendalltau.html"" rel=""nofollow"">http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.kendalltau.html</a></p>

<p>Best of luck.</p>
"
"['dataset', 'recommender-system']",Benchmark datasets for collaborative filtering,"<p>The obvious answer would be the Netflix prize dataset, there is a lot of research into it and most CF algorithms have known scores in it. </p>

<p>There are other available datasets that are usually used as benchmarks:</p>

<ul>
<li><p><a href=""http://grouplens.org/datasets/movielens/"" rel=""nofollow"">Movie lens Dataset</a>: a 20 million ratings dataset used for benchmarking CF algorithms;</p></li>
<li><p><a href=""http://eigentaste.berkeley.edu/dataset/"" rel=""nofollow"">Jester Dataset</a>: a joke recommendation dataset with more than 6 million ratings;</p></li>
<li><p>You can find many more datasets in this <a href=""https://gist.github.com/entaroadun/1653794"" rel=""nofollow"">link</a> </p></li>
</ul>
"
"['machine-learning', 'predictive-modeling', 'statistics']","What are the relationships/differences between Bias, Variance and Residuals","<p>I think your confusion arises from mixing two different kinds of terms together here. Bias and variance are general concepts which can be measured and quantified in a number of different ways. A residual is a specific measurement of the differences between a predicted value and a true value.</p>

<p><strong>Bias</strong>, loosely speaking, is how far away the average prediction is from the actual average. One way to measure it is in the difference of the means. You could also use difference of medians, difference in range, or several other calculations. To a get a complete picture of the bias of a model, you will want to look at several different measures.</p>

<p><strong>Variance</strong>, when used informally in data science, is a property of single sets (whether predictions or true values). The variance of a model, loosely speaking, is how far from the average prediction a randomly selected prediction will be. It's very often assessed using cross-validation. You construct multiple models using slightly different training sets but the same algorithm and tuning parameters. You then calculate an evaluation metric for each model, and calculate the standard deviation of this evaluation over all your models. This gives you a sense of the ""stability"" of a given algorithm/parameter set when exposed to different training and testing sets.</p>

<p>(N.B. This can be confusing because there is a specific definition of ""variance"" used in statistics, $v = \sigma^2$. In data science, it's usually used more informally.)</p>

<p><strong>Residuals</strong> are a specific quantity associated with a single prediction/true value set pair. You've got the right definition there. This makes it not a general concept, but instead a measurement that you can use to assess either bias or variance. They're also frequently used in fitting regression models and otherwise performing gradient descent-style optimization. The mean or median of a residual set can be a way to assess bias, while the standard deviation of a residual set can be used to assess a variance.</p>
"
"['feature-selection', 'correlation', 'feature-engineering']",Dissmissing features based on correlation with target variable,"<p>You've really got a classification problem on your hands, not a regression problem. Your target is not continuous, and Pearson correlation measures a relationship between continuous variables really. That's problematic enough to start.</p>

<p>Low correlation means there's no <em>linear</em> relationship; it doesn't mean there's no information in the feature that predicts the target.</p>

<p>I think you're really looking for <em>mutual information</em>, in this case between continuous and categorical variables. (I assume your other inputs are continuous?) This is a little involved; see <a href=""http://stats.stackexchange.com/questions/29489/how-do-i-study-the-correlation-between-a-continuous-variable-and-a-categorical"">http://stats.stackexchange.com/questions/29489/how-do-i-study-the-correlation-between-a-continuous-variable-and-a-categorical</a></p>

<p>If you're attempting to do feature selection then you could perform a logistic regression with L1 regularization and select features based on the absolute value of their coefficients.</p>
"
"['predictive-modeling', 'time-series', 'deep-learning']",Deep Learning for Time series,"<p>Recurrent neural networks (RNNs) can work with series as input or output or both.</p>

<p>Even a simple one-layer RNN is effectively ""deep"" because it has to solve similar problems as multi-layer non-recursive networks. That is because backpropagation logic in a RNN has to account for delay between input and target, which is solved by <a href=""https://en.wikipedia.org/wiki/Backpropagation_through_time"" rel=""nofollow"">backpropagation through time</a> - essentially adding a layer to the network for every time step of delay between first input and last output.</p>

<p>RNN architecture has become more sophisticated in recent years by using ""gating"" techniques such as <a href=""https://en.wikipedia.org/wiki/Gated_recurrent_unit"" rel=""nofollow"">Gated Recurrent Units</a> (GRU) or <a href=""https://en.wikipedia.org/wiki/Long_short-term_memory"" rel=""nofollow"">Long Short Term Memory</a> (LSTM). These have multiple trainable params - 3 or 4 - per neuron, and the schematics are more complicated than feed-forward networks. They have been demonstrated as <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">very effective in practice</a>, so this extra complexity does seem to pay off.</p>

<p>Although you can research and implement RNNs yourself in a library like Theano or Tensor Flow, several neural network libraries already implement RNN architectures (e.g. <a href=""http://keras.io/"" rel=""nofollow"">Keras</a>, <a href=""https://github.com/jcjohnson/torch-rnn"" rel=""nofollow"">torch-rnn</a>)</p>
"
"['machine-learning', 'data-mining', 'classification']",Rough vs Fuzzy vs Granular Computing,"<p>""<strong>Granularity</strong>"" refers to the <strong><em>resolution</em></strong> of the variables under analysis.  If you are analyzing <em>height</em> of people, you could use <em>course-grained variables</em> that have only a few possible values -- e.g. ""above-average, average, below-average"" -- or a <em>fine-grained variable</em>, with many or an infinite number of values -- e.g. integer values or real number values.</p>

<p>A measure is ""<strong>fuzzy</strong>"" if the distinction between alternative values is not crisp.  In the course-grained variable for <em>height</em>, a ""crisp"" measure would mean that any given individual could <em>only</em> be assigned one value -- e.g. a tall-ish person is either ""above-average"", or ""average"".  In contrast, a ""fuzzy"" measure allows for <em>degrees of membership</em> for each value, with ""membership"" taking values from 0 to 1.0.  Thus, a tall-ish person could be a value of ""0.5 above-average"", ""0.5 average"", ""0.0 below-average"".</p>

<p>Finally, a measure is ""<strong>rough</strong>"" when two values are given: upper and lower bounds as an estimate of the ""crisp"" measure.  In our example of a tall-ish person, the rough measure would be {UPPER = above-average, LOWER = average}.</p>

<p>Why use granular, fuzzy, or rough measures at all, you might ask?  Why not measure everything in nice, precise real numbers?  Because many real-world phenomena don't have a good, reliable intrinsic measure and measurement procedure that results in a real number.  If you ask married couples to rate the quality of their marriage on a scale from 1 to 10, or 1.00 to 10.00, they might give you a number (or range of numbers), but how reliable are those reports? Using a course-grained measure (e.g. ""happy"", ""neutral/mixed"", ""unhappy""), or fuzzy measure, or rough measure can be more reliable and more credible in your analysis.  Generally, it's much better to use rough/crude measures well than to use precise/fine-grained measures poorly.</p>
"
['classification'],K nearest neighbour,"<p>See a similar answer <a href=""http://stats.stackexchange.com/questions/105979/is-knn-a-discriminative-learning-algorithm"">here</a>. To clarify, k nearest neighbor is a discriminative classifier. </p>

<p>The difference between a generative and a discriminative classifier is that the former models the joint probability where as the latter models the conditional probability (the posterior) starting from the prior. </p>

<p>In the case of nearest neighbors, the conditional probability of a class given a data point is modeled. To do this, one starts with the prior probability on the classes.  </p>
"
['machine-learning'],Choice of weights for the Laplacian Eigenmaps algorithm,"<p>Recall the definition he makes for the graph Laplacian earlier, $L = D -W $. Now consider the map in the RHS parentheses which I'll call $L^*$,
$$ L^*f(x_i) := f(x_i) - \alpha \sum_{x_j, ||x_i-x_j||&lt;\epsilon}e^{-\frac{||x_i-x_j||^2}{4t}}f(x_j).$$
The suggested weight matrix definition is natural because it lets us write
$$ L^* := I - D^{-1}W. $$ Here's <a href=""http://www.cs.yale.edu/homes/singer/publications/laplacian_ACHA.pdf"" rel=""nofollow"">a reference</a> to a related paper with some easy to read exposition. Hope this helps!</p>
"
['clustering'],Determining correlated product categories using store purchase history,"<p>This is classic <strong>market basket analysis</strong>.</p>

<p>Clustering is the weong tool, you want frequent item set mining and association rules instead.</p>

<p><a href=""https://en.wikipedia.org/wiki/Association_rule_learning"" rel=""nofollow"">https://en.wikipedia.org/wiki/Association_rule_learning</a></p>
"
"['visualization', 'tools', 'tableau']",List of used aliases in Tableau,"<p>Finally solved the issue.
You should go to:</p>

<pre><code>Data –&gt; your_data_source –&gt; Edit Aliases –&gt; Measure Names
</code></pre>

<p>And see full mapping between variables/aggregations and aliases.</p>

<p><img src=""http://i.stack.imgur.com/uSoHC.png"" alt=""enter image description here""></p>
"
"['bigdata', 'databases', 'binary', 'version-control']",How to deal with version control of large amounts of (binary) data,"<p>What I am ending up using is a sort of hybrid solution:</p>

<ul>
<li>backup of the raw data</li>
<li>git of the workflow</li>
<li>manual snapshots of workflow + processed data, that are of relevance, e.g.:

<ul>
<li>standard preprocessing</li>
<li>really time-consuming</li>
<li>for publication</li>
</ul></li>
</ul>

<p>I believe it is seldom sensible to have a full revision history of large amount of binary data, because the time required to review the changes will eventually be so overwhelming that it will not pay off in the long run.
Maybe a semi-automatic snapshot procedure (eventually to save some disk-space, by not replicating the unchanged data across different snapshots) would be of help.</p>
"
"['feature-selection', 'feature-extraction', 'dimensionality-reduction']",What is dimensionality reduction? What is the difference between feature selection and extraction?,"<p>Simply put: </p>

<ul>
<li>feature selection: you select a subset of the original feature set; while</li>
<li>feature extraction: you build a new set of features from the original feature set. </li>
</ul>

<p>Examples of feature extraction: extraction of contours in images, extraction of digrams from a text, extraction of phonemes from recording of spoken text, etc.</p>

<p>Feature extraction involves a transformation of the features, which often is not reversible because some information is lost in the process of dimensionality reduction.</p>
"
"['scalability', 'hadoop', 'map-reduce', 'mongodb']",Can map-reduce algorithms written for MongoDB be ported to Hadoop later?,"<p>There will definitely be a translation task at the end if you prototype using just mongo.</p>

<p>When you run a MapReduce task on mongodb, it has the data source and structure built in.  When you eventually convert to hadoop, your data structures might not look the same.  You could leverage the mongodb-hadoop connector to access mongo data directly from within hadoop, but that won't be quite as straightforward as you might think.  The time to figure out how exactly to do the conversion most optimally will be easier to justify once you have a prototype in place, IMO.</p>

<p>While you will need to translate mapreduce functions, the basic pseudocode should apply well to both systems.  You won't find anything that can be done in MongoDB that can't be done using Java or that is significantly more complex to do with Java.</p>
"
"['bigdata', 'efficiency', 'performance']",Why is it hard to grant efficiency while using libraries?,"<p>Having done the rewriting game over and over myself (and still doing it), my immediate reaction was <em>adaptability</em>.</p>

<p>While frameworks and libraries have a huge arsenal of (possibly intertwinable) routines for standard tasks, their framework property often (always?) disallows shortcuts.  In fact, most frameworks have some sort of core infrastructure around which a core layer of basic functionality is implemented.  More specific functionality makes use of the basic layer and is placed in a second layer around the core.</p>

<p>Now by shortcuts I mean going straight from a second layer routine to another second layer routine without using the core.  Typical example (from my domain) would be timestamps: You have a timestamped data source of some kind.  Thus far the job is simply to read the data off the wire and pass it to the core so your other code can feast on it.</p>

<p>Now your industry changes the default timestamp format for a very good reason (in my case they went from unix time to GPS time).  Unless your framework is industry-specific it is very unlikely that they're willing to change the core representation of time, so you end up using a framework that <em>almost</em> does what you want.  Every time you access your data you have to convert it to industry-time-format first, and every time you want it modified you have to convert it back to whatever the core deems appropriate.  There is no way that you can hand over data straight from the source to a sink without double conversion.</p>

<p>This is where your hand-crafted frameworks will shine, it's just a minor change and you're back modelling the real world whereas all other (non-industry-specific) frameworks will now have a performance disadvantage.</p>

<p>Over time, the discrepancy between the real world and the model will add up.  With an off-the-shelf framework you'd soon be facing questions like: How can I represent <code>this</code> in <code>that</code> or how do make routine <code>X</code> accept/produce <code>Y</code>.</p>

<p>So far this wasn't about C/C++.  But if, for some reason, you can't change the framework, i.e. you do have to put up with double conversion of data to go from one end to another, then you'd typically employ something that minimises the additional overhead.  In my case, a TAI->UTC or UTC->TAI converter is best left to raw C (or an FPGA).  There is no elegance possible, no profound smart data structure that makes the problem trivial.  It's just a boring switch statement, and why not use a language whose compilers are good at optimising exactly that?</p>
"
"['r', 'rstudio']",Rules by which RStudio sets Headings,"<p>Check out <a href=""https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections"">Code Folding and Sections</a>:</p>

<blockquote>
  <p><strong>Code sections allow you to break a larger source file into a set of
  discrete regions for easy navigation between them</strong>. Code sections are
  automatically foldable—for example, the following source file has
  three sections (one expanded and the other two folded):</p>
  
  <p>To insert a new code section you can use the Code -> Insert Section
  command. Alternatively, any comment line which includes at least four
  trailing dashes (-), equal signs (=), or pound signs (#) automatically
  creates a code section. For example, all of the following lines create
  code sections:</p>
  
  <p># Section One ---------------------------------<br>
   # Section Two =================================<br>
   ### Section Three ############################# </p>
  
  <p>Note that as illustrated above the line can start with any number of
  pound signs (#) so long as <strong>it ends with four or more -, =, or #
  characters</strong>.</p>
</blockquote>

<p>(highlights by myself)</p>
"
['machine-learning'],Approximating density of test set in train,"<p>The approach that comes to mind, is to calculate the kullback-leibler divergence between the kernel density estimations of your train dataset and of your test dataset. </p>

<p>The kernel density estimation of each of your datasets will give you an approximation to the pdf's of your datasets. The kullback-leibler divergence will give you a number that will represent the divergence in bits from one distribution to another (if you use base 2 for your logarithm). Below are some references I think you would fine useful. </p>

<p><a href=""https://en.wikipedia.org/wiki/Kernel_density_estimation"" rel=""nofollow"">https://en.wikipedia.org/wiki/Kernel_density_estimation</a>
<a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow"">https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</a>
<a href=""https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/"" rel=""nofollow"">https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/</a></p>

<p>If you would like me to show the math behind this method. Feel free to ask.</p>

<p><strong>EDIT: Added math as asked by author of question</strong>    </p>

<p>Let $\hat x_1, \hat x_2,\hat x_3...,\hat x_n$ be your training dataset while $x_1,x_2,x_3...,x_n$ is your testing dataset, where both $\hat x_i$ and $x_i$ belong to $\mathbb{R}^d$.
$$\hat f(x;H)=\frac{1}{n} \sum_{i=1}^{n}K(x-\hat x_i;H)$$
$$f(x;H)=\frac{1}{n} \sum_{i=1}^{n}K(x-x_i;H)$$
$\hat f$and $f$ represent the kernel density estimation for training set and testing set respectively. The parameter $H$ represents the bandwidth parameter and is a symmetric positive definite $d \times d$ matrix. $K(u;H)$ can be rewritten as $$|H|^{-\frac{1}{2}} K(H^{\frac{1}{2}} u)$$ where K can be any kernel function. I would recommend for simplicity purposes the standard multivariate normal kernel. Okay so now that we have the kernel density estimations of both our training and our testing dataset, we can use the kullback-leibler divergence in order to estimate the difference between the two.<br>
Optimally we would like to calculate the kullback-leibler divergence with respect to every point in our space. Mathematically speaking.  </p>

<p>$$\int_X f(x;H) \ log_2(\frac{f(x;H)}{\hat f(x;H)})dx$$</p>

<p>But this is computationally unpractical to compute. We can approximate this integral by sampling a set of points from the $f(x;H)$ and then computing the discrete sum.
$$\sum_{x \in X} f(x;H) \ log_2(\frac{f(x;H)}{\hat f(x;H)})$$</p>

<p><strong>Quick Note:</strong> To sample from a kernel density estimator, uniformly randomly select a point from the respective dataset. Then sample from the kernel of choice centered around the point chosen. </p>
"
['neuralnetwork'],How data representation affects neural networks?,"<p><strong>It will have very little effect</strong></p>

<p>The answer most will give is that it will have no effect, but adding one more feature will decrease the ratio of records to features so will slightly increase the bias and will hence make your model slightly less accurate.  Unless, of course, you have overfit your model , in which case it will make your model slightly more accurate (a good data scientist would never do this because they understand the importance of cross-validation :-).</p>

<p>If you normalize your data and then attempt some sort of dimensionality reduction, your algorithm will immediately eliminate the feature that you added since it is perfectly negatively (linearly) correlated with the first feature.  <strong>In this case it will have no effect</strong>.</p>

<p><strong>Please also consider the following:</strong></p>

<p>I always see big red flags when someone asks a very fundamental data science question with the words <code>neural network</code> included.  Neural networks are very powerful and receive a great deal of attention in the media and on Kaggle, but they take more data to train, are difficult to configure, and require much more computing power.  If you are just starting out, I suggest getting a foundation in linear regression, logistic regression, clustering, SVMs, decision trees, random forests, and naive Bayes before delving into artificial neural networks.  Just some food for thought.</p>

<p>Hope this helps! </p>
"
"['time-series', 'data', 'experiments', 'csv', 'metadata']",Recommendations for storing time series data,"<p>There are two solutions that are worth looking at:</p>

<p><strong><a href=""http://www.influxdb.com"">InfluxDB</a> is an open source database platform specifically designed for time series data.</strong> The platform includes many optimized functions related to time and you can collect data on any interval and compute rollups/aggregations when reporting. The company recently launched a query app called <a href=""https://influxdb.com/chronograf/index.html"">Chronograf</a>. I have not used this - but if its no good, you can also check out Grafana (which is very widely used and stable).</p>

<p><strong>The alternative strategy you may want to pursue is an elasticsearch index.</strong> Elasticsearch is great at running aggregations and other mathematical functions on data. Many use it to store server log data and then query said data using Kibana.</p>
"
['bigdata'],What's an efficient way to compare and group millions of store names?,"<p>This is an <a href=""http://www.umiacs.umd.edu/~getoor/Tutorials/ER_VLDB2012.pdf"">entity resolution</a> aka <a href=""https://en.wikipedia.org/wiki/Record_linkage"">record linkage</a> aka <a href=""http://www.springer.com/us/book/9783642311635"">data matching</a> problem.</p>

<p>I would solve this by removing all of the non-alphabetical characters including numbers, casting into all uppercase and then employing a hierarchical match.  First match up the exact cases and then move to a Levenshtein scoring between the fields.  Make some sort of a decision about how large you will allow the Levenshtein or normalized Levenshtein score to get before you declare something a non-match.</p>

<p>Assign every row an id and when you have a match, reassign the lower of the IDs to both members of the match.</p>

<p>The <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"">Levenshtein distance</a> algorithm is simple but brilliant (<a href=""http://hetland.org/coding/python/levenshtein.py"">taken from here</a>):</p>

<pre><code>def levenshtein(a,b):
    ""Calculates the Levenshtein distance between a and b.""
    n, m = len(a), len(b)
    if n &gt; m:
        # Make sure n &lt;= m, to use O(min(n,m)) space
        a,b = b,a
        n,m = m,n

    current = range(n+1)
    for i in range(1,m+1):
        previous, current = current, [i]+[0]*n
        for j in range(1,n+1):
            add, delete = previous[j]+1, current[j-1]+1
            change = previous[j-1]
            if a[j-1] != b[i-1]:
                change = change + 1
            current[j] = min(add, delete, change)

    return current[n]
</code></pre>

<p>This <a href=""http://rads.stackoverflow.com/amzn/click/3642311636"">Data Matching</a> book is a good resource and is free for seven days on Amazon.</p>

<p>Nominally, this is an $n^2$ algorithm without exploiting some sorting efficiencies, so I would expect to have to use multiple cores on $2\times10^7$ rows.  But this should run just fine on an 8 core <a href=""https://aws.amazon.com/"">AWS instance</a>.  It will eventually finish on a single core, but might take several hours.</p>

<p>Hope this helps!</p>
"
"['bigdata', 'google', 'search']",How does a query into a huge database return with negligible latency?,"<p>Well, I'm not sure if it is MapReduce that solves the problem, but it surely wouldn't be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it <em>feasible</em> to have such low latency on queries from all these TBs of data in different machines:</p>

<ol>
<li>distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);</li>
<li>caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;</li>
<li>lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.</li>
</ol>

<p>Considering that, lets try to address your questions:</p>

<blockquote>
  <p>but I imagine it infeasible for the results of every single possible query to be indexed</p>
</blockquote>

<p>Yes, it would be, and actually is infeasible to have results for <em>every single possible query</em>. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these <code>n -&gt; inf</code> terms (<code>2^n</code>). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.</p>

<blockquote>
  <p>wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs</p>
</blockquote>

<p>Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing <em>ruling</em> market is money, and the investors are not interested in wasting it. So what is done?</p>

<p>The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of <a href=""http://i.stack.imgur.com/Uf6al.gif"">performance</a>. But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.</p>

<p>Memory cards are <em>expensive</em> for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it's not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending <em>specific geographical regions</em>, which allows for more <em>specialized</em> data caching, and even better response times.</p>

<blockquote>
  <p>Does MapReduce help solve this problem?</p>
</blockquote>

<p>Although I don't think that using or not MapReduce is restricted information inside Google, I'm not conversant about this point. However, Google's implementation of MapReduce (which is surely <em>not</em> Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time.</p>

<blockquote>
  <p>Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches?</p>
</blockquote>

<p>The graph below presents a curve of how the <em>kinds</em> of queries occur. You can see that there are three main kinds of searches, each of them holding approximately 1/3 of the volume of queries (area below curve). The plot shows power law, and reinforces the fact that smaller queries are the most popular. The second third of queries are still feasible to process, since they hold few words. But the set of so-called <em>obscure queries</em>, which usually consist of non-experienced users' queries, are not a negligible part of the queries.</p>

<p><img src=""http://i.stack.imgur.com/CpcNf.jpg"" alt=""Heavy-tailed distribution""></p>

<p>And there lies space for novel solutions. Since it's not just one or two queries (but one third of them), they must have <em>relevant</em> results. If you type in something <em>much too obscure</em> in a Google search, it shan't take longer to return a list of results, but will most probably show you something it <em>inferred</em> you'd like to say. Or it may simply state that there was no document with such terms -- or even cut down your search to 32 words (which just happened to me in a random test here).</p>

<p>There are dozens of appliable heuristics, which may be either to ignore some words, or to try to break the query into smaller ones, and gather the most <em>popular</em> results. And all these solutions can be tailored and tweaked to respect <em>feasible waiting times</em> of, say, less then a second? :D</p>
"
"['definitions', 'parallel', 'distributed']",Parallel and distributed computing,"<p>Simply set, 'parallel' means running concurrently on distinct resources (CPUs), while 'distributed' means running across distinct computers, involving issues related to networks.</p>

<p>Parallel computing using for instance <a href=""http://en.wikipedia.org/wiki/OpenMP"">OpenMP</a> is not distributed, while parallel computing with <a href=""http://en.wikipedia.org/wiki/Message_Passing_Interface"">Message Passing</a> is often distributed.</p>

<p>Being in a 'distributed but not parallel' setting would mean under-using resources so it is seldom encountered but it is conceptually possible.</p>
"
['clustering'],"Clustering unique visitors by useragent, ip, session_id","<p>One possibility here (and this is really an extension of what Sean Owen posted) is to define a ""stable user.""</p>

<p>For the given info you have you can imagine making a user_id that is a hash of ip and some user agent info (pseudo code):</p>

<pre><code>uid = MD5Hash(ip + UA.device + UA.model)
</code></pre>

<p>Then you flag these ids with ""stable"" or ""unstable"" based on usage heuristics you observe for your users.  This can be a threshold of # of visits in a given time window, length of time their cookies persist, some end action on your site (I realize this wasn't stated in your original log), etc...</p>

<p>The idea here is to separate the users that don't drop cookies from those that do.</p>

<p>From here you can attribute session_ids to stable uids from your logs.  You will then have ""left over"" session_ids for unstable users that you are relatively unsure about.  You may be over or under counting sessions, attributing behavior to multiple people when there is only one, etc...  But this is at least limited to the users you are now ""less certain"" about.</p>

<p>You then perform analytics on your stable group and project that to the unstable group.  Take a user count for example, you know the total # of sessions, but you are unsure of how many users generated those sessions.  You can find the # sessions / unique stable user and use this to project the ""estimated"" number of unique users in the unstable group since you know the number of sessions attributed to that group.</p>

<pre><code>projected_num_unstable_users = num_sess_unstable / num_sess_per_stable_uid
</code></pre>

<p>This doesn't help with per user level investigation on unstable users but you can at least get some mileage out of a cohort of stable users that persist for some time.  You can, by various methods, project behavior and counts into the unstable group.  The above is a simple example of something you might want to know.  The general idea is again to define a set of users you are confident persist, measure what you want to measure, and use certain ground truths (num searches, visits, clicks, etc...) to project into the unknown user space and estimate counts for them.</p>

<p>This is a longstanding problem in unique user counting, logging, etc... for services that don't require log in.</p>
"
"['statistics', 'correlation']",On interpreting the statistical significance of R squared,"<p>The p-value is the strength of evidence against the null hypotheses. In this case the null is that the coefficient is equal to zero. So your p-value says that this is very weak evidence against the null so you model is likely to be describing the underlying system of the data.</p>

<p>R-squared describes the percent of variation that is explained by the model. Your value is very low; 14.5%. Of all the ""activity"" in the data your model is only explaining 14.5% of it.</p>

<p>So you have a situation were model is most likely explaining variation in data but not explaining very much of it.  I would suggest altering the model and refitting. </p>
"
"['machine-learning', 'classification', 'naive-bayes-classifier']",Predicting New Data with Naive Bayes,"<p>Among Naive Bayes assumptions the main one is that features are conditionally independent. For our problem we would have:</p>

<p>$$P(Play|Outlook,Person) \propto P(Play)P(Outlook|Play)P(Name|Play)$$</p>

<p>To address question <em>is Harry going to play on a sunny day?</em>, you have to compute the following:</p>

<p>$$P(Yes|Sunny,Harry) = P(Yes)P(Sunny|Yes)P(Harry|Yes)$$
$$P(No|Sunny,Harry) = P(No)P(Sunny|No)P(Harry|No)$$</p>

<p>and choose the probability with bigger value. </p>

<p>That is what theory says. To address your question I will rephrase the main assumption of Naive Bayes. The assumptions that features are independent given the output means basically that the information given by joint distribution can be obtained by product of marginals. In plain English: assume you can find if Harry plays on sunny days if you only know how much Harry plays in general and how much anybody plays on sunny days. As you can see, <em>you simply you would not use the fact that Harry plays on sunny days</em> <strong>even if you would have had that record in your data</strong>. Simply because Naive Bayes assumes there is no useful information in the interaction between the features, and this is the precise meaning of conditional independence, which Naive Bayes relies upon. </p>

<p>That said if you would want to use the interaction of features than you would have either to use a different model, or simply add a new <em>combined</em> feature like a concatenation of factors of names and outlook. </p>

<p>As a conclusion when you do not include names in your input features you will have a <em>general wisdom</em> classifier like <em>everybody plays no matter outlook</em>, since most of the instances have play=yes. If you include the name in your input variables you allow to alter that <em>general wisdom</em> with something specific to player. So your classifier <em>wisdom</em> would look like <em>players prefer in general to play, no matter outlook, but Marry like less to play less on Rainy</em>. </p>

<p>There is however a potential problem with Naive Bayes on your data set. This problem is related with the potential big number of levels for variable Name. In order to approximate the probability there is a general thing that happens: more data, better estimates. This probably would happen with variable Outlook, since there are two levels and adding more data would probably not increase number of levels. So the estimates for Outlook would be probably better with more data. However for name you will not have the same situation. Adding more instances would be possible perhaps only by adding more names. Which means that on average the number of instances for each name would be relatively stable. And if you would have a single instance, like it is the case for Harry, you do not have enough data to estimate $P(Harry|No)$. </p>

<p>As it happens this problem can be alleviated using smoothing. Perhaps Laplace smoothing (or a more general for like Lindstone) is very helpful. The reason is that estimates based on maximum likelihood have big problems with cases like that.</p>

<p>I hope it answers at least partially your question.</p>
"
"['machine-learning', 'beginner']",Beggining in machine learning,"<p>Coursera is currently offering a course on Machine learning with collaboration from MIT. Many says its strongly recommended.</p>

<p><a href=""https://www.coursera.org/learn/machine-learning"" rel=""nofollow"">https://www.coursera.org/learn/machine-learning</a></p>

<p>But I found the below course from Edx more interesting.</p>

<p><a href=""https://www.coursera.org/learn/machine-learning"" rel=""nofollow"">https://www.coursera.org/learn/machine-learning</a></p>

<p>It also provides hands on the Microsofts New exclusive Machine learning platform On Azure. </p>
"
"['nlp', 'word-embeddings']",What is a better input for Word2Vec?,"<p>The answer to this question is that <em>it depends</em>. The primary approach is to pass in the tokenized sentences (so <code>SentenceCorpus</code> in your example), but depending on what your goal is and what the corpus is you're looking at, you might want to instead use the entire article to learn the embeddings. This is something you might not know ahead of time -- so you'll have to think about how you want to evaluate the quality of the embeddings, and do some experiments to see which 'kind' of embeddings are more useful for your task(s).</p>
"
['neuralnetwork'],Lack of activation function in output layer at regression?,"<p>Activation ""linear"" is identical to ""no activation function"". The term ""linear output layer"" also means precisely ""the last layer has no activation function"". Whether you use one or the other term might be down to how your NN library implements it. You may also see it described either way around in documents, but it is exactly the same thing mathematically:</p>

<p>$$a^{out}_j = b^{out}_j + \sum_{i=1}^{N^{hidden}} W_{ij}a^{hidden}_i$$</p>

<p>Where $a$ values are activation, $b$ are biases, $W$ is weight matrix.  </p>

<p>For a regression problem with a mean squared error objective, this is the most common approach.</p>

<p>There is nothing stopping you using other activation functions. They might help you if they match the target variable distribution. About the only rule is that your network output should be able to cover possible values of the target variable. So if the target variable is always between -1.0 and 1.0, with higher density around 0.0, perhaps tanh could also work for you.</p>
"
['data-formats'],What is the best file format to store an uncompressed 2D matrix?,"<p>The most compatible format is surely CSV/TSV. It's text and you can usually Gzip it on the fly with the software package you are using. There is no widely standardized format for storing matrix array data. Matlab has its *.mat files, NumPy has *.npz, Stata and SAS have their own, ... Best just use a clear-text file.</p>

<p>If the matrix is symmetric, if it is very large or if there will be a lot of them, you could spare 50% in space requirement by storing only the lower (or upper) triangular part of it. If you chose to do so, there is, again, no widely accepted format. Just store the shape first and then the flattened, 1D data.</p>
"
"['machine-learning', 'confusion-matrix', 'google-prediction-api']",Understanding ConfusionMatrix for Google Prediction API,"<p><a href=""https://cloud.google.com/prediction/docs/reference/v1.6/trainedmodels/analyze"" rel=""nofollow"">As explained in the documentation</a>:</p>

<blockquote>
  <p>This shows an estimate for how this model will do in predictions. This is first indexed by the true class label. For each true class label, this provides a pair {predicted_label, count}, where count is the estimated number of times the model will predict the predicted label given the true label.</p>
</blockquote>

<p>If you are not sure what a confusion matrix, <a href=""https://en.wikipedia.org/wiki/Confusion_matrix"" rel=""nofollow"">see Wikipedia</a>, where the ""actual class"" refers to the same thing as the ""true class"" in the Google documentation.</p>
"
['classification'],How are selected the features for a decision tree in CART?,"<p>At each split point, CART will choose the feature which ""best"" splits the observations. What qualifies as best varies, but generally the split is done so that the subsequent nodes are more homogenous/pure with respect to the target. There are different ways of measuring homogeneity, for example Gini, Entropy, Chi-square. If you are using software, it may allow you to choose the measure of homogenity that the tree algorithm will use.</p>

<p>Distance is not a factor with trees - what matters is whether the value is greater than or less than the split point, not the distance from the split point.</p>
"
['classification'],Which non-training classification methods are available?,"<p>What you are asking about is <a href=""http://en.wikipedia.org/wiki/Instance-based_learning"">Instance-Based Learning</a>. k-Nearest Neighbors (kNN) appears to be the most popular of these methods and is applicable to a wide variety of problem domains. Another general type of instance-based learning is <a href=""http://en.wikipedia.org/wiki/Analogical_modeling"">Analogical Modeling</a>, which uses instances as exemplars for comparison with new data.</p>

<p>You referred to kNN as an application that uses training but that is not correct (the Wikipedia entry you linked is somewhat misleading in that regard). Yes, there are ""training examples"" (labeled instances) but the classifier doesn't learn/train from these data. Rather, they are only used whenever you actually want to classify a new instance, which is why it is considered a ""lazy"" learner.</p>

<p>Note that the Nearest Template Prediction method you mention effectively is a form of kNN with <code>k=1</code> and cosine distance as the distance measure.</p>
"
"['apache-pig', 'etl']",Convert date into number - Apache PIG,"<p>I don't use PIG, but from looking online I think you should be able to use the Replace function built into PIG.</p>

<p>Assuming your Date field is already a string:</p>

<pre><code>Data_ID = FOREACH File GENERATE(int) REPLACE(Date,'-','');
</code></pre>

<p>Here's the reference if it's helpful: <a href=""https://pig.apache.org/docs/r0.9.1/func.html#replace"" rel=""nofollow"">https://pig.apache.org/docs/r0.9.1/func.html#replace</a></p>
"
"['deep-learning', 'convnet', 'backpropagation']",Question about bias in Convolutional Networks,"<p>Bias operates per virtual neuron, so there is no value in having multiple bias inputs where there is a single output - that would equivalent to just adding up the different bias weights into a single bias.</p>

<p>In the feature maps that are the output of the first hidden layer, the colours are no longer kept separate*. Effectively each feature map is a ""channel"" in the next layer, although they are usually visualised separately where the input is visualised with channels combined. Another way of thinking about this is that the separate RGB channels in the original image are 3 ""feature maps"" in the input.</p>

<p>It doesn't matter how many channels or features are in a previous layer, the output to each feature map in the next layer is a single value in that map. One output value corresponds to a single virtual neuron, needing one bias weight.</p>

<p>In a CNN, as you explain in the question, the same weights (including bias weight) are shared at each point in the output feature map. So each feature map has its own bias weight as well as <code>previous_layer_num_features x kernel_width x kernel_height</code> connection weights.</p>

<p>So yes, your example resulting in <code>(3 x (5x5) + 1) x 32</code> weights total for the first layer is correct for a CNN with first hidden layer processing RGB input into 32 separate feature maps.</p>

<hr>

<p><sup>*</sup> You may be getting confused by seeing visualisation of CNN <em>weights</em> which can be separated into the colour channels that they operate on.</p>
"
['visualization'],"R Programming, how to replicate for districts in a city","<p>The latitude and longitude are stored in a <a href=""http://www.rforscience.com/wpmain/wp-content/uploads/2014/06/Koeppen-Geiger-ASCII.txt"" rel=""nofollow"">text file</a> that is read into a table, <code>Dat</code>.  It has the form:</p>

<pre><code>lat long climate-group
</code></pre>

<p>For your particular case, you would calculate <code>Dat[, 3]</code>.</p>
"
"['data-mining', 'text-mining', 'feature-selection', 'feature-extraction']",How to select features from text data?,"<p>If I understand you correctly, you're looking to take the text of these questions and train classifiers to identify which of 10 categories they belong to. And you'd like to come up with a decent feature representation in order to do this. </p>

<p>I think your finding about part-of-speech is intuitive. It makes sense that in grammatical English (assuming your question data is written in English), most questions would follow similar part-of-speech sequences since grammatically correct questions follow a particular syntactic form (at least when posed interrogatively as in the case of ""When was George Washington born?"")</p>

<p>So, you've ruled something out - which you should actually view as progress. If you haven't tried it already, one simple thing you might do is use the actual words within the questions as features. You could use any order n-gram you like, but unigrams stick out as an immediate linguistic feature to try. It seems likely to me that while the POS-tags are similar across classes, making them difficult to distinguish between, the actual <i>words</i> being used in the questions may vary from class to class, giving your model a better shot of differentiating between classes.</p>

<p>That is, maybe words like ""time"", ""year"", and ""when"" co-occur more highly with the <code>Date</code> class while words like ""numerical"" and ""quantity"" co-occur more with the <code>Number</code> class (obviously, this is speculation - I haven't seen your data). You might also look at bigrams, trigrams, or any other number n-gram for this feature set as well.</p>

<p>Finally, there may be other features you could generate using NLP methods that may be useful. I'm not familiar with the Shiftreducer software, but <a href=""http://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow"">Named-Entity Recognition</a> could be helpful in generating features for the <code>Factoid</code> class if there are many questions about proper nouns. Other really simple features such as length of the question (counted in number of tokens). A final thought would be to use only the tagged <i>verbs</i> from your POS-tagger, tab these up and to see whether they differ between classes. This may be a useful feature for identifying questions present in your <code>Verb</code> class. Hopefully, those are some ideas to get you started.</p>
"
['data-stream-mining'],Analysis of Real-Time Bidding,"<p>A simplified example: suppose two companies: the webpage owner and a book store. And a customer named Jane, interested in reading.</p>

<p>The web page owner has some free space on its page, which can be sold for advertisement. The book store wants to place an advertisement on the same web page, in order to increase sales. Both companies meet eachother at an auction, where the web space is sold to the highest bidder in real-time.</p>

<p>So, the book store is bidding on the right to show an advertisement to Jane, who is visiting the page of the webpage owner.</p>

<p>The machine learning is done on the part of the book store, who receives information about the web page visitor. This can be all sort of information that the web page owner wants to release, and that could be of any use to the bookstore. </p>

<p>Based on this information, it is decided weather to make a bidding for the advertisement space or not, and the amount up to which the book store will want to make a bidding. </p>

<p>Without cookie matching, the webpage visitor Jane will probably not be identifiable by the bookstore, so the store must decide on bidding, based on parameters like geographical location of the customer, browser version (just to name a few ).</p>

<p>With cookie matching, each visitor/customer gets a unique identifier at the book store. Based on this identifier, the book store has more information of the visitor, like: what ads have been served before, and how long ago ? This visit to the web page can be linked to earlier visits and this information will likely ease the decision making process for doing the bidding on the advertisement space.</p>

<p>( There is more to it as there can be two more intermediary companies: one that holds the auction and one that delivers the ad )</p>
"
"['dataset', 'recommendation', 'data', 'databases']",Data structure design for supporting arbitrary number of columns in table or database,"<p>I've done something like you're describing using <a href=""https://www.mongodb.com/"" rel=""nofollow"">mongoDB</a>--I think you'll best use your time using some sort of NoSQL approach, rather than creating a specialized one-off solution. If you're using Python, I've had excellent experiences using <a href=""https://api.mongodb.org/python/current/"" rel=""nofollow"">PyMongo</a> to handle reads and writes from within my code. </p>

<p>I would strongly caution you against adopting your approach #1. This could break very easily in the future, and there are databases designed to handle your exact problem!</p>
"
['neuralnetwork'],Simple ANN visualisation,"<p>I believe this is the representation you're after, please excuse the rough sketch but I think it explains the structure appropriately.</p>

<ul>
<li>Single input going to three hidden units, each with a bias and tansig activation.</li>
<li>The outputs of the hidden layer are summed (via linear activation) with a bias to produce the output.</li>
</ul>

<p><a href=""http://i.stack.imgur.com/CdMAf.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CdMAf.jpg"" alt=""enter image description here""></a></p>
"
"['python', 'classification', 'decision-trees', 'dimensionality-reduction']",Pruning and parameter reduction for decision trees,"<p>Pruning and feature reduction are different things.</p>

<p><strong>Pruning:</strong> It basically compares the <em>purity</em> of the two leafs separately, and, together. If the leafs together are purer, than the two leafs are pruned. Thus, the decision over the parameter(s) at the node is wiped off.</p>

<p>Let's say you have <em>N</em> different parameters. You tree might be tall enough such that pruning has been used over all the parameters at different nodes. And in the same time, all these parameters might have been used in other nodes. If not, the decision tree will take the decision itself not to use this parameter - doesn't prevent from overfitting though.</p>

<p><strong>Dimensionality Reduction:</strong>
It you reduce the number of parameters, then these parameters would never appear in your tree, at any node. Whereas they might have been relevant at some point.</p>

<p>They are not uncompatible, and performing a dimensionality reduction may increase the accuracy of your task for a further classifier (as decision trees). </p>

<p>However, decision trees are also used for dimensionality reduction: After being trained, one might scan the features importance within the decision tree, i.e. how much each feature is used to create a split a different nodes. based on this new knowledge, you can used only the most important features to train another classifier.</p>
"
"['machine-learning', 'neuralnetwork', 'deep-learning', 'optimization', 'supervised-learning']",How to form Hessian matrix in BFGS Quasi-Newton Method,"<p>In optimization, you are looking for minima and maxima of a certain function <strong>f</strong>. Equivalently, under certain derivative conditions, you are looking for the zeros of <strong>g</strong>, the derivative of <strong>f</strong>.</p>

<p>The first equation of your link describes one step of the Newton Method - not BFGS. It involves the Hessian of <strong>f</strong> which is not accessible in practice - it's ineficcient in terms of time and memory storage. Due to this fact, some have developed methods not to calculate the hessian directly. </p>

<p>Thanks to Taylor expansion, you can write</p>

<blockquote>
  <p><strong>g</strong>(x) = <strong>g</strong>(y) + <strong>g'</strong>(y)(x-y) + <strong>Θ</strong>(|x-y|²)</p>
</blockquote>

<p>Replacing x by x(k+1) and y by x(k), and <strong>g'</strong> by the Hessian of <strong>f</strong> gives you (with the following convention : <strong>g</strong>(k+1) = <strong>g</strong>( x(k+1) )</p>

<blockquote>
  <p><strong>g</strong>(k+1) ≈ <strong>g</strong>(k) + <strong>∇²f</strong>(x(k+1)) ( x(k+1) - x(k) )</p>
  
  <p><strong>H</strong>(k+1) (x(k+1) - x(k)) ≈ <strong>g</strong>(k+1) - <strong>g</strong>(k)
  where <strong>H</strong>(k) aims to approximate ∇²<strong>f</strong>(x(k)). But instead of calculating it at each step, we'll try to <em>approximate</em> it thanks to the previous <strong>H</strong>(k-1)</p>
</blockquote>

<p>In 1970, Broyden, Fletcher, Goldfard and Shamo proposed a rank 2 update of <strong>H</strong>(k), ie </p>

<blockquote>
  <p><strong>H</strong>(k+1) = <strong>H</strong>(k) + Term1 + Term2</p>
</blockquote>

<p>where Term1 does not depend on <strong>H</strong> and Term2 depends on <strong>H(k)</strong>. You can find these terms <a href=""https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm"" rel=""nofollow"">here</a>.</p>

<p>But this solves only the time issue, not the memory storage. As for that, some have developed <a href=""https://en.wikipedia.org/wiki/Limited-memory_BFGS"" rel=""nofollow"">Limited-memory BFGS</a> (or L-BFGS) to limit the amount of memory used. </p>
"
"['feature-selection', 'mutual-information']",feature selection techniques,"<p>Doing that is a very good idea. The problem is that doing that is very hard.
<a href=""https://en.wikipedia.org/wiki/Feature_selection"" rel=""nofollow"">Feature selection</a> is a <a href=""https://en.wikipedia.org/wiki/NP-completeness"" rel=""nofollow"">NP-complete</a> problem.
The practical meaning as that we don't know any fast algorithm that can select only the needed feature.</p>

<p>In the other direction, omitting features that don't have mutual information (MI) with the concept might cause you to throw the features you need most.
There are cases in which a single feature is useless but given more features it becomes important.</p>

<p>Consider a concept which is the XOR of some features. Given all the features, the concept is totally predictable. Given one of them, you have 0 MI.</p>

<p>A more real life example is of age at death. Birth date and death date give you the age. One of them will have very low correlation (due to increase in life expectancy). </p>

<p>In practice, omitting features with low MI is OK. Many learning algorithms are using MI so they won't be able to use the omitted variables anyway.
As for the selection itself, there are many algorithms, usually heuristics or approximation algorithm that are quite handy.</p>
"
"['dataset', 'data-cleaning']",InterquartileRange takes up most instances in data set,"<p>The InterQuartileRangeFilter from weka library uses an IQR formula to designate some values as outliers/extreme values. Any value outside this range $[Q_1 - k(Q_3-Q_1), Q_3 + k(Q_3-Q_1)]$ is considered some sort of an outlier, where $k$ is some constant, and $ IQR = Q_3 - Q_1$. </p>

<p>By default weka uses $k=3$ to define something as outlier, and $k=3*2$ to define something as extreme value (extreme outlier). </p>

<p>The formula guarantees that at least 50% value are considered non-outliers. Having a single variable (univariate sample of values), it's practically impossible to reproduce your result.</p>

<p>Note however that this filter can be applied to a data frame. When applied like this, it will consider as an outlier any instance of the data frame which has at least one value of the instance considered as outlier for that variable. </p>

<p>Now, supposing that you have a data frame with 2 variables, totally uncorrelated (independent). Considering again that only 10% of the values from each variable are considered outliers, due to independence, one can expect that $(1-0.9)^2$ values will not be outliers. If you have $p$ variables like that in your data frame, you might expect to have only $(1-0.9)^k$ normal values, and is not very hard to arrive in that situation.</p>

<p>There are two things which you will have to consider. One is to increase the factors for outliers if in general too many values are considered outliers (ideally you would like to take a look at each variable graphically and if possible to get some idea about the distribution beneath). The second one is to check if you have many values which are totally independent. The second hint does not solve your problem but might give you a reason why it happens. </p>
"
"['data-mining', 'python']",Python for data analytics,"<p>You're looking for this answer: <a href=""https://www.quora.com/Why-is-Python-a-language-of-choice-for-data-scientists"" rel=""nofollow"">https://www.quora.com/Why-is-Python-a-language-of-choice-for-data-scientists</a></p>
"
"['scalability', 'scala']",Data Science Tools Using Scala,"<h2>Re: size of data</h2>

<h3>The short answer</h3>

<p>Scala works for both small and large data, but its creation and development is motivated by needing something scalable.  <a href=""http://www.scala-lang.org/what-is-scala.html"">Scala is an acronym for “Scalable Language”</a>.</p>

<h3>The long answer</h3>

<p>Scala is a <a href=""http://en.wikipedia.org/wiki/Functional_programming"">functional programming language</a> that runs on the <a href=""http://en.wikipedia.org/wiki/Java_virtual_machine"">jvm</a>.  The 'functional' part of this is a fundamental difference in the language that makes you think differently about programming.  If you like that way of thinking, it lets you quickly work with small data.  Whether you like it or not, functional languages are fundamentally easier to massively scale.  The jvm piece is also important because the jvm is basically everywhere and, thus, Scala code can run basically everywhere.  (Note there are plenty of other <a href=""http://en.wikipedia.org/wiki/List_of_JVM_languages"">languages written on the jvm</a> and plenty of other <a href=""http://en.wikipedia.org/wiki/List_of_programming_languages_by_type#Functional_languages"">functional programming languages</a>, and languages beyond Scala do appear in both lists.)</p>

<p><a href=""https://www.youtube.com/watch?v=3jg1AheF4n0"">This talk</a> give a good overview of the motivation behind Scala.</p>

<h2>Re: other tools that have good Scala support:</h2>

<p>As you mentioned, Spark (distributable batch processing better at iteratative algorithms than its counterpart) is a big one.  With Spark comes its libraries <a href=""https://spark.apache.org/mllib/"">Mllib</a> for machine learning and <a href=""https://spark.apache.org/graphx/"">GraphX</a> for graphs.  As mentioned by  Erik Allik and Tris Nefzger, <a href=""http://akka.io"">Akka</a> and <a href=""https://github.com/factorie/factorie"">Factorie</a> exist.  There is also <a href=""https://www.playframework.com/"">Play</a>.</p>

<p>Generally,  I can't tell if there is a specific use case you're digging for (if so, make that a part of your question), or just want a survey of big data tools and happen to know Scala a bit and want to start there.</p>
"
"['efficiency', 'javascript']",Implementation of Association Rules in Javascript,"<p>I don't know how efficient they are, but I did find some implementations:</p>

<ul>
<li><a href=""https://github.com/dmarges/apriori"" rel=""nofollow"">https://github.com/dmarges/apriori</a></li>
<li><a href=""https://github.com/seratch/apriori.js"" rel=""nofollow"">https://github.com/seratch/apriori.js</a></li>
</ul>
"
['python'],Need help with python code as part of a data analysis project,"<p>All of your plots are appearing on top of each other.  You need to invoke plt.subplot(xxx) before you create each plot.  For info on how the xxx command works, go to <a href=""http://www.mathworks.com/help/matlab/ref/subplot.html"" rel=""nofollow"">the MATLAB documentation</a>.</p>

<p>You might end up with multiple figures - see <a href=""http://stackoverflow.com/questions/21321764/matplotlib-multiple-plots-on-one-figure"">this page</a> for info about that.</p>
"
"['machine-learning', 'data-mining', 'deep-learning']",Difference between Validation data and Testing data?,"<p>There are two uses for the validation set:</p>

<p><strong>1) Knowing when to stop training</strong></p>

<p>Some models are trained iteratively - like neural nets. Sooner or later the model might start to overfit the training data. That's why you repeatedly measure the model's score on the validation set (like after each epoch) and you stop training once the score on the validation set starts degrading again.</p>

<p>From <a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow"">Wikipedia on Overfitting</a>:</p>

<p><a href=""http://i.stack.imgur.com/5dDzL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5dDzL.png"" alt=""Learning Curves - Training vs Validation Error""></a></p>

<p>""Training error is shown in blue, validation error in red, both as a function of the number of training cycles. If the validation error increases (positive slope) while the training error steadily decreases (negative slope) then a situation of overfitting may have occurred. The best predictive and fitted model would be where the validation error has its global minimum.""</p>

<p><strong>2) Parameter selection</strong></p>

<p>Your model needs some hyper-parameters to be set, like the learning rate, what optimizer to use, number and type of layers / neurons, activation functions, or even different algorithms like neural net vs SVM... you'll have to fiddle with these parameters, trying to find the ones that work best.</p>

<p>To do that you train a model with each set of parameters and then evaluate each model using the validation set. Finally you select the model / the set of parameters that yielded the best score on the validation set.</p>

<p>In both of the above cases the model might have fit the data in the validation set, resulting in a biased (slightly too optimistic) score - which is why you evaluate the final model on the test-set before publishing its score.</p>
"
['tensorflow'],Tensorflow skewed cost function,"<p>The equation I ended up using is listed in the link below, provided by Emre:</p>

<p><a href=""https://datascience.stackexchange.com/questions/10471/linear-regression-with-non-symmetric-cost-function/10474"">Linear regression with non-symmetric cost function?</a></p>
"
"['dataset', 'clustering', 'similarity']",Measuring similarity for sets with same cardinality,"<p>Yes, the Jaccard similarity score is normalized by the union to deal with sets of different cardinality. Without this normalization (if you used just the intersection), very small sets would always have very low scores. </p>

<p>When the cardinalities of all your sets are the same, the union of any two sets will be a straightforward function of the intersection (this is easy to visualize - as the two sets intersect more and more, their unions get smaller and smaller). The formula is:</p>

<pre><code>union = 2 * cardinality - intersection
</code></pre>

<p>So the Jaccard score in your case would be:</p>

<pre><code>intersection / (200 - intersection)
</code></pre>

<p>If you plot this, you'll see it's monotonically the same as what you did. </p>
"
"['machine-learning', 'logistic-regression', 'linear-regression', 'regularization']",Should I use regularization every time?,"<p>Normally you use regularization. The exception is if you know the data generating process and can model it exactly. Then you merely estimate the model parameters. In general you will not know the process, so you will have to approximate with a flexible enough model. If the model is not flexible enough you will not need to regularize but you won't approximate well anyway. If you use a more flexible model, you will get closer on average (low bias) but you will have more variance, thus the need for increased regularization. In other words, the amount of regularization you need depends on the model. This is related to the <a href=""https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff"" rel=""nofollow"">bias-variance trade-off</a>.</p>

<p>Welcome to DataScience.SE. </p>
"
"['data-cleaning', 'similarity', 'distance']",Fixing data inconsistencies,"<p>Since this dataset is already organised in a table, you can leverage standard SQL functions to perform a large part of the cleanup.
A record seems to be composed of 4 fields, for example:</p>

<pre><code>university name, city, state, country
stanford law school - stanford - ca - united states of america
</code></pre>

<p>You could follow these steps to get a cleaner representation of this dataset:</p>

<ol>
<li>Starting with the highest level (country), find unique values use it to identify all similar sounding words by listing soundex matches with itself (build a join query of the table with itself).</li>
<li>Use these suggested similar matches to fix all mistakes by updating the names.</li>
<li>Continue in this manner till you've fixed all four fields.</li>
<li>Identify missing states by using the city name to query the correct state from the rest of the table; if state and country are missing for ""leland stanford junior university"", then use the city name ""stanford"" to join it with itself and get the state/country name from the other records in the table.</li>
<li>For the university name, identify all abbreviations using grep to search for words ending in a dot character. Replace them with full expansions.</li>
<li>Next, breakup the university name into individual words and dump these into a temporary table in a single column. De-duplicate the values in the column so it only contains unique values.</li>
<li>Run the same soundex matching join query as in step 1 to identify similar sounding names, append these suggested similar names in a second column of the temp table.</li>
<li>Manually do a quick sanity check of what you've obtained as suggestions and delete any invalid matches.</li>
<li>Write a procedure to replace the words in each university name with suggested replacements, and you will have a much cleaner dataset.</li>
</ol>
"
['neuralnetwork'],How to fight underfitting in a deep neural net,"<p>The problem with deep networks is that they have lots of hyperparameters to tune and very small solution space. Thus, finding good ones is more like an art rather than engineering task. I would start with working example from tutorial and play around with its parameters to see how results change - this gives a good intuition (though not formal explanation) about dependencies between parameters and results (both - final and intermediate). </p>

<p>Also I found following papers very useful: </p>

<ul>
<li><a href=""http://yosinski.com/media/papers/Yosinski2012VisuallyDebuggingRestrictedBoltzmannMachine.pdf"">Visually Debugging Restricted Boltzmann Machine Training
with a 3D Example</a></li>
<li><a href=""https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"">A Practical Guide to Training Restricted Boltzmann
Machines</a></li>
</ul>

<p>They both describe RBMs, but contain some insights on deep networks in general. For example, one of key points is that networks need to be debugged layer-wise - if previous layer doesn't provide good representation of features, further layers have almost no chance to fix it. </p>
"
"['bigdata', 'data-mining', 'efficiency', 'state-of-the-art']","Is FPGrowth still considered ""state of the art"" in frequent pattern mining?","<p>State of the art as in: used in practise or worked on in theory?</p>

<p>APRIORI is used everywhere, except in developing new frequent itemset algorithms. It's easy to implement, and easy to reuse in very different domains. You'll find hundreds of APRIORI implementations of varying quality. And it's easy to get APRIORI wrong, actually.</p>

<p>FPgrowth is much harder to implement, but also much more interesting. So from an academic point of view, everybody tries to improve FPgrowth - getting work based on APRIORI accepted will be very hard by now.</p>

<p>If you have a good implementation, every algorithm has it's good and it's bad situations in my opinion. A good APRIORI implementation will <em>only</em> need to scan the database <em>k</em> times to find all frequent itemsets of length <em>k</em>. In particular if your data fits into main memory this is cheap. What can kill APRIORI is too many frequent 2-itemsets (in particular when you don't use a Trie and similar acceleration techniques etc.). It works best on large data with a low number of frequent itemsets.</p>

<p>Eclat works on columns; but it needs to read each column much more often. There is some work on diffsets to reduce this work. If your data does not fit into main memory, Eclat suffers probably more than Apriori. By going depth first, it will also be able to return a first interesting result much earlier than Apriori, and you can use these results to adjust parameters; so you need less iterations to find good parameters. But by design, it cannot exploit pruning as neatly as Apriori did.</p>

<p>FPGrowth compresses the data set into the tree. This works best when you have lots of duplicate records. You could probably reap of quite some gains for Apriori and Eclat too if you can presort your data and merge duplicates into weighted vectors. FPGrowth does this at an extreme level. The drawback is that the implementation is much harder; and once this tree does not fit into memory anymore it gets a mess to implement.</p>

<p>As for performance results and benchmarks - don't trust them. There are so many things to implement incorrectly. Try 10 different implementations, and you get 10 very different performance results. In particular for APRIORI, I have the impression that most implementations are broken in the sense of missing some of the main contributions of APRIORI... and of those that have these parts right, the quality of optimizations varies a lot.</p>

<p>There are actually even papers on how to implement these algorithms efficiently:</p>

<blockquote>
  <p>Efficient Implementations of Apriori and Eclat.<br /> Christian Borgelt<br />Workshop of Frequent Item Set Mining Implementations (FIMI 2003, Melbourne, FL, USA).</p>
</blockquote>

<p>You may also want to read these surveys on this domain:</p>

<ul>
<li><blockquote>
  <p>Goethals, Bart. ""Survey on frequent pattern mining."" Univ. of Helsinki (2003).</p>
</blockquote></li>
<li><blockquote>
  <p>Ferenc Bodon, A Survey on Frequent Itemset Mining, Technical Report, Budapest University of Technology and Economic, 2006, </p>
</blockquote></li>
<li><blockquote>
  <p>Frequent Item Set Mining<br />Christian Borgelt<br />Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 2(6):437-456. 2012</p>
</blockquote></li>
</ul>
"
"['classification', 'svm']",Skewed multi-class data,"<p>I would suggest you to use libsvm, which already has adjustable class weights implemented in it. Rather than replicating the training samples, one modifies the C parameter for different classes in the SVM optimization. For example if your data has 2 classes, and the first class is only 10% of the data, you would choose class weights to be 10 and 1 for class 1 and 2 respectively. Therefore, margin violations of the first class would cost 10 times more than the margin violations for second class, and per-class accuracies would be more balanced.</p>
"
"['education', 'definitions', 'career']","Starting my career as Data Scientist, is Software Engineering experience required?","<p>1) I think that there's no need to question whether your background is adequate for a career in data science. CS degree IMHO is <strong>more than enough</strong> for data scientist from software engineering point of view. Having said that, theoretical knowledge is not very helpful without matching <strong>practical experience</strong>, so I would definitely try to <strong>enrich</strong> my experience through participating in <em>additional school projects, internships or open source projects</em> (maybe ones, focused on data science / machine learning / artificial intelligence).</p>

<p>2) I believe your concern about <strong>focusing</strong> on data science <strong>too early</strong> is unfounded, as long as you will be practicing software engineering either as a part of your data science job, or additionally in your spare time.</p>

<p>3) I find the following <strong>definition of a data scientist</strong> rather accurate and hope it will be helpful in your future career success:</p>

<blockquote>
  <p>A <em>data scientist</em> is someone who is better at statistics than any
  software engineer and better at software engineering than any
  statistician.</p>
</blockquote>

<p>P.S. Today's <strong>enormous</strong> number of various resources on data science topics is mind-blowing, but this <strong>open source curriculum for learning data science</strong> might fill some gaps between your BSc/MSc respective curricula and reality of the data science career (or, at least, provide some direction for further research and maybe answer some of your concerns): <a href=""http://datasciencemasters.org"">http://datasciencemasters.org</a>, or on GitHub: <a href=""https://github.com/datasciencemasters/go"">https://github.com/datasciencemasters/go</a>.</p>
"
"['time-series', 'visualization']",Represent outlier days,"<p>One idea would be to plot the daily average power consumption in a bar plot:</p>

<p><a href=""http://i.stack.imgur.com/ym3Ws.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ym3Ws.png"" alt=""enter image description here""></a></p>

<p>For a finer visualization of day-hour peaks, you can try to plot it in a matrix format:</p>

<p><a href=""http://i.stack.imgur.com/o6jUH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/o6jUH.png"" alt=""enter image description here""></a></p>
"
['neuralnetwork'],Extra output layer in a neural network,"<p>The question is asking you to make the following mapping between old representation and new representation:</p>

<pre><code>Represent    Old                     New
0            1 0 0 0 0 0 0 0 0 0     0 0 0 0 
1            0 1 0 0 0 0 0 0 0 0     0 0 0 1 
2            0 0 1 0 0 0 0 0 0 0     0 0 1 0 

3            0 0 0 1 0 0 0 0 0 0     0 0 1 1 
4            0 0 0 0 1 0 0 0 0 0     0 1 0 0 
5            0 0 0 0 0 1 0 0 0 0     0 1 0 1 

6            0 0 0 0 0 0 1 0 0 0     0 1 1 0 
7            0 0 0 0 0 0 0 1 0 0     0 1 1 1 
8            0 0 0 0 0 0 0 0 1 0     1 0 0 0 

9            0 0 0 0 0 0 0 0 0 1     1 0 0 1
</code></pre>

<p>Because the old output layer has a simple form, this is quite easy to achieve. Each output neuron should have a positive weight between itself and output neurons which should be on to represent it, and a negative weight between itself and output neurons that should be off. The values should combine to be large enough to cleanly switch on or off, so I would use largish weights, such as +10 and -10.</p>

<p>If you have sigmoid activations here, the bias is not that relevant. You just want to simply saturate each neuron towards on or off. The question has allowed you to assume very clear signals in the old output layer.</p>

<p>So taking example of representing a 3 and using zero-indexing for the neurons in the order I am showing them (these options are not set in the question), I might have weights going from activation of old output $i=3$, $A_3^{Old}$ to logit of new outputs $Z_j^{New}$, where $Z_j^{New} = \Sigma_{i=0}^{i=9} W_{ij} * A_i^{Old}$ as follows:</p>

<p>$$W_{3,0} = -10$$
$$W_{3,1} = -10$$
$$W_{3,2} = +10$$
$$W_{3,3} = +10$$</p>

<p>This should clearly produce close to <code>0 0 1 1</code> output when only the old output layer's neuron representing a ""3"" is active. In the question, you can assume 0.99 activation of one neuron and &lt;0.01 for competing ones in the old layer. So, if you use the same magnitude of weights throughout, then relatively small values coming from +-0.1 (0.01 * 10) from the other old layer activation values will not seriously affect the +-9.9 value, and the outputs in the new layer will be saturated at very close to either 0 or 1.</p>
"
"['data-cleaning', 'processing']",Alignment of square nonorientable images/data,"<p><strong>Fun with Group Theory!</strong></p>

<p>There are only 8 unique rotation-inversion operations for a square matrix.</p>

<p>The four rotation operators are <code>(0,90,180,270)</code>.  Further rotation or rotation in the reverse direction is the same as these four.  Two successive rotations just yields one of the rotation operators, so we will only consider these four rotations applied one time.</p>

<p>The five inversion operators are <code>(0,/,\,|,-)</code>.  Two successive inversions just yields a rotation, so we will only allow for a single inversion.</p>

<p>We can thus derive all operators by combining these two vectors, which yields <code>4*5=20</code> possible states.</p>

<pre><code>(0,90,180,270,0/,90/,180/,270/,0\,90\,180\,270\,0|,90|,180|,270|,0-,90-,180-,270-)
</code></pre>

<p>But there is still symmetry to be exploited in the inversion operators.  You can probably intuit that the 4x4 matrix only has 8 final states: inverted or not and rotated by (0,90,180,270).  It turns out that you can arrive at any possible state involving an inversion using any of the other inversion operators followed by one of the rotations.  So we only need to retain a single inversion operator!  So the final set of 8 orthogonal operations are:</p>

<pre><code>(0,90,180,270,0|,90|,180|,270|)
</code></pre>

<p>If there is any symmetry in the matrix's members then some of the resulting states may be degenerate.</p>

<p>In terms of mapping possible states into a ground state, it makes sense to apply a set of successive deterministic rules to determine the ground state orientation.  I suggest finding the largest corner square and locating it in the lower right corner. If there are multiple candidates with equally large values in the corner square then use the next closest square as a tie breaker.  There are 16 squares, so you can eventually break all ties or declare degeneracy. There is one remaining <code>\</code> operation that you can decide to apply in order to locate the larger of the two squares adjacent to the lower right corner at the bottom.  Again, you can use squares adjacent to these as tie breakers. </p>
"
"['r', 'visualization']",Which packages or functions can I use in R to plot 3D data like this?,"<p>You could use the <code>wireframe</code> function from the <code>lattice</code> package:</p>

<pre><code>library(""lattice"")
wireframe(volcano[1:30, 1:30], shade=TRUE, zlab="""")
</code></pre>

<p><img src=""http://i.stack.imgur.com/BQHGX.png"" alt=""wireframe plot example""></p>
"
"['visualization', 'tableau']",Change aliases of filter items in Tableau,"<p>Found it myself.</p>

<ul>
<li>Go to context menu right clicking to the dimension field.</li>
<li>Go to <strong>Aliases...</strong> and change the labels.</li>
</ul>

<p><img src=""http://i.stack.imgur.com/qKq73.png"" alt=""enter image description here""></p>
"
"['machine-learning', 'classification', 'neuralnetwork', 'text-mining', 'feature-selection']",Which features do I select from text?,"<p>Using NLTK in python you should first <strong>Tokenize</strong> the sentences into words, even you can use <a href=""https://spark.apache.org/docs/latest/ml-features.html#n-gram"" rel=""nofollow"">Ngram</a> for 2-Gram or 3-Gram bags of word, the reason I am suggesting <strong>N-Gram</strong> is that let's suppose you have sentence like: <code>I am not happy with this product</code>, then <strong>2-Gram</strong> tokenize it as <code>['not happy', 'happy with', 'with this', 'this product']</code> here <code>I</code> and <code>am</code> are assumed as <code>STOPWORDS</code>. Using <strong>HashingTF</strong> you can hash the sentence into a <strong>feature vector</strong> as <code>['word position': frequency of word, ...]</code> i.e <strong>highly sparse vectors</strong>, For Hashing in PySpark check this <a href=""https://spark.apache.org/docs/latest/ml-features.html#tf-idf-hashingtf-and-idf"" rel=""nofollow"">documentation</a>. </p>

<p>Here below python code will help you to <strong>tokenize</strong> in bags of word</p>

<pre><code>import string

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

PUNCTUATION = set(string.punctuation)
STOPWORDS = set(stopwords.words('english'))
STEMMER = PorterStemmer()

example = ['Hello Krishna Prasad, this is test file for spark testing',
            'Another episode of star',
            'There are far and away many stars'
            'A galloping horse using two coconuts'
            'My kingdom for a horse'
            'A long time ago in a galaxy far']

def tokenize(text):
    tokens = word_tokenize(text)
    lowercased = [t.lower() for t in tokens]
    no_punctuation = []
    for word in lowercased:
        punct_removed = ''.join([letter for letter in word if not letter in PUNCTUATION])
        no_punctuation.append(punct_removed)
    no_stopwords = [w for w in no_punctuation if not w in STOPWORDS]
    stemmed = [STEMMER.stem(w) for w in no_stopwords]
    return [w for w in stemmed if w]

tokenized_word = [tokenize(text) for text in example]
for word in tokenized_word:
    print word
</code></pre>

<p>Out of the above code as:</p>

<pre><code>$python WordFrequencyHash.py
[u'hello', u'krishna', u'prasad', u'test', u'file', u'spark', u'test']
[u'anoth', u'episod', u'star']
[u'far', u'away', u'mani', u'starsa', u'gallop', u'hors', u'use', u'two', u'coconutsmi', u'kingdom', u'horsea', u'long', u'time', u'ago', u'galaxi', u'far']
</code></pre>

<p>You can also use <a href=""https://spark.apache.org/docs/latest/ml-features.html#word2vec"" rel=""nofollow"">word2vec</a> or <a href=""https://spark.apache.org/docs/latest/ml-features.html#countvectorizer"" rel=""nofollow"">countvectorizer</a> for tokenization.</p>
"
"['data-mining', 'machine-learning']",Is there any APIs for crawling abstract of paper?,"<p>Look it up on:</p>

<ul>
<li>Google Scholar <a href=""http://scholar.google.com/scholar?hl=en&amp;q=Assessment%20of%20Utility%20in%20Web%20Mining%20for%20the%20Domain%20of%20Public%20Health&amp;btnG=&amp;as_sdt=1,5&amp;as_sdtp="" rel=""nofollow"">link</a></li>
<li>Citeseer <a href=""http://citeseerx.ist.psu.edu/search?q=Assessment%20of%20Utility%20in%20Web%20Mining%20for%20the%20Domain%20of%20Public%20Health&amp;submit=Search&amp;sort=rlv&amp;t=doc"" rel=""nofollow"">link</a></li>
</ul>

<p>If you get a single exact title match then you have probably found the right article, and can fill in the rest of the info from there.  Both give you download links and bibtex-style output.  What you would likely want to do though to get perfect metadata is download and parse the pdf (if any) and look for DOI-style identifier.</p>

<p>Please be nice and rate-limit your requests if you do this.</p>
"
"['machine-learning', 'dimensionality-reduction', 'python']",Machine learning techniques for estimating users' age based on Facebook sites they like,"<p>One thing to start off with would be k-NN.  The idea here is that you have a user/item matrix and for some of the users you have a reported age.  The age for a person in the user item matrix might be well determined by something like the mean or median age of some nearest neighbors in the item space.</p>

<p>So you have each user expressed as a vector in item space, find the k nearest neighbors and assign the vector in question some summary stat of the nearest neighbor ages.  You can choose k on a distance cutoff or more realistically by iteratively assigning ages to a train hold out and choosing the k that minimizes the error in that assignment.</p>

<p>If the dimensionality is a problem you can easily perform reduction in this setup by single value decomposition choosing the m vectors that capture the most variance across the group.</p>

<p>In all cases since each feature is binary it seems that cosine similarity would be your go to distance metric.</p>

<p>I need to think a bit more about other approaches (regression, rf, etc...) given the narrow focus of your feature space (all variants of the same action, liking) I think the user/item approach might be the best.</p>

<p>One note of caution, if the ages you have for train are self reported you might need to correct some of them.  People on facebook tend to report ages in the decade they were born.  Plot a histogram of the birth dates (derived from ages) and see if you have spikes at decades like 70s, 80s, 90s.</p>
"
"['bigdata', 'nosql', 'databases']",When a relational database has better performance than a no relational,"<p>It depends on your data and what you're doing with it. For example, if the processing you have to do requires transactions to synchronize across nodes, it will likely be faster to use transactions implemented in an RDBMS rather than implementing it yourself on top of NoSQL databases which don't support it natively. </p>
"
"['r', 'regression', 'linear-regression', 'parameter-estimation']",Gibbs sampling in R,"<p>Here is the code I wrote to answer my question. It might not be the most efficient one but it works. Sharing is caring :)</p>

<pre><code>niter&lt;-10000
N&lt;-nrow(Y)

T&lt;-10 # I will take into consideration until t=10 to estimate my parametres and then I will forecast the rest values t=11,12,... etc
result &lt;- matrix(0,nrow = niter, ncol = 3) #Here I store 3 parameters: alpha, sigma_epsilon, sigma_eta. I do not store gamma here
result_betas&lt;-matrix(0,nrow=niter,ncol=N) #the betas
result_gammas&lt;-matrix(0,nrow=niter,ncol=nrow(Z)) #here I store the gammas

alpha&lt;-0
gamma&lt;-0


sigma_epsilon_squared&lt;-1
sigma_eta_squared&lt;-1
beta&lt;-matrix(0, nrow=N, ncol=1)
beta2&lt;-0



for (i in 1:niter) 
{
  mean_alpha&lt;-0
  for (obs in 1:N)
  {
    for (time in 1:T)
    {
      mean_alpha&lt;-mean_alpha+Y[obs,time] - beta[obs]*X[obs,time]
    }
  }
  alpha&lt;-rnorm(1, mean = (1/(T*N))* mean_alpha, sd = sqrt((sigma_epsilon_squared)/(T*N)))


  param_sigma_epsilon_squared&lt;-0
  for (obs in 1:N)
  {
    for (time in 1:T)
    {
      param_sigma_epsilon_squared&lt;-param_sigma_epsilon_squared + (Y[obs,time] - alpha - beta[obs]*X[obs,time])^2
    }
  }
  sigma_epsilon_squared &lt;- rinvgamma(1, (N*T)/2, param_sigma_epsilon_squared/2 )


  mean_gamma&lt;-0 #this is a vector
  mean_gamma1&lt;-0 #this is a matrix
  mean_gamma2&lt;-0
  sigma_gamma&lt;-0
  for(obs in 1:N)
  {
    mean_gamma1&lt;-mean_gamma1 + Z[,obs]%*%t(Z[,obs]) #this is (k x k)
    mean_gamma2&lt;-mean_gamma2 + Z[,obs]*beta[obs]

    #mean_gamma&lt;-mean_gamma + 1/(Z[obs,]%*%t(Z[obs,])) * (Z[obs,]*beta[obs,1])
    #sigma_gamma&lt;-sigma_gamma+1/(Z[obs,]%*%t(Z[obs,]))
  }
  mean_gamma&lt;-solve(mean_gamma1)%*%mean_gamma2
  sigma_gamma&lt;-solve(mean_gamma1)
  gamma&lt;-mvrnorm(n = 1, mu=mean_gamma, Sigma=sqrt(sigma_eta_squared)*sigma_gamma)

  result_gammas[i,]&lt;-gamma

  mean_sigma_eta_squared&lt;-0
  for (obs in 1:N)
  {
    mean_sigma_eta_squared&lt;- mean_sigma_eta_squared + (beta[obs] - t(Z[,obs])%*%gamma)^2
  }
  sigma_eta_squared&lt;-rinvgamma(1, N/2, mean_sigma_eta_squared/2 )

  result[i,]&lt;-c(alpha,sigma_epsilon_squared,sigma_eta_squared)




  mean_beta&lt;-0
  mean_beta1&lt;-0 #this is scalar
  mean_beta2&lt;-0 #this is scalar
  sigma_beta&lt;-0
  for (obs in 1:N)
  {
    for (time in 1:T)
    {
      mean_beta1&lt;-mean_beta1 + X[obs,time]^2 + (sigma_epsilon_squared/sigma_eta_squared)
      mean_beta2&lt;-mean_beta2 + X[obs,time]*Y[obs,time] + (sigma_epsilon_squared/sigma_eta_squared)*(t(Z[,obs])%*%gamma)


    }
    mean_beta&lt;-(1/mean_beta1)*mean_beta2
    sigma_beta&lt;-1/mean_beta1
    beta2&lt;-rnorm(1, mean = mean_beta, sd = sqrt((sigma_epsilon_squared)*sigma_beta))

    beta[obs]&lt;-beta2
    result_betas[i,obs]&lt;-beta2
  }
  if(i%%10==0)
  {
    print(i)
  }

} 
</code></pre>
"
"['clustering', 'matlab', 'computer-vision']",Agglomerative Clustering Stopping Criteria,"<p>Clearly, 2 standard deviations beyond Omega is not the same as twice the mean.</p>

<p>Apparently, their process is this:</p>

<ol>
<li>compute the distance matrix</li>
<li>compute the mean</li>
<li>compute the standard deviation</li>
<li>compute hierarchical clustering with maximum linkage</li>
<li>cut the tree at mu+2*sigma</li>
</ol>

<p>Because complete linkage is in O(n^3), this approach will not scale to longer videos or higher frame rates.</p>
"
"['feature-selection', 'correlation', 'xgboost', 'gbm']",Does XGBoost handle multicollinearity by itself?,"<p>Decision trees are by nature immune to multi-collinearity. For example, if you have 2 features which are 99% correlated, when deciding upon a split the tree will choose only one of them. Other models such as Logistic regression would use both the features.</p>

<p>Since boosted trees use individual decision trees, they also are unaffected by multi-collinearity. However, its a good practice to remove any redundant features from any dataset used for training, irrespective of the model's  algorithm. In your case since you're deriving new features, you could use this approach, evaluate each feature's importance and retain only the best features for your final model.</p>

<p>The importance matrix of an xgboost model is actually a data.table object with the first column listing the names of all the features actually used in the boosted trees. The second column is the Gain metric which implies the relative contribution of the corresponding feature to the model calculated by taking each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction.</p>
"
"['neuralnetwork', 'gradient-descent']","Implementing RMSProp, but finding differences between reference versions","<p>RMSProp is indeed an unpublished method, and in the lecture Geoffrey Hinton gives just the general idea behind RMSProp - to divide the gradient by a moving average of the gradient magnitude. You can watch the lecture here:</p>

<p><a href=""https://youtu.be/O3sxAc4hxZU"" rel=""nofollow"">https://youtu.be/O3sxAc4hxZU</a></p>

<p>When this principle is applied to Stochastic Gradient Descent, the update rule you showed is obtained. Since Hinton did not propose an exact algorithm, this principle has been applied to different optimization methods. I agree it's confusing all these methods call themselves RMSProp.</p>

<p><code>climin</code> implements RMSProp with Nesterov momentum. Momentum methods try to avoid the oscillation that often happens with SGD by slowly changing the current direction of updates. The algorithm given in <code>climin</code> documentation introduces the $\beta$ parameter that controls how much of the previous update direction is retained. Nesterov momentum is implemented by first taking a step towards the previous update direction $v_t$, calculating gradient at that position, using the gradient to obtain the new update direction $v_{t+1}$, and finally updating the parameters. The <code>climin</code> implementation also includes the smoothing term $\iota$ inside the square root for stability (1e-8), even though it's not mentioned in the documentation.</p>

<p>The implementation in the Downhill library is based on the algorithm described in the paper by A. Graves. It is not mentioned in the article why the square of the average gradient is subtracted from the average square gradient. I've seen this done sometimes when RMSProp is implemented with a momentum method, but I'm not sure why.</p>
"
['dataset'],Developing an Empirical Model for Non-Linear Data,"<p>A couple of thoughts...</p>

<p><strong>First as a (former) physicist:</strong></p>

<p>As a scientist/physicist I would likely never be satisfied to analyze the data that you have presented because there are some obvious issues with it.  I was initially thinking that it was way to schizophrenic, but then I noticed that you referred to the time units as seconds, in which case this is a pretty slowly varying time series.  Are the units really seconds?  If they happen to be milliseconds or microseconds then you might want to think about a probe that is less noisy (i.e. a larger probe with more of a temperature sink) and check weather you are seeing the effects of ground loop feedback in your system or just EMF interference from your electrical system.</p>

<p>Secondly, there are clear patterns in the data and these do not line up very well, so I would rerun the experiment with things timed much more accurately.  For instance, the event that occurs midway through the experiment that drops the temperature (looks like an icicle or dagger) occurs 5-8 minutes apart in comparing the dark blue experiment to the purple experiment.  It would be great if these could be synced better.  The point being that if you can't sync them better, then the noise of the time series renders most phenomenon apart from a flat line, almost useless.</p>

<p><strong>Now as a data scientist:</strong></p>

<p>This data is really noisy or at least fluctuates significantly!  You should stay away from averaging minima and maxima as you suggest in your post.  extrema are very sensitive to noise where as you are trying to reduce the noise in your system.</p>

<p>You should perhaps think about applying some sort of smoothing method to reduce the jitter in your data.  Perhaps a second-order exponentially weighted moving average like <a href=""http://connor-johnson.com/2014/11/23/time-series-forecasting-in-python-and-r/"" rel=""nofollow"">Holt-Winters</a>.</p>

<p>Finally, you could probably just average the three signals to produce a mean signal.  Means are more suseptable to outliers than medians,so the median signal is also an option. </p>

<p>Some other options include separating the trend from the general behavior using some type of <a href=""https://www.stat.auckland.ac.nz/~wild/ChanceEnc/Ch14.pdf"" rel=""nofollow"">de-seasoning</a> or <a href=""http://www.research.att.com/techdocs/TD_100381.pdf"" rel=""nofollow"">double ARIMA</a> procedure.  But, it seems like these are too extreme given that the dagger/icicle event looks significant and you don't want to transform it away.  So I would only use these if there is some type of periodicity to your data.</p>

<p>Hope this helps!</p>
"
"['clustering', 'k-means']",How to convert vector values to fit k-means algorithm function?,"<p>You are 95% there, you just have one hangup...</p>

<p>The vectorization that you are doing is alternatively known as binarization or <a href=""https://en.wikipedia.org/wiki/One-hot"" rel=""nofollow"">one-hot encoding</a>.  The only thing you need to do now is break apart all of those vectors and think of them as individual features.  </p>

<p>So instead of thinking of the question one vector as $(0,0,1,0)$ and the question two vector as $(0,1,0,0)$, you can now think of them as individual features.</p>

<p>So this:</p>

<pre><code>-      q1,        q2
-      (a,b,c,d), (a,b,c,d)
user1  (0,0,1,0), (0,1,0,0)
user2  (1,0,0,0), (0,0,0,1)
</code></pre>

<p>Becomes this:</p>

<pre><code>-      q1a,q1b,q1c,q1d,q2a,q2b,q2c,q2d
user1  0   0   1   0   0   1   0   0
user2  1   0   0   0   0   0   0   1
</code></pre>

<p>And you can think of each one of those binary features as an orthogonal  dimension in your data that lies in a 8-dimensional space. </p>

<p>Hope this helps!</p>
"
"['machine-learning', 'svm', 'random-forest', 'data']",When to use Random Forest over SVM and vice versa?,"<p>I would say, the choice depends very much on what data you have and what is your purpose. A few ""rules of thumb"".</p>

<p>Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems.</p>

<p>Random Forest works well with a mixture of numerical and categorical features. When features are on the various scales, it is also fine. Roughly speaking, with Random Forest you can use data as they are. SVM maximizes the ""margin"" and thus relies on the concept of ""distance"" between different points. It is up to you to decide if ""distance"" is meaningful. As a consequence, one-hot encoding for categorical features is a must-do. Further, min-max or other scaling is highly recommended at preprocessing step. </p>

<p>If you have data with $n$ points and $m$ features, an intermediate step in SVM is constructing an $n\times n$ matrix (think about memory requirements for storage) by calculating $n^2$ dot products (computational complexity).
Therefore, as a rule of thumb, SVM is hardly scalable beyond 10^5 points.
Large number of features (homogeneous features with meaningful distance, pixel of image would be a perfect example) is generally not a problem. </p>

<p>For a classification problem Random Forest gives you probability of belonging to class. SVM gives you distance to the boundary, you still need to convert it to probability somehow if you need probability.</p>

<p>For those problems, where SVM applies, it generally performs better than Random Forest.</p>

<p>SVM gives you ""support vectors"", that is points in each class closest to the boundary between classes. They may be of interest by themselves for interpretation. </p>
"
"['svm', 'scikit', 'image-classification']",SVM prediction time increase with number of test cases,"<p>The number of support vectors must be increasing. The prediction time is proportional to that; after all, the kernel classifier is $f(x) = \sum_i \alpha_i k(x, x_i)$, where the summation is over the support vectors. With sklearn you can find out how many you have by inspecting <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"" rel=""nofollow"">n_support_</a></p>
"
"['r', 'text-mining']",Examining a DocumentTermMatrix in RTextTools,"<p>a documenttermmatrix is a simple_triplet_matrix. You can turn this into a simple matrix with the as.matrix command and then use all matrix functions.</p>

<pre><code># turn into simple matrix
mat &lt;- as.matrix(doc_matrix)

# vector of the words 
word_vector &lt;- colnames(mat)

# Dataframe containing words and their frequency
df_words &lt;- data.frame(words = colnames(mat), frequency = colSums(mat), row.names = NULL)
</code></pre>
"
"['predictive-modeling', 'time-series']",Events prediction with time series of continuous variables as features,"<p>First of all, you will not be able to prove anything with a model, you will have false positives/negatives. With a good model you may be able show what variables may be an indicator of component failure. </p>

<p>In problems like this feature generation can have the most important influence on the accuracy of the model. The time stamps can be used for aggregation. For example, you may aggregate metrics per device per hour. The metrics/features that you might create for input into the model might be average/max temperature or fan speed, rate of change in temperature or fan speed, number of seconds device was above some threshold temperature or fan speed, boolean indicator of voltage spike, etc. There could be any number of features you may create. You can then find which features are not strong predictors and remove these columns to reduce noise, if need be.</p>
"
['decision-trees'],"Decision tree, how to understand or calculate the probability/confidence of prediction result","<p>What data mining package do you use?</p>

<p>In sklearn, the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba"" rel=""nofollow"">DecisionTreeClassifier</a> can give you probabilities, but you have to use things like <code>max_depth</code> in order to truncate the tree. The probabilities that it returns is $P=n_A/(n_A+n_B)$, that is, the number of observations of class A that have been ""captured"" by that leaf over the entire number of observations captured by that leaf (during training). But again, you must prune or truncate your decision tree, because otherwise the decision tree grows until $n=1$ in each leaf and so $P=1$.</p>

<p>That being said, I think you want to use something like a random forest. In a random forest, multiple decision trees are trained, by using different resamples of your data. In the end, probabilities can be calculated by the proportion of decision trees which vote for each class. This I think is a much more robust approach to estimate probabilities than using individual decision trees.</p>

<p>But random forests are not interpretable, so if interpertability is a requirement, use the decision tree like I mentioned. You can use grid search to maximize the ROC AUC score by changing hyperparameters such as maximum depth to find whatever decision tree gives the most reliable probabilities.</p>
"
['clustering'],Can KMeans clustering be used on word2vec output?,"<p>It's totally fine to cluster word2vec output to know semantically similar words. KMeans is an option, you might also want to checkout some approximate neighbor scheme such as <a href=""https://en.wikipedia.org/wiki/Locality-sensitive_hashing"" rel=""nofollow"">Locality Sensitive Hashing</a>.</p>
"
"['predictive-modeling', 'image-classification']",Predicting earthquakes using disturbances in DTH TV transmission,"<p>I think that if there are disturbances caused by something preceding earthquake, they might be used.</p>

<p>The problem is that you need to find which cause produces that effect and measure it. For example you can team up with geologists, geophysicists or someone similar and try to build as complete hypothesis as possible and then design experiment to gather data. If your experiment would bring some non noise data then you can start to thinking about machine learning algorithms which will work in this situation.</p>

<p>But you might also try to do it on ""brute force"" way by actually recording DTH TV images in seismically unstable regions and try to correlate those videos with seismic data. Then after some manual examination and categorization you could define (or not if this cause and effect hypothesis is wrong) some possible glitches that are observed and try to develop software that tries to detect them (<a href=""http://opencv.org/"" rel=""nofollow"">OpenCV</a> might be useful for example).</p>

<p>IMHO in either way, some domain knowledge related to earthquakes would be more useful on beginning stage, and experience related to machine learning would be more useful on latest stages of such ambitious project.</p>
"
"['r', 'classification']",Why would removing a variable in adaboost decrease error rate?,"<p>Imagine that one of the column is just random data -- then it's not informative at all, so no classifier will be improved by including it.</p>

<p>However, <code>ada</code>'s stochastic boosting implementations will always have some chance of including that variable in the classifier it generates. As a result, removing it has the potential to improve the classifiers generated.</p>

<p>(In your case, you might check whether that variable is part of the final model generated.)</p>
"
['r'],Why does ada (adaboost) in R return different training error graphs and variable importance plots when running the same function multiple times?,"<p>Boosting, together with bagging, falls into the realm of so cold ensemble models: you randomly draw a sample from the data, fit a model, adjust your predictions, sample once again. Unless your samples are fixed, every time you run the algo you'll get slightly different results.</p>
"
